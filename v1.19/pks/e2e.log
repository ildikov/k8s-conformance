I0129 10:49:37.309325      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-769685040
I0129 10:49:37.309416      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0129 10:49:37.309626      20 e2e.go:129] Starting e2e run "ec637448-0d54-4e57-8fe0-5c916fc150a3" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1611917375 - Will randomize all specs
Will run 305 of 5238 specs

Jan 29 10:49:37.323: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:49:37.325: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0129 10:49:37.327743      20 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Jan 29 10:49:37.338: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 29 10:49:37.361: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 29 10:49:37.361: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jan 29 10:49:37.361: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 29 10:49:37.368: INFO: e2e test version: v1.19.6
Jan 29 10:49:37.368: INFO: kube-apiserver version: v1.19.6+vmware.1
Jan 29 10:49:37.369: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:49:37.372: INFO: Cluster IP family: ipv4
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:49:37.372: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-preemption
Jan 29 10:49:37.401: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jan 29 10:49:37.405: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jan 29 10:49:37.420: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 10:50:37.440: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jan 29 10:50:37.463: INFO: Created pod: pod0-sched-preemption-low-priority
Jan 29 10:50:37.485: INFO: Created pod: pod1-sched-preemption-medium-priority
Jan 29 10:50:37.505: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:15.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9290" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:98.219 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":1,"skipped":1,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:15.591: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:51:15.655: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ff59c837-aa9e-49fa-bc15-de829d9152b3", Controller:(*bool)(0xc002ade23a), BlockOwnerDeletion:(*bool)(0xc002ade23b)}}
Jan 29 10:51:15.663: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a1e8f4e6-f756-41c2-99f7-b71bc6570c18", Controller:(*bool)(0xc002d315c6), BlockOwnerDeletion:(*bool)(0xc002d315c7)}}
Jan 29 10:51:15.672: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a84e3c5a-1570-41fc-b21a-7c47914cc13b", Controller:(*bool)(0xc002ade44e), BlockOwnerDeletion:(*bool)(0xc002ade44f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7192" for this suite.

• [SLOW TEST:5.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":2,"skipped":11,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:20.725: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:51:20.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84" in namespace "projected-8621" to be "Succeeded or Failed"
Jan 29 10:51:20.777: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84": Phase="Pending", Reason="", readiness=false. Elapsed: 11.611988ms
Jan 29 10:51:22.779: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013847882s
Jan 29 10:51:24.781: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016411434s
Jan 29 10:51:26.784: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018694627s
Jan 29 10:51:28.787: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.021594785s
STEP: Saw pod success
Jan 29 10:51:28.787: INFO: Pod "downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84" satisfied condition "Succeeded or Failed"
Jan 29 10:51:28.789: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84 container client-container: <nil>
STEP: delete the pod
Jan 29 10:51:28.812: INFO: Waiting for pod downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84 to disappear
Jan 29 10:51:28.819: INFO: Pod downwardapi-volume-8f188f5c-2f46-46ba-b6de-d2d995a73e84 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:28.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8621" for this suite.

• [SLOW TEST:8.100 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":21,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:28.827: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:28.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3446" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":4,"skipped":28,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:28.855: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:51:28.887: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388" in namespace "projected-7312" to be "Succeeded or Failed"
Jan 29 10:51:28.889: INFO: Pod "downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02535ms
Jan 29 10:51:30.893: INFO: Pod "downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005231811s
Jan 29 10:51:32.896: INFO: Pod "downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008652816s
STEP: Saw pod success
Jan 29 10:51:32.896: INFO: Pod "downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388" satisfied condition "Succeeded or Failed"
Jan 29 10:51:32.898: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388 container client-container: <nil>
STEP: delete the pod
Jan 29 10:51:32.914: INFO: Waiting for pod downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388 to disappear
Jan 29 10:51:32.918: INFO: Pod downwardapi-volume-f67a96b4-b85e-4132-a5bb-dcb011206388 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:32.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7312" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":5,"skipped":70,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:32.926: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-69c6edd1-2921-4f1d-b25a-7de557c51f51
STEP: Creating a pod to test consume configMaps
Jan 29 10:51:32.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-d465334f-a755-4652-8112-7fe674af9051" in namespace "configmap-6119" to be "Succeeded or Failed"
Jan 29 10:51:32.965: INFO: Pod "pod-configmaps-d465334f-a755-4652-8112-7fe674af9051": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133876ms
Jan 29 10:51:34.968: INFO: Pod "pod-configmaps-d465334f-a755-4652-8112-7fe674af9051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0060782s
Jan 29 10:51:36.971: INFO: Pod "pod-configmaps-d465334f-a755-4652-8112-7fe674af9051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008661015s
STEP: Saw pod success
Jan 29 10:51:36.971: INFO: Pod "pod-configmaps-d465334f-a755-4652-8112-7fe674af9051" satisfied condition "Succeeded or Failed"
Jan 29 10:51:36.973: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-d465334f-a755-4652-8112-7fe674af9051 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:51:36.985: INFO: Waiting for pod pod-configmaps-d465334f-a755-4652-8112-7fe674af9051 to disappear
Jan 29 10:51:36.991: INFO: Pod pod-configmaps-d465334f-a755-4652-8112-7fe674af9051 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:36.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6119" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":72,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:36.999: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:51:37.018: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:38.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-765" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":7,"skipped":96,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-b01edc06-d79b-4549-a7d2-e38b134dd1a0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:51:42.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7708" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":109,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:51:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 29 10:52:22.177: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0129 10:52:22.177399      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 10:52:22.177418      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 10:52:22.177422      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 10:52:22.177: INFO: Deleting pod "simpletest.rc-6h6tq" in namespace "gc-325"
Jan 29 10:52:22.194: INFO: Deleting pod "simpletest.rc-7hbnv" in namespace "gc-325"
Jan 29 10:52:22.208: INFO: Deleting pod "simpletest.rc-8f5gz" in namespace "gc-325"
Jan 29 10:52:22.232: INFO: Deleting pod "simpletest.rc-99nrj" in namespace "gc-325"
Jan 29 10:52:22.253: INFO: Deleting pod "simpletest.rc-btrbd" in namespace "gc-325"
Jan 29 10:52:22.274: INFO: Deleting pod "simpletest.rc-jvwfn" in namespace "gc-325"
Jan 29 10:52:22.291: INFO: Deleting pod "simpletest.rc-lqgj6" in namespace "gc-325"
Jan 29 10:52:22.314: INFO: Deleting pod "simpletest.rc-w75pn" in namespace "gc-325"
Jan 29 10:52:22.334: INFO: Deleting pod "simpletest.rc-xv8w6" in namespace "gc-325"
Jan 29 10:52:22.359: INFO: Deleting pod "simpletest.rc-xxmqr" in namespace "gc-325"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:52:22.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-325" for this suite.

• [SLOW TEST:40.257 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":9,"skipped":114,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:52:22.382: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:52:22.414: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 29 10:52:25.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 create -f -'
Jan 29 10:52:26.418: INFO: stderr: ""
Jan 29 10:52:26.418: INFO: stdout: "e2e-test-crd-publish-openapi-2310-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 10:52:26.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 delete e2e-test-crd-publish-openapi-2310-crds test-foo'
Jan 29 10:52:26.494: INFO: stderr: ""
Jan 29 10:52:26.494: INFO: stdout: "e2e-test-crd-publish-openapi-2310-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 29 10:52:26.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 apply -f -'
Jan 29 10:52:26.686: INFO: stderr: ""
Jan 29 10:52:26.686: INFO: stdout: "e2e-test-crd-publish-openapi-2310-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 29 10:52:26.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 delete e2e-test-crd-publish-openapi-2310-crds test-foo'
Jan 29 10:52:26.758: INFO: stderr: ""
Jan 29 10:52:26.758: INFO: stdout: "e2e-test-crd-publish-openapi-2310-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 29 10:52:26.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 create -f -'
Jan 29 10:52:26.944: INFO: rc: 1
Jan 29 10:52:26.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 apply -f -'
Jan 29 10:52:27.126: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 29 10:52:27.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 create -f -'
Jan 29 10:52:27.296: INFO: rc: 1
Jan 29 10:52:27.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 --namespace=crd-publish-openapi-3049 apply -f -'
Jan 29 10:52:27.464: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 29 10:52:27.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 explain e2e-test-crd-publish-openapi-2310-crds'
Jan 29 10:52:27.635: INFO: stderr: ""
Jan 29 10:52:27.635: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2310-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 29 10:52:27.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 explain e2e-test-crd-publish-openapi-2310-crds.metadata'
Jan 29 10:52:27.805: INFO: stderr: ""
Jan 29 10:52:27.805: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2310-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 29 10:52:27.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 explain e2e-test-crd-publish-openapi-2310-crds.spec'
Jan 29 10:52:27.982: INFO: stderr: ""
Jan 29 10:52:27.982: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2310-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 29 10:52:27.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 explain e2e-test-crd-publish-openapi-2310-crds.spec.bars'
Jan 29 10:52:28.157: INFO: stderr: ""
Jan 29 10:52:28.157: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2310-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 29 10:52:28.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3049 explain e2e-test-crd-publish-openapi-2310-crds.spec.bars2'
Jan 29 10:52:28.328: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:52:31.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3049" for this suite.

• [SLOW TEST:8.848 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":10,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:52:31.232: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:52:35.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9336" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":11,"skipped":166,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:52:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5521
STEP: creating service affinity-clusterip in namespace services-5521
STEP: creating replication controller affinity-clusterip in namespace services-5521
I0129 10:52:35.337962      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-5521, replica count: 3
I0129 10:52:38.388405      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 10:52:41.388555      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 10:52:44.388768      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 10:52:44.393: INFO: Creating new exec pod
Jan 29 10:52:47.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5521 exec execpod-affinityskpjt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jan 29 10:52:47.578: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 29 10:52:47.578: INFO: stdout: ""
Jan 29 10:52:47.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5521 exec execpod-affinityskpjt -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.185 80'
Jan 29 10:52:47.734: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.185 80\nConnection to 10.100.200.185 80 port [tcp/http] succeeded!\n"
Jan 29 10:52:47.734: INFO: stdout: ""
Jan 29 10:52:47.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5521 exec execpod-affinityskpjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.185:80/ ; done'
Jan 29 10:52:47.962: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.185:80/\n"
Jan 29 10:52:47.962: INFO: stdout: "\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs\naffinity-clusterip-mjzvs"
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Received response from host: affinity-clusterip-mjzvs
Jan 29 10:52:47.962: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5521, will wait for the garbage collector to delete the pods
Jan 29 10:52:48.035: INFO: Deleting ReplicationController affinity-clusterip took: 5.423109ms
Jan 29 10:52:48.535: INFO: Terminating ReplicationController affinity-clusterip pods took: 500.160315ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:52:58.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5521" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.677 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":12,"skipped":172,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:52:58.966: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-3357/secret-test-6bf5942c-bc80-425a-a2b0-44207bcc4e20
STEP: Creating a pod to test consume secrets
Jan 29 10:52:58.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06" in namespace "secrets-3357" to be "Succeeded or Failed"
Jan 29 10:52:59.002: INFO: Pod "pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.923997ms
Jan 29 10:53:01.005: INFO: Pod "pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00766552s
STEP: Saw pod success
Jan 29 10:53:01.005: INFO: Pod "pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06" satisfied condition "Succeeded or Failed"
Jan 29 10:53:01.007: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06 container env-test: <nil>
STEP: delete the pod
Jan 29 10:53:01.028: INFO: Waiting for pod pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06 to disappear
Jan 29 10:53:01.033: INFO: Pod pod-configmaps-68fd3907-dbc0-4816-abf0-09c0d9e39a06 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:01.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3357" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":186,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:01.039: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0129 10:53:11.087324      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 10:53:11.087384      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 10:53:11.087521      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 10:53:11.087: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:11.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5900" for this suite.

• [SLOW TEST:10.054 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":14,"skipped":192,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:11.094: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 10:53:11.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 10:53:14.645: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 29 10:53:14.666: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5338" for this suite.
STEP: Destroying namespace "webhook-5338-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":15,"skipped":207,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:18.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-108" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":224,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:18.806: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8749
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 10:53:18.826: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 10:53:18.883: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 10:53:20.886: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 10:53:22.886: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 10:53:24.886: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 10:53:26.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:28.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:30.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:32.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:34.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:36.886: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 10:53:38.886: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 29 10:53:38.890: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 29 10:53:38.893: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 29 10:53:40.912: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.50.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8749 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:53:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:53:41.995: INFO: Found all expected endpoints: [netserver-0]
Jan 29 10:53:41.997: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.95.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8749 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:53:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:53:43.071: INFO: Found all expected endpoints: [netserver-1]
Jan 29 10:53:43.074: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.24.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8749 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:53:43.074: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:53:44.161: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:44.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8749" for this suite.

• [SLOW TEST:25.363 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:44.169: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 10:53:44.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 10:53:46.728: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514424, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514424, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514424, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514424, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 10:53:49.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:49.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4976" for this suite.
STEP: Destroying namespace "webhook-4976-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":18,"skipped":251,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:49.853: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:49.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9750" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":19,"skipped":261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:49.930: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-1951/configmap-test-ac803d9a-0a35-40cc-80e8-0c7a117c8a69
STEP: Creating a pod to test consume configMaps
Jan 29 10:53:49.971: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2" in namespace "configmap-1951" to be "Succeeded or Failed"
Jan 29 10:53:49.980: INFO: Pod "pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003725ms
Jan 29 10:53:51.983: INFO: Pod "pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011988911s
STEP: Saw pod success
Jan 29 10:53:51.983: INFO: Pod "pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2" satisfied condition "Succeeded or Failed"
Jan 29 10:53:51.984: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2 container env-test: <nil>
STEP: delete the pod
Jan 29 10:53:51.997: INFO: Waiting for pod pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2 to disappear
Jan 29 10:53:52.004: INFO: Pod pod-configmaps-bf0e0db2-bbb0-4c18-80e6-17b86f967de2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:52.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1951" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jan 29 10:53:52.561: INFO: created pod pod-service-account-defaultsa
Jan 29 10:53:52.561: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 29 10:53:52.572: INFO: created pod pod-service-account-mountsa
Jan 29 10:53:52.572: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 29 10:53:52.575: INFO: created pod pod-service-account-nomountsa
Jan 29 10:53:52.575: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 29 10:53:52.584: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 29 10:53:52.584: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 29 10:53:52.596: INFO: created pod pod-service-account-mountsa-mountspec
Jan 29 10:53:52.596: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 29 10:53:52.608: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 29 10:53:52.608: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 29 10:53:52.618: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 29 10:53:52.619: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 29 10:53:52.650: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 29 10:53:52.650: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 29 10:53:52.662: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 29 10:53:52.662: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:52.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9588" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":21,"skipped":332,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:52.691: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 10:53:53.057: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 10:53:56.092: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 29 10:53:58.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=webhook-1226 attach --namespace=webhook-1226 to-be-attached-pod -i -c=container1'
Jan 29 10:53:58.229: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:53:58.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1226" for this suite.
STEP: Destroying namespace "webhook-1226-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.615 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":22,"skipped":335,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:53:58.306: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jan 29 10:53:58.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 create -f -'
Jan 29 10:53:58.653: INFO: stderr: ""
Jan 29 10:53:58.653: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 10:53:58.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:53:58.736: INFO: stderr: ""
Jan 29 10:53:58.736: INFO: stdout: "update-demo-nautilus-8x2qv update-demo-nautilus-r2fsx "
Jan 29 10:53:58.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-8x2qv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:53:58.811: INFO: stderr: ""
Jan 29 10:53:58.811: INFO: stdout: ""
Jan 29 10:53:58.811: INFO: update-demo-nautilus-8x2qv is created but not running
Jan 29 10:54:03.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:54:03.881: INFO: stderr: ""
Jan 29 10:54:03.881: INFO: stdout: "update-demo-nautilus-8x2qv update-demo-nautilus-r2fsx "
Jan 29 10:54:03.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-8x2qv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:03.948: INFO: stderr: ""
Jan 29 10:54:03.948: INFO: stdout: "true"
Jan 29 10:54:03.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-8x2qv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 10:54:04.024: INFO: stderr: ""
Jan 29 10:54:04.024: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 10:54:04.024: INFO: validating pod update-demo-nautilus-8x2qv
Jan 29 10:54:04.034: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 10:54:04.034: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 10:54:04.034: INFO: update-demo-nautilus-8x2qv is verified up and running
Jan 29 10:54:04.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:04.100: INFO: stderr: ""
Jan 29 10:54:04.100: INFO: stdout: "true"
Jan 29 10:54:04.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 10:54:04.165: INFO: stderr: ""
Jan 29 10:54:04.165: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 10:54:04.165: INFO: validating pod update-demo-nautilus-r2fsx
Jan 29 10:54:04.168: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 10:54:04.168: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 10:54:04.168: INFO: update-demo-nautilus-r2fsx is verified up and running
STEP: scaling down the replication controller
Jan 29 10:54:04.170: INFO: scanned /root for discovery docs: <nil>
Jan 29 10:54:04.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 29 10:54:05.271: INFO: stderr: ""
Jan 29 10:54:05.271: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 10:54:05.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:54:05.341: INFO: stderr: ""
Jan 29 10:54:05.341: INFO: stdout: "update-demo-nautilus-8x2qv update-demo-nautilus-r2fsx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 29 10:54:10.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:54:10.412: INFO: stderr: ""
Jan 29 10:54:10.412: INFO: stdout: "update-demo-nautilus-r2fsx "
Jan 29 10:54:10.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:10.477: INFO: stderr: ""
Jan 29 10:54:10.477: INFO: stdout: "true"
Jan 29 10:54:10.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 10:54:10.540: INFO: stderr: ""
Jan 29 10:54:10.540: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 10:54:10.540: INFO: validating pod update-demo-nautilus-r2fsx
Jan 29 10:54:10.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 10:54:10.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 10:54:10.542: INFO: update-demo-nautilus-r2fsx is verified up and running
STEP: scaling up the replication controller
Jan 29 10:54:10.544: INFO: scanned /root for discovery docs: <nil>
Jan 29 10:54:10.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 29 10:54:11.644: INFO: stderr: ""
Jan 29 10:54:11.644: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 10:54:11.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:54:11.712: INFO: stderr: ""
Jan 29 10:54:11.712: INFO: stdout: "update-demo-nautilus-bdwks update-demo-nautilus-r2fsx "
Jan 29 10:54:11.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-bdwks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:11.776: INFO: stderr: ""
Jan 29 10:54:11.776: INFO: stdout: ""
Jan 29 10:54:11.776: INFO: update-demo-nautilus-bdwks is created but not running
Jan 29 10:54:16.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 10:54:16.849: INFO: stderr: ""
Jan 29 10:54:16.849: INFO: stdout: "update-demo-nautilus-bdwks update-demo-nautilus-r2fsx "
Jan 29 10:54:16.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-bdwks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:16.914: INFO: stderr: ""
Jan 29 10:54:16.914: INFO: stdout: "true"
Jan 29 10:54:16.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-bdwks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 10:54:16.991: INFO: stderr: ""
Jan 29 10:54:16.991: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 10:54:16.991: INFO: validating pod update-demo-nautilus-bdwks
Jan 29 10:54:16.994: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 10:54:16.994: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 10:54:16.994: INFO: update-demo-nautilus-bdwks is verified up and running
Jan 29 10:54:16.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 10:54:17.059: INFO: stderr: ""
Jan 29 10:54:17.059: INFO: stdout: "true"
Jan 29 10:54:17.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods update-demo-nautilus-r2fsx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 10:54:17.128: INFO: stderr: ""
Jan 29 10:54:17.128: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 10:54:17.128: INFO: validating pod update-demo-nautilus-r2fsx
Jan 29 10:54:17.131: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 10:54:17.131: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 10:54:17.131: INFO: update-demo-nautilus-r2fsx is verified up and running
STEP: using delete to clean up resources
Jan 29 10:54:17.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 delete --grace-period=0 --force -f -'
Jan 29 10:54:17.208: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 10:54:17.208: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 10:54:17.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get rc,svc -l name=update-demo --no-headers'
Jan 29 10:54:17.284: INFO: stderr: "No resources found in kubectl-1827 namespace.\n"
Jan 29 10:54:17.285: INFO: stdout: ""
Jan 29 10:54:17.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 10:54:17.360: INFO: stderr: ""
Jan 29 10:54:17.360: INFO: stdout: "update-demo-nautilus-bdwks\nupdate-demo-nautilus-r2fsx\n"
Jan 29 10:54:17.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get rc,svc -l name=update-demo --no-headers'
Jan 29 10:54:17.953: INFO: stderr: "No resources found in kubectl-1827 namespace.\n"
Jan 29 10:54:17.953: INFO: stdout: ""
Jan 29 10:54:17.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1827 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 10:54:18.031: INFO: stderr: ""
Jan 29 10:54:18.031: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:54:18.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1827" for this suite.

• [SLOW TEST:19.732 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":23,"skipped":336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:54:18.040: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-e526745c-a74e-418b-bad0-7f1f239034c4
STEP: Creating a pod to test consume secrets
Jan 29 10:54:18.073: INFO: Waiting up to 5m0s for pod "pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6" in namespace "secrets-241" to be "Succeeded or Failed"
Jan 29 10:54:18.085: INFO: Pod "pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.516838ms
Jan 29 10:54:20.088: INFO: Pod "pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014624166s
Jan 29 10:54:22.091: INFO: Pod "pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017538279s
STEP: Saw pod success
Jan 29 10:54:22.091: INFO: Pod "pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6" satisfied condition "Succeeded or Failed"
Jan 29 10:54:22.092: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 10:54:22.105: INFO: Waiting for pod pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6 to disappear
Jan 29 10:54:22.112: INFO: Pod pod-secrets-917d61ec-2b40-4bbd-9fc8-18cd8ef368a6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:54:22.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-241" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:54:22.119: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 10:54:22.476: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 10:54:24.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514462, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514462, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514462, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514462, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 10:54:27.493: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:54:37.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4117" for this suite.
STEP: Destroying namespace "webhook-4117-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.523 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":25,"skipped":399,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:54:37.643: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 10:54:37.697: INFO: Waiting up to 5m0s for pod "pod-87851de8-15b2-4972-960c-d44a0d814c94" in namespace "emptydir-7306" to be "Succeeded or Failed"
Jan 29 10:54:37.701: INFO: Pod "pod-87851de8-15b2-4972-960c-d44a0d814c94": Phase="Pending", Reason="", readiness=false. Elapsed: 3.970156ms
Jan 29 10:54:39.704: INFO: Pod "pod-87851de8-15b2-4972-960c-d44a0d814c94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006875875s
STEP: Saw pod success
Jan 29 10:54:39.704: INFO: Pod "pod-87851de8-15b2-4972-960c-d44a0d814c94" satisfied condition "Succeeded or Failed"
Jan 29 10:54:39.706: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-87851de8-15b2-4972-960c-d44a0d814c94 container test-container: <nil>
STEP: delete the pod
Jan 29 10:54:39.722: INFO: Waiting for pod pod-87851de8-15b2-4972-960c-d44a0d814c94 to disappear
Jan 29 10:54:39.728: INFO: Pod pod-87851de8-15b2-4972-960c-d44a0d814c94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:54:39.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7306" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:54:39.737: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-v7tm
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 10:54:39.777: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v7tm" in namespace "subpath-6100" to be "Succeeded or Failed"
Jan 29 10:54:39.788: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Pending", Reason="", readiness=false. Elapsed: 10.636855ms
Jan 29 10:54:41.791: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 2.013749772s
Jan 29 10:54:43.794: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 4.016254607s
Jan 29 10:54:45.797: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 6.019250893s
Jan 29 10:54:47.799: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 8.021754053s
Jan 29 10:54:49.803: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 10.025479778s
Jan 29 10:54:51.806: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 12.028294161s
Jan 29 10:54:53.808: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 14.030716837s
Jan 29 10:54:55.811: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 16.033262459s
Jan 29 10:54:57.813: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 18.035918282s
Jan 29 10:54:59.816: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Running", Reason="", readiness=true. Elapsed: 20.039041587s
Jan 29 10:55:01.820: INFO: Pod "pod-subpath-test-configmap-v7tm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.042580899s
STEP: Saw pod success
Jan 29 10:55:01.820: INFO: Pod "pod-subpath-test-configmap-v7tm" satisfied condition "Succeeded or Failed"
Jan 29 10:55:01.822: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-subpath-test-configmap-v7tm container test-container-subpath-configmap-v7tm: <nil>
STEP: delete the pod
Jan 29 10:55:01.834: INFO: Waiting for pod pod-subpath-test-configmap-v7tm to disappear
Jan 29 10:55:01.852: INFO: Pod pod-subpath-test-configmap-v7tm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v7tm
Jan 29 10:55:01.852: INFO: Deleting pod "pod-subpath-test-configmap-v7tm" in namespace "subpath-6100"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:01.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6100" for this suite.

• [SLOW TEST:22.123 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":27,"skipped":430,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:01.863: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 29 10:55:03.901: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4739 PodName:pod-sharedvolume-89313aef-7d8a-43a4-a5c4-dd4df5d58810 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 10:55:03.901: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:55:03.972: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4739" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":28,"skipped":451,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:03.980: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:04.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8652" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":29,"skipped":461,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:04.012: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:04.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8630" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":30,"skipped":462,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:04.059: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 10:55:04.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645" in namespace "downward-api-2871" to be "Succeeded or Failed"
Jan 29 10:55:04.087: INFO: Pod "downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645": Phase="Pending", Reason="", readiness=false. Elapsed: 1.814264ms
Jan 29 10:55:06.090: INFO: Pod "downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004914169s
STEP: Saw pod success
Jan 29 10:55:06.090: INFO: Pod "downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645" satisfied condition "Succeeded or Failed"
Jan 29 10:55:06.092: INFO: Trying to get logs from node 7624137d-137a-468c-9e03-7a04d16d3fbf pod downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645 container client-container: <nil>
STEP: delete the pod
Jan 29 10:55:06.122: INFO: Waiting for pod downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645 to disappear
Jan 29 10:55:06.127: INFO: Pod downwardapi-volume-a5a4f686-069e-42b9-a30e-c4bd5e3cd645 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:06.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2871" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6121" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":32,"skipped":565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jan 29 10:55:06.214: INFO: Major version: 1
STEP: Confirm minor version
Jan 29 10:55:06.214: INFO: cleanMinorVersion: 19
Jan 29 10:55:06.214: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:06.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7019" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":33,"skipped":597,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:06.240: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:55:06.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-1654 version'
Jan 29 10:55:06.342: INFO: stderr: ""
Jan 29 10:55:06.342: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6\", GitCommit:\"fbf646b339dc52336b55d8ec85c181981b86331a\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:09:30Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6+vmware.1\", GitCommit:\"afe4eda368a924376517be0c262541f358673efa\", GitTreeState:\"clean\", BuildDate:\"2020-12-22T01:08:16Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:06.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1654" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":34,"skipped":602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:06.349: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 10:55:10.403: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:10.405: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 10:55:12.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:12.408: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 10:55:14.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:14.408: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 10:55:16.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:16.408: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 10:55:18.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:18.408: INFO: Pod pod-with-poststart-http-hook still exists
Jan 29 10:55:20.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 29 10:55:20.408: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4496" for this suite.

• [SLOW TEST:14.068 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":35,"skipped":637,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 29 10:55:20.448: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:28.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6425" for this suite.

• [SLOW TEST:8.011 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":643,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:28.429: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d721a5d5-c122-427e-a6e4-4e722cbcfdf2
STEP: Creating a pod to test consume configMaps
Jan 29 10:55:28.459: INFO: Waiting up to 5m0s for pod "pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a" in namespace "configmap-4711" to be "Succeeded or Failed"
Jan 29 10:55:28.461: INFO: Pod "pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.987095ms
Jan 29 10:55:30.463: INFO: Pod "pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004833966s
STEP: Saw pod success
Jan 29 10:55:30.463: INFO: Pod "pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a" satisfied condition "Succeeded or Failed"
Jan 29 10:55:30.466: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:55:30.480: INFO: Waiting for pod pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a to disappear
Jan 29 10:55:30.485: INFO: Pod pod-configmaps-1216bbbf-35bd-4a2e-9f29-b6e6c535f08a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4711" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:55:30.530: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-1b651f98-a5f1-4615-9674-96add71e2294" in namespace "security-context-test-6260" to be "Succeeded or Failed"
Jan 29 10:55:30.534: INFO: Pod "alpine-nnp-false-1b651f98-a5f1-4615-9674-96add71e2294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991372ms
Jan 29 10:55:32.536: INFO: Pod "alpine-nnp-false-1b651f98-a5f1-4615-9674-96add71e2294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005652095s
Jan 29 10:55:34.539: INFO: Pod "alpine-nnp-false-1b651f98-a5f1-4615-9674-96add71e2294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008630067s
Jan 29 10:55:34.539: INFO: Pod "alpine-nnp-false-1b651f98-a5f1-4615-9674-96add71e2294" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6260" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":707,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:34.552: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jan 29 10:55:34.586: INFO: Waiting up to 5m0s for pod "var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff" in namespace "var-expansion-8788" to be "Succeeded or Failed"
Jan 29 10:55:34.601: INFO: Pod "var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.520292ms
Jan 29 10:55:36.604: INFO: Pod "var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017638362s
STEP: Saw pod success
Jan 29 10:55:36.604: INFO: Pod "var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff" satisfied condition "Succeeded or Failed"
Jan 29 10:55:36.606: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff container dapi-container: <nil>
STEP: delete the pod
Jan 29 10:55:36.619: INFO: Waiting for pod var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff to disappear
Jan 29 10:55:36.626: INFO: Pod var-expansion-19d791bd-c2e6-4e37-809c-b377dc05dbff no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:55:36.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8788" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":707,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:55:36.632: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 29 10:55:36.655: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 29 10:55:47.993: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 10:55:49.883: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:00.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3053" for this suite.

• [SLOW TEST:23.661 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":40,"skipped":742,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:00.294: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-1b7b052c-148f-48f9-9044-728c58c18bee
STEP: Creating a pod to test consume secrets
Jan 29 10:56:00.325: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360" in namespace "projected-905" to be "Succeeded or Failed"
Jan 29 10:56:00.327: INFO: Pod "pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360": Phase="Pending", Reason="", readiness=false. Elapsed: 1.250155ms
Jan 29 10:56:02.329: INFO: Pod "pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0038387s
STEP: Saw pod success
Jan 29 10:56:02.329: INFO: Pod "pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360" satisfied condition "Succeeded or Failed"
Jan 29 10:56:02.331: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 10:56:02.343: INFO: Waiting for pod pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360 to disappear
Jan 29 10:56:02.350: INFO: Pod pod-projected-secrets-dead2a67-b838-486d-9811-f169e9782360 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:02.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-905" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":755,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:02.356: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-937ecba2-49b2-491b-9a97-dee29340b184
STEP: Creating a pod to test consume configMaps
Jan 29 10:56:02.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7" in namespace "projected-5162" to be "Succeeded or Failed"
Jan 29 10:56:02.411: INFO: Pod "pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.286764ms
Jan 29 10:56:04.414: INFO: Pod "pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023332025s
STEP: Saw pod success
Jan 29 10:56:04.414: INFO: Pod "pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7" satisfied condition "Succeeded or Failed"
Jan 29 10:56:04.416: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 10:56:04.429: INFO: Waiting for pod pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7 to disappear
Jan 29 10:56:04.435: INFO: Pod pod-projected-configmaps-f77b4b11-3c7a-403f-8370-e399c50713f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:04.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5162" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":765,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:04.441: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-761c8ebc-1426-4299-9c8a-5681c4f073fa
STEP: Creating a pod to test consume secrets
Jan 29 10:56:04.475: INFO: Waiting up to 5m0s for pod "pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656" in namespace "secrets-240" to be "Succeeded or Failed"
Jan 29 10:56:04.477: INFO: Pod "pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820055ms
Jan 29 10:56:06.480: INFO: Pod "pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004757282s
STEP: Saw pod success
Jan 29 10:56:06.480: INFO: Pod "pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656" satisfied condition "Succeeded or Failed"
Jan 29 10:56:06.482: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 10:56:06.494: INFO: Waiting for pod pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656 to disappear
Jan 29 10:56:06.511: INFO: Pod pod-secrets-328f849e-1963-4f7a-aeed-22a92fa9d656 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:06.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-240" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:06.517: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7506.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7506.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7506.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 10:56:16.574: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.576: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.592: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.595: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.597: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.599: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:16.604: INFO: Lookups using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local]

Jan 29 10:56:21.609: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.612: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.614: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.616: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.622: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.624: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.625: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.627: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:21.631: INFO: Lookups using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local]

Jan 29 10:56:26.608: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.610: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.612: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.614: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.619: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.621: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.623: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.625: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:26.629: INFO: Lookups using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local]

Jan 29 10:56:31.608: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.610: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.613: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.614: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.620: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.622: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.624: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.625: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:31.633: INFO: Lookups using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local]

Jan 29 10:56:36.608: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.611: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.613: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.615: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.621: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.623: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.625: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.627: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local from pod dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528: the server could not find the requested resource (get pods dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528)
Jan 29 10:56:36.631: INFO: Lookups using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7506.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7506.svc.cluster.local jessie_udp@dns-test-service-2.dns-7506.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7506.svc.cluster.local]

Jan 29 10:56:41.630: INFO: DNS probes using dns-7506/dns-test-c0df543f-2c6c-41ef-bccf-822d81f0a528 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:41.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7506" for this suite.

• [SLOW TEST:35.163 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":44,"skipped":815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0129 10:56:42.749030      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 10:56:42.749115      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 10:56:42.749142      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 10:56:42.749: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7486" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":45,"skipped":863,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:42.763: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:44.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3189" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":874,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:44.880: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:44.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2704" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":47,"skipped":922,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:44.917: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jan 29 10:56:44.935: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6336 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:56:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6336" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":48,"skipped":925,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:56:45.003: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:57:14.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6855" for this suite.
STEP: Destroying namespace "nsdeletetest-3732" for this suite.
Jan 29 10:57:14.114: INFO: Namespace nsdeletetest-3732 was already deleted
STEP: Destroying namespace "nsdeletetest-4749" for this suite.

• [SLOW TEST:29.113 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":49,"skipped":946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:57:14.117: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jan 29 10:57:14.145: INFO: Waiting up to 5m0s for pod "client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6" in namespace "containers-2025" to be "Succeeded or Failed"
Jan 29 10:57:14.156: INFO: Pod "client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.387067ms
Jan 29 10:57:16.160: INFO: Pod "client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014146885s
Jan 29 10:57:18.163: INFO: Pod "client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01798649s
STEP: Saw pod success
Jan 29 10:57:18.163: INFO: Pod "client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6" satisfied condition "Succeeded or Failed"
Jan 29 10:57:18.165: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6 container test-container: <nil>
STEP: delete the pod
Jan 29 10:57:18.179: INFO: Waiting for pod client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6 to disappear
Jan 29 10:57:18.185: INFO: Pod client-containers-09cfdb71-a983-4a42-9396-d7b37b7863c6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:57:18.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2025" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":969,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:57:18.191: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jan 29 10:57:22.755: INFO: Successfully updated pod "annotationupdate8b533a78-3524-453c-872f-c259ed0c3e5c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:57:24.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3781" for this suite.

• [SLOW TEST:6.581 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:57:24.773: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 10:57:24.816: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 10:57:24.823: INFO: Number of nodes with available pods: 0
Jan 29 10:57:24.823: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:25.830: INFO: Number of nodes with available pods: 0
Jan 29 10:57:25.830: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:26.830: INFO: Number of nodes with available pods: 0
Jan 29 10:57:26.830: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:27.830: INFO: Number of nodes with available pods: 0
Jan 29 10:57:27.830: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:28.829: INFO: Number of nodes with available pods: 0
Jan 29 10:57:28.829: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:29.832: INFO: Number of nodes with available pods: 0
Jan 29 10:57:29.832: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:30.830: INFO: Number of nodes with available pods: 0
Jan 29 10:57:30.830: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:31.830: INFO: Number of nodes with available pods: 0
Jan 29 10:57:31.830: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 10:57:32.829: INFO: Number of nodes with available pods: 3
Jan 29 10:57:32.829: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 29 10:57:32.887: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:32.887: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:32.887: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:33.901: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:33.901: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:33.901: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:34.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:34.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:34.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:35.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:35.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:35.902: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:35.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:36.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:36.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:36.902: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:36.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:37.901: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:37.901: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:37.901: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:37.901: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:38.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:38.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:38.902: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:38.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:39.903: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:39.903: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:39.903: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:39.903: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:40.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:40.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:40.902: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:40.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:41.903: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:41.903: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:41.903: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:41.903: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:42.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:42.902: INFO: Wrong image for pod: daemon-set-nqp2r. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:42.902: INFO: Pod daemon-set-nqp2r is not available
Jan 29 10:57:42.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:43.901: INFO: Pod daemon-set-6hncm is not available
Jan 29 10:57:43.901: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:43.901: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:44.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:44.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:45.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:45.902: INFO: Pod daemon-set-fs4rt is not available
Jan 29 10:57:45.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:46.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:46.902: INFO: Pod daemon-set-fs4rt is not available
Jan 29 10:57:46.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:47.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:47.902: INFO: Pod daemon-set-fs4rt is not available
Jan 29 10:57:47.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:48.902: INFO: Wrong image for pod: daemon-set-fs4rt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:48.902: INFO: Pod daemon-set-fs4rt is not available
Jan 29 10:57:48.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:49.901: INFO: Pod daemon-set-jkscf is not available
Jan 29 10:57:49.901: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:50.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:51.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:51.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:52.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:52.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:53.901: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:53.901: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:54.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:54.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:55.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:55.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:56.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:56.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:57.902: INFO: Wrong image for pod: daemon-set-whhlw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Jan 29 10:57:57.902: INFO: Pod daemon-set-whhlw is not available
Jan 29 10:57:58.902: INFO: Pod daemon-set-qd52x is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 29 10:57:58.909: INFO: Number of nodes with available pods: 2
Jan 29 10:57:58.909: INFO: Node c66ef63f-50cf-427f-af2a-b340450b0b53 is running more than one daemon pod
Jan 29 10:57:59.915: INFO: Number of nodes with available pods: 2
Jan 29 10:57:59.915: INFO: Node c66ef63f-50cf-427f-af2a-b340450b0b53 is running more than one daemon pod
Jan 29 10:58:00.915: INFO: Number of nodes with available pods: 3
Jan 29 10:58:00.915: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2636, will wait for the garbage collector to delete the pods
Jan 29 10:58:00.983: INFO: Deleting DaemonSet.extensions daemon-set took: 7.330997ms
Jan 29 10:58:01.483: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.53182ms
Jan 29 10:58:08.485: INFO: Number of nodes with available pods: 0
Jan 29 10:58:08.485: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 10:58:08.487: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2636/daemonsets","resourceVersion":"8159"},"items":null}

Jan 29 10:58:08.489: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2636/pods","resourceVersion":"8159"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 10:58:08.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2636" for this suite.

• [SLOW TEST:43.728 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":52,"skipped":1000,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 10:58:08.502: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-66a5853a-ec7c-4bf7-8588-02677fd074d7 in namespace container-probe-288
Jan 29 10:58:10.543: INFO: Started pod liveness-66a5853a-ec7c-4bf7-8588-02677fd074d7 in namespace container-probe-288
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 10:58:10.546: INFO: Initial restart count of pod liveness-66a5853a-ec7c-4bf7-8588-02677fd074d7 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-288" for this suite.

• [SLOW TEST:242.398 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":1018,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:10.900: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:13.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1639" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":54,"skipped":1023,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:13.953: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:30.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9186" for this suite.

• [SLOW TEST:16.110 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":55,"skipped":1027,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:30.063: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 29 11:02:30.083: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 11:02:30.094: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 11:02:30.097: INFO: 
Logging pods the apiserver thinks is on node 16fc05cd-9790-4b41-b13f-11160f9262cc before test
Jan 29 11:02:30.105: INFO: coredns-664d7f64c5-lwq6l from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:02:30.105: INFO: event-controller-85d6bb4d4c-qrl9c from pks-system started at 2021-01-29 10:32:14 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container event-controller ready: true, restart count 0
Jan 29 11:02:30.105: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:02:30.105: INFO: fluent-bit-82lt4 from pks-system started at 2021-01-29 10:32:33 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:02:30.105: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:02:30.105: INFO: node-exporter-hhk27 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:02:30.105: INFO: telegraf-t9hn6 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:02:30.105: INFO: wavefront-proxy-5bf7b49596-6vwv9 from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container wavefront-proxy ready: true, restart count 0
Jan 29 11:02:30.105: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:02:30.105: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:02:30.105: INFO: 
Logging pods the apiserver thinks is on node 7624137d-137a-468c-9e03-7a04d16d3fbf before test
Jan 29 11:02:30.113: INFO: coredns-664d7f64c5-8bv7k from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:02:30.113: INFO: fluent-bit-r4pxl from pks-system started at 2021-01-29 10:32:28 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:02:30.113: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:02:30.113: INFO: metric-controller-7f5cb8ff6d-cm2b2 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container metric-controller ready: true, restart count 0
Jan 29 11:02:30.113: INFO: node-exporter-kjb4q from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:02:30.113: INFO: observability-manager-6cf797f97-wshfj from pks-system started at 2021-01-29 10:32:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container observability-manager ready: true, restart count 0
Jan 29 11:02:30.113: INFO: telegraf-459jf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.113: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:02:30.114: INFO: telemetry-agent-8f56d9865-rt7qc from pks-system started at 2021-01-29 10:36:57 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.114: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jan 29 11:02:30.114: INFO: validator-69f557d7c6-hz8vq from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.114: INFO: 	Container validator ready: true, restart count 0
Jan 29 11:02:30.114: INFO: sonobuoy-e2e-job-a3be688aee3e44de from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.114: INFO: 	Container e2e ready: true, restart count 0
Jan 29 11:02:30.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:02:30.114: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:02:30.114: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:02:30.114: INFO: 
Logging pods the apiserver thinks is on node c66ef63f-50cf-427f-af2a-b340450b0b53 before test
Jan 29 11:02:30.122: INFO: coredns-664d7f64c5-nqvrn from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.123: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:02:30.123: INFO: metrics-server-7d476fdfbd-n4b2n from kube-system started at 2021-01-29 10:32:08 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.123: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 11:02:30.123: INFO: cert-generator-e484f2435fc830429fc9747bd002fcda56dd053e-4lc5d from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.123: INFO: 	Container cert-generator ready: false, restart count 0
Jan 29 11:02:30.123: INFO: fluent-bit-xbbgf from pks-system started at 2021-01-29 10:32:16 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.123: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:02:30.123: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:02:30.123: INFO: node-exporter-bx9js from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.123: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:02:30.123: INFO: sink-controller-f6bc7f774-wn8vp from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.124: INFO: 	Container sink-controller ready: true, restart count 0
Jan 29 11:02:30.124: INFO: telegraf-w42nf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.124: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:02:30.124: INFO: wavefront-collector-7648f897c7-sb8kc from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.124: INFO: 	Container wavefront-collector ready: true, restart count 0
Jan 29 11:02:30.124: INFO: sonobuoy from sonobuoy started at 2021-01-29 10:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 11:02:30.124: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 11:02:30.124: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:02:30.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:02:30.124: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node 16fc05cd-9790-4b41-b13f-11160f9262cc
STEP: verifying the node has the label node 7624137d-137a-468c-9e03-7a04d16d3fbf
STEP: verifying the node has the label node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod coredns-664d7f64c5-8bv7k requesting resource cpu=100m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.183: INFO: Pod coredns-664d7f64c5-lwq6l requesting resource cpu=100m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.183: INFO: Pod coredns-664d7f64c5-nqvrn requesting resource cpu=100m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod metrics-server-7d476fdfbd-n4b2n requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod event-controller-85d6bb4d4c-qrl9c requesting resource cpu=0m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.183: INFO: Pod fluent-bit-82lt4 requesting resource cpu=0m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.183: INFO: Pod fluent-bit-r4pxl requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.183: INFO: Pod fluent-bit-xbbgf requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod metric-controller-7f5cb8ff6d-cm2b2 requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.183: INFO: Pod node-exporter-bx9js requesting resource cpu=10m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod node-exporter-hhk27 requesting resource cpu=10m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.183: INFO: Pod node-exporter-kjb4q requesting resource cpu=10m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.183: INFO: Pod observability-manager-6cf797f97-wshfj requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.183: INFO: Pod sink-controller-f6bc7f774-wn8vp requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.183: INFO: Pod telegraf-459jf requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.184: INFO: Pod telegraf-t9hn6 requesting resource cpu=0m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.184: INFO: Pod telegraf-w42nf requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.184: INFO: Pod telemetry-agent-8f56d9865-rt7qc requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.184: INFO: Pod validator-69f557d7c6-hz8vq requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.184: INFO: Pod wavefront-collector-7648f897c7-sb8kc requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.184: INFO: Pod wavefront-proxy-5bf7b49596-6vwv9 requesting resource cpu=0m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.184: INFO: Pod sonobuoy requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.184: INFO: Pod sonobuoy-e2e-job-a3be688aee3e44de requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.184: INFO: Pod sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj requesting resource cpu=0m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
Jan 29 11:02:30.184: INFO: Pod sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks requesting resource cpu=0m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.184: INFO: Pod sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 requesting resource cpu=0m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
STEP: Starting Pods to consume most of the cluster CPU.
Jan 29 11:02:30.184: INFO: Creating a pod which consumes cpu=973m on Node 16fc05cd-9790-4b41-b13f-11160f9262cc
Jan 29 11:02:30.191: INFO: Creating a pod which consumes cpu=973m on Node 7624137d-137a-468c-9e03-7a04d16d3fbf
Jan 29 11:02:30.204: INFO: Creating a pod which consumes cpu=973m on Node c66ef63f-50cf-427f-af2a-b340450b0b53
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def.165eaf046dec25f4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5860/filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def to 16fc05cd-9790-4b41-b13f-11160f9262cc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def.165eaf04a2f73e11], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def.165eaf04a757a577], Reason = [Created], Message = [Created container filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def.165eaf04af661556], Reason = [Started], Message = [Started container filler-pod-27473c64-ec63-4a2a-ab82-d52180af9def]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851.165eaf046f2888b7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5860/filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851 to c66ef63f-50cf-427f-af2a-b340450b0b53]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851.165eaf04a228db9d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851.165eaf04a5f5bd0f], Reason = [Created], Message = [Created container filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851.165eaf04aded90bb], Reason = [Started], Message = [Started container filler-pod-d64f9a9d-97b1-45a3-b32e-42f1938ac851]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c.165eaf046e132f52], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5860/filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c to 7624137d-137a-468c-9e03-7a04d16d3fbf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c.165eaf04a24fd8bb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c.165eaf04a6926e83], Reason = [Created], Message = [Created container filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c.165eaf04ae92d9cc], Reason = [Started], Message = [Started container filler-pod-dd183dfc-5a93-465b-a108-e66fbf52750c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.165eaf04e82ebbcc], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.165eaf04e8f788be], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 7624137d-137a-468c-9e03-7a04d16d3fbf
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node c66ef63f-50cf-427f-af2a-b340450b0b53
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 16fc05cd-9790-4b41-b13f-11160f9262cc
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:33.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5860" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":56,"skipped":1040,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:33.291: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:02:33.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:02:35.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514953, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514953, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514953, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747514953, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:02:38.937: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:38.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2575" for this suite.
STEP: Destroying namespace "webhook-2575-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":57,"skipped":1053,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7937
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7937
I0129 11:02:39.086605      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7937, replica count: 2
I0129 11:02:42.137171      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:02:42.137: INFO: Creating new exec pod
Jan 29 11:02:47.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 29 11:02:48.295: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 11:02:48.295: INFO: stdout: ""
Jan 29 11:02:48.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.124 80'
Jan 29 11:02:48.438: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.124 80\nConnection to 10.100.200.124 80 port [tcp/http] succeeded!\n"
Jan 29 11:02:48.438: INFO: stdout: ""
Jan 29 11:02:48.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30650'
Jan 29 11:02:48.582: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30650\nConnection to 30.0.0.11 30650 port [tcp/30650] succeeded!\n"
Jan 29 11:02:48.582: INFO: stdout: ""
Jan 29 11:02:48.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30650'
Jan 29 11:02:48.730: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30650\nConnection to 30.0.0.10 30650 port [tcp/30650] succeeded!\n"
Jan 29 11:02:48.730: INFO: stdout: ""
Jan 29 11:02:48.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30650'
Jan 29 11:02:48.878: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30650\nConnection to 30.0.0.11 30650 port [tcp/30650] succeeded!\n"
Jan 29 11:02:48.878: INFO: stdout: ""
Jan 29 11:02:48.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-7937 exec execpodf5vm8 -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30650'
Jan 29 11:02:49.037: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30650\nConnection to 30.0.0.10 30650 port [tcp/30650] succeeded!\n"
Jan 29 11:02:49.037: INFO: stdout: ""
Jan 29 11:02:49.037: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:49.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7937" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.038 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":58,"skipped":1064,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:49.070: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-1252e8f1-6697-497f-9f0e-afc63ac34bb6
STEP: Creating a pod to test consume configMaps
Jan 29 11:02:49.099: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126" in namespace "projected-5855" to be "Succeeded or Failed"
Jan 29 11:02:49.105: INFO: Pod "pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096004ms
Jan 29 11:02:51.108: INFO: Pod "pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008904437s
STEP: Saw pod success
Jan 29 11:02:51.108: INFO: Pod "pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126" satisfied condition "Succeeded or Failed"
Jan 29 11:02:51.110: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:02:51.131: INFO: Waiting for pod pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126 to disappear
Jan 29 11:02:51.138: INFO: Pod pod-projected-configmaps-6dcf7002-6729-4b9a-87e0-6eba42973126 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:51.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5855" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":59,"skipped":1068,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:51.144: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 29 11:02:51.180: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:02:51.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7268" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":60,"skipped":1079,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:02:51.204: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3806
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-3806
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3806
Jan 29 11:02:51.246: INFO: Found 0 stateful pods, waiting for 1
Jan 29 11:03:01.249: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 29 11:03:01.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:03:01.420: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:03:01.420: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:03:01.420: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:03:01.423: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 11:03:11.426: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:03:11.426: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:03:11.434: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:11.434: INFO: ss-0  c66ef63f-50cf-427f-af2a-b340450b0b53  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  }]
Jan 29 11:03:11.434: INFO: 
Jan 29 11:03:11.434: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 29 11:03:12.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99790984s
Jan 29 11:03:13.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994477771s
Jan 29 11:03:14.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991335881s
Jan 29 11:03:15.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985516441s
Jan 29 11:03:16.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981763641s
Jan 29 11:03:17.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978698173s
Jan 29 11:03:18.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.975395192s
Jan 29 11:03:19.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971235136s
Jan 29 11:03:20.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.977868ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3806
Jan 29 11:03:21.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:03:21.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:03:21.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:03:21.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:03:21.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:03:21.791: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 11:03:21.791: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:03:21.791: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:03:21.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:03:21.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 29 11:03:21.945: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:03:21.946: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:03:21.949: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 29 11:03:31.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:03:31.952: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:03:31.952: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 29 11:03:31.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:03:32.102: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:03:32.102: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:03:32.102: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:03:32.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:03:32.276: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:03:32.276: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:03:32.276: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:03:32.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3806 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:03:32.443: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:03:32.443: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:03:32.443: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:03:32.443: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:03:32.446: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 29 11:03:42.451: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:03:42.451: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:03:42.451: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:03:42.461: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:42.461: INFO: ss-0  c66ef63f-50cf-427f-af2a-b340450b0b53  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  }]
Jan 29 11:03:42.461: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:42.461: INFO: ss-2  c66ef63f-50cf-427f-af2a-b340450b0b53  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:42.461: INFO: 
Jan 29 11:03:42.461: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 11:03:43.465: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:43.465: INFO: ss-0  c66ef63f-50cf-427f-af2a-b340450b0b53  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  }]
Jan 29 11:03:43.465: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:43.465: INFO: ss-2  c66ef63f-50cf-427f-af2a-b340450b0b53  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:43.465: INFO: 
Jan 29 11:03:43.465: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 11:03:44.468: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:44.468: INFO: ss-0  c66ef63f-50cf-427f-af2a-b340450b0b53  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:02:51 +0000 UTC  }]
Jan 29 11:03:44.468: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:44.468: INFO: ss-2  c66ef63f-50cf-427f-af2a-b340450b0b53  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:44.468: INFO: 
Jan 29 11:03:44.468: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 29 11:03:45.471: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:45.471: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:45.471: INFO: 
Jan 29 11:03:45.471: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 11:03:46.475: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:46.475: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:46.475: INFO: 
Jan 29 11:03:46.475: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 11:03:47.478: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:47.478: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:47.478: INFO: 
Jan 29 11:03:47.478: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 11:03:48.481: INFO: POD   NODE                                  PHASE    GRACE  CONDITIONS
Jan 29 11:03:48.481: INFO: ss-1  7624137d-137a-468c-9e03-7a04d16d3fbf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-01-29 11:03:11 +0000 UTC  }]
Jan 29 11:03:48.481: INFO: 
Jan 29 11:03:48.481: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 29 11:03:49.484: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975906579s
Jan 29 11:03:50.487: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.973111792s
Jan 29 11:03:51.489: INFO: Verifying statefulset ss doesn't scale past 0 for another 970.284571ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3806
Jan 29 11:03:52.492: INFO: Scaling statefulset ss to 0
Jan 29 11:03:52.498: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 11:03:52.499: INFO: Deleting all statefulset in ns statefulset-3806
Jan 29 11:03:52.501: INFO: Scaling statefulset ss to 0
Jan 29 11:03:52.515: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:03:52.517: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:03:52.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3806" for this suite.

• [SLOW TEST:61.332 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":61,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:03:52.537: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:03:54.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2005" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":1122,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:03:54.581: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:03:55.166: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:03:57.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515035, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515035, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515035, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515035, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:04:00.180: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:00.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-716" for this suite.
STEP: Destroying namespace "webhook-716-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.820 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":63,"skipped":1127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:00.403: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:04:00.437: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 11:04:03.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3789 --namespace=crd-publish-openapi-3789 create -f -'
Jan 29 11:04:04.589: INFO: stderr: ""
Jan 29 11:04:04.589: INFO: stdout: "e2e-test-crd-publish-openapi-5290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 11:04:04.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3789 --namespace=crd-publish-openapi-3789 delete e2e-test-crd-publish-openapi-5290-crds test-cr'
Jan 29 11:04:04.659: INFO: stderr: ""
Jan 29 11:04:04.659: INFO: stdout: "e2e-test-crd-publish-openapi-5290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 29 11:04:04.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3789 --namespace=crd-publish-openapi-3789 apply -f -'
Jan 29 11:04:04.864: INFO: stderr: ""
Jan 29 11:04:04.864: INFO: stdout: "e2e-test-crd-publish-openapi-5290-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 29 11:04:04.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3789 --namespace=crd-publish-openapi-3789 delete e2e-test-crd-publish-openapi-5290-crds test-cr'
Jan 29 11:04:04.937: INFO: stderr: ""
Jan 29 11:04:04.937: INFO: stdout: "e2e-test-crd-publish-openapi-5290-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 29 11:04:04.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-3789 explain e2e-test-crd-publish-openapi-5290-crds'
Jan 29 11:04:05.105: INFO: stderr: ""
Jan 29 11:04:05.105: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5290-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:06.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3789" for this suite.

• [SLOW TEST:6.586 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":64,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:06.988: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:04:07.026: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 29 11:04:07.032: INFO: Number of nodes with available pods: 0
Jan 29 11:04:07.032: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 29 11:04:07.052: INFO: Number of nodes with available pods: 0
Jan 29 11:04:07.052: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:08.054: INFO: Number of nodes with available pods: 0
Jan 29 11:04:08.055: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:09.055: INFO: Number of nodes with available pods: 1
Jan 29 11:04:09.055: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 29 11:04:09.075: INFO: Number of nodes with available pods: 0
Jan 29 11:04:09.075: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 29 11:04:09.094: INFO: Number of nodes with available pods: 0
Jan 29 11:04:09.094: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:10.097: INFO: Number of nodes with available pods: 0
Jan 29 11:04:10.097: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:11.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:11.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:12.102: INFO: Number of nodes with available pods: 0
Jan 29 11:04:12.102: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:13.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:13.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:14.097: INFO: Number of nodes with available pods: 0
Jan 29 11:04:14.097: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:15.097: INFO: Number of nodes with available pods: 0
Jan 29 11:04:15.097: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:16.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:16.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:17.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:17.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:18.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:18.097: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:19.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:19.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:20.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:20.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:21.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:21.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:22.097: INFO: Number of nodes with available pods: 0
Jan 29 11:04:22.097: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:23.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:23.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:24.096: INFO: Number of nodes with available pods: 0
Jan 29 11:04:24.096: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:04:25.096: INFO: Number of nodes with available pods: 1
Jan 29 11:04:25.097: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-20, will wait for the garbage collector to delete the pods
Jan 29 11:04:25.155: INFO: Deleting DaemonSet.extensions daemon-set took: 3.655086ms
Jan 29 11:04:25.655: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.12746ms
Jan 29 11:04:29.157: INFO: Number of nodes with available pods: 0
Jan 29 11:04:29.157: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 11:04:29.158: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-20/daemonsets","resourceVersion":"9851"},"items":null}

Jan 29 11:04:29.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-20/pods","resourceVersion":"9851"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:29.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-20" for this suite.

• [SLOW TEST:22.188 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":65,"skipped":1209,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:29.178: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-3ae31761-09cc-42fb-88c4-d9911410b02d
STEP: Creating secret with name s-test-opt-upd-d352bf6a-fa5c-4378-95e5-1518d9f9e0af
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3ae31761-09cc-42fb-88c4-d9911410b02d
STEP: Updating secret s-test-opt-upd-d352bf6a-fa5c-4378-95e5-1518d9f9e0af
STEP: Creating secret with name s-test-opt-create-6b11150e-e859-4e4e-93ff-27300a28f856
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9714" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1220,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:33.290: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 29 11:04:33.319: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 11:04:33.329: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 11:04:33.332: INFO: 
Logging pods the apiserver thinks is on node 16fc05cd-9790-4b41-b13f-11160f9262cc before test
Jan 29 11:04:33.341: INFO: coredns-664d7f64c5-lwq6l from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:04:33.341: INFO: event-controller-85d6bb4d4c-qrl9c from pks-system started at 2021-01-29 10:32:14 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container event-controller ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:04:33.341: INFO: fluent-bit-82lt4 from pks-system started at 2021-01-29 10:32:33 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:04:33.341: INFO: node-exporter-hhk27 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:04:33.341: INFO: telegraf-t9hn6 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:04:33.341: INFO: wavefront-proxy-5bf7b49596-6vwv9 from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container wavefront-proxy ready: true, restart count 0
Jan 29 11:04:33.341: INFO: pod-projected-secrets-565d4d12-aad2-4a27-a83d-adbb5d41e9f5 from projected-9714 started at 2021-01-29 11:04:29 +0000 UTC (3 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container creates-volume-test ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 	Container dels-volume-test ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 	Container upds-volume-test ready: true, restart count 0
Jan 29 11:04:33.341: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.341: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:04:33.341: INFO: 
Logging pods the apiserver thinks is on node 7624137d-137a-468c-9e03-7a04d16d3fbf before test
Jan 29 11:04:33.349: INFO: coredns-664d7f64c5-8bv7k from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.349: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:04:33.349: INFO: fluent-bit-r4pxl from pks-system started at 2021-01-29 10:32:28 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.349: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:04:33.349: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:04:33.349: INFO: metric-controller-7f5cb8ff6d-cm2b2 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container metric-controller ready: true, restart count 0
Jan 29 11:04:33.350: INFO: node-exporter-kjb4q from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:04:33.350: INFO: observability-manager-6cf797f97-wshfj from pks-system started at 2021-01-29 10:32:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container observability-manager ready: true, restart count 0
Jan 29 11:04:33.350: INFO: telegraf-459jf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:04:33.350: INFO: telemetry-agent-8f56d9865-rt7qc from pks-system started at 2021-01-29 10:36:57 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jan 29 11:04:33.350: INFO: validator-69f557d7c6-hz8vq from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container validator ready: true, restart count 0
Jan 29 11:04:33.350: INFO: sonobuoy-e2e-job-a3be688aee3e44de from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container e2e ready: true, restart count 0
Jan 29 11:04:33.350: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:04:33.350: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.350: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:04:33.350: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:04:33.350: INFO: 
Logging pods the apiserver thinks is on node c66ef63f-50cf-427f-af2a-b340450b0b53 before test
Jan 29 11:04:33.358: INFO: coredns-664d7f64c5-nqvrn from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:04:33.358: INFO: metrics-server-7d476fdfbd-n4b2n from kube-system started at 2021-01-29 10:32:08 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 11:04:33.358: INFO: cert-generator-e484f2435fc830429fc9747bd002fcda56dd053e-4lc5d from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container cert-generator ready: false, restart count 0
Jan 29 11:04:33.358: INFO: fluent-bit-xbbgf from pks-system started at 2021-01-29 10:32:16 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:04:33.358: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:04:33.358: INFO: node-exporter-bx9js from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:04:33.358: INFO: sink-controller-f6bc7f774-wn8vp from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container sink-controller ready: true, restart count 0
Jan 29 11:04:33.358: INFO: telegraf-w42nf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:04:33.358: INFO: wavefront-collector-7648f897c7-sb8kc from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container wavefront-collector ready: true, restart count 0
Jan 29 11:04:33.358: INFO: sonobuoy from sonobuoy started at 2021-01-29 10:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 11:04:33.358: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:04:33.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:04:33.358: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.165eaf211bb44ded], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.165eaf211c887a46], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4951" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":67,"skipped":1228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:34.386: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-52652c0d-022f-4388-9b94-5787d35cfc51
STEP: Creating a pod to test consume secrets
Jan 29 11:04:34.424: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e" in namespace "projected-6706" to be "Succeeded or Failed"
Jan 29 11:04:34.426: INFO: Pod "pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612399ms
Jan 29 11:04:36.429: INFO: Pod "pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005425659s
STEP: Saw pod success
Jan 29 11:04:36.429: INFO: Pod "pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e" satisfied condition "Succeeded or Failed"
Jan 29 11:04:36.431: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:04:36.447: INFO: Waiting for pod pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e to disappear
Jan 29 11:04:36.453: INFO: Pod pod-projected-secrets-1fb94d0f-0969-484e-971c-14e13da07b9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:36.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6706" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1260,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:36.459: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 11:04:40.521: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 11:04:40.523: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 11:04:42.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 11:04:42.526: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 11:04:44.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 11:04:44.526: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 11:04:46.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 11:04:46.527: INFO: Pod pod-with-prestop-http-hook still exists
Jan 29 11:04:48.523: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 29 11:04:48.526: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:04:48.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2288" for this suite.

• [SLOW TEST:12.078 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1263,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:04:48.538: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4673 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4673;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4673 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4673;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4673.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4673.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4673.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4673.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4673.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4673.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 56.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.56_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4673 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4673;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4673 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4673;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4673.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4673.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4673.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4673.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4673.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4673.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4673.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4673.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4673.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 56.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.56_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:04:58.621: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.624: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.626: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.628: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.634: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.636: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.650: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.653: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.656: INFO: Unable to read jessie_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.658: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.663: INFO: Unable to read jessie_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.667: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.669: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:04:58.682: INFO: Lookups using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4673 wheezy_tcp@dns-test-service.dns-4673 wheezy_udp@dns-test-service.dns-4673.svc wheezy_tcp@dns-test-service.dns-4673.svc wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4673 jessie_tcp@dns-test-service.dns-4673 jessie_udp@dns-test-service.dns-4673.svc jessie_tcp@dns-test-service.dns-4673.svc jessie_udp@_http._tcp.dns-test-service.dns-4673.svc jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc]

Jan 29 11:05:03.686: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.689: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.691: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.693: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.695: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.697: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.699: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.701: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.713: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.715: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.717: INFO: Unable to read jessie_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.720: INFO: Unable to read jessie_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.722: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.724: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.726: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:03.736: INFO: Lookups using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4673 wheezy_tcp@dns-test-service.dns-4673 wheezy_udp@dns-test-service.dns-4673.svc wheezy_tcp@dns-test-service.dns-4673.svc wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4673 jessie_tcp@dns-test-service.dns-4673 jessie_udp@dns-test-service.dns-4673.svc jessie_tcp@dns-test-service.dns-4673.svc jessie_udp@_http._tcp.dns-test-service.dns-4673.svc jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc]

Jan 29 11:05:08.687: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.690: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.696: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.698: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.700: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.702: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.704: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.722: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.726: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.728: INFO: Unable to read jessie_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.732: INFO: Unable to read jessie_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.734: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.737: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.748: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:08.763: INFO: Lookups using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4673 wheezy_tcp@dns-test-service.dns-4673 wheezy_udp@dns-test-service.dns-4673.svc wheezy_tcp@dns-test-service.dns-4673.svc wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4673 jessie_tcp@dns-test-service.dns-4673 jessie_udp@dns-test-service.dns-4673.svc jessie_tcp@dns-test-service.dns-4673.svc jessie_udp@_http._tcp.dns-test-service.dns-4673.svc jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc]

Jan 29 11:05:13.686: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.690: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.692: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.694: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.696: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.698: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.700: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.714: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.716: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.718: INFO: Unable to read jessie_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.723: INFO: Unable to read jessie_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.725: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.727: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.729: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:13.742: INFO: Lookups using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4673 wheezy_tcp@dns-test-service.dns-4673 wheezy_udp@dns-test-service.dns-4673.svc wheezy_tcp@dns-test-service.dns-4673.svc wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4673 jessie_tcp@dns-test-service.dns-4673 jessie_udp@dns-test-service.dns-4673.svc jessie_tcp@dns-test-service.dns-4673.svc jessie_udp@_http._tcp.dns-test-service.dns-4673.svc jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc]

Jan 29 11:05:18.686: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.688: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.690: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.693: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.695: INFO: Unable to read wheezy_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.696: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.698: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.700: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.721: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.733: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.736: INFO: Unable to read jessie_udp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.738: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673 from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.739: INFO: Unable to read jessie_udp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.741: INFO: Unable to read jessie_tcp@dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.744: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.746: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc from pod dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83: the server could not find the requested resource (get pods dns-test-45219375-85a2-472e-a238-90ed96935a83)
Jan 29 11:05:18.757: INFO: Lookups using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4673 wheezy_tcp@dns-test-service.dns-4673 wheezy_udp@dns-test-service.dns-4673.svc wheezy_tcp@dns-test-service.dns-4673.svc wheezy_udp@_http._tcp.dns-test-service.dns-4673.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4673.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4673 jessie_tcp@dns-test-service.dns-4673 jessie_udp@dns-test-service.dns-4673.svc jessie_tcp@dns-test-service.dns-4673.svc jessie_udp@_http._tcp.dns-test-service.dns-4673.svc jessie_tcp@_http._tcp.dns-test-service.dns-4673.svc]

Jan 29 11:05:23.747: INFO: DNS probes using dns-4673/dns-test-45219375-85a2-472e-a238-90ed96935a83 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:23.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4673" for this suite.

• [SLOW TEST:35.294 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":70,"skipped":1275,"failed":0}
SSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:23.834: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:05:23.862: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-75f9e6aa-2b43-4a1e-964c-2d2370f85ebc" in namespace "security-context-test-6559" to be "Succeeded or Failed"
Jan 29 11:05:23.865: INFO: Pod "busybox-privileged-false-75f9e6aa-2b43-4a1e-964c-2d2370f85ebc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909382ms
Jan 29 11:05:25.868: INFO: Pod "busybox-privileged-false-75f9e6aa-2b43-4a1e-964c-2d2370f85ebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00554393s
Jan 29 11:05:25.868: INFO: Pod "busybox-privileged-false-75f9e6aa-2b43-4a1e-964c-2d2370f85ebc" satisfied condition "Succeeded or Failed"
Jan 29 11:05:25.873: INFO: Got logs for pod "busybox-privileged-false-75f9e6aa-2b43-4a1e-964c-2d2370f85ebc": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:25.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6559" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1281,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:25.881: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jan 29 11:05:25.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4639 api-versions'
Jan 29 11:05:26.000: INFO: stderr: ""
Jan 29 11:05:26.000: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npksapi.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:26.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4639" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":72,"skipped":1281,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:26.006: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:05:26.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd" in namespace "downward-api-5345" to be "Succeeded or Failed"
Jan 29 11:05:26.043: INFO: Pod "downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.330241ms
Jan 29 11:05:28.045: INFO: Pod "downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005855174s
STEP: Saw pod success
Jan 29 11:05:28.045: INFO: Pod "downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd" satisfied condition "Succeeded or Failed"
Jan 29 11:05:28.047: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd container client-container: <nil>
STEP: delete the pod
Jan 29 11:05:28.067: INFO: Waiting for pod downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd to disappear
Jan 29 11:05:28.074: INFO: Pod downwardapi-volume-f30e892a-3b51-4de6-872b-46bbb1e9d3cd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5345" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":73,"skipped":1296,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:28.080: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:05:28.113: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-0a8608b9-e0cf-4816-8d0b-a1cd1c226545" in namespace "security-context-test-2799" to be "Succeeded or Failed"
Jan 29 11:05:28.116: INFO: Pod "busybox-readonly-false-0a8608b9-e0cf-4816-8d0b-a1cd1c226545": Phase="Pending", Reason="", readiness=false. Elapsed: 3.552611ms
Jan 29 11:05:30.119: INFO: Pod "busybox-readonly-false-0a8608b9-e0cf-4816-8d0b-a1cd1c226545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00636206s
Jan 29 11:05:30.119: INFO: Pod "busybox-readonly-false-0a8608b9-e0cf-4816-8d0b-a1cd1c226545" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:30.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2799" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":74,"skipped":1298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:30.128: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3084
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3084
STEP: Deleting pre-stop pod
Jan 29 11:05:39.194: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:05:39.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3084" for this suite.

• [SLOW TEST:9.082 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":75,"skipped":1329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:05:39.211: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-hx8x
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 11:05:39.265: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hx8x" in namespace "subpath-162" to be "Succeeded or Failed"
Jan 29 11:05:39.275: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Pending", Reason="", readiness=false. Elapsed: 9.658775ms
Jan 29 11:05:41.278: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 2.013335748s
Jan 29 11:05:43.281: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 4.016086006s
Jan 29 11:05:45.284: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 6.019341289s
Jan 29 11:05:47.287: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 8.021781486s
Jan 29 11:05:49.290: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 10.024766499s
Jan 29 11:05:51.293: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 12.027775334s
Jan 29 11:05:53.296: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 14.030397825s
Jan 29 11:05:55.299: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 16.03373966s
Jan 29 11:05:57.301: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 18.035921247s
Jan 29 11:05:59.304: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Running", Reason="", readiness=true. Elapsed: 20.039004152s
Jan 29 11:06:01.311: INFO: Pod "pod-subpath-test-secret-hx8x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045532896s
STEP: Saw pod success
Jan 29 11:06:01.311: INFO: Pod "pod-subpath-test-secret-hx8x" satisfied condition "Succeeded or Failed"
Jan 29 11:06:01.313: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-subpath-test-secret-hx8x container test-container-subpath-secret-hx8x: <nil>
STEP: delete the pod
Jan 29 11:06:01.326: INFO: Waiting for pod pod-subpath-test-secret-hx8x to disappear
Jan 29 11:06:01.332: INFO: Pod pod-subpath-test-secret-hx8x no longer exists
STEP: Deleting pod pod-subpath-test-secret-hx8x
Jan 29 11:06:01.333: INFO: Deleting pod "pod-subpath-test-secret-hx8x" in namespace "subpath-162"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-162" for this suite.

• [SLOW TEST:22.129 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":76,"skipped":1353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:01.340: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 29 11:06:05.882: INFO: Successfully updated pod "adopt-release-r5s8w"
STEP: Checking that the Job readopts the Pod
Jan 29 11:06:05.882: INFO: Waiting up to 15m0s for pod "adopt-release-r5s8w" in namespace "job-9589" to be "adopted"
Jan 29 11:06:05.885: INFO: Pod "adopt-release-r5s8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.987498ms
Jan 29 11:06:07.888: INFO: Pod "adopt-release-r5s8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.006324138s
Jan 29 11:06:07.889: INFO: Pod "adopt-release-r5s8w" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 29 11:06:08.398: INFO: Successfully updated pod "adopt-release-r5s8w"
STEP: Checking that the Job releases the Pod
Jan 29 11:06:08.398: INFO: Waiting up to 15m0s for pod "adopt-release-r5s8w" in namespace "job-9589" to be "released"
Jan 29 11:06:08.401: INFO: Pod "adopt-release-r5s8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.261941ms
Jan 29 11:06:10.403: INFO: Pod "adopt-release-r5s8w": Phase="Running", Reason="", readiness=true. Elapsed: 2.004938194s
Jan 29 11:06:10.403: INFO: Pod "adopt-release-r5s8w" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:10.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9589" for this suite.

• [SLOW TEST:9.070 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":77,"skipped":1385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:10.412: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 29 11:06:10.439: INFO: Waiting up to 5m0s for pod "pod-29369770-0cff-41a4-924d-9651ef173af5" in namespace "emptydir-7326" to be "Succeeded or Failed"
Jan 29 11:06:10.442: INFO: Pod "pod-29369770-0cff-41a4-924d-9651ef173af5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.898198ms
Jan 29 11:06:12.444: INFO: Pod "pod-29369770-0cff-41a4-924d-9651ef173af5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005460288s
STEP: Saw pod success
Jan 29 11:06:12.445: INFO: Pod "pod-29369770-0cff-41a4-924d-9651ef173af5" satisfied condition "Succeeded or Failed"
Jan 29 11:06:12.446: INFO: Trying to get logs from node 16fc05cd-9790-4b41-b13f-11160f9262cc pod pod-29369770-0cff-41a4-924d-9651ef173af5 container test-container: <nil>
STEP: delete the pod
Jan 29 11:06:12.466: INFO: Waiting for pod pod-29369770-0cff-41a4-924d-9651ef173af5 to disappear
Jan 29 11:06:12.472: INFO: Pod pod-29369770-0cff-41a4-924d-9651ef173af5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7326" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1408,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:12.478: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-q229m in namespace proxy-2184
I0129 11:06:12.520170      20 runners.go:190] Created replication controller with name: proxy-service-q229m, namespace: proxy-2184, replica count: 1
I0129 11:06:13.570414      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 11:06:14.570597      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 11:06:15.570720      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 11:06:16.570883      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 11:06:17.571016      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0129 11:06:18.571148      20 runners.go:190] proxy-service-q229m Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:06:18.573: INFO: setup took 6.076383821s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 29 11:06:18.580: INFO: (0) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 7.192696ms)
Jan 29 11:06:18.584: INFO: (0) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 11.260042ms)
Jan 29 11:06:18.584: INFO: (0) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.205233ms)
Jan 29 11:06:18.587: INFO: (0) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 13.206055ms)
Jan 29 11:06:18.587: INFO: (0) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 13.453435ms)
Jan 29 11:06:18.587: INFO: (0) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 13.342324ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 16.883843ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 17.162723ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 16.973575ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 17.132357ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 17.107634ms)
Jan 29 11:06:18.590: INFO: (0) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 17.114004ms)
Jan 29 11:06:18.591: INFO: (0) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 17.253121ms)
Jan 29 11:06:18.592: INFO: (0) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 18.29276ms)
Jan 29 11:06:18.592: INFO: (0) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 18.555957ms)
Jan 29 11:06:18.592: INFO: (0) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 18.576926ms)
Jan 29 11:06:18.603: INFO: (1) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 10.846723ms)
Jan 29 11:06:18.603: INFO: (1) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.197268ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 11.212839ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 11.439731ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 11.520193ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 11.369197ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 12.039686ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 11.851307ms)
Jan 29 11:06:18.604: INFO: (1) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 11.794482ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 15.096994ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 15.179017ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 15.003773ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 14.969429ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 15.31805ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 15.170519ms)
Jan 29 11:06:18.607: INFO: (1) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 15.218118ms)
Jan 29 11:06:18.616: INFO: (2) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 7.748518ms)
Jan 29 11:06:18.616: INFO: (2) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 7.932408ms)
Jan 29 11:06:18.616: INFO: (2) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 8.052271ms)
Jan 29 11:06:18.616: INFO: (2) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 8.388439ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 11.705919ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 12.41344ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 11.803883ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 11.833604ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.861328ms)
Jan 29 11:06:18.620: INFO: (2) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 11.810545ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 14.463213ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 14.427804ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 14.012986ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 14.197332ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 14.634092ms)
Jan 29 11:06:18.622: INFO: (2) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 14.699643ms)
Jan 29 11:06:18.637: INFO: (3) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 14.047466ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 16.773259ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 17.262072ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 16.736389ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 16.713055ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 17.139031ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 17.40144ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 17.46541ms)
Jan 29 11:06:18.640: INFO: (3) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 16.737725ms)
Jan 29 11:06:18.642: INFO: (3) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 19.026679ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 19.990334ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 19.988518ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 19.882896ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 20.279208ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 19.958565ms)
Jan 29 11:06:18.643: INFO: (3) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 20.199054ms)
Jan 29 11:06:18.648: INFO: (4) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 5.086289ms)
Jan 29 11:06:18.653: INFO: (4) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 9.326824ms)
Jan 29 11:06:18.653: INFO: (4) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 9.317702ms)
Jan 29 11:06:18.653: INFO: (4) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 9.742308ms)
Jan 29 11:06:18.653: INFO: (4) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 9.298449ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 10.219748ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 10.186089ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 10.173191ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 10.888559ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 10.848179ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.05652ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 10.795044ms)
Jan 29 11:06:18.654: INFO: (4) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.980063ms)
Jan 29 11:06:18.656: INFO: (4) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 11.941273ms)
Jan 29 11:06:18.657: INFO: (4) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 13.108329ms)
Jan 29 11:06:18.657: INFO: (4) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 13.110938ms)
Jan 29 11:06:18.665: INFO: (5) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 8.379743ms)
Jan 29 11:06:18.665: INFO: (5) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.146336ms)
Jan 29 11:06:18.665: INFO: (5) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 7.986928ms)
Jan 29 11:06:18.666: INFO: (5) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 8.370916ms)
Jan 29 11:06:18.666: INFO: (5) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 8.898767ms)
Jan 29 11:06:18.666: INFO: (5) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.876154ms)
Jan 29 11:06:18.667: INFO: (5) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 9.042461ms)
Jan 29 11:06:18.667: INFO: (5) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 9.693395ms)
Jan 29 11:06:18.667: INFO: (5) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 9.66099ms)
Jan 29 11:06:18.667: INFO: (5) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 9.79704ms)
Jan 29 11:06:18.668: INFO: (5) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 9.937023ms)
Jan 29 11:06:18.668: INFO: (5) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.305609ms)
Jan 29 11:06:18.668: INFO: (5) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 11.094651ms)
Jan 29 11:06:18.668: INFO: (5) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.025209ms)
Jan 29 11:06:18.668: INFO: (5) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 11.348843ms)
Jan 29 11:06:18.669: INFO: (5) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 11.245201ms)
Jan 29 11:06:18.674: INFO: (6) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 5.104974ms)
Jan 29 11:06:18.674: INFO: (6) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.144998ms)
Jan 29 11:06:18.674: INFO: (6) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.143898ms)
Jan 29 11:06:18.677: INFO: (6) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 7.742006ms)
Jan 29 11:06:18.677: INFO: (6) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 7.804094ms)
Jan 29 11:06:18.678: INFO: (6) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 9.505785ms)
Jan 29 11:06:18.679: INFO: (6) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 9.884013ms)
Jan 29 11:06:18.679: INFO: (6) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 10.304438ms)
Jan 29 11:06:18.679: INFO: (6) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 10.145435ms)
Jan 29 11:06:18.679: INFO: (6) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 10.418939ms)
Jan 29 11:06:18.680: INFO: (6) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 11.216897ms)
Jan 29 11:06:18.680: INFO: (6) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 11.367212ms)
Jan 29 11:06:18.680: INFO: (6) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 11.44028ms)
Jan 29 11:06:18.680: INFO: (6) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.613061ms)
Jan 29 11:06:18.680: INFO: (6) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 11.439759ms)
Jan 29 11:06:18.681: INFO: (6) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.655201ms)
Jan 29 11:06:18.685: INFO: (7) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 4.137706ms)
Jan 29 11:06:18.685: INFO: (7) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 4.130285ms)
Jan 29 11:06:18.686: INFO: (7) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 5.432489ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.880903ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 5.905431ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 6.145481ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 6.2832ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 6.585346ms)
Jan 29 11:06:18.687: INFO: (7) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 6.491142ms)
Jan 29 11:06:18.688: INFO: (7) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 6.660356ms)
Jan 29 11:06:18.688: INFO: (7) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 7.009314ms)
Jan 29 11:06:18.688: INFO: (7) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 7.249139ms)
Jan 29 11:06:18.690: INFO: (7) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 8.959589ms)
Jan 29 11:06:18.690: INFO: (7) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 9.028994ms)
Jan 29 11:06:18.690: INFO: (7) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 9.551636ms)
Jan 29 11:06:18.691: INFO: (7) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 9.970738ms)
Jan 29 11:06:18.697: INFO: (8) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 6.236788ms)
Jan 29 11:06:18.698: INFO: (8) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 7.240806ms)
Jan 29 11:06:18.698: INFO: (8) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 7.338305ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 9.552266ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 9.922706ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 9.709323ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 9.964115ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 10.087829ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 10.503898ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 10.493548ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 10.493566ms)
Jan 29 11:06:18.701: INFO: (8) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 10.488963ms)
Jan 29 11:06:18.702: INFO: (8) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 10.638355ms)
Jan 29 11:06:18.702: INFO: (8) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 10.496518ms)
Jan 29 11:06:18.702: INFO: (8) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.624922ms)
Jan 29 11:06:18.702: INFO: (8) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 10.631262ms)
Jan 29 11:06:18.706: INFO: (9) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 4.275054ms)
Jan 29 11:06:18.706: INFO: (9) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 4.520162ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 8.552979ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.472395ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 8.726739ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 8.891462ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 8.835777ms)
Jan 29 11:06:18.711: INFO: (9) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 9.042601ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 9.485459ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 9.649434ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 9.684495ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 9.752126ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 9.855456ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 10.144339ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 10.054724ms)
Jan 29 11:06:18.712: INFO: (9) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 10.342391ms)
Jan 29 11:06:18.715: INFO: (10) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 3.112741ms)
Jan 29 11:06:18.716: INFO: (10) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 3.523948ms)
Jan 29 11:06:18.716: INFO: (10) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 4.041853ms)
Jan 29 11:06:18.719: INFO: (10) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 7.077011ms)
Jan 29 11:06:18.720: INFO: (10) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 7.032495ms)
Jan 29 11:06:18.721: INFO: (10) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 8.78068ms)
Jan 29 11:06:18.721: INFO: (10) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.844754ms)
Jan 29 11:06:18.721: INFO: (10) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.890711ms)
Jan 29 11:06:18.721: INFO: (10) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 9.166981ms)
Jan 29 11:06:18.722: INFO: (10) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 9.235552ms)
Jan 29 11:06:18.722: INFO: (10) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 9.282155ms)
Jan 29 11:06:18.726: INFO: (10) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 13.150543ms)
Jan 29 11:06:18.726: INFO: (10) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 13.832014ms)
Jan 29 11:06:18.726: INFO: (10) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 13.952744ms)
Jan 29 11:06:18.726: INFO: (10) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 13.978602ms)
Jan 29 11:06:18.727: INFO: (10) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 13.951194ms)
Jan 29 11:06:18.733: INFO: (11) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 6.034313ms)
Jan 29 11:06:18.733: INFO: (11) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 6.13673ms)
Jan 29 11:06:18.733: INFO: (11) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 6.346161ms)
Jan 29 11:06:18.738: INFO: (11) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 10.635359ms)
Jan 29 11:06:18.739: INFO: (11) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 12.205695ms)
Jan 29 11:06:18.739: INFO: (11) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 12.6457ms)
Jan 29 11:06:18.739: INFO: (11) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 12.498186ms)
Jan 29 11:06:18.740: INFO: (11) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 12.534599ms)
Jan 29 11:06:18.740: INFO: (11) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 13.458039ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 13.394528ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 13.807517ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 13.800123ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 14.255105ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 14.057913ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 14.245337ms)
Jan 29 11:06:18.741: INFO: (11) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 14.592872ms)
Jan 29 11:06:18.747: INFO: (12) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.719088ms)
Jan 29 11:06:18.747: INFO: (12) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 5.862434ms)
Jan 29 11:06:18.747: INFO: (12) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.852232ms)
Jan 29 11:06:18.748: INFO: (12) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 6.151277ms)
Jan 29 11:06:18.748: INFO: (12) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 6.177051ms)
Jan 29 11:06:18.749: INFO: (12) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 7.196581ms)
Jan 29 11:06:18.749: INFO: (12) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 7.275096ms)
Jan 29 11:06:18.749: INFO: (12) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 7.555523ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 11.797412ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.965565ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.958421ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 12.253438ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 12.036571ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 12.135238ms)
Jan 29 11:06:18.754: INFO: (12) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 12.29872ms)
Jan 29 11:06:18.756: INFO: (12) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 13.419344ms)
Jan 29 11:06:18.758: INFO: (13) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 2.33234ms)
Jan 29 11:06:18.759: INFO: (13) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 3.063952ms)
Jan 29 11:06:18.761: INFO: (13) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 5.106099ms)
Jan 29 11:06:18.761: INFO: (13) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.190945ms)
Jan 29 11:06:18.762: INFO: (13) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.62927ms)
Jan 29 11:06:18.764: INFO: (13) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 7.783625ms)
Jan 29 11:06:18.764: INFO: (13) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 7.815235ms)
Jan 29 11:06:18.764: INFO: (13) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 8.083602ms)
Jan 29 11:06:18.764: INFO: (13) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 8.39148ms)
Jan 29 11:06:18.765: INFO: (13) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.40184ms)
Jan 29 11:06:18.765: INFO: (13) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 8.592792ms)
Jan 29 11:06:18.767: INFO: (13) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.738774ms)
Jan 29 11:06:18.767: INFO: (13) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 11.482583ms)
Jan 29 11:06:18.768: INFO: (13) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 11.260062ms)
Jan 29 11:06:18.768: INFO: (13) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 11.364955ms)
Jan 29 11:06:18.768: INFO: (13) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.530642ms)
Jan 29 11:06:18.771: INFO: (14) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 2.781008ms)
Jan 29 11:06:18.771: INFO: (14) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 3.342553ms)
Jan 29 11:06:18.772: INFO: (14) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 3.926926ms)
Jan 29 11:06:18.772: INFO: (14) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 4.176004ms)
Jan 29 11:06:18.778: INFO: (14) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 9.537101ms)
Jan 29 11:06:18.778: INFO: (14) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 9.568542ms)
Jan 29 11:06:18.778: INFO: (14) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 9.561345ms)
Jan 29 11:06:18.779: INFO: (14) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 10.480942ms)
Jan 29 11:06:18.779: INFO: (14) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.853498ms)
Jan 29 11:06:18.779: INFO: (14) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 10.830189ms)
Jan 29 11:06:18.779: INFO: (14) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 11.104549ms)
Jan 29 11:06:18.780: INFO: (14) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 11.332009ms)
Jan 29 11:06:18.779: INFO: (14) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 10.97746ms)
Jan 29 11:06:18.780: INFO: (14) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 11.44067ms)
Jan 29 11:06:18.780: INFO: (14) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 11.03068ms)
Jan 29 11:06:18.780: INFO: (14) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 11.189213ms)
Jan 29 11:06:18.791: INFO: (15) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 6.3328ms)
Jan 29 11:06:18.791: INFO: (15) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 6.42487ms)
Jan 29 11:06:18.791: INFO: (15) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 6.524164ms)
Jan 29 11:06:18.794: INFO: (15) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 8.962939ms)
Jan 29 11:06:18.794: INFO: (15) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 9.184288ms)
Jan 29 11:06:18.794: INFO: (15) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 9.441765ms)
Jan 29 11:06:18.794: INFO: (15) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 9.57474ms)
Jan 29 11:06:18.794: INFO: (15) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 10.013683ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 10.241565ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 9.968445ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 10.180239ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 10.338306ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 10.177937ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 10.563855ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.396046ms)
Jan 29 11:06:18.795: INFO: (15) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 10.560864ms)
Jan 29 11:06:18.798: INFO: (16) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 2.380508ms)
Jan 29 11:06:18.798: INFO: (16) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 3.09458ms)
Jan 29 11:06:18.800: INFO: (16) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 5.22134ms)
Jan 29 11:06:18.801: INFO: (16) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 5.648814ms)
Jan 29 11:06:18.807: INFO: (16) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 11.733873ms)
Jan 29 11:06:18.807: INFO: (16) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 11.941219ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 12.120272ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 11.913604ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 11.683401ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 11.94234ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 12.167361ms)
Jan 29 11:06:18.808: INFO: (16) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 12.210239ms)
Jan 29 11:06:18.814: INFO: (16) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 18.436415ms)
Jan 29 11:06:18.815: INFO: (16) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 19.443177ms)
Jan 29 11:06:18.815: INFO: (16) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 20.11623ms)
Jan 29 11:06:18.817: INFO: (16) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 21.461281ms)
Jan 29 11:06:18.826: INFO: (17) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.717021ms)
Jan 29 11:06:18.826: INFO: (17) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 8.526385ms)
Jan 29 11:06:18.826: INFO: (17) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 8.451217ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 9.009215ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 9.109395ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 8.919585ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 9.391726ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 9.374846ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 9.764325ms)
Jan 29 11:06:18.827: INFO: (17) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 9.099992ms)
Jan 29 11:06:18.829: INFO: (17) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 11.102883ms)
Jan 29 11:06:18.829: INFO: (17) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 11.660753ms)
Jan 29 11:06:18.830: INFO: (17) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 11.699124ms)
Jan 29 11:06:18.830: INFO: (17) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 12.325512ms)
Jan 29 11:06:18.830: INFO: (17) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 12.232729ms)
Jan 29 11:06:18.830: INFO: (17) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 12.654629ms)
Jan 29 11:06:18.833: INFO: (18) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 2.811021ms)
Jan 29 11:06:18.834: INFO: (18) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 4.071285ms)
Jan 29 11:06:18.834: INFO: (18) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 3.982114ms)
Jan 29 11:06:18.835: INFO: (18) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 4.701481ms)
Jan 29 11:06:18.838: INFO: (18) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 7.467622ms)
Jan 29 11:06:18.838: INFO: (18) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 7.163454ms)
Jan 29 11:06:18.839: INFO: (18) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 8.003524ms)
Jan 29 11:06:18.839: INFO: (18) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 8.071218ms)
Jan 29 11:06:18.839: INFO: (18) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 8.795885ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 9.053491ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 9.043355ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 9.47696ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 9.464609ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 9.635122ms)
Jan 29 11:06:18.840: INFO: (18) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 10.04304ms)
Jan 29 11:06:18.841: INFO: (18) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 10.179052ms)
Jan 29 11:06:18.846: INFO: (19) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:460/proxy/: tls baz (200; 5.16336ms)
Jan 29 11:06:18.846: INFO: (19) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:1080/proxy/rewriteme">test<... (200; 5.373737ms)
Jan 29 11:06:18.846: INFO: (19) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:160/proxy/: foo (200; 5.295271ms)
Jan 29 11:06:18.846: INFO: (19) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww:162/proxy/: bar (200; 5.449083ms)
Jan 29 11:06:18.846: INFO: (19) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:1080/proxy/rewriteme">... (200; 5.609865ms)
Jan 29 11:06:18.850: INFO: (19) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname2/proxy/: bar (200; 9.655106ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/services/http:proxy-service-q229m:portname1/proxy/: foo (200; 9.817133ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname1/proxy/: tls baz (200; 9.739305ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/proxy-service-q229m-6k4ww/proxy/rewriteme">test</a> (200; 9.902511ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/: <a href="/api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:443/proxy/tlsrewritem... (200; 10.072854ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname1/proxy/: foo (200; 10.110428ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:162/proxy/: bar (200; 10.418953ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/pods/https:proxy-service-q229m-6k4ww:462/proxy/: tls qux (200; 10.336662ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/services/https:proxy-service-q229m:tlsportname2/proxy/: tls qux (200; 10.47365ms)
Jan 29 11:06:18.851: INFO: (19) /api/v1/namespaces/proxy-2184/pods/http:proxy-service-q229m-6k4ww:160/proxy/: foo (200; 10.544079ms)
Jan 29 11:06:18.852: INFO: (19) /api/v1/namespaces/proxy-2184/services/proxy-service-q229m:portname2/proxy/: bar (200; 10.870317ms)
STEP: deleting ReplicationController proxy-service-q229m in namespace proxy-2184, will wait for the garbage collector to delete the pods
Jan 29 11:06:18.908: INFO: Deleting ReplicationController proxy-service-q229m took: 4.410421ms
Jan 29 11:06:19.408: INFO: Terminating ReplicationController proxy-service-q229m pods took: 500.150232ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:21.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2184" for this suite.

• [SLOW TEST:8.937 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":79,"skipped":1411,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:21.415: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jan 29 11:06:21.445: INFO: Waiting up to 5m0s for pod "downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e" in namespace "downward-api-7730" to be "Succeeded or Failed"
Jan 29 11:06:21.447: INFO: Pod "downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548146ms
Jan 29 11:06:23.449: INFO: Pod "downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004130544s
STEP: Saw pod success
Jan 29 11:06:23.449: INFO: Pod "downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e" satisfied condition "Succeeded or Failed"
Jan 29 11:06:23.451: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e container dapi-container: <nil>
STEP: delete the pod
Jan 29 11:06:23.466: INFO: Waiting for pod downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e to disappear
Jan 29 11:06:23.473: INFO: Pod downward-api-3bc6d6b9-d5e4-4f32-9ad7-184f6af9de0e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:23.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7730" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1435,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:06:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:24.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-983" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":81,"skipped":1440,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:24.052: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:06:24.691: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:06:26.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515184, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515184, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515184, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515184, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:06:29.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:29.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1085" for this suite.
STEP: Destroying namespace "webhook-1085-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.921 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":82,"skipped":1449,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:29.972: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-fcd8a5f7-2a8d-4637-bcbe-d0faa2958b2f
STEP: Creating a pod to test consume configMaps
Jan 29 11:06:30.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83" in namespace "projected-348" to be "Succeeded or Failed"
Jan 29 11:06:30.022: INFO: Pod "pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83": Phase="Pending", Reason="", readiness=false. Elapsed: 11.412303ms
Jan 29 11:06:32.024: INFO: Pod "pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014180437s
STEP: Saw pod success
Jan 29 11:06:32.024: INFO: Pod "pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83" satisfied condition "Succeeded or Failed"
Jan 29 11:06:32.026: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:06:32.040: INFO: Waiting for pod pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83 to disappear
Jan 29 11:06:32.046: INFO: Pod pod-projected-configmaps-acdc3b8c-30fc-4cb3-bdbb-f0507241db83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:32.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-348" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1449,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:32.052: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6153
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 11:06:32.072: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 11:06:32.135: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:06:34.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:36.138: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:38.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:40.140: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:42.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:44.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:46.137: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:06:48.138: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 29 11:06:48.142: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 29 11:06:48.145: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 29 11:06:52.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.50.30:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6153 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:06:52.174: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:06:52.261: INFO: Found all expected endpoints: [netserver-0]
Jan 29 11:06:52.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.95.30:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6153 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:06:52.263: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:06:52.350: INFO: Found all expected endpoints: [netserver-1]
Jan 29 11:06:52.353: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.24.81:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6153 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:06:52.353: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:06:52.439: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:06:52.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6153" for this suite.

• [SLOW TEST:20.394 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":84,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:06:52.447: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jan 29 11:06:52.471: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-986" for this suite.

• [SLOW TEST:16.547 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":85,"skipped":1480,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:08.993: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:26.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2976" for this suite.

• [SLOW TEST:17.057 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":86,"skipped":1480,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:26.050: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 11:07:26.077: INFO: Waiting up to 5m0s for pod "pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671" in namespace "emptydir-1804" to be "Succeeded or Failed"
Jan 29 11:07:26.079: INFO: Pod "pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005052ms
Jan 29 11:07:28.082: INFO: Pod "pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005026123s
STEP: Saw pod success
Jan 29 11:07:28.082: INFO: Pod "pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671" satisfied condition "Succeeded or Failed"
Jan 29 11:07:28.084: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671 container test-container: <nil>
STEP: delete the pod
Jan 29 11:07:28.096: INFO: Waiting for pod pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671 to disappear
Jan 29 11:07:28.103: INFO: Pod pod-d117e0e8-b4c8-4f36-b510-f1c14e4e9671 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:28.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1804" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:28.112: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-547" for this suite.

• [SLOW TEST:25.220 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1532,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:53.334: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jan 29 11:07:53.366: INFO: created test-podtemplate-1
Jan 29 11:07:53.368: INFO: created test-podtemplate-2
Jan 29 11:07:53.371: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 29 11:07:53.379: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 29 11:07:53.388: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:53.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-705" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":89,"skipped":1553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:53.398: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:07:53.426: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79" in namespace "projected-2981" to be "Succeeded or Failed"
Jan 29 11:07:53.435: INFO: Pod "downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79": Phase="Pending", Reason="", readiness=false. Elapsed: 8.846697ms
Jan 29 11:07:55.437: INFO: Pod "downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011711672s
STEP: Saw pod success
Jan 29 11:07:55.437: INFO: Pod "downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79" satisfied condition "Succeeded or Failed"
Jan 29 11:07:55.439: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79 container client-container: <nil>
STEP: delete the pod
Jan 29 11:07:55.458: INFO: Waiting for pod downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79 to disappear
Jan 29 11:07:55.464: INFO: Pod downwardapi-volume-4c11f0c1-b16f-4ebd-8593-aa66e8e15e79 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:07:55.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2981" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1617,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:07:55.470: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:07:55.539: INFO: Creating deployment "webserver-deployment"
Jan 29 11:07:55.545: INFO: Waiting for observed generation 1
Jan 29 11:07:57.552: INFO: Waiting for all required pods to come up
Jan 29 11:07:57.556: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 29 11:07:59.567: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 29 11:07:59.570: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 29 11:07:59.575: INFO: Updating deployment webserver-deployment
Jan 29 11:07:59.575: INFO: Waiting for observed generation 2
Jan 29 11:08:01.579: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 29 11:08:01.581: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 29 11:08:01.583: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 11:08:01.587: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 29 11:08:01.588: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 29 11:08:01.589: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 29 11:08:01.592: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 29 11:08:01.592: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 29 11:08:01.597: INFO: Updating deployment webserver-deployment
Jan 29 11:08:01.597: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 29 11:08:01.600: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 29 11:08:01.628: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jan 29 11:08:01.662: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7889 /apis/apps/v1/namespaces/deployment-7889/deployments/webserver-deployment 3ef946eb-d3fc-4bf0-a46a-5a53e6946b0e 11512 3 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037927c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-01-29 11:07:59 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-01-29 11:08:01 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 29 11:08:01.683: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7889 /apis/apps/v1/namespaces/deployment-7889/replicasets/webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 11506 3 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3ef946eb-d3fc-4bf0-a46a-5a53e6946b0e 0xc00376d947 0xc00376d948}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ef946eb-d3fc-4bf0-a46a-5a53e6946b0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00376d9d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:08:01.683: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 29 11:08:01.683: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-7889 /apis/apps/v1/namespaces/deployment-7889/replicasets/webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 11503 3 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3ef946eb-d3fc-4bf0-a46a-5a53e6946b0e 0xc00376da67 0xc00376da68}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:07:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ef946eb-d3fc-4bf0-a46a-5a53e6946b0e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00376dae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:08:01.716: INFO: Pod "webserver-deployment-795d758f88-7wr2x" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7wr2x webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-7wr2x 82896a80-dab1-4f57-994b-e657ddc0f791 11545 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003792d57 0xc003792d58}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.716: INFO: Pod "webserver-deployment-795d758f88-8s46n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8s46n webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-8s46n 154bba06-aee2-420c-8ae8-7b1d83c142cd 11528 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003792ea7 0xc003792ea8}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.716: INFO: Pod "webserver-deployment-795d758f88-hx4fs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hx4fs webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-hx4fs b27ae734-62a5-4d24-818f-3bc9d347aba8 11546 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793010 0xc003793011}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.716: INFO: Pod "webserver-deployment-795d758f88-jkbm6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jkbm6 webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-jkbm6 ae2ff333-bf03-43f3-90b4-1367e0fe9e05 11482 0 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793147 0xc003793148}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-01-29 11:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-khh42" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-khh42 webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-khh42 852154f6-c04e-41d2-9a52-9ff42a5fcd2c 11453 0 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793360 0xc003793361}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-01-29 11:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-q2ptx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-q2ptx webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-q2ptx f9902310-3d47-48fd-bcbb-ab5133f66c04 11462 0 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793530 0xc003793531}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-01-29 11:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-qdflm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qdflm webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-qdflm 756f9a8e-93f7-4457-b2b9-69495c4dd052 11539 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793720 0xc003793721}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-qjv6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qjv6k webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-qjv6k cec19dac-9780-4c90-9fab-21c56cb90396 11544 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc0037938a0 0xc0037938a1}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-tjq4h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tjq4h webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-tjq4h c38d9f40-9849-431a-8a87-7d97bad163de 11481 0 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc0037939e7 0xc0037939e8}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-01-29 11:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-tsdmf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tsdmf webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-tsdmf 63997db0-9fe5-42e4-9fa6-416dc0949526 11542 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793bb0 0xc003793bb1}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.717: INFO: Pod "webserver-deployment-795d758f88-wcmhl" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wcmhl webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-wcmhl acc3dfb5-6f27-4a53-ab41-7a72a3667ee6 11458 0 2021-01-29 11:07:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793d07 0xc003793d08}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-01-29 11:07:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-795d758f88-z55br" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z55br webserver-deployment-795d758f88- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-795d758f88-z55br 3e1031f6-2f78-4198-9596-5a5f5f1d24aa 11537 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 8120b01c-ac6e-4e70-b944-70a1d77f8ddd 0xc003793eb0 0xc003793eb1}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8120b01c-ac6e-4e70-b944-70a1d77f8ddd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-867f44f6fb-2cf9m" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-2cf9m webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-2cf9m 805f5042-2f09-4f54-814f-34d5737d0ede 11400 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc0e0 0xc0037bc0e1}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.50.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:10.200.50.33,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c0632a3e5cae36e25af3fa3bf98b214db2b5c8bb84dc4f9682ff1565ba5a93ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.50.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-867f44f6fb-2flq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-2flq8 webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-2flq8 e8cebaf6-2199-467e-ac57-6b9293cd8335 11541 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc2e0 0xc0037bc2e1}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-867f44f6fb-4f6cv" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-4f6cv webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-4f6cv 22f16cde-7cf5-4b95-86ec-bee48beb7066 11529 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc447 0xc0037bc448}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-867f44f6fb-5jljk" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-5jljk webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-5jljk 23653933-c147-44bf-a643-f9ba1a0808df 11531 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc590 0xc0037bc591}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-01-29 11:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.718: INFO: Pod "webserver-deployment-867f44f6fb-5lr69" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-5lr69 webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-5lr69 bae3ed42-ec25-4a2d-9664-dc81fa23ea8b 11363 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc710 0xc0037bc711}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.50.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:10.200.50.31,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d4309de46988bafafe9df8cdce53bcbdeadc9c780660f916bdf752181ef1f16f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.50.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.719: INFO: Pod "webserver-deployment-867f44f6fb-7dww5" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-7dww5 webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-7dww5 d727f3f7-b77a-4d79-b678-4d07a0436d34 11538 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bc920 0xc0037bc921}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.719: INFO: Pod "webserver-deployment-867f44f6fb-928fc" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-928fc webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-928fc 0fd767c3-93b1-48ba-bb48-b74a326d4ef6 11543 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bca47 0xc0037bca48}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.10,PodIP:,StartTime:2021-01-29 11:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.719: INFO: Pod "webserver-deployment-867f44f6fb-9f9qn" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-9f9qn webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-9f9qn 40f437a6-a4ca-40c5-9cdc-63e31ac66aa9 11386 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bcca0 0xc0037bcca1}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.95.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.95.31,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://de9da0e15fa99dce027df6c03505983c0eb0aa33acc76c278cb9901c79c20570,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.95.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.720: INFO: Pod "webserver-deployment-867f44f6fb-b4zk8" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-b4zk8 webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-b4zk8 11965c3c-0ec8-4ece-9176-614be413810b 11378 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bce70 0xc0037bce71}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.88,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8da6b211d1f6b224e529ec7d4a2fbc174250357ad687d966d4a01ef5e8b08f11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.720: INFO: Pod "webserver-deployment-867f44f6fb-cg77g" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-cg77g webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-cg77g 90e272f5-dc9d-424d-973a-1388c7f4bc9f 11527 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd090 0xc0037bd091}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.720: INFO: Pod "webserver-deployment-867f44f6fb-fj7wn" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-fj7wn webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-fj7wn 9ca7e1bc-d127-4e02-9793-3777bcc29cb8 11532 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd210 0xc0037bd211}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.720: INFO: Pod "webserver-deployment-867f44f6fb-g4fgh" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-g4fgh webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-g4fgh 4905dc83-be95-4d70-a57e-402d8a2dce64 11533 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd3c0 0xc0037bd3c1}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-h87dr" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-h87dr webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-h87dr 81d574e4-5aa0-426a-9744-d1fed28cfb82 11388 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd507 0xc0037bd508}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.95.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.95.32,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://95d70e62e770eede33e95b41ca44f64e4c760e36ad4f1cb56aab0d01a5b7623b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.95.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-j5tjn" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-j5tjn webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-j5tjn 47d365be-e010-4948-b5d9-836477e8b16c 11382 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd6f0 0xc0037bd6f1}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.89,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e9975e10490f2f5e664baaf32728ae0a00718b77f0cda27186b546b3d148ddc9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-nf7zw" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-nf7zw webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-nf7zw ee883908-98f5-4c64-b9c2-295a127265e0 11384 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bd8d0 0xc0037bd8d1}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.95.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.95.33,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d35c455dcabc1ef7b0a94e307604e3f1bd5af9d11232f224d18273a035fc6481,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.95.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-rr84v" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-rr84v webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-rr84v fdac86e0-35d1-46a1-aa8f-9c45b99a7ce8 11376 0 2021-01-29 11:07:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bda70 0xc0037bda71}] []  [{kube-controller-manager Update v1 2021-01-29 11:07:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:07:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:07:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.90,StartTime:2021-01-29 11:07:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:07:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ada50af8ce8c160eaedfbb0ea0bc7d38939a576ed2421a67e937bd28bd649e64,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-rtzfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-rtzfh webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-rtzfh 538d262a-335d-4207-b109-57bc535f5e59 11524 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bdc00 0xc0037bdc01}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:16fc05cd-9790-4b41-b13f-11160f9262cc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.721: INFO: Pod "webserver-deployment-867f44f6fb-wff6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-wff6c webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-wff6c 42369cd4-45bf-4539-97d5-76d093d903c7 11530 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bdd70 0xc0037bdd71}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:08:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:,StartTime:2021-01-29 11:08:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.722: INFO: Pod "webserver-deployment-867f44f6fb-wqxvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-wqxvf webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-wqxvf 819d18cf-8ee3-492a-986f-9156adbb25ce 11535 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037bdf30 0xc0037bdf31}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:08:01.722: INFO: Pod "webserver-deployment-867f44f6fb-xsjnz" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-xsjnz webserver-deployment-867f44f6fb- deployment-7889 /api/v1/namespaces/deployment-7889/pods/webserver-deployment-867f44f6fb-xsjnz 6ef67319-2f20-4964-b9a7-1fe23f9f65e1 11540 0 2021-01-29 11:08:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb e56210d8-6173-4677-bd38-ec148057d32f 0xc0037da0a7 0xc0037da0a8}] []  [{kube-controller-manager Update v1 2021-01-29 11:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56210d8-6173-4677-bd38-ec148057d32f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5pn6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5pn6q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5pn6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:01.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7889" for this suite.

• [SLOW TEST:6.303 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":91,"skipped":1627,"failed":0}
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:01.774: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jan 29 11:08:01.879: INFO: namespace kubectl-8704
Jan 29 11:08:01.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8704 create -f -'
Jan 29 11:08:02.254: INFO: stderr: ""
Jan 29 11:08:02.254: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 29 11:08:03.262: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 11:08:03.262: INFO: Found 0 / 1
Jan 29 11:08:04.265: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 11:08:04.265: INFO: Found 0 / 1
Jan 29 11:08:05.257: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 11:08:05.257: INFO: Found 1 / 1
Jan 29 11:08:05.257: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 11:08:05.260: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 11:08:05.260: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 11:08:05.260: INFO: wait on agnhost-primary startup in kubectl-8704 
Jan 29 11:08:05.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8704 logs agnhost-primary-bmw4h agnhost-primary'
Jan 29 11:08:05.348: INFO: stderr: ""
Jan 29 11:08:05.348: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 29 11:08:05.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8704 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 29 11:08:05.461: INFO: stderr: ""
Jan 29 11:08:05.461: INFO: stdout: "service/rm2 exposed\n"
Jan 29 11:08:05.465: INFO: Service rm2 in namespace kubectl-8704 found.
STEP: exposing service
Jan 29 11:08:07.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8704 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 29 11:08:07.647: INFO: stderr: ""
Jan 29 11:08:07.647: INFO: stdout: "service/rm3 exposed\n"
Jan 29 11:08:07.666: INFO: Service rm3 in namespace kubectl-8704 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:09.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8704" for this suite.

• [SLOW TEST:7.901 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":92,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:09.675: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:09.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9426" for this suite.
STEP: Destroying namespace "nspatchtest-ad32f29b-5cd6-46a9-8822-b6e8d07f8ab0-4005" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":93,"skipped":1652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:20.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6968" for this suite.

• [SLOW TEST:11.055 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":94,"skipped":1693,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:20.793: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-d6fb9b0a-64dd-4a1b-b134-d55d1e32d475
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:20.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6185" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":95,"skipped":1709,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 11:08:20.853: INFO: Waiting up to 5m0s for pod "pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393" in namespace "emptydir-4833" to be "Succeeded or Failed"
Jan 29 11:08:20.855: INFO: Pod "pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64879ms
Jan 29 11:08:22.858: INFO: Pod "pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004412806s
STEP: Saw pod success
Jan 29 11:08:22.858: INFO: Pod "pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393" satisfied condition "Succeeded or Failed"
Jan 29 11:08:22.859: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393 container test-container: <nil>
STEP: delete the pod
Jan 29 11:08:22.873: INFO: Waiting for pod pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393 to disappear
Jan 29 11:08:22.878: INFO: Pod pod-3dac22ac-e443-44a6-b1c0-348b2fd0a393 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:22.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4833" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1710,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:22.884: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:22.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2186" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":97,"skipped":1723,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:22.974: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 29 11:08:22.993: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:08:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:37.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1316" for this suite.

• [SLOW TEST:14.333 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":98,"skipped":1741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:37.308: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:08:37.853: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:08:39.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515317, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515317, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515317, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:08:42.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:08:42.869: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3659-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:08:43.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2352" for this suite.
STEP: Destroying namespace "webhook-2352-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.711 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":99,"skipped":1763,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:08:44.019: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jan 29 11:08:44.055: INFO: PodSpec: initContainers in spec.initContainers
Jan 29 11:09:32.613: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e1b2e755-7a2b-4618-b922-1162742015bc", GenerateName:"", Namespace:"init-container-7738", SelfLink:"/api/v1/namespaces/init-container-7738/pods/pod-init-e1b2e755-7a2b-4618-b922-1162742015bc", UID:"6536aa9f-1d54-41b9-96fd-79e2b29ddd8d", ResourceVersion:"12302", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63747515324, loc:(*time.Location)(0x76f4840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"55908308"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e99f20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e99f40)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e99f60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e99f80)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hwl6f", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00713cd80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwl6f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwl6f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwl6f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006d04b18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"c66ef63f-50cf-427f-af2a-b340450b0b53", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003347880), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006d04bc0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006d04be0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006d04be8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006d04bec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004e56e10), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515324, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515324, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515324, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515324, loc:(*time.Location)(0x76f4840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"30.0.0.11", PodIP:"10.200.24.101", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.200.24.101"}}, StartTime:(*v1.Time)(0xc003e99fa0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003347960)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0033479d0)}, Ready:false, RestartCount:3, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"docker-pullable://mirror.gcr.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://1b1b764fe25c815526269cfb46ede75842f3e167cb8d78f99c27813c8c661f01", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e99fe0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e99fc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc006d04c9f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:09:32.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7738" for this suite.

• [SLOW TEST:48.616 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":100,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:09:32.636: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 11:09:37.178: INFO: Successfully updated pod "pod-update-ab798a32-4ba6-48f7-9d74-e8642bb24ce4"
STEP: verifying the updated pod is in kubernetes
Jan 29 11:09:37.181: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:09:37.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1298" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1789,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:09:37.187: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 29 11:09:37.211: INFO: Waiting up to 5m0s for pod "pod-651e1abb-da33-4613-9f92-2e95bd160676" in namespace "emptydir-1539" to be "Succeeded or Failed"
Jan 29 11:09:37.226: INFO: Pod "pod-651e1abb-da33-4613-9f92-2e95bd160676": Phase="Pending", Reason="", readiness=false. Elapsed: 14.357456ms
Jan 29 11:09:39.229: INFO: Pod "pod-651e1abb-da33-4613-9f92-2e95bd160676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017232867s
STEP: Saw pod success
Jan 29 11:09:39.229: INFO: Pod "pod-651e1abb-da33-4613-9f92-2e95bd160676" satisfied condition "Succeeded or Failed"
Jan 29 11:09:39.230: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-651e1abb-da33-4613-9f92-2e95bd160676 container test-container: <nil>
STEP: delete the pod
Jan 29 11:09:39.243: INFO: Waiting for pod pod-651e1abb-da33-4613-9f92-2e95bd160676 to disappear
Jan 29 11:09:39.249: INFO: Pod pod-651e1abb-da33-4613-9f92-2e95bd160676 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:09:39.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1539" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":102,"skipped":1817,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:09:39.256: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 29 11:09:41.302: INFO: &Pod{ObjectMeta:{send-events-5c39fc53-57ac-41b9-8d59-f1dbfab26c71  events-1734 /api/v1/namespaces/events-1734/pods/send-events-5c39fc53-57ac-41b9-8d59-f1dbfab26c71 2110895c-4a68-44d2-8599-ed49588de429 12381 0 2021-01-29 11:09:39 +0000 UTC <nil> <nil> map[name:foo time:286590429] map[] [] []  [{e2e.test Update v1 2021-01-29 11:09:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:09:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.95.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xsztp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xsztp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xsztp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:7624137d-137a-468c-9e03-7a04d16d3fbf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:09:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:09:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:09:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:09:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.12,PodIP:10.200.95.42,StartTime:2021-01-29 11:09:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:09:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://cde4481e237c69e85d796fe36f889f88d0f4a3339432b2cf23145a58d2b2f23e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.95.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 29 11:09:43.305: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 29 11:09:45.308: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:09:45.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1734" for this suite.

• [SLOW TEST:6.069 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":103,"skipped":1830,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:09:45.326: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:09:45.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639" in namespace "projected-4159" to be "Succeeded or Failed"
Jan 29 11:09:45.364: INFO: Pod "downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639": Phase="Pending", Reason="", readiness=false. Elapsed: 9.663807ms
Jan 29 11:09:47.369: INFO: Pod "downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014817544s
STEP: Saw pod success
Jan 29 11:09:47.369: INFO: Pod "downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639" satisfied condition "Succeeded or Failed"
Jan 29 11:09:47.371: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639 container client-container: <nil>
STEP: delete the pod
Jan 29 11:09:47.382: INFO: Waiting for pod downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639 to disappear
Jan 29 11:09:47.389: INFO: Pod downwardapi-volume-126f69b5-3c0c-4aa1-943a-acef137c1639 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:09:47.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4159" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":104,"skipped":1839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:09:47.400: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-9999cf7d-c434-4662-b77a-346b0fc5547c
STEP: Creating secret with name s-test-opt-upd-c9acc43a-85e7-423e-a318-4cbc841858d6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9999cf7d-c434-4662-b77a-346b0fc5547c
STEP: Updating secret s-test-opt-upd-c9acc43a-85e7-423e-a318-4cbc841858d6
STEP: Creating secret with name s-test-opt-create-f1821504-5044-4cba-abaf-2eebfc946400
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:11:01.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6695" for this suite.

• [SLOW TEST:74.349 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1894,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:11:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:11:02.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:11:05.214: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:11:05.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9244" for this suite.
STEP: Destroying namespace "webhook-9244-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":106,"skipped":1903,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:11:05.284: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jan 29 11:11:07.832: INFO: Successfully updated pod "labelsupdate243c96b1-a3aa-4635-b3a9-f852f6446521"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:11:11.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6988" for this suite.

• [SLOW TEST:6.573 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":107,"skipped":1913,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:11:11.857: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-9824ef54-bad7-4386-8bf6-8111bf83bef6
STEP: Creating a pod to test consume configMaps
Jan 29 11:11:11.890: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d" in namespace "projected-5086" to be "Succeeded or Failed"
Jan 29 11:11:11.892: INFO: Pod "pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803407ms
Jan 29 11:11:13.894: INFO: Pod "pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003761997s
STEP: Saw pod success
Jan 29 11:11:13.894: INFO: Pod "pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d" satisfied condition "Succeeded or Failed"
Jan 29 11:11:13.896: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:11:13.908: INFO: Waiting for pod pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d to disappear
Jan 29 11:11:13.914: INFO: Pod pod-projected-configmaps-a691d4b2-a101-4170-9465-eb20aa243a6d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:11:13.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5086" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:11:13.921: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jan 29 11:11:13.955: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 11:12:13.978: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:12:13.980: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 29 11:12:18.029: INFO: found a healthy node: c66ef63f-50cf-427f-af2a-b340450b0b53
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:12:32.093: INFO: pods created so far: [1 1 1]
Jan 29 11:12:32.093: INFO: length of pods created so far: 3
Jan 29 11:12:44.100: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:12:51.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3973" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:12:51.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2406" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:97.250 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":109,"skipped":1953,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:12:51.172: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-67478144-a582-4920-84c6-baa98894073e
STEP: Creating a pod to test consume secrets
Jan 29 11:12:51.213: INFO: Waiting up to 5m0s for pod "pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f" in namespace "secrets-6930" to be "Succeeded or Failed"
Jan 29 11:12:51.225: INFO: Pod "pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.580081ms
Jan 29 11:12:53.229: INFO: Pod "pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015695318s
STEP: Saw pod success
Jan 29 11:12:53.229: INFO: Pod "pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f" satisfied condition "Succeeded or Failed"
Jan 29 11:12:53.230: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:12:53.251: INFO: Waiting for pod pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f to disappear
Jan 29 11:12:53.256: INFO: Pod pod-secrets-3cc03103-61ec-4a52-b9e5-c28d40b69e9f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:12:53.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6930" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1956,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:12:53.262: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:12:53.282: INFO: Creating ReplicaSet my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc
Jan 29 11:12:53.291: INFO: Pod name my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc: Found 0 pods out of 1
Jan 29 11:12:58.294: INFO: Pod name my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc: Found 1 pods out of 1
Jan 29 11:12:58.294: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc" is running
Jan 29 11:12:58.296: INFO: Pod "my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc-4bmxr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 11:12:53 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 11:12:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 11:12:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 11:12:53 +0000 UTC Reason: Message:}])
Jan 29 11:12:58.296: INFO: Trying to dial the pod
Jan 29 11:13:03.304: INFO: Controller my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc: Got expected result from replica 1 [my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc-4bmxr]: "my-hostname-basic-65bfd297-4b03-437b-a872-550dc4d030dc-4bmxr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:03.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4177" for this suite.

• [SLOW TEST:10.048 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":111,"skipped":1973,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:03.311: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9153" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":112,"skipped":1988,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:03.352: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:13:03.423: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:04.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5748" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":113,"skipped":2011,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:04.575: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 29 11:13:04.612: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13367 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:13:04.612: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13368 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:13:04.612: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13369 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 29 11:13:14.629: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13427 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:13:14.629: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13428 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:13:14.629: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-996 /api/v1/namespaces/watch-996/configmaps/e2e-watch-test-label-changed e63b08f9-0057-47f8-8522-350457d5f165 13429 0 2021-01-29 11:13:04 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:14.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-996" for this suite.

• [SLOW TEST:10.066 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":114,"skipped":2042,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:14.641: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jan 29 11:13:18.674: INFO: Pod pod-hostip-249ced29-8111-445d-aed6-62450cf7bdee has hostIP: 30.0.0.11
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:18.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6970" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":115,"skipped":2054,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:18.680: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:18.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8584" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":116,"skipped":2062,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:18.734: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:13:18.755: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 29 11:13:19.780: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:19.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6282" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":117,"skipped":2067,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:19.789: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 29 11:13:19.816: INFO: Created pod &Pod{ObjectMeta:{dns-9731  dns-9731 /api/v1/namespaces/dns-9731/pods/dns-9731 7f90bf94-55ee-4ae9-a55e-4d8b86b5adab 13510 0 2021-01-29 11:13:19 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-01-29 11:13:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b52rf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b52rf,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b52rf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:13:19.825: INFO: The status of Pod dns-9731 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:13:21.828: INFO: The status of Pod dns-9731 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:13:23.828: INFO: The status of Pod dns-9731 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 29 11:13:23.828: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9731 PodName:dns-9731 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:13:23.828: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Verifying customized DNS server is configured on pod...
Jan 29 11:13:23.967: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9731 PodName:dns-9731 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:13:23.967: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:13:24.077: INFO: Deleting pod dns-9731...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:24.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9731" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":118,"skipped":2067,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:24.101: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jan 29 11:13:24.141: INFO: Waiting up to 5m0s for pod "downward-api-7696af54-9cd1-4634-be53-935ae8e365ad" in namespace "downward-api-8796" to be "Succeeded or Failed"
Jan 29 11:13:24.142: INFO: Pod "downward-api-7696af54-9cd1-4634-be53-935ae8e365ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.484784ms
Jan 29 11:13:26.146: INFO: Pod "downward-api-7696af54-9cd1-4634-be53-935ae8e365ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005087778s
STEP: Saw pod success
Jan 29 11:13:26.146: INFO: Pod "downward-api-7696af54-9cd1-4634-be53-935ae8e365ad" satisfied condition "Succeeded or Failed"
Jan 29 11:13:26.150: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downward-api-7696af54-9cd1-4634-be53-935ae8e365ad container dapi-container: <nil>
STEP: delete the pod
Jan 29 11:13:26.173: INFO: Waiting for pod downward-api-7696af54-9cd1-4634-be53-935ae8e365ad to disappear
Jan 29 11:13:26.174: INFO: Pod downward-api-7696af54-9cd1-4634-be53-935ae8e365ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:26.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8796" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":2079,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Jan 29 11:13:26.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 29 11:13:26.287: INFO: stderr: ""
Jan 29 11:13:26.287: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jan 29 11:13:26.287: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 29 11:13:26.287: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4640" to be "running and ready, or succeeded"
Jan 29 11:13:26.291: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.982892ms
Jan 29 11:13:28.294: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006827522s
Jan 29 11:13:28.294: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 29 11:13:28.294: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 29 11:13:28.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator'
Jan 29 11:13:28.369: INFO: stderr: ""
Jan 29 11:13:28.369: INFO: stdout: "I0129 11:13:27.364576       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/cdhs 296\nI0129 11:13:27.564650       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/7c5s 332\nI0129 11:13:27.764679       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/7sd 430\nI0129 11:13:27.964671       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/w9f 354\nI0129 11:13:28.164675       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lk48 544\nI0129 11:13:28.364670       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9vd 393\n"
STEP: limiting log lines
Jan 29 11:13:28.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator --tail=1'
Jan 29 11:13:28.443: INFO: stderr: ""
Jan 29 11:13:28.443: INFO: stdout: "I0129 11:13:28.364670       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9vd 393\n"
Jan 29 11:13:28.443: INFO: got output "I0129 11:13:28.364670       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9vd 393\n"
STEP: limiting log bytes
Jan 29 11:13:28.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator --limit-bytes=1'
Jan 29 11:13:28.514: INFO: stderr: ""
Jan 29 11:13:28.514: INFO: stdout: "I"
Jan 29 11:13:28.514: INFO: got output "I"
STEP: exposing timestamps
Jan 29 11:13:28.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 29 11:13:28.584: INFO: stderr: ""
Jan 29 11:13:28.584: INFO: stdout: "2021-01-29T11:13:28.564755912Z I0129 11:13:28.564664       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/xd4 448\n"
Jan 29 11:13:28.584: INFO: got output "2021-01-29T11:13:28.564755912Z I0129 11:13:28.564664       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/xd4 448\n"
STEP: restricting to a time range
Jan 29 11:13:31.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator --since=1s'
Jan 29 11:13:31.168: INFO: stderr: ""
Jan 29 11:13:31.168: INFO: stdout: "I0129 11:13:30.364671       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/5r7 402\nI0129 11:13:30.564656       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/xgwc 536\nI0129 11:13:30.764659       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bm5r 567\nI0129 11:13:30.964661       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/5rqh 460\nI0129 11:13:31.164719       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/fx5 562\n"
Jan 29 11:13:31.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 logs logs-generator logs-generator --since=24h'
Jan 29 11:13:31.252: INFO: stderr: ""
Jan 29 11:13:31.252: INFO: stdout: "I0129 11:13:27.364576       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/cdhs 296\nI0129 11:13:27.564650       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/7c5s 332\nI0129 11:13:27.764679       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/7sd 430\nI0129 11:13:27.964671       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/w9f 354\nI0129 11:13:28.164675       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/lk48 544\nI0129 11:13:28.364670       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/9vd 393\nI0129 11:13:28.564664       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/xd4 448\nI0129 11:13:28.764627       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/tc7 355\nI0129 11:13:28.964677       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/c67 520\nI0129 11:13:29.164665       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/5fq6 453\nI0129 11:13:29.364669       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/rrvf 547\nI0129 11:13:29.564667       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/x8m 356\nI0129 11:13:29.764658       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/9lgp 264\nI0129 11:13:29.964661       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/4r8t 524\nI0129 11:13:30.164673       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/l8n6 574\nI0129 11:13:30.364671       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/5r7 402\nI0129 11:13:30.564656       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/xgwc 536\nI0129 11:13:30.764659       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bm5r 567\nI0129 11:13:30.964661       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/5rqh 460\nI0129 11:13:31.164719       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/fx5 562\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Jan 29 11:13:31.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-4640 delete pod logs-generator'
Jan 29 11:13:33.149: INFO: stderr: ""
Jan 29 11:13:33.149: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:33.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4640" for this suite.

• [SLOW TEST:6.977 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":120,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:33.158: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jan 29 11:13:33.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-2119 create -f -'
Jan 29 11:13:33.416: INFO: stderr: ""
Jan 29 11:13:33.416: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 29 11:13:33.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-2119 diff -f -'
Jan 29 11:13:33.752: INFO: rc: 1
Jan 29 11:13:33.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-2119 delete -f -'
Jan 29 11:13:33.827: INFO: stderr: ""
Jan 29 11:13:33.827: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:33.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2119" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":121,"skipped":2134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:33.842: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-plxn
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 11:13:33.879: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-plxn" in namespace "subpath-2670" to be "Succeeded or Failed"
Jan 29 11:13:33.889: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005582ms
Jan 29 11:13:35.892: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 2.012708247s
Jan 29 11:13:37.895: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 4.015868845s
Jan 29 11:13:39.901: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 6.021704273s
Jan 29 11:13:41.904: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 8.02476643s
Jan 29 11:13:43.906: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 10.026980403s
Jan 29 11:13:45.909: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 12.030380876s
Jan 29 11:13:47.912: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 14.033149554s
Jan 29 11:13:49.915: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 16.035564182s
Jan 29 11:13:51.918: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 18.038887044s
Jan 29 11:13:53.920: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Running", Reason="", readiness=true. Elapsed: 20.04104401s
Jan 29 11:13:55.924: INFO: Pod "pod-subpath-test-configmap-plxn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045135599s
STEP: Saw pod success
Jan 29 11:13:55.924: INFO: Pod "pod-subpath-test-configmap-plxn" satisfied condition "Succeeded or Failed"
Jan 29 11:13:55.926: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-subpath-test-configmap-plxn container test-container-subpath-configmap-plxn: <nil>
STEP: delete the pod
Jan 29 11:13:55.940: INFO: Waiting for pod pod-subpath-test-configmap-plxn to disappear
Jan 29 11:13:55.946: INFO: Pod pod-subpath-test-configmap-plxn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-plxn
Jan 29 11:13:55.946: INFO: Deleting pod "pod-subpath-test-configmap-plxn" in namespace "subpath-2670"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:13:55.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2670" for this suite.

• [SLOW TEST:22.111 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":122,"skipped":2160,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:13:55.954: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:13:55.990: INFO: Create a RollingUpdate DaemonSet
Jan 29 11:13:55.993: INFO: Check that daemon pods launch on every node of the cluster
Jan 29 11:13:56.003: INFO: Number of nodes with available pods: 0
Jan 29 11:13:56.003: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:13:57.010: INFO: Number of nodes with available pods: 0
Jan 29 11:13:57.010: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:13:58.010: INFO: Number of nodes with available pods: 2
Jan 29 11:13:58.010: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:13:59.009: INFO: Number of nodes with available pods: 3
Jan 29 11:13:59.009: INFO: Number of running nodes: 3, number of available pods: 3
Jan 29 11:13:59.009: INFO: Update the DaemonSet to trigger a rollout
Jan 29 11:13:59.015: INFO: Updating DaemonSet daemon-set
Jan 29 11:14:13.039: INFO: Roll back the DaemonSet before rollout is complete
Jan 29 11:14:13.043: INFO: Updating DaemonSet daemon-set
Jan 29 11:14:13.044: INFO: Make sure DaemonSet rollback is complete
Jan 29 11:14:13.046: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:13.046: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:14.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:14.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:15.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:15.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:16.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:16.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:17.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:17.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:18.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:18.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:19.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:19.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:20.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:20.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:21.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:21.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:22.058: INFO: Wrong image for pod: daemon-set-8zlxl. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 29 11:14:22.058: INFO: Pod daemon-set-8zlxl is not available
Jan 29 11:14:23.058: INFO: Pod daemon-set-h4d4m is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2338, will wait for the garbage collector to delete the pods
Jan 29 11:14:23.133: INFO: Deleting DaemonSet.extensions daemon-set took: 17.016831ms
Jan 29 11:14:23.633: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.140918ms
Jan 29 11:14:25.335: INFO: Number of nodes with available pods: 0
Jan 29 11:14:25.335: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 11:14:25.336: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2338/daemonsets","resourceVersion":"13940"},"items":null}

Jan 29 11:14:25.338: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2338/pods","resourceVersion":"13940"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:14:25.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2338" for this suite.

• [SLOW TEST:29.397 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":123,"skipped":2167,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:14:25.352: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:14:25.960: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:14:28.973: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:14:28.976: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:14:30.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7473" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":124,"skipped":2168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:14:30.187: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8496
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 11:14:30.214: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 11:14:30.270: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:14:32.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:14:34.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:14:36.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:14:38.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:14:40.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:14:42.273: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 29 11:14:42.276: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 29 11:14:44.279: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 29 11:14:46.279: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 29 11:14:46.282: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 29 11:14:48.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.125:8080/dial?request=hostname&protocol=http&host=10.200.50.49&port=8080&tries=1'] Namespace:pod-network-test-8496 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:14:48.295: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:14:48.383: INFO: Waiting for responses: map[]
Jan 29 11:14:48.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.125:8080/dial?request=hostname&protocol=http&host=10.200.95.46&port=8080&tries=1'] Namespace:pod-network-test-8496 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:14:48.386: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:14:48.469: INFO: Waiting for responses: map[]
Jan 29 11:14:48.471: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.125:8080/dial?request=hostname&protocol=http&host=10.200.24.124&port=8080&tries=1'] Namespace:pod-network-test-8496 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:14:48.471: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:14:48.543: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:14:48.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8496" for this suite.

• [SLOW TEST:18.363 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2206,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:14:48.552: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 29 11:14:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:14:51.445: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:15:02.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7213" for this suite.

• [SLOW TEST:14.368 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":126,"skipped":2215,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:15:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-1c489e9a-d90e-40ad-a10a-2f77b3f59a27 in namespace container-probe-402
Jan 29 11:15:04.966: INFO: Started pod busybox-1c489e9a-d90e-40ad-a10a-2f77b3f59a27 in namespace container-probe-402
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 11:15:04.968: INFO: Initial restart count of pod busybox-1c489e9a-d90e-40ad-a10a-2f77b3f59a27 is 0
Jan 29 11:15:59.055: INFO: Restart count of pod container-probe-402/busybox-1c489e9a-d90e-40ad-a10a-2f77b3f59a27 is now 1 (54.086709268s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:15:59.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-402" for this suite.

• [SLOW TEST:56.156 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2229,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:15:59.077: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1492
STEP: creating service affinity-clusterip-transition in namespace services-1492
STEP: creating replication controller affinity-clusterip-transition in namespace services-1492
I0129 11:15:59.110358      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1492, replica count: 3
I0129 11:16:02.160770      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:16:02.164: INFO: Creating new exec pod
Jan 29 11:16:05.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-1492 exec execpod-affinity28mb4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jan 29 11:16:06.305: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 29 11:16:06.305: INFO: stdout: ""
Jan 29 11:16:06.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-1492 exec execpod-affinity28mb4 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.136 80'
Jan 29 11:16:06.453: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.136 80\nConnection to 10.100.200.136 80 port [tcp/http] succeeded!\n"
Jan 29 11:16:06.453: INFO: stdout: ""
Jan 29 11:16:06.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-1492 exec execpod-affinity28mb4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.136:80/ ; done'
Jan 29 11:16:06.695: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n"
Jan 29 11:16:06.695: INFO: stdout: "\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-l8wjx\naffinity-clusterip-transition-l8wjx\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-l8wjx\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-l8wjx\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-lhwws\naffinity-clusterip-transition-l8wjx\naffinity-clusterip-transition-lhwws"
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-l8wjx
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-l8wjx
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-l8wjx
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-l8wjx
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-l8wjx
Jan 29 11:16:06.695: INFO: Received response from host: affinity-clusterip-transition-lhwws
Jan 29 11:16:06.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-1492 exec execpod-affinity28mb4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.136:80/ ; done'
Jan 29 11:16:06.935: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.136:80/\n"
Jan 29 11:16:06.935: INFO: stdout: "\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z\naffinity-clusterip-transition-24q4z"
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Received response from host: affinity-clusterip-transition-24q4z
Jan 29 11:16:06.935: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1492, will wait for the garbage collector to delete the pods
Jan 29 11:16:07.011: INFO: Deleting ReplicationController affinity-clusterip-transition took: 8.407934ms
Jan 29 11:16:07.511: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 500.093739ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:16:18.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1492" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:19.476 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":128,"skipped":2250,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:16:18.553: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 11:16:20.592: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:16:20.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6131" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":129,"skipped":2267,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:16:20.614: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6608
Jan 29 11:16:22.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 29 11:16:22.829: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 29 11:16:22.829: INFO: stdout: "iptables"
Jan 29 11:16:22.829: INFO: proxyMode: iptables
Jan 29 11:16:22.835: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:16:22.837: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:16:24.837: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:16:24.839: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:16:26.837: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:16:26.840: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:16:28.837: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:16:28.839: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6608
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6608
I0129 11:16:28.847963      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6608, replica count: 3
I0129 11:16:31.898222      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:16:31.901: INFO: Creating new exec pod
Jan 29 11:16:36.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec execpod-affinityk8z6t -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jan 29 11:16:37.079: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 29 11:16:37.079: INFO: stdout: ""
Jan 29 11:16:37.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec execpod-affinityk8z6t -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.29 80'
Jan 29 11:16:37.230: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.29 80\nConnection to 10.100.200.29 80 port [tcp/http] succeeded!\n"
Jan 29 11:16:37.230: INFO: stdout: ""
Jan 29 11:16:37.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec execpod-affinityk8z6t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.200.29:80/ ; done'
Jan 29 11:16:37.444: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n"
Jan 29 11:16:37.444: INFO: stdout: "\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht\naffinity-clusterip-timeout-q8mht"
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Received response from host: affinity-clusterip-timeout-q8mht
Jan 29 11:16:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec execpod-affinityk8z6t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.200.29:80/'
Jan 29 11:16:37.597: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n"
Jan 29 11:16:37.597: INFO: stdout: "affinity-clusterip-timeout-q8mht"
Jan 29 11:16:52.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-6608 exec execpod-affinityk8z6t -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.100.200.29:80/'
Jan 29 11:16:52.753: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.100.200.29:80/\n"
Jan 29 11:16:52.753: INFO: stdout: "affinity-clusterip-timeout-nt2pb"
Jan 29 11:16:52.753: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6608, will wait for the garbage collector to delete the pods
Jan 29 11:16:52.826: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.745665ms
Jan 29 11:16:53.326: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 500.120463ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:17:08.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6608" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:47.936 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":130,"skipped":2286,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:17:08.550: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:17:08.583: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 29 11:17:13.586: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 11:17:13.586: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 29 11:17:15.588: INFO: Creating deployment "test-rollover-deployment"
Jan 29 11:17:15.594: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 29 11:17:17.598: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 29 11:17:17.601: INFO: Ensure that both replica sets have 1 created replica
Jan 29 11:17:17.604: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 29 11:17:17.609: INFO: Updating deployment test-rollover-deployment
Jan 29 11:17:17.609: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 29 11:17:19.613: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 29 11:17:19.616: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 29 11:17:19.620: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 11:17:19.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515839, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:17:21.625: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 11:17:21.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515839, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:17:23.625: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 11:17:23.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515839, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:17:25.625: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 11:17:25.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515839, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:17:27.625: INFO: all replica sets need to contain the pod-template-hash label
Jan 29 11:17:27.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515839, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747515835, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:17:29.624: INFO: 
Jan 29 11:17:29.624: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jan 29 11:17:29.630: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3338 /apis/apps/v1/namespaces/deployment-3338/deployments/test-rollover-deployment dd1f2636-0b1d-425b-bb14-bb1322b7a31c 14978 2 2021-01-29 11:17:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-01-29 11:17:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:17:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005447638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-01-29 11:17:15 +0000 UTC,LastTransitionTime:2021-01-29 11:17:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-01-29 11:17:29 +0000 UTC,LastTransitionTime:2021-01-29 11:17:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 11:17:29.632: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-3338 /apis/apps/v1/namespaces/deployment-3338/replicasets/test-rollover-deployment-5797c7764 92135a9c-bfbc-47f8-8df0-59a211fd604a 14967 2 2021-01-29 11:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment dd1f2636-0b1d-425b-bb14-bb1322b7a31c 0xc003c39e90 0xc003c39e91}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:17:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd1f2636-0b1d-425b-bb14-bb1322b7a31c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c39f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:17:29.632: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 29 11:17:29.632: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3338 /apis/apps/v1/namespaces/deployment-3338/replicasets/test-rollover-controller e82f54b0-7387-4a99-8185-aecaa2b26eb6 14977 2 2021-01-29 11:17:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment dd1f2636-0b1d-425b-bb14-bb1322b7a31c 0xc003c39d77 0xc003c39d78}] []  [{e2e.test Update apps/v1 2021-01-29 11:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:17:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd1f2636-0b1d-425b-bb14-bb1322b7a31c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c39e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:17:29.632: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-3338 /apis/apps/v1/namespaces/deployment-3338/replicasets/test-rollover-deployment-78bc8b888c 85b7d04d-93d2-4f59-9b20-cb0bc7ea531f 14928 2 2021-01-29 11:17:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment dd1f2636-0b1d-425b-bb14-bb1322b7a31c 0xc003c39f77 0xc003c39f78}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:17:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd1f2636-0b1d-425b-bb14-bb1322b7a31c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038d2008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:17:29.634: INFO: Pod "test-rollover-deployment-5797c7764-8fpvq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-8fpvq test-rollover-deployment-5797c7764- deployment-3338 /api/v1/namespaces/deployment-3338/pods/test-rollover-deployment-5797c7764-8fpvq b2750de6-ea2b-4b74-918e-a42acb732f07 14940 0 2021-01-29 11:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 92135a9c-bfbc-47f8-8df0-59a211fd604a 0xc005447a40 0xc005447a41}] []  [{kube-controller-manager Update v1 2021-01-29 11:17:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92135a9c-bfbc-47f8-8df0-59a211fd604a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:17:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t6k5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t6k5h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t6k5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:17:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:17:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.136,StartTime:2021-01-29 11:17:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:17:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://319fb45497d512565a87446616e6064f8197224f9b215ef9d5bbd51908fc5389,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:17:29.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3338" for this suite.

• [SLOW TEST:21.091 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":131,"skipped":2298,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:17:29.641: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-cb879524-aa03-4734-89fe-c845bbe95ca4
STEP: Creating a pod to test consume secrets
Jan 29 11:17:29.676: INFO: Waiting up to 5m0s for pod "pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb" in namespace "secrets-5638" to be "Succeeded or Failed"
Jan 29 11:17:29.685: INFO: Pod "pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.829292ms
Jan 29 11:17:31.688: INFO: Pod "pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01233924s
STEP: Saw pod success
Jan 29 11:17:31.688: INFO: Pod "pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb" satisfied condition "Succeeded or Failed"
Jan 29 11:17:31.690: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:17:31.750: INFO: Waiting for pod pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb to disappear
Jan 29 11:17:31.751: INFO: Pod pod-secrets-074364fd-c4c7-4f8f-8ea1-41a3cbc650fb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:17:31.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5638" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2309,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:17:31.757: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-050b578a-377e-4c8e-9cc4-2f577c7c1187 in namespace container-probe-384
Jan 29 11:17:33.795: INFO: Started pod busybox-050b578a-377e-4c8e-9cc4-2f577c7c1187 in namespace container-probe-384
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 11:17:33.796: INFO: Initial restart count of pod busybox-050b578a-377e-4c8e-9cc4-2f577c7c1187 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:34.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-384" for this suite.

• [SLOW TEST:242.411 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2315,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:34.168: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:21:35.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:21:38.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:21:38.372: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-389-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:39.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4819" for this suite.
STEP: Destroying namespace "webhook-4819-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.406 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":134,"skipped":2317,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:39.576: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 29 11:21:39.594: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 11:21:39.604: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 11:21:39.606: INFO: 
Logging pods the apiserver thinks is on node 16fc05cd-9790-4b41-b13f-11160f9262cc before test
Jan 29 11:21:39.614: INFO: coredns-664d7f64c5-lwq6l from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:21:39.614: INFO: event-controller-85d6bb4d4c-qrl9c from pks-system started at 2021-01-29 10:32:14 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container event-controller ready: true, restart count 0
Jan 29 11:21:39.614: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:21:39.614: INFO: fluent-bit-82lt4 from pks-system started at 2021-01-29 10:32:33 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:21:39.614: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:21:39.614: INFO: node-exporter-hhk27 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:21:39.614: INFO: telegraf-t9hn6 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:21:39.614: INFO: wavefront-proxy-5bf7b49596-6vwv9 from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container wavefront-proxy ready: true, restart count 0
Jan 29 11:21:39.614: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.614: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:21:39.614: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:21:39.614: INFO: 
Logging pods the apiserver thinks is on node 7624137d-137a-468c-9e03-7a04d16d3fbf before test
Jan 29 11:21:39.623: INFO: coredns-664d7f64c5-8bv7k from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:21:39.623: INFO: fluent-bit-r4pxl from pks-system started at 2021-01-29 10:32:28 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:21:39.623: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:21:39.623: INFO: metric-controller-7f5cb8ff6d-cm2b2 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container metric-controller ready: true, restart count 0
Jan 29 11:21:39.623: INFO: node-exporter-kjb4q from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:21:39.623: INFO: observability-manager-6cf797f97-wshfj from pks-system started at 2021-01-29 10:32:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container observability-manager ready: true, restart count 0
Jan 29 11:21:39.623: INFO: telegraf-459jf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:21:39.623: INFO: telemetry-agent-8f56d9865-rt7qc from pks-system started at 2021-01-29 10:36:57 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jan 29 11:21:39.623: INFO: validator-69f557d7c6-hz8vq from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container validator ready: true, restart count 0
Jan 29 11:21:39.623: INFO: sonobuoy-e2e-job-a3be688aee3e44de from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container e2e ready: true, restart count 0
Jan 29 11:21:39.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:21:39.623: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:21:39.623: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:21:39.623: INFO: 
Logging pods the apiserver thinks is on node c66ef63f-50cf-427f-af2a-b340450b0b53 before test
Jan 29 11:21:39.631: INFO: coredns-664d7f64c5-nqvrn from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:21:39.631: INFO: metrics-server-7d476fdfbd-n4b2n from kube-system started at 2021-01-29 10:32:08 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 11:21:39.631: INFO: cert-generator-e484f2435fc830429fc9747bd002fcda56dd053e-4lc5d from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container cert-generator ready: false, restart count 0
Jan 29 11:21:39.631: INFO: fluent-bit-xbbgf from pks-system started at 2021-01-29 10:32:16 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:21:39.631: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:21:39.631: INFO: node-exporter-bx9js from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:21:39.631: INFO: sink-controller-f6bc7f774-wn8vp from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container sink-controller ready: true, restart count 0
Jan 29 11:21:39.631: INFO: telegraf-w42nf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.631: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:21:39.631: INFO: wavefront-collector-7648f897c7-sb8kc from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.632: INFO: 	Container wavefront-collector ready: true, restart count 0
Jan 29 11:21:39.632: INFO: sonobuoy from sonobuoy started at 2021-01-29 10:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.632: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 11:21:39.632: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:21:39.632: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:21:39.632: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:21:39.632: INFO: sample-webhook-deployment-cbccbf6bb-687vz from webhook-4819 started at 2021-01-29 11:21:35 +0000 UTC (1 container statuses recorded)
Jan 29 11:21:39.632: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aaf9c9ff-4257-435f-8a68-f2116a40ce7f 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-aaf9c9ff-4257-435f-8a68-f2116a40ce7f off the node c66ef63f-50cf-427f-af2a-b340450b0b53
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaf9c9ff-4257-435f-8a68-f2116a40ce7f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8130" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":135,"skipped":2331,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-1da536f6-4a8e-486d-bff0-14fbfdccd86e
STEP: Creating a pod to test consume secrets
Jan 29 11:21:43.740: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e" in namespace "projected-838" to be "Succeeded or Failed"
Jan 29 11:21:43.745: INFO: Pod "pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.415872ms
Jan 29 11:21:45.752: INFO: Pod "pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011080249s
STEP: Saw pod success
Jan 29 11:21:45.752: INFO: Pod "pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e" satisfied condition "Succeeded or Failed"
Jan 29 11:21:45.755: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:21:45.782: INFO: Waiting for pod pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e to disappear
Jan 29 11:21:45.788: INFO: Pod pod-projected-secrets-1a89c598-4b37-46ab-9aef-d7c12ad4925e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:45.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-838" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2336,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:45.806: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-2a6a0e3c-f654-4b84-ad86-49e415ff7c1e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2a6a0e3c-f654-4b84-ad86-49e415ff7c1e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:49.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1895" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":137,"skipped":2345,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jan 29 11:21:49.900: INFO: Waiting up to 5m0s for pod "downward-api-71857534-d43f-4765-bff5-233d8e2e001e" in namespace "downward-api-2241" to be "Succeeded or Failed"
Jan 29 11:21:49.910: INFO: Pod "downward-api-71857534-d43f-4765-bff5-233d8e2e001e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.76189ms
Jan 29 11:21:51.912: INFO: Pod "downward-api-71857534-d43f-4765-bff5-233d8e2e001e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012364332s
STEP: Saw pod success
Jan 29 11:21:51.912: INFO: Pod "downward-api-71857534-d43f-4765-bff5-233d8e2e001e" satisfied condition "Succeeded or Failed"
Jan 29 11:21:51.914: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downward-api-71857534-d43f-4765-bff5-233d8e2e001e container dapi-container: <nil>
STEP: delete the pod
Jan 29 11:21:51.925: INFO: Waiting for pod downward-api-71857534-d43f-4765-bff5-233d8e2e001e to disappear
Jan 29 11:21:51.933: INFO: Pod downward-api-71857534-d43f-4765-bff5-233d8e2e001e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2241" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2356,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:51.940: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-c11d8eab-84f0-413f-bc6c-8e018f80e231
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1288" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":139,"skipped":2360,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:51.969: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-58e98899-dd76-4046-ac3d-c3e489f85d3e
STEP: Creating a pod to test consume secrets
Jan 29 11:21:52.003: INFO: Waiting up to 5m0s for pod "pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317" in namespace "secrets-124" to be "Succeeded or Failed"
Jan 29 11:21:52.016: INFO: Pod "pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317": Phase="Pending", Reason="", readiness=false. Elapsed: 13.127778ms
Jan 29 11:21:54.019: INFO: Pod "pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016046591s
STEP: Saw pod success
Jan 29 11:21:54.019: INFO: Pod "pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317" satisfied condition "Succeeded or Failed"
Jan 29 11:21:54.020: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:21:54.035: INFO: Waiting for pod pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317 to disappear
Jan 29 11:21:54.040: INFO: Pod pod-secrets-1399bd29-855d-493f-86f6-3ce27e7d1317 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:21:54.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-124" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2364,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:21:54.047: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:01.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3197" for this suite.

• [SLOW TEST:7.037 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":141,"skipped":2374,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:01.084: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-861091f5-4fa3-48a0-a7ed-2f7de9e486dd
STEP: Creating a pod to test consume configMaps
Jan 29 11:22:01.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe" in namespace "configmap-7868" to be "Succeeded or Failed"
Jan 29 11:22:01.120: INFO: Pod "pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186591ms
Jan 29 11:22:03.123: INFO: Pod "pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005004044s
STEP: Saw pod success
Jan 29 11:22:03.123: INFO: Pod "pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe" satisfied condition "Succeeded or Failed"
Jan 29 11:22:03.125: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:22:03.141: INFO: Waiting for pod pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe to disappear
Jan 29 11:22:03.147: INFO: Pod pod-configmaps-91ca301f-64eb-4bd6-a792-0e31a96d90fe no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7868" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2374,"failed":0}

------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 29 11:22:03.170: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jan 29 11:22:03.899: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 29 11:22:05.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:07.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:09.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:11.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:13.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:15.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516123, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 29 11:22:18.761: INFO: Waited 813.947842ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:19.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5344" for this suite.

• [SLOW TEST:16.353 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":143,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:19.507: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:19.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2462" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":144,"skipped":2407,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:19.566: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:30.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-600" for this suite.

• [SLOW TEST:11.070 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":145,"skipped":2408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:30.637: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:22:30.655: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2667" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2460,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:32.748: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:22:32.766: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:38.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3600" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":147,"skipped":2461,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:38.976: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:22:39.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed" in namespace "projected-7757" to be "Succeeded or Failed"
Jan 29 11:22:39.054: INFO: Pod "downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 14.65589ms
Jan 29 11:22:41.056: INFO: Pod "downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017422927s
STEP: Saw pod success
Jan 29 11:22:41.056: INFO: Pod "downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed" satisfied condition "Succeeded or Failed"
Jan 29 11:22:41.058: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed container client-container: <nil>
STEP: delete the pod
Jan 29 11:22:41.074: INFO: Waiting for pod downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed to disappear
Jan 29 11:22:41.081: INFO: Pod downwardapi-volume-b42ceb74-5c36-41cc-90a1-430c7fc7d3ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:41.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7757" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":148,"skipped":2474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:41.089: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-7b3fad00-481b-481d-a13d-4713f1480ef2
STEP: Creating a pod to test consume configMaps
Jan 29 11:22:41.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074" in namespace "configmap-2747" to be "Succeeded or Failed"
Jan 29 11:22:41.122: INFO: Pod "pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074": Phase="Pending", Reason="", readiness=false. Elapsed: 2.409102ms
Jan 29 11:22:43.124: INFO: Pod "pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004432103s
STEP: Saw pod success
Jan 29 11:22:43.124: INFO: Pod "pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074" satisfied condition "Succeeded or Failed"
Jan 29 11:22:43.125: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:22:43.140: INFO: Waiting for pod pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074 to disappear
Jan 29 11:22:43.146: INFO: Pod pod-configmaps-91061a5f-a985-4a88-ad2f-e8e5bcf02074 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:43.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2747" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":149,"skipped":2536,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 11:22:43.189: INFO: Waiting up to 5m0s for pod "pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f" in namespace "emptydir-4827" to be "Succeeded or Failed"
Jan 29 11:22:43.193: INFO: Pod "pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030167ms
Jan 29 11:22:45.196: INFO: Pod "pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00684259s
STEP: Saw pod success
Jan 29 11:22:45.196: INFO: Pod "pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f" satisfied condition "Succeeded or Failed"
Jan 29 11:22:45.197: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f container test-container: <nil>
STEP: delete the pod
Jan 29 11:22:45.211: INFO: Waiting for pod pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f to disappear
Jan 29 11:22:45.216: INFO: Pod pod-ad86458a-eb7b-49e9-96a4-b80cd902a53f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:22:45.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4827" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2539,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:22:45.222: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3305
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3305
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3305
Jan 29 11:22:45.276: INFO: Found 0 stateful pods, waiting for 1
Jan 29 11:22:55.279: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 29 11:22:55.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:22:55.466: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:22:55.466: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:22:55.466: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:22:55.469: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 29 11:23:05.472: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:23:05.472: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:23:05.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999845s
Jan 29 11:23:06.492: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991672364s
Jan 29 11:23:07.496: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988751156s
Jan 29 11:23:08.499: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985506651s
Jan 29 11:23:09.502: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982546487s
Jan 29 11:23:10.504: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979588414s
Jan 29 11:23:11.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976673025s
Jan 29 11:23:12.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.973390346s
Jan 29 11:23:13.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.97042143s
Jan 29 11:23:14.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 966.841853ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3305
Jan 29 11:23:15.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:23:15.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:23:15.681: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:23:15.681: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:23:15.685: INFO: Found 1 stateful pods, waiting for 3
Jan 29 11:23:25.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:23:25.688: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:23:25.688: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 29 11:23:25.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:23:25.852: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:23:25.852: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:23:25.852: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:23:25.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:23:26.028: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:23:26.028: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:23:26.028: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:23:26.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:23:26.205: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:23:26.205: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:23:26.205: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:23:26.205: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:23:26.207: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 29 11:23:36.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:23:36.212: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:23:36.212: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 29 11:23:36.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999982s
Jan 29 11:23:37.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989246901s
Jan 29 11:23:38.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986719905s
Jan 29 11:23:39.239: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983305855s
Jan 29 11:23:40.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979997109s
Jan 29 11:23:41.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976713016s
Jan 29 11:23:42.249: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973456861s
Jan 29 11:23:43.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969905184s
Jan 29 11:23:44.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966080454s
Jan 29 11:23:45.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.733269ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3305
Jan 29 11:23:46.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:23:46.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:23:46.415: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:23:46.415: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:23:46.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:23:46.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:23:46.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:23:46.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:23:46.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-3305 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:23:46.724: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:23:46.724: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:23:46.724: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:23:46.724: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 11:23:56.735: INFO: Deleting all statefulset in ns statefulset-3305
Jan 29 11:23:56.736: INFO: Scaling statefulset ss to 0
Jan 29 11:23:56.742: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:23:56.743: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:23:56.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3305" for this suite.

• [SLOW TEST:71.537 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":151,"skipped":2540,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:23:56.761: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:23:56.788: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 29 11:24:01.791: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 11:24:01.791: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jan 29 11:24:01.835: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9919 /apis/apps/v1/namespaces/deployment-9919/deployments/test-cleanup-deployment e5434fcf-1a41-4a13-b3f2-37c908264c30 16780 1 2021-01-29 11:24:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb68f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetCreated,Message:Created new replica set "test-cleanup-deployment-5d446bdd47",LastUpdateTime:2021-01-29 11:24:01 +0000 UTC,LastTransitionTime:2021-01-29 11:24:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 11:24:01.854: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-9919 /apis/apps/v1/namespaces/deployment-9919/replicasets/test-cleanup-deployment-5d446bdd47 4d1b401c-5d08-4421-b915-7acb34fa08a5 16783 1 2021-01-29 11:24:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e5434fcf-1a41-4a13-b3f2-37c908264c30 0xc003eb6e27 0xc003eb6e28}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5434fcf-1a41-4a13-b3f2-37c908264c30\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eb6eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:24:01.854: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 29 11:24:01.855: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9919 /apis/apps/v1/namespaces/deployment-9919/replicasets/test-cleanup-controller 229aeba6-dc17-4ff3-adfe-df03cbdbd8dd 16775 1 2021-01-29 11:23:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e5434fcf-1a41-4a13-b3f2-37c908264c30 0xc003eb6d17 0xc003eb6d18}] []  [{e2e.test Update apps/v1 2021-01-29 11:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"e5434fcf-1a41-4a13-b3f2-37c908264c30\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003eb6db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:24:01.877: INFO: Pod "test-cleanup-controller-mdzxc" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mdzxc test-cleanup-controller- deployment-9919 /api/v1/namespaces/deployment-9919/pods/test-cleanup-controller-mdzxc 4bd356a1-fe1d-4acf-b096-b83ee9c12763 16759 0 2021-01-29 11:23:56 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 229aeba6-dc17-4ff3-adfe-df03cbdbd8dd 0xc003eb7377 0xc003eb7378}] []  [{kube-controller-manager Update v1 2021-01-29 11:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"229aeba6-dc17-4ff3-adfe-df03cbdbd8dd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4zhc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4zhc2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4zhc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.155,StartTime:2021-01-29 11:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://39f069a178c24568786deecaef4f5c681db609b21fd39cfd263e810441b8279c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 29 11:24:01.878: INFO: Pod "test-cleanup-deployment-5d446bdd47-bsvm4" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-bsvm4 test-cleanup-deployment-5d446bdd47- deployment-9919 /api/v1/namespaces/deployment-9919/pods/test-cleanup-deployment-5d446bdd47-bsvm4 bd50f425-8009-4c27-87d1-31582e5ed9a1 16788 0 2021-01-29 11:24:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 4d1b401c-5d08-4421-b915-7acb34fa08a5 0xc003eb7517 0xc003eb7518}] []  [{kube-controller-manager Update v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d1b401c-5d08-4421-b915-7acb34fa08a5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:24:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4zhc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4zhc2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4zhc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [agnhost],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [agnhost],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-01-29 11:24:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:01.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9919" for this suite.

• [SLOW TEST:5.137 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":152,"skipped":2555,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:01.898: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:05.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2814" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2575,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:24:06.432: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:24:08.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516246, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516246, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516246, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516246, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:24:11.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:23.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2627" for this suite.
STEP: Destroying namespace "webhook-2627-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":154,"skipped":2581,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:23.602: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 29 11:24:27.657: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:27.661: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:29.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:29.664: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:31.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:31.664: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:33.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:33.664: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:35.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:35.664: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:37.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:37.664: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 29 11:24:39.661: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 29 11:24:39.664: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:39.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7662" for this suite.

• [SLOW TEST:16.069 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":155,"skipped":2586,"failed":0}
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:39.671: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:39.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8956" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":156,"skipped":2588,"failed":0}
SS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:39.730: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:24:39.757: INFO: Waiting up to 5m0s for pod "busybox-user-65534-df74b4fd-207c-4060-b7d4-e427bb3143d2" in namespace "security-context-test-5348" to be "Succeeded or Failed"
Jan 29 11:24:39.766: INFO: Pod "busybox-user-65534-df74b4fd-207c-4060-b7d4-e427bb3143d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.752814ms
Jan 29 11:24:41.769: INFO: Pod "busybox-user-65534-df74b4fd-207c-4060-b7d4-e427bb3143d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012560316s
Jan 29 11:24:41.769: INFO: Pod "busybox-user-65534-df74b4fd-207c-4060-b7d4-e427bb3143d2" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:41.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5348" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2590,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 29 11:24:41.804: INFO: Waiting up to 5m0s for pod "pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1" in namespace "emptydir-6462" to be "Succeeded or Failed"
Jan 29 11:24:41.806: INFO: Pod "pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049739ms
Jan 29 11:24:43.809: INFO: Pod "pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004966262s
STEP: Saw pod success
Jan 29 11:24:43.809: INFO: Pod "pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1" satisfied condition "Succeeded or Failed"
Jan 29 11:24:43.811: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1 container test-container: <nil>
STEP: delete the pod
Jan 29 11:24:43.823: INFO: Waiting for pod pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1 to disappear
Jan 29 11:24:43.831: INFO: Pod pod-ad488b39-a2a2-4dbd-ad0d-0541495702e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6462" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2590,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:43.837: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:24:43.852: INFO: Creating deployment "test-recreate-deployment"
Jan 29 11:24:43.859: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 29 11:24:43.873: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 29 11:24:45.878: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 29 11:24:45.879: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 29 11:24:45.884: INFO: Updating deployment test-recreate-deployment
Jan 29 11:24:45.884: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jan 29 11:24:45.999: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5861 /apis/apps/v1/namespaces/deployment-5861/deployments/test-recreate-deployment 08caa97b-6d77-483c-92d2-4e9f53f4edcb 17183 2 2021-01-29 11:24:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c083d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-01-29 11:24:45 +0000 UTC,LastTransitionTime:2021-01-29 11:24:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-01-29 11:24:45 +0000 UTC,LastTransitionTime:2021-01-29 11:24:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 29 11:24:46.001: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-5861 /apis/apps/v1/namespaces/deployment-5861/replicasets/test-recreate-deployment-f756bcb56 f46ed984-5ebb-4bc0-840a-c7d929090d01 17180 1 2021-01-29 11:24:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 08caa97b-6d77-483c-92d2-4e9f53f4edcb 0xc003c088e0 0xc003c088e1}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08caa97b-6d77-483c-92d2-4e9f53f4edcb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c08958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:24:46.001: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 29 11:24:46.001: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-5861 /apis/apps/v1/namespaces/deployment-5861/replicasets/test-recreate-deployment-c96cf48f bc2fceaf-9b24-4ae6-9787-969a0a37d6f5 17171 2 2021-01-29 11:24:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 08caa97b-6d77-483c-92d2-4e9f53f4edcb 0xc003c087ef 0xc003c08800}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08caa97b-6d77-483c-92d2-4e9f53f4edcb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c08878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:24:46.003: INFO: Pod "test-recreate-deployment-f756bcb56-mqbbj" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-mqbbj test-recreate-deployment-f756bcb56- deployment-5861 /api/v1/namespaces/deployment-5861/pods/test-recreate-deployment-f756bcb56-mqbbj 1fef08af-e7cd-4082-87d3-99583859cd84 17182 0 2021-01-29 11:24:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 f46ed984-5ebb-4bc0-840a-c7d929090d01 0xc003c08e40 0xc003c08e41}] []  [{kube-controller-manager Update v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f46ed984-5ebb-4bc0-840a-c7d929090d01\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:24:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zcpmx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zcpmx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zcpmx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:24:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:,StartTime:2021-01-29 11:24:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:46.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5861" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":159,"skipped":2599,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:46.010: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-913d60a6-7a26-46ab-8e38-91df841eb740
STEP: Creating a pod to test consume secrets
Jan 29 11:24:46.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c" in namespace "projected-5135" to be "Succeeded or Failed"
Jan 29 11:24:46.054: INFO: Pod "pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.254795ms
Jan 29 11:24:48.057: INFO: Pod "pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013956838s
STEP: Saw pod success
Jan 29 11:24:48.057: INFO: Pod "pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c" satisfied condition "Succeeded or Failed"
Jan 29 11:24:48.059: INFO: Trying to get logs from node 7624137d-137a-468c-9e03-7a04d16d3fbf pod pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:24:48.079: INFO: Waiting for pod pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c to disappear
Jan 29 11:24:48.084: INFO: Pod pod-projected-secrets-8812a867-ec06-4da6-a3f1-0dac8350ee9c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:48.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5135" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2617,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:48.091: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:24:48.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4592" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":161,"skipped":2618,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:24:48.156: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 29 11:24:48.187: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17236 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:24:48.188: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17236 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 29 11:24:58.194: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17310 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:24:58.194: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17310 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 29 11:25:08.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17335 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:25:08.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17335 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 29 11:25:18.204: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17360 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:25:18.205: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-a 7df4408f-7f4a-4dac-903c-fbac06b3f324 17360 0 2021-01-29 11:24:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-01-29 11:24:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 29 11:25:28.210: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-b 67323fdb-d6cc-48cb-a62e-ef8d8b7bfdd8 17385 0 2021-01-29 11:25:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-01-29 11:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:25:28.210: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-b 67323fdb-d6cc-48cb-a62e-ef8d8b7bfdd8 17385 0 2021-01-29 11:25:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-01-29 11:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 29 11:25:38.216: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-b 67323fdb-d6cc-48cb-a62e-ef8d8b7bfdd8 17410 0 2021-01-29 11:25:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-01-29 11:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:25:38.216: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6394 /api/v1/namespaces/watch-6394/configmaps/e2e-watch-test-configmap-b 67323fdb-d6cc-48cb-a62e-ef8d8b7bfdd8 17410 0 2021-01-29 11:25:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-01-29 11:25:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:25:48.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6394" for this suite.

• [SLOW TEST:60.067 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":162,"skipped":2623,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:25:48.224: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Jan 29 11:25:48.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9431 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jan 29 11:25:48.317: INFO: stderr: ""
Jan 29 11:25:48.317: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 29 11:25:53.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9431 get pod e2e-test-httpd-pod -o json'
Jan 29 11:25:53.441: INFO: stderr: ""
Jan 29 11:25:53.441: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-01-29T11:25:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-01-29T11:25:48Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.200.24.165\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-01-29T11:25:50Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9431\",\n        \"resourceVersion\": \"17453\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9431/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ea5affc5-9784-48c0-b3b6-dd06b7da0c4e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4drvl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"c66ef63f-50cf-427f-af2a-b340450b0b53\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4drvl\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4drvl\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-01-29T11:25:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-01-29T11:25:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-01-29T11:25:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-01-29T11:25:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://94a99799b7f82b2f7cff9fe5f959787d6ee86f9d6ef3b05c4245d90cc2643811\",\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://mirror.gcr.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-01-29T11:25:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"30.0.0.11\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.24.165\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.200.24.165\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-01-29T11:25:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 29 11:25:53.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9431 replace -f -'
Jan 29 11:25:53.678: INFO: stderr: ""
Jan 29 11:25:53.678: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jan 29 11:25:53.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9431 delete pods e2e-test-httpd-pod'
Jan 29 11:25:58.414: INFO: stderr: ""
Jan 29 11:25:58.414: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:25:58.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9431" for this suite.

• [SLOW TEST:10.199 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":163,"skipped":2624,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:25:58.423: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jan 29 11:25:58.460: INFO: Waiting up to 5m0s for pod "var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a" in namespace "var-expansion-5644" to be "Succeeded or Failed"
Jan 29 11:25:58.463: INFO: Pod "var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.67965ms
Jan 29 11:26:00.466: INFO: Pod "var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00556956s
STEP: Saw pod success
Jan 29 11:26:00.466: INFO: Pod "var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a" satisfied condition "Succeeded or Failed"
Jan 29 11:26:00.468: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a container dapi-container: <nil>
STEP: delete the pod
Jan 29 11:26:00.481: INFO: Waiting for pod var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a to disappear
Jan 29 11:26:00.487: INFO: Pod var-expansion-911d5abb-45dc-426c-bb7c-3983562be17a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:00.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5644" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:00.494: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:26:00.820: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:26:02.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516360, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516360, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516360, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516360, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:26:05.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:05.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4739" for this suite.
STEP: Destroying namespace "webhook-4739-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.469 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":165,"skipped":2673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:05.963: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1015.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1015.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1015.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1015.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:26:08.020: INFO: DNS probes using dns-1015/dns-test-21ed4e78-6cc6-488f-92dc-95540060270e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:08.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1015" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":166,"skipped":2716,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:08.041: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 29 11:26:11.088: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:12.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5656" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":167,"skipped":2727,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:12.109: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 11:26:14.144: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4917" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":168,"skipped":2740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:14.164: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:14.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9973" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:14.229: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 29 11:26:14.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2465 /api/v1/namespaces/watch-2465/configmaps/e2e-watch-test-watch-closed dec1dec1-631d-4f7b-ab5d-d7fa7b58bfff 17734 0 2021-01-29 11:26:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-01-29 11:26:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:26:14.257: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2465 /api/v1/namespaces/watch-2465/configmaps/e2e-watch-test-watch-closed dec1dec1-631d-4f7b-ab5d-d7fa7b58bfff 17735 0 2021-01-29 11:26:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-01-29 11:26:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 29 11:26:14.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2465 /api/v1/namespaces/watch-2465/configmaps/e2e-watch-test-watch-closed dec1dec1-631d-4f7b-ab5d-d7fa7b58bfff 17736 0 2021-01-29 11:26:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-01-29 11:26:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:26:14.265: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2465 /api/v1/namespaces/watch-2465/configmaps/e2e-watch-test-watch-closed dec1dec1-631d-4f7b-ab5d-d7fa7b58bfff 17737 0 2021-01-29 11:26:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-01-29 11:26:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:14.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2465" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":170,"skipped":2782,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-12fe29fb-ee15-4cad-a1e6-aff0fa6e5f6e
STEP: Creating configMap with name cm-test-opt-upd-40cb75c3-4b99-4fc1-8bc9-c4b310c50109
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-12fe29fb-ee15-4cad-a1e6-aff0fa6e5f6e
STEP: Updating configmap cm-test-opt-upd-40cb75c3-4b99-4fc1-8bc9-c4b310c50109
STEP: Creating configMap with name cm-test-opt-create-3e330282-d8f6-439b-9909-7f4b24d20ccb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:18.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7528" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:18.380: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:26:18.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f" in namespace "downward-api-1310" to be "Succeeded or Failed"
Jan 29 11:26:18.418: INFO: Pod "downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.526537ms
Jan 29 11:26:20.421: INFO: Pod "downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013547028s
STEP: Saw pod success
Jan 29 11:26:20.421: INFO: Pod "downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f" satisfied condition "Succeeded or Failed"
Jan 29 11:26:20.423: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f container client-container: <nil>
STEP: delete the pod
Jan 29 11:26:20.436: INFO: Waiting for pod downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f to disappear
Jan 29 11:26:20.442: INFO: Pod downwardapi-volume-78e5e665-bea2-4924-a449-2fb006378f0f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:26:20.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1310" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2806,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:26:20.448: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:28:20.494: INFO: Deleting pod "var-expansion-0a7bd6bc-1895-4d22-9509-9f63c7311021" in namespace "var-expansion-8391"
Jan 29 11:28:20.500: INFO: Wait up to 5m0s for pod "var-expansion-0a7bd6bc-1895-4d22-9509-9f63c7311021" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:24.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8391" for this suite.

• [SLOW TEST:124.063 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":173,"skipped":2816,"failed":0}
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:24.511: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:28:24.530: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7458
I0129 11:28:24.542229      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7458, replica count: 1
I0129 11:28:25.592492      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0129 11:28:26.592765      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:28:26.697: INFO: Created: latency-svc-7c68z
Jan 29 11:28:26.703: INFO: Got endpoints: latency-svc-7c68z [10.940928ms]
Jan 29 11:28:26.713: INFO: Created: latency-svc-jgpvk
Jan 29 11:28:26.728: INFO: Created: latency-svc-vfmjl
Jan 29 11:28:26.729: INFO: Got endpoints: latency-svc-jgpvk [25.28671ms]
Jan 29 11:28:26.731: INFO: Created: latency-svc-j2b4p
Jan 29 11:28:26.738: INFO: Got endpoints: latency-svc-j2b4p [33.664476ms]
Jan 29 11:28:26.738: INFO: Got endpoints: latency-svc-vfmjl [33.805819ms]
Jan 29 11:28:26.746: INFO: Created: latency-svc-n87j8
Jan 29 11:28:26.754: INFO: Got endpoints: latency-svc-n87j8 [49.623703ms]
Jan 29 11:28:26.757: INFO: Created: latency-svc-b9wb2
Jan 29 11:28:26.773: INFO: Got endpoints: latency-svc-b9wb2 [68.838194ms]
Jan 29 11:28:26.777: INFO: Created: latency-svc-6xwm2
Jan 29 11:28:26.781: INFO: Got endpoints: latency-svc-6xwm2 [76.63128ms]
Jan 29 11:28:26.785: INFO: Created: latency-svc-2t8xj
Jan 29 11:28:26.790: INFO: Got endpoints: latency-svc-2t8xj [85.449773ms]
Jan 29 11:28:26.794: INFO: Created: latency-svc-27jlt
Jan 29 11:28:26.802: INFO: Got endpoints: latency-svc-27jlt [97.806778ms]
Jan 29 11:28:26.805: INFO: Created: latency-svc-qn6wd
Jan 29 11:28:26.814: INFO: Got endpoints: latency-svc-qn6wd [109.479598ms]
Jan 29 11:28:26.828: INFO: Created: latency-svc-zckzt
Jan 29 11:28:26.830: INFO: Created: latency-svc-xv8m4
Jan 29 11:28:26.837: INFO: Got endpoints: latency-svc-zckzt [132.452574ms]
Jan 29 11:28:26.838: INFO: Got endpoints: latency-svc-xv8m4 [132.408854ms]
Jan 29 11:28:26.848: INFO: Created: latency-svc-dq6vg
Jan 29 11:28:26.857: INFO: Got endpoints: latency-svc-dq6vg [152.154136ms]
Jan 29 11:28:26.859: INFO: Created: latency-svc-xj4b9
Jan 29 11:28:26.868: INFO: Got endpoints: latency-svc-xj4b9 [162.519073ms]
Jan 29 11:28:26.872: INFO: Created: latency-svc-hlw8g
Jan 29 11:28:26.879: INFO: Got endpoints: latency-svc-hlw8g [173.929261ms]
Jan 29 11:28:26.890: INFO: Created: latency-svc-mfvnq
Jan 29 11:28:26.895: INFO: Created: latency-svc-kdbd9
Jan 29 11:28:26.895: INFO: Got endpoints: latency-svc-mfvnq [190.26139ms]
Jan 29 11:28:26.905: INFO: Got endpoints: latency-svc-kdbd9 [26.098922ms]
Jan 29 11:28:26.905: INFO: Created: latency-svc-rpnll
Jan 29 11:28:26.914: INFO: Created: latency-svc-87k85
Jan 29 11:28:26.915: INFO: Got endpoints: latency-svc-rpnll [185.178731ms]
Jan 29 11:28:26.927: INFO: Got endpoints: latency-svc-87k85 [188.610723ms]
Jan 29 11:28:26.927: INFO: Created: latency-svc-wktl8
Jan 29 11:28:26.936: INFO: Got endpoints: latency-svc-wktl8 [198.161811ms]
Jan 29 11:28:26.938: INFO: Created: latency-svc-qtjl5
Jan 29 11:28:26.949: INFO: Got endpoints: latency-svc-qtjl5 [194.745978ms]
Jan 29 11:28:26.959: INFO: Created: latency-svc-8ghxp
Jan 29 11:28:26.963: INFO: Got endpoints: latency-svc-8ghxp [190.22228ms]
Jan 29 11:28:26.963: INFO: Created: latency-svc-kbp96
Jan 29 11:28:26.971: INFO: Got endpoints: latency-svc-kbp96 [190.189176ms]
Jan 29 11:28:26.973: INFO: Created: latency-svc-b8cn5
Jan 29 11:28:26.983: INFO: Got endpoints: latency-svc-b8cn5 [193.208984ms]
Jan 29 11:28:27.000: INFO: Created: latency-svc-7gbxv
Jan 29 11:28:27.003: INFO: Got endpoints: latency-svc-7gbxv [200.486232ms]
Jan 29 11:28:27.003: INFO: Created: latency-svc-q6cn2
Jan 29 11:28:27.014: INFO: Got endpoints: latency-svc-q6cn2 [199.250188ms]
Jan 29 11:28:27.014: INFO: Created: latency-svc-2rdpx
Jan 29 11:28:27.024: INFO: Got endpoints: latency-svc-2rdpx [185.912821ms]
Jan 29 11:28:27.027: INFO: Created: latency-svc-fv6sk
Jan 29 11:28:27.035: INFO: Created: latency-svc-z7jwz
Jan 29 11:28:27.037: INFO: Got endpoints: latency-svc-fv6sk [199.906396ms]
Jan 29 11:28:27.039: INFO: Got endpoints: latency-svc-z7jwz [181.87727ms]
Jan 29 11:28:27.049: INFO: Created: latency-svc-c8s97
Jan 29 11:28:27.057: INFO: Got endpoints: latency-svc-c8s97 [189.81638ms]
Jan 29 11:28:27.060: INFO: Created: latency-svc-hk4zb
Jan 29 11:28:27.068: INFO: Got endpoints: latency-svc-hk4zb [172.68791ms]
Jan 29 11:28:27.070: INFO: Created: latency-svc-q46dl
Jan 29 11:28:27.081: INFO: Got endpoints: latency-svc-q46dl [175.554528ms]
Jan 29 11:28:27.082: INFO: Created: latency-svc-5w5r5
Jan 29 11:28:27.090: INFO: Got endpoints: latency-svc-5w5r5 [175.129238ms]
Jan 29 11:28:27.101: INFO: Created: latency-svc-z77wm
Jan 29 11:28:27.104: INFO: Created: latency-svc-5z84j
Jan 29 11:28:27.104: INFO: Got endpoints: latency-svc-z77wm [177.63685ms]
Jan 29 11:28:27.114: INFO: Got endpoints: latency-svc-5z84j [177.656869ms]
Jan 29 11:28:27.116: INFO: Created: latency-svc-pnzm6
Jan 29 11:28:27.119: INFO: Got endpoints: latency-svc-pnzm6 [169.519864ms]
Jan 29 11:28:27.136: INFO: Created: latency-svc-5fclr
Jan 29 11:28:27.140: INFO: Created: latency-svc-8xkw7
Jan 29 11:28:27.148: INFO: Got endpoints: latency-svc-8xkw7 [176.70139ms]
Jan 29 11:28:27.148: INFO: Got endpoints: latency-svc-5fclr [185.151629ms]
Jan 29 11:28:27.150: INFO: Created: latency-svc-m2pgp
Jan 29 11:28:27.168: INFO: Got endpoints: latency-svc-m2pgp [185.220719ms]
Jan 29 11:28:27.174: INFO: Created: latency-svc-hx2v5
Jan 29 11:28:27.184: INFO: Created: latency-svc-97q7m
Jan 29 11:28:27.196: INFO: Created: latency-svc-zkr79
Jan 29 11:28:27.196: INFO: Created: latency-svc-rclq2
Jan 29 11:28:27.200: INFO: Created: latency-svc-8xgqk
Jan 29 11:28:27.210: INFO: Got endpoints: latency-svc-hx2v5 [206.628316ms]
Jan 29 11:28:27.210: INFO: Created: latency-svc-t5vjg
Jan 29 11:28:27.219: INFO: Created: latency-svc-scgj6
Jan 29 11:28:27.222: INFO: Created: latency-svc-tdk9z
Jan 29 11:28:27.224: INFO: Created: latency-svc-n8rx8
Jan 29 11:28:27.235: INFO: Created: latency-svc-p8qtg
Jan 29 11:28:27.237: INFO: Created: latency-svc-nbrlg
Jan 29 11:28:27.239: INFO: Created: latency-svc-bn76c
Jan 29 11:28:27.251: INFO: Created: latency-svc-z86qr
Jan 29 11:28:27.253: INFO: Got endpoints: latency-svc-97q7m [239.0724ms]
Jan 29 11:28:27.253: INFO: Created: latency-svc-dprjm
Jan 29 11:28:27.264: INFO: Created: latency-svc-wpnq2
Jan 29 11:28:27.275: INFO: Created: latency-svc-d8rk2
Jan 29 11:28:27.277: INFO: Created: latency-svc-jjrlc
Jan 29 11:28:27.303: INFO: Got endpoints: latency-svc-zkr79 [279.326961ms]
Jan 29 11:28:27.313: INFO: Created: latency-svc-j44mh
Jan 29 11:28:27.351: INFO: Got endpoints: latency-svc-rclq2 [313.839983ms]
Jan 29 11:28:27.363: INFO: Created: latency-svc-rkfnk
Jan 29 11:28:27.402: INFO: Got endpoints: latency-svc-8xgqk [363.082508ms]
Jan 29 11:28:27.413: INFO: Created: latency-svc-56479
Jan 29 11:28:27.452: INFO: Got endpoints: latency-svc-t5vjg [394.930085ms]
Jan 29 11:28:27.464: INFO: Created: latency-svc-4v5z9
Jan 29 11:28:27.501: INFO: Got endpoints: latency-svc-scgj6 [432.052971ms]
Jan 29 11:28:27.506: INFO: Created: latency-svc-5b7rb
Jan 29 11:28:27.553: INFO: Got endpoints: latency-svc-tdk9z [471.810007ms]
Jan 29 11:28:27.556: INFO: Created: latency-svc-xpqrs
Jan 29 11:28:27.601: INFO: Got endpoints: latency-svc-n8rx8 [510.899757ms]
Jan 29 11:28:27.616: INFO: Created: latency-svc-lqp79
Jan 29 11:28:27.651: INFO: Got endpoints: latency-svc-p8qtg [546.652857ms]
Jan 29 11:28:27.662: INFO: Created: latency-svc-bgwxz
Jan 29 11:28:27.701: INFO: Got endpoints: latency-svc-nbrlg [587.575228ms]
Jan 29 11:28:27.712: INFO: Created: latency-svc-vsdp8
Jan 29 11:28:27.753: INFO: Got endpoints: latency-svc-bn76c [634.147375ms]
Jan 29 11:28:27.762: INFO: Created: latency-svc-z2glx
Jan 29 11:28:27.802: INFO: Got endpoints: latency-svc-z86qr [654.221115ms]
Jan 29 11:28:27.812: INFO: Created: latency-svc-j58hm
Jan 29 11:28:27.852: INFO: Got endpoints: latency-svc-dprjm [704.180188ms]
Jan 29 11:28:27.862: INFO: Created: latency-svc-dh9vw
Jan 29 11:28:27.900: INFO: Got endpoints: latency-svc-wpnq2 [731.983303ms]
Jan 29 11:28:27.912: INFO: Created: latency-svc-zbl82
Jan 29 11:28:27.951: INFO: Got endpoints: latency-svc-d8rk2 [741.811817ms]
Jan 29 11:28:27.962: INFO: Created: latency-svc-qfkf6
Jan 29 11:28:28.000: INFO: Got endpoints: latency-svc-jjrlc [747.585982ms]
Jan 29 11:28:28.012: INFO: Created: latency-svc-f8xxv
Jan 29 11:28:28.051: INFO: Got endpoints: latency-svc-j44mh [748.106123ms]
Jan 29 11:28:28.055: INFO: Created: latency-svc-wzfmw
Jan 29 11:28:28.101: INFO: Got endpoints: latency-svc-rkfnk [750.211801ms]
Jan 29 11:28:28.106: INFO: Created: latency-svc-fzl2r
Jan 29 11:28:28.152: INFO: Got endpoints: latency-svc-56479 [749.702157ms]
Jan 29 11:28:28.157: INFO: Created: latency-svc-h9pd9
Jan 29 11:28:28.207: INFO: Got endpoints: latency-svc-4v5z9 [754.895619ms]
Jan 29 11:28:28.218: INFO: Created: latency-svc-sfvrx
Jan 29 11:28:28.251: INFO: Got endpoints: latency-svc-5b7rb [750.203723ms]
Jan 29 11:28:28.261: INFO: Created: latency-svc-h8hdp
Jan 29 11:28:28.301: INFO: Got endpoints: latency-svc-xpqrs [748.107047ms]
Jan 29 11:28:28.313: INFO: Created: latency-svc-v2vl8
Jan 29 11:28:28.352: INFO: Got endpoints: latency-svc-lqp79 [751.245214ms]
Jan 29 11:28:28.362: INFO: Created: latency-svc-7fzdm
Jan 29 11:28:28.402: INFO: Got endpoints: latency-svc-bgwxz [750.924846ms]
Jan 29 11:28:28.413: INFO: Created: latency-svc-sv678
Jan 29 11:28:28.453: INFO: Got endpoints: latency-svc-vsdp8 [751.671117ms]
Jan 29 11:28:28.463: INFO: Created: latency-svc-5jnsh
Jan 29 11:28:28.502: INFO: Got endpoints: latency-svc-z2glx [748.545103ms]
Jan 29 11:28:28.512: INFO: Created: latency-svc-l9tn6
Jan 29 11:28:28.553: INFO: Got endpoints: latency-svc-j58hm [750.257928ms]
Jan 29 11:28:28.563: INFO: Created: latency-svc-m7wnj
Jan 29 11:28:28.603: INFO: Got endpoints: latency-svc-dh9vw [750.186435ms]
Jan 29 11:28:28.616: INFO: Created: latency-svc-27wj9
Jan 29 11:28:28.651: INFO: Got endpoints: latency-svc-zbl82 [750.608678ms]
Jan 29 11:28:28.655: INFO: Created: latency-svc-w5vdf
Jan 29 11:28:28.701: INFO: Got endpoints: latency-svc-qfkf6 [749.429527ms]
Jan 29 11:28:28.707: INFO: Created: latency-svc-gk88t
Jan 29 11:28:28.752: INFO: Got endpoints: latency-svc-f8xxv [751.807621ms]
Jan 29 11:28:28.762: INFO: Created: latency-svc-nn7rm
Jan 29 11:28:28.803: INFO: Got endpoints: latency-svc-wzfmw [751.354847ms]
Jan 29 11:28:28.813: INFO: Created: latency-svc-xflxr
Jan 29 11:28:28.851: INFO: Got endpoints: latency-svc-fzl2r [749.722181ms]
Jan 29 11:28:28.863: INFO: Created: latency-svc-6n8fb
Jan 29 11:28:28.900: INFO: Got endpoints: latency-svc-h9pd9 [748.200161ms]
Jan 29 11:28:28.913: INFO: Created: latency-svc-n65wg
Jan 29 11:28:28.952: INFO: Got endpoints: latency-svc-sfvrx [744.136663ms]
Jan 29 11:28:28.961: INFO: Created: latency-svc-vwmqn
Jan 29 11:28:29.000: INFO: Got endpoints: latency-svc-h8hdp [749.166485ms]
Jan 29 11:28:29.014: INFO: Created: latency-svc-tkmh9
Jan 29 11:28:29.052: INFO: Got endpoints: latency-svc-v2vl8 [751.192435ms]
Jan 29 11:28:29.063: INFO: Created: latency-svc-cdt76
Jan 29 11:28:29.102: INFO: Got endpoints: latency-svc-7fzdm [749.99826ms]
Jan 29 11:28:29.113: INFO: Created: latency-svc-jkhgj
Jan 29 11:28:29.150: INFO: Got endpoints: latency-svc-sv678 [748.388759ms]
Jan 29 11:28:29.155: INFO: Created: latency-svc-8gszr
Jan 29 11:28:29.200: INFO: Got endpoints: latency-svc-5jnsh [747.274443ms]
Jan 29 11:28:29.205: INFO: Created: latency-svc-8lcmk
Jan 29 11:28:29.252: INFO: Got endpoints: latency-svc-l9tn6 [749.922653ms]
Jan 29 11:28:29.257: INFO: Created: latency-svc-vxflw
Jan 29 11:28:29.302: INFO: Got endpoints: latency-svc-m7wnj [748.660014ms]
Jan 29 11:28:29.312: INFO: Created: latency-svc-8jcm4
Jan 29 11:28:29.351: INFO: Got endpoints: latency-svc-27wj9 [748.704776ms]
Jan 29 11:28:29.361: INFO: Created: latency-svc-xk77h
Jan 29 11:28:29.400: INFO: Got endpoints: latency-svc-w5vdf [749.282502ms]
Jan 29 11:28:29.414: INFO: Created: latency-svc-mh94d
Jan 29 11:28:29.451: INFO: Got endpoints: latency-svc-gk88t [749.845823ms]
Jan 29 11:28:29.461: INFO: Created: latency-svc-k92mg
Jan 29 11:28:29.501: INFO: Got endpoints: latency-svc-nn7rm [749.029837ms]
Jan 29 11:28:29.512: INFO: Created: latency-svc-bbthf
Jan 29 11:28:29.553: INFO: Got endpoints: latency-svc-xflxr [750.537085ms]
Jan 29 11:28:29.560: INFO: Created: latency-svc-qk2lk
Jan 29 11:28:29.601: INFO: Got endpoints: latency-svc-6n8fb [749.895114ms]
Jan 29 11:28:29.614: INFO: Created: latency-svc-lbf75
Jan 29 11:28:29.652: INFO: Got endpoints: latency-svc-n65wg [751.670505ms]
Jan 29 11:28:29.663: INFO: Created: latency-svc-dxprd
Jan 29 11:28:29.704: INFO: Got endpoints: latency-svc-vwmqn [752.015884ms]
Jan 29 11:28:29.716: INFO: Created: latency-svc-9nkkd
Jan 29 11:28:29.751: INFO: Got endpoints: latency-svc-tkmh9 [750.838246ms]
Jan 29 11:28:29.762: INFO: Created: latency-svc-2gjgh
Jan 29 11:28:29.800: INFO: Got endpoints: latency-svc-cdt76 [748.244042ms]
Jan 29 11:28:29.812: INFO: Created: latency-svc-xz8fz
Jan 29 11:28:29.852: INFO: Got endpoints: latency-svc-jkhgj [749.699853ms]
Jan 29 11:28:29.864: INFO: Created: latency-svc-hp64v
Jan 29 11:28:29.900: INFO: Got endpoints: latency-svc-8gszr [749.772629ms]
Jan 29 11:28:29.912: INFO: Created: latency-svc-kd4vn
Jan 29 11:28:29.952: INFO: Got endpoints: latency-svc-8lcmk [751.392147ms]
Jan 29 11:28:29.957: INFO: Created: latency-svc-74jwg
Jan 29 11:28:30.001: INFO: Got endpoints: latency-svc-vxflw [748.978471ms]
Jan 29 11:28:30.006: INFO: Created: latency-svc-nw9xt
Jan 29 11:28:30.051: INFO: Got endpoints: latency-svc-8jcm4 [749.270396ms]
Jan 29 11:28:30.061: INFO: Created: latency-svc-qrbq5
Jan 29 11:28:30.101: INFO: Got endpoints: latency-svc-xk77h [749.207264ms]
Jan 29 11:28:30.113: INFO: Created: latency-svc-t7wm6
Jan 29 11:28:30.153: INFO: Got endpoints: latency-svc-mh94d [752.215973ms]
Jan 29 11:28:30.161: INFO: Created: latency-svc-sss5m
Jan 29 11:28:30.202: INFO: Got endpoints: latency-svc-k92mg [750.849503ms]
Jan 29 11:28:30.213: INFO: Created: latency-svc-mvhnh
Jan 29 11:28:30.253: INFO: Got endpoints: latency-svc-bbthf [751.29641ms]
Jan 29 11:28:30.262: INFO: Created: latency-svc-pf8hm
Jan 29 11:28:30.302: INFO: Got endpoints: latency-svc-qk2lk [748.456508ms]
Jan 29 11:28:30.306: INFO: Created: latency-svc-dlztg
Jan 29 11:28:30.353: INFO: Got endpoints: latency-svc-lbf75 [752.174458ms]
Jan 29 11:28:30.357: INFO: Created: latency-svc-hgmgr
Jan 29 11:28:30.402: INFO: Got endpoints: latency-svc-dxprd [749.89724ms]
Jan 29 11:28:30.407: INFO: Created: latency-svc-8qsqk
Jan 29 11:28:30.453: INFO: Got endpoints: latency-svc-9nkkd [748.817075ms]
Jan 29 11:28:30.463: INFO: Created: latency-svc-6znwc
Jan 29 11:28:30.501: INFO: Got endpoints: latency-svc-2gjgh [749.590276ms]
Jan 29 11:28:30.512: INFO: Created: latency-svc-xzf97
Jan 29 11:28:30.551: INFO: Got endpoints: latency-svc-xz8fz [750.330562ms]
Jan 29 11:28:30.562: INFO: Created: latency-svc-v5f7k
Jan 29 11:28:30.602: INFO: Got endpoints: latency-svc-hp64v [750.412323ms]
Jan 29 11:28:30.612: INFO: Created: latency-svc-wntz7
Jan 29 11:28:30.651: INFO: Got endpoints: latency-svc-kd4vn [751.213873ms]
Jan 29 11:28:30.662: INFO: Created: latency-svc-jf68b
Jan 29 11:28:30.702: INFO: Got endpoints: latency-svc-74jwg [750.462054ms]
Jan 29 11:28:30.713: INFO: Created: latency-svc-vsnpf
Jan 29 11:28:30.751: INFO: Got endpoints: latency-svc-nw9xt [750.519115ms]
Jan 29 11:28:30.763: INFO: Created: latency-svc-z689t
Jan 29 11:28:30.800: INFO: Got endpoints: latency-svc-qrbq5 [749.316086ms]
Jan 29 11:28:30.812: INFO: Created: latency-svc-cjdn2
Jan 29 11:28:30.851: INFO: Got endpoints: latency-svc-t7wm6 [750.519684ms]
Jan 29 11:28:30.856: INFO: Created: latency-svc-2rh8v
Jan 29 11:28:30.903: INFO: Got endpoints: latency-svc-sss5m [749.947798ms]
Jan 29 11:28:30.908: INFO: Created: latency-svc-lkxnn
Jan 29 11:28:30.952: INFO: Got endpoints: latency-svc-mvhnh [749.850748ms]
Jan 29 11:28:30.957: INFO: Created: latency-svc-2bxrp
Jan 29 11:28:31.003: INFO: Got endpoints: latency-svc-pf8hm [749.902283ms]
Jan 29 11:28:31.011: INFO: Created: latency-svc-gmx4l
Jan 29 11:28:31.052: INFO: Got endpoints: latency-svc-dlztg [750.280152ms]
Jan 29 11:28:31.062: INFO: Created: latency-svc-kzq5b
Jan 29 11:28:31.101: INFO: Got endpoints: latency-svc-hgmgr [748.144447ms]
Jan 29 11:28:31.112: INFO: Created: latency-svc-dqbrx
Jan 29 11:28:31.155: INFO: Got endpoints: latency-svc-8qsqk [752.971264ms]
Jan 29 11:28:31.165: INFO: Created: latency-svc-t52rr
Jan 29 11:28:31.204: INFO: Got endpoints: latency-svc-6znwc [751.361413ms]
Jan 29 11:28:31.213: INFO: Created: latency-svc-wclx4
Jan 29 11:28:31.251: INFO: Got endpoints: latency-svc-xzf97 [749.618295ms]
Jan 29 11:28:31.264: INFO: Created: latency-svc-rcc2d
Jan 29 11:28:31.302: INFO: Got endpoints: latency-svc-v5f7k [751.366688ms]
Jan 29 11:28:31.313: INFO: Created: latency-svc-6sw8l
Jan 29 11:28:31.350: INFO: Got endpoints: latency-svc-wntz7 [747.733096ms]
Jan 29 11:28:31.364: INFO: Created: latency-svc-gg8tv
Jan 29 11:28:31.402: INFO: Got endpoints: latency-svc-jf68b [750.30744ms]
Jan 29 11:28:31.406: INFO: Created: latency-svc-cxdmc
Jan 29 11:28:31.451: INFO: Got endpoints: latency-svc-vsnpf [748.788647ms]
Jan 29 11:28:31.456: INFO: Created: latency-svc-4t9tc
Jan 29 11:28:31.502: INFO: Got endpoints: latency-svc-z689t [750.313742ms]
Jan 29 11:28:31.507: INFO: Created: latency-svc-l6gp7
Jan 29 11:28:31.551: INFO: Got endpoints: latency-svc-cjdn2 [749.996367ms]
Jan 29 11:28:31.562: INFO: Created: latency-svc-6598n
Jan 29 11:28:31.606: INFO: Got endpoints: latency-svc-2rh8v [754.061793ms]
Jan 29 11:28:31.615: INFO: Created: latency-svc-8dcc7
Jan 29 11:28:31.652: INFO: Got endpoints: latency-svc-lkxnn [748.854716ms]
Jan 29 11:28:31.662: INFO: Created: latency-svc-jnkmm
Jan 29 11:28:31.700: INFO: Got endpoints: latency-svc-2bxrp [748.648592ms]
Jan 29 11:28:31.713: INFO: Created: latency-svc-k2dbw
Jan 29 11:28:31.754: INFO: Got endpoints: latency-svc-gmx4l [750.753467ms]
Jan 29 11:28:31.762: INFO: Created: latency-svc-zkvch
Jan 29 11:28:31.802: INFO: Got endpoints: latency-svc-kzq5b [749.793748ms]
Jan 29 11:28:31.813: INFO: Created: latency-svc-lqmtk
Jan 29 11:28:31.851: INFO: Got endpoints: latency-svc-dqbrx [749.515037ms]
Jan 29 11:28:31.862: INFO: Created: latency-svc-26jmf
Jan 29 11:28:31.902: INFO: Got endpoints: latency-svc-t52rr [747.193163ms]
Jan 29 11:28:31.913: INFO: Created: latency-svc-dgb2s
Jan 29 11:28:31.962: INFO: Got endpoints: latency-svc-wclx4 [757.603243ms]
Jan 29 11:28:31.966: INFO: Created: latency-svc-bjl5c
Jan 29 11:28:32.001: INFO: Got endpoints: latency-svc-rcc2d [750.648352ms]
Jan 29 11:28:32.006: INFO: Created: latency-svc-nzl2f
Jan 29 11:28:32.051: INFO: Got endpoints: latency-svc-6sw8l [749.259498ms]
Jan 29 11:28:32.056: INFO: Created: latency-svc-gd9m9
Jan 29 11:28:32.101: INFO: Got endpoints: latency-svc-gg8tv [750.48675ms]
Jan 29 11:28:32.112: INFO: Created: latency-svc-55nmv
Jan 29 11:28:32.152: INFO: Got endpoints: latency-svc-cxdmc [750.412182ms]
Jan 29 11:28:32.165: INFO: Created: latency-svc-c4k26
Jan 29 11:28:32.201: INFO: Got endpoints: latency-svc-4t9tc [749.766273ms]
Jan 29 11:28:32.214: INFO: Created: latency-svc-lbv4l
Jan 29 11:28:32.250: INFO: Got endpoints: latency-svc-l6gp7 [748.422555ms]
Jan 29 11:28:32.261: INFO: Created: latency-svc-qm4lb
Jan 29 11:28:32.302: INFO: Got endpoints: latency-svc-6598n [751.096946ms]
Jan 29 11:28:32.313: INFO: Created: latency-svc-vkm42
Jan 29 11:28:32.352: INFO: Got endpoints: latency-svc-8dcc7 [746.299896ms]
Jan 29 11:28:32.363: INFO: Created: latency-svc-cdgsw
Jan 29 11:28:32.400: INFO: Got endpoints: latency-svc-jnkmm [748.534024ms]
Jan 29 11:28:32.413: INFO: Created: latency-svc-nkgjh
Jan 29 11:28:32.452: INFO: Got endpoints: latency-svc-k2dbw [751.17751ms]
Jan 29 11:28:32.462: INFO: Created: latency-svc-hwdm6
Jan 29 11:28:32.501: INFO: Got endpoints: latency-svc-zkvch [746.961566ms]
Jan 29 11:28:32.505: INFO: Created: latency-svc-6fpr4
Jan 29 11:28:32.552: INFO: Got endpoints: latency-svc-lqmtk [750.19986ms]
Jan 29 11:28:32.557: INFO: Created: latency-svc-c4n5v
Jan 29 11:28:32.601: INFO: Got endpoints: latency-svc-26jmf [749.810933ms]
Jan 29 11:28:32.606: INFO: Created: latency-svc-xgjvb
Jan 29 11:28:32.650: INFO: Got endpoints: latency-svc-dgb2s [747.673777ms]
Jan 29 11:28:32.661: INFO: Created: latency-svc-j5gtr
Jan 29 11:28:32.702: INFO: Got endpoints: latency-svc-bjl5c [740.109131ms]
Jan 29 11:28:32.712: INFO: Created: latency-svc-f4527
Jan 29 11:28:32.751: INFO: Got endpoints: latency-svc-nzl2f [749.536733ms]
Jan 29 11:28:32.763: INFO: Created: latency-svc-75kzw
Jan 29 11:28:32.803: INFO: Got endpoints: latency-svc-gd9m9 [751.342564ms]
Jan 29 11:28:32.812: INFO: Created: latency-svc-nwtn6
Jan 29 11:28:32.850: INFO: Got endpoints: latency-svc-55nmv [749.165127ms]
Jan 29 11:28:32.861: INFO: Created: latency-svc-h8rvn
Jan 29 11:28:32.901: INFO: Got endpoints: latency-svc-c4k26 [748.514397ms]
Jan 29 11:28:32.914: INFO: Created: latency-svc-mphzx
Jan 29 11:28:32.953: INFO: Got endpoints: latency-svc-lbv4l [751.719804ms]
Jan 29 11:28:32.964: INFO: Created: latency-svc-bcbg5
Jan 29 11:28:33.001: INFO: Got endpoints: latency-svc-qm4lb [750.189497ms]
Jan 29 11:28:33.013: INFO: Created: latency-svc-r77t8
Jan 29 11:28:33.051: INFO: Got endpoints: latency-svc-vkm42 [749.557008ms]
Jan 29 11:28:33.055: INFO: Created: latency-svc-7lhkn
Jan 29 11:28:33.102: INFO: Got endpoints: latency-svc-cdgsw [749.480951ms]
Jan 29 11:28:33.106: INFO: Created: latency-svc-rn5zc
Jan 29 11:28:33.150: INFO: Got endpoints: latency-svc-nkgjh [749.905847ms]
Jan 29 11:28:33.156: INFO: Created: latency-svc-64hzr
Jan 29 11:28:33.203: INFO: Got endpoints: latency-svc-hwdm6 [751.234765ms]
Jan 29 11:28:33.214: INFO: Created: latency-svc-m4trk
Jan 29 11:28:33.251: INFO: Got endpoints: latency-svc-6fpr4 [749.669348ms]
Jan 29 11:28:33.261: INFO: Created: latency-svc-l6zzq
Jan 29 11:28:33.301: INFO: Got endpoints: latency-svc-c4n5v [748.978111ms]
Jan 29 11:28:33.312: INFO: Created: latency-svc-g5mqd
Jan 29 11:28:33.350: INFO: Got endpoints: latency-svc-xgjvb [749.19017ms]
Jan 29 11:28:33.362: INFO: Created: latency-svc-mcfjv
Jan 29 11:28:33.401: INFO: Got endpoints: latency-svc-j5gtr [750.749552ms]
Jan 29 11:28:33.413: INFO: Created: latency-svc-cqv84
Jan 29 11:28:33.450: INFO: Got endpoints: latency-svc-f4527 [747.887539ms]
Jan 29 11:28:33.462: INFO: Created: latency-svc-hqv6t
Jan 29 11:28:33.500: INFO: Got endpoints: latency-svc-75kzw [748.657836ms]
Jan 29 11:28:33.514: INFO: Created: latency-svc-jxmx9
Jan 29 11:28:33.552: INFO: Got endpoints: latency-svc-nwtn6 [749.275621ms]
Jan 29 11:28:33.562: INFO: Created: latency-svc-jhw2w
Jan 29 11:28:33.601: INFO: Got endpoints: latency-svc-h8rvn [750.841035ms]
Jan 29 11:28:33.605: INFO: Created: latency-svc-5cdms
Jan 29 11:28:33.652: INFO: Got endpoints: latency-svc-mphzx [750.514221ms]
Jan 29 11:28:33.655: INFO: Created: latency-svc-99kmz
Jan 29 11:28:33.701: INFO: Got endpoints: latency-svc-bcbg5 [748.172558ms]
Jan 29 11:28:33.708: INFO: Created: latency-svc-ftqg4
Jan 29 11:28:33.751: INFO: Got endpoints: latency-svc-r77t8 [750.118014ms]
Jan 29 11:28:33.762: INFO: Created: latency-svc-xptkr
Jan 29 11:28:33.803: INFO: Got endpoints: latency-svc-7lhkn [751.019681ms]
Jan 29 11:28:33.813: INFO: Created: latency-svc-46ktj
Jan 29 11:28:33.853: INFO: Got endpoints: latency-svc-rn5zc [751.124835ms]
Jan 29 11:28:33.862: INFO: Created: latency-svc-hnlh7
Jan 29 11:28:33.904: INFO: Got endpoints: latency-svc-64hzr [753.454784ms]
Jan 29 11:28:33.914: INFO: Created: latency-svc-gpx27
Jan 29 11:28:33.952: INFO: Got endpoints: latency-svc-m4trk [748.748019ms]
Jan 29 11:28:33.964: INFO: Created: latency-svc-2w9qg
Jan 29 11:28:34.002: INFO: Got endpoints: latency-svc-l6zzq [751.357816ms]
Jan 29 11:28:34.013: INFO: Created: latency-svc-fjtk9
Jan 29 11:28:34.053: INFO: Got endpoints: latency-svc-g5mqd [751.405524ms]
Jan 29 11:28:34.062: INFO: Created: latency-svc-g58lw
Jan 29 11:28:34.101: INFO: Got endpoints: latency-svc-mcfjv [750.622738ms]
Jan 29 11:28:34.105: INFO: Created: latency-svc-wvjxr
Jan 29 11:28:34.152: INFO: Got endpoints: latency-svc-cqv84 [750.912498ms]
Jan 29 11:28:34.156: INFO: Created: latency-svc-w2kn6
Jan 29 11:28:34.201: INFO: Got endpoints: latency-svc-hqv6t [750.960063ms]
Jan 29 11:28:34.206: INFO: Created: latency-svc-vvvsm
Jan 29 11:28:34.252: INFO: Got endpoints: latency-svc-jxmx9 [751.813959ms]
Jan 29 11:28:34.271: INFO: Created: latency-svc-9f4fj
Jan 29 11:28:34.300: INFO: Got endpoints: latency-svc-jhw2w [748.310375ms]
Jan 29 11:28:34.311: INFO: Created: latency-svc-bspj4
Jan 29 11:28:34.351: INFO: Got endpoints: latency-svc-5cdms [750.443953ms]
Jan 29 11:28:34.362: INFO: Created: latency-svc-pcp7b
Jan 29 11:28:34.402: INFO: Got endpoints: latency-svc-99kmz [750.721933ms]
Jan 29 11:28:34.412: INFO: Created: latency-svc-qj4n5
Jan 29 11:28:34.451: INFO: Got endpoints: latency-svc-ftqg4 [749.610794ms]
Jan 29 11:28:34.462: INFO: Created: latency-svc-hql92
Jan 29 11:28:34.501: INFO: Got endpoints: latency-svc-xptkr [750.034063ms]
Jan 29 11:28:34.514: INFO: Created: latency-svc-6zktq
Jan 29 11:28:34.552: INFO: Got endpoints: latency-svc-46ktj [749.041691ms]
Jan 29 11:28:34.602: INFO: Got endpoints: latency-svc-hnlh7 [749.158714ms]
Jan 29 11:28:34.653: INFO: Got endpoints: latency-svc-gpx27 [748.578374ms]
Jan 29 11:28:34.704: INFO: Got endpoints: latency-svc-2w9qg [752.694135ms]
Jan 29 11:28:34.751: INFO: Got endpoints: latency-svc-fjtk9 [748.75854ms]
Jan 29 11:28:34.802: INFO: Got endpoints: latency-svc-g58lw [749.114262ms]
Jan 29 11:28:34.850: INFO: Got endpoints: latency-svc-wvjxr [749.634375ms]
Jan 29 11:28:34.901: INFO: Got endpoints: latency-svc-w2kn6 [748.861804ms]
Jan 29 11:28:34.955: INFO: Got endpoints: latency-svc-vvvsm [753.432742ms]
Jan 29 11:28:35.003: INFO: Got endpoints: latency-svc-9f4fj [751.023786ms]
Jan 29 11:28:35.052: INFO: Got endpoints: latency-svc-bspj4 [751.397493ms]
Jan 29 11:28:35.102: INFO: Got endpoints: latency-svc-pcp7b [750.811946ms]
Jan 29 11:28:35.150: INFO: Got endpoints: latency-svc-qj4n5 [747.813491ms]
Jan 29 11:28:35.203: INFO: Got endpoints: latency-svc-hql92 [751.794948ms]
Jan 29 11:28:35.254: INFO: Got endpoints: latency-svc-6zktq [752.507251ms]
Jan 29 11:28:35.254: INFO: Latencies: [25.28671ms 26.098922ms 33.664476ms 33.805819ms 49.623703ms 68.838194ms 76.63128ms 85.449773ms 97.806778ms 109.479598ms 132.408854ms 132.452574ms 152.154136ms 162.519073ms 169.519864ms 172.68791ms 173.929261ms 175.129238ms 175.554528ms 176.70139ms 177.63685ms 177.656869ms 181.87727ms 185.151629ms 185.178731ms 185.220719ms 185.912821ms 188.610723ms 189.81638ms 190.189176ms 190.22228ms 190.26139ms 193.208984ms 194.745978ms 198.161811ms 199.250188ms 199.906396ms 200.486232ms 206.628316ms 239.0724ms 279.326961ms 313.839983ms 363.082508ms 394.930085ms 432.052971ms 471.810007ms 510.899757ms 546.652857ms 587.575228ms 634.147375ms 654.221115ms 704.180188ms 731.983303ms 740.109131ms 741.811817ms 744.136663ms 746.299896ms 746.961566ms 747.193163ms 747.274443ms 747.585982ms 747.673777ms 747.733096ms 747.813491ms 747.887539ms 748.106123ms 748.107047ms 748.144447ms 748.172558ms 748.200161ms 748.244042ms 748.310375ms 748.388759ms 748.422555ms 748.456508ms 748.514397ms 748.534024ms 748.545103ms 748.578374ms 748.648592ms 748.657836ms 748.660014ms 748.704776ms 748.748019ms 748.75854ms 748.788647ms 748.817075ms 748.854716ms 748.861804ms 748.978111ms 748.978471ms 749.029837ms 749.041691ms 749.114262ms 749.158714ms 749.165127ms 749.166485ms 749.19017ms 749.207264ms 749.259498ms 749.270396ms 749.275621ms 749.282502ms 749.316086ms 749.429527ms 749.480951ms 749.515037ms 749.536733ms 749.557008ms 749.590276ms 749.610794ms 749.618295ms 749.634375ms 749.669348ms 749.699853ms 749.702157ms 749.722181ms 749.766273ms 749.772629ms 749.793748ms 749.810933ms 749.845823ms 749.850748ms 749.895114ms 749.89724ms 749.902283ms 749.905847ms 749.922653ms 749.947798ms 749.996367ms 749.99826ms 750.034063ms 750.118014ms 750.186435ms 750.189497ms 750.19986ms 750.203723ms 750.211801ms 750.257928ms 750.280152ms 750.30744ms 750.313742ms 750.330562ms 750.412182ms 750.412323ms 750.443953ms 750.462054ms 750.48675ms 750.514221ms 750.519115ms 750.519684ms 750.537085ms 750.608678ms 750.622738ms 750.648352ms 750.721933ms 750.749552ms 750.753467ms 750.811946ms 750.838246ms 750.841035ms 750.849503ms 750.912498ms 750.924846ms 750.960063ms 751.019681ms 751.023786ms 751.096946ms 751.124835ms 751.17751ms 751.192435ms 751.213873ms 751.234765ms 751.245214ms 751.29641ms 751.342564ms 751.354847ms 751.357816ms 751.361413ms 751.366688ms 751.392147ms 751.397493ms 751.405524ms 751.670505ms 751.671117ms 751.719804ms 751.794948ms 751.807621ms 751.813959ms 752.015884ms 752.174458ms 752.215973ms 752.507251ms 752.694135ms 752.971264ms 753.432742ms 753.454784ms 754.061793ms 754.895619ms 757.603243ms]
Jan 29 11:28:35.254: INFO: 50 %ile: 749.270396ms
Jan 29 11:28:35.254: INFO: 90 %ile: 751.392147ms
Jan 29 11:28:35.254: INFO: 99 %ile: 754.895619ms
Jan 29 11:28:35.254: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:35.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7458" for this suite.

• [SLOW TEST:10.755 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":174,"skipped":2816,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jan 29 11:28:35.285: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 29 11:28:35.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:36.415: INFO: stderr: ""
Jan 29 11:28:36.415: INFO: stdout: "service/agnhost-replica created\n"
Jan 29 11:28:36.415: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 29 11:28:36.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:36.600: INFO: stderr: ""
Jan 29 11:28:36.600: INFO: stdout: "service/agnhost-primary created\n"
Jan 29 11:28:36.600: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 29 11:28:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:36.847: INFO: stderr: ""
Jan 29 11:28:36.847: INFO: stdout: "service/frontend created\n"
Jan 29 11:28:36.847: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 29 11:28:36.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:37.053: INFO: stderr: ""
Jan 29 11:28:37.053: INFO: stdout: "deployment.apps/frontend created\n"
Jan 29 11:28:37.053: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 11:28:37.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:37.235: INFO: stderr: ""
Jan 29 11:28:37.235: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 29 11:28:37.236: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 29 11:28:37.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 create -f -'
Jan 29 11:28:37.413: INFO: stderr: ""
Jan 29 11:28:37.413: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 29 11:28:37.413: INFO: Waiting for all frontend pods to be Running.
Jan 29 11:28:42.464: INFO: Waiting for frontend to serve content.
Jan 29 11:28:42.473: INFO: Trying to add a new entry to the guestbook.
Jan 29 11:28:42.488: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 29 11:28:42.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:42.610: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:42.610: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 11:28:42.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:42.691: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:42.691: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 11:28:42.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:42.800: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:42.800: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 11:28:42.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:42.896: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:42.896: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 11:28:42.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:43.007: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:43.008: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 29 11:28:43.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-7653 delete --grace-period=0 --force -f -'
Jan 29 11:28:43.120: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:28:43.120: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:43.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7653" for this suite.

• [SLOW TEST:7.871 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":175,"skipped":2817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jan 29 11:28:43.184: INFO: Waiting up to 5m0s for pod "client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc" in namespace "containers-4204" to be "Succeeded or Failed"
Jan 29 11:28:43.195: INFO: Pod "client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.744995ms
Jan 29 11:28:45.198: INFO: Pod "client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014057591s
STEP: Saw pod success
Jan 29 11:28:45.198: INFO: Pod "client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc" satisfied condition "Succeeded or Failed"
Jan 29 11:28:45.199: INFO: Trying to get logs from node 16fc05cd-9790-4b41-b13f-11160f9262cc pod client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc container test-container: <nil>
STEP: delete the pod
Jan 29 11:28:45.218: INFO: Waiting for pod client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc to disappear
Jan 29 11:28:45.224: INFO: Pod client-containers-9d2948a0-f19d-4fee-8263-6deef3c40adc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:45.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4204" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2839,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:45.230: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5598.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5598.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:28:49.286: INFO: DNS probes using dns-5598/dns-test-c872dee4-6a69-4c90-a4ad-8995a77d85c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:49.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5598" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":177,"skipped":2847,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:49.314: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:28:49.760: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:28:51.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516529, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516529, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516529, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747516529, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:28:54.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:28:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:55.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1481" for this suite.
STEP: Destroying namespace "webhook-1481-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.634 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":178,"skipped":2865,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:28:55.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3" in namespace "downward-api-5474" to be "Succeeded or Failed"
Jan 29 11:28:55.997: INFO: Pod "downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.169481ms
Jan 29 11:28:58.000: INFO: Pod "downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017256745s
STEP: Saw pod success
Jan 29 11:28:58.000: INFO: Pod "downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3" satisfied condition "Succeeded or Failed"
Jan 29 11:28:58.002: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3 container client-container: <nil>
STEP: delete the pod
Jan 29 11:28:58.025: INFO: Waiting for pod downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3 to disappear
Jan 29 11:28:58.033: INFO: Pod downwardapi-volume-e0691b97-2b61-434f-94e1-9e7264d0fdf3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:28:58.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5474" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2874,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:28:58.042: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:28:58.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9" in namespace "downward-api-6667" to be "Succeeded or Failed"
Jan 29 11:28:58.085: INFO: Pod "downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.578728ms
Jan 29 11:29:00.089: INFO: Pod "downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013943999s
STEP: Saw pod success
Jan 29 11:29:00.089: INFO: Pod "downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9" satisfied condition "Succeeded or Failed"
Jan 29 11:29:00.091: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9 container client-container: <nil>
STEP: delete the pod
Jan 29 11:29:00.103: INFO: Waiting for pod downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9 to disappear
Jan 29 11:29:00.110: INFO: Pod downwardapi-volume-82b423f5-6ca4-4a47-86cf-0ca2587369d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:00.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6667" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":180,"skipped":2883,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:00.116: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 29 11:29:02.659: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2013 pod-service-account-f6976f63-3692-4a57-bc07-1aeb9c6ec851 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 29 11:29:02.821: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2013 pod-service-account-f6976f63-3692-4a57-bc07-1aeb9c6ec851 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 29 11:29:02.965: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2013 pod-service-account-f6976f63-3692-4a57-bc07-1aeb9c6ec851 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:03.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2013" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":181,"skipped":2899,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-c7b239c2-112a-45b8-8496-0e18e3c650da
STEP: Creating a pod to test consume configMaps
Jan 29 11:29:03.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8" in namespace "projected-4356" to be "Succeeded or Failed"
Jan 29 11:29:03.197: INFO: Pod "pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.655697ms
Jan 29 11:29:05.200: INFO: Pod "pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012976334s
STEP: Saw pod success
Jan 29 11:29:05.200: INFO: Pod "pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8" satisfied condition "Succeeded or Failed"
Jan 29 11:29:05.202: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:29:05.225: INFO: Waiting for pod pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8 to disappear
Jan 29 11:29:05.227: INFO: Pod pod-projected-configmaps-1ca215fa-d511-4695-874e-a29762e7fff8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:05.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4356" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":2901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:05.234: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jan 29 11:29:05.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-5541 cluster-info'
Jan 29 11:29:05.325: INFO: stderr: ""
Jan 29 11:29:05.325: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.200.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:05.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5541" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":183,"skipped":2931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:05.331: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-1497988a-f636-44df-9a58-acde76e07564
STEP: Creating a pod to test consume configMaps
Jan 29 11:29:05.375: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5" in namespace "projected-5152" to be "Succeeded or Failed"
Jan 29 11:29:05.379: INFO: Pod "pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.946229ms
Jan 29 11:29:07.382: INFO: Pod "pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007142114s
STEP: Saw pod success
Jan 29 11:29:07.382: INFO: Pod "pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5" satisfied condition "Succeeded or Failed"
Jan 29 11:29:07.384: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:29:07.397: INFO: Waiting for pod pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5 to disappear
Jan 29 11:29:07.405: INFO: Pod pod-projected-configmaps-b6f0d105-2c23-40d0-bb51-857b043d36a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:07.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5152" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2957,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:07.410: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0129 11:29:08.073016      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 11:29:08.073033      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 11:29:08.073038      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 11:29:08.073: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:08.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6535" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":185,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:08.080: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 11:29:08.133: INFO: Number of nodes with available pods: 0
Jan 29 11:29:08.133: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:29:09.139: INFO: Number of nodes with available pods: 0
Jan 29 11:29:09.139: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:29:10.139: INFO: Number of nodes with available pods: 3
Jan 29 11:29:10.139: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 29 11:29:10.161: INFO: Number of nodes with available pods: 2
Jan 29 11:29:10.161: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:11.167: INFO: Number of nodes with available pods: 2
Jan 29 11:29:11.167: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:12.167: INFO: Number of nodes with available pods: 2
Jan 29 11:29:12.167: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:13.172: INFO: Number of nodes with available pods: 2
Jan 29 11:29:13.172: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:14.166: INFO: Number of nodes with available pods: 2
Jan 29 11:29:14.166: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:15.166: INFO: Number of nodes with available pods: 2
Jan 29 11:29:15.166: INFO: Node 7624137d-137a-468c-9e03-7a04d16d3fbf is running more than one daemon pod
Jan 29 11:29:16.168: INFO: Number of nodes with available pods: 3
Jan 29 11:29:16.168: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2708, will wait for the garbage collector to delete the pods
Jan 29 11:29:16.226: INFO: Deleting DaemonSet.extensions daemon-set took: 4.552526ms
Jan 29 11:29:16.726: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.192247ms
Jan 29 11:29:23.029: INFO: Number of nodes with available pods: 0
Jan 29 11:29:23.029: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 11:29:23.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2708/daemonsets","resourceVersion":"20605"},"items":null}

Jan 29 11:29:23.031: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2708/pods","resourceVersion":"20605"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:23.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2708" for this suite.

• [SLOW TEST:14.963 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":186,"skipped":2977,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:23.044: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 29 11:29:27.098: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 11:29:27.100: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 11:29:29.100: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 11:29:29.103: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 29 11:29:31.100: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 29 11:29:31.103: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5635" for this suite.

• [SLOW TEST:8.070 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":2983,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:29:31.492: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 29 11:29:31.493: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 29 11:29:31.493: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.493: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 29 11:29:31.493: INFO: Checking APIGroup: extensions
Jan 29 11:29:31.494: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jan 29 11:29:31.494: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jan 29 11:29:31.494: INFO: extensions/v1beta1 matches extensions/v1beta1
Jan 29 11:29:31.494: INFO: Checking APIGroup: apps
Jan 29 11:29:31.495: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 29 11:29:31.495: INFO: Versions found [{apps/v1 v1}]
Jan 29 11:29:31.495: INFO: apps/v1 matches apps/v1
Jan 29 11:29:31.495: INFO: Checking APIGroup: events.k8s.io
Jan 29 11:29:31.496: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 29 11:29:31.496: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.496: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 29 11:29:31.496: INFO: Checking APIGroup: authentication.k8s.io
Jan 29 11:29:31.497: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 29 11:29:31.497: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.497: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 29 11:29:31.497: INFO: Checking APIGroup: authorization.k8s.io
Jan 29 11:29:31.497: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 29 11:29:31.497: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.497: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 29 11:29:31.497: INFO: Checking APIGroup: autoscaling
Jan 29 11:29:31.498: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jan 29 11:29:31.498: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 29 11:29:31.498: INFO: autoscaling/v1 matches autoscaling/v1
Jan 29 11:29:31.498: INFO: Checking APIGroup: batch
Jan 29 11:29:31.499: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 29 11:29:31.499: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 29 11:29:31.499: INFO: batch/v1 matches batch/v1
Jan 29 11:29:31.499: INFO: Checking APIGroup: certificates.k8s.io
Jan 29 11:29:31.499: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 29 11:29:31.499: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.499: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 29 11:29:31.499: INFO: Checking APIGroup: networking.k8s.io
Jan 29 11:29:31.500: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 29 11:29:31.500: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.500: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 29 11:29:31.500: INFO: Checking APIGroup: policy
Jan 29 11:29:31.500: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jan 29 11:29:31.501: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jan 29 11:29:31.501: INFO: policy/v1beta1 matches policy/v1beta1
Jan 29 11:29:31.501: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 29 11:29:31.501: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 29 11:29:31.501: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.501: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 29 11:29:31.501: INFO: Checking APIGroup: storage.k8s.io
Jan 29 11:29:31.502: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 29 11:29:31.502: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.502: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 29 11:29:31.502: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 29 11:29:31.502: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 29 11:29:31.502: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.502: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 29 11:29:31.502: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 29 11:29:31.503: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 29 11:29:31.503: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.503: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 29 11:29:31.503: INFO: Checking APIGroup: scheduling.k8s.io
Jan 29 11:29:31.505: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 29 11:29:31.505: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.505: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 29 11:29:31.505: INFO: Checking APIGroup: coordination.k8s.io
Jan 29 11:29:31.506: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 29 11:29:31.506: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.506: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 29 11:29:31.506: INFO: Checking APIGroup: node.k8s.io
Jan 29 11:29:31.507: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jan 29 11:29:31.507: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.507: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jan 29 11:29:31.507: INFO: Checking APIGroup: discovery.k8s.io
Jan 29 11:29:31.507: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jan 29 11:29:31.507: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.507: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jan 29 11:29:31.507: INFO: Checking APIGroup: pksapi.io
Jan 29 11:29:31.508: INFO: PreferredVersion.GroupVersion: pksapi.io/v1beta1
Jan 29 11:29:31.508: INFO: Versions found [{pksapi.io/v1beta1 v1beta1}]
Jan 29 11:29:31.508: INFO: pksapi.io/v1beta1 matches pksapi.io/v1beta1
Jan 29 11:29:31.508: INFO: Checking APIGroup: metrics.k8s.io
Jan 29 11:29:31.509: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 29 11:29:31.509: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 29 11:29:31.509: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:31.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7166" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":188,"skipped":2996,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:31.515: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-3942702a-4d4d-4f6e-8473-ec9a6e1153e2
STEP: Creating a pod to test consume configMaps
Jan 29 11:29:31.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d" in namespace "configmap-4236" to be "Succeeded or Failed"
Jan 29 11:29:31.554: INFO: Pod "pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20403ms
Jan 29 11:29:33.557: INFO: Pod "pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007342391s
STEP: Saw pod success
Jan 29 11:29:33.557: INFO: Pod "pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d" satisfied condition "Succeeded or Failed"
Jan 29 11:29:33.559: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:29:33.573: INFO: Waiting for pod pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d to disappear
Jan 29 11:29:33.578: INFO: Pod pod-configmaps-6a277229-3620-4894-8fc5-ec725e33857d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:33.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4236" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3003,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:49.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9916" for this suite.

• [SLOW TEST:16.105 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":190,"skipped":3003,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:49.691: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:29:55.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4829" for this suite.
STEP: Destroying namespace "nsdeletetest-1341" for this suite.
Jan 29 11:29:55.794: INFO: Namespace nsdeletetest-1341 was already deleted
STEP: Destroying namespace "nsdeletetest-8556" for this suite.

• [SLOW TEST:6.106 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":191,"skipped":3015,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:29:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:29:55.817: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 11:29:57.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-499 --namespace=crd-publish-openapi-499 create -f -'
Jan 29 11:29:58.867: INFO: stderr: ""
Jan 29 11:29:58.867: INFO: stdout: "e2e-test-crd-publish-openapi-4574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 11:29:58.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-499 --namespace=crd-publish-openapi-499 delete e2e-test-crd-publish-openapi-4574-crds test-cr'
Jan 29 11:29:58.942: INFO: stderr: ""
Jan 29 11:29:58.942: INFO: stdout: "e2e-test-crd-publish-openapi-4574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 29 11:29:58.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-499 --namespace=crd-publish-openapi-499 apply -f -'
Jan 29 11:29:59.135: INFO: stderr: ""
Jan 29 11:29:59.135: INFO: stdout: "e2e-test-crd-publish-openapi-4574-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 29 11:29:59.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-499 --namespace=crd-publish-openapi-499 delete e2e-test-crd-publish-openapi-4574-crds test-cr'
Jan 29 11:29:59.208: INFO: stderr: ""
Jan 29 11:29:59.208: INFO: stdout: "e2e-test-crd-publish-openapi-4574-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 29 11:29:59.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-499 explain e2e-test-crd-publish-openapi-4574-crds'
Jan 29 11:29:59.386: INFO: stderr: ""
Jan 29 11:29:59.386: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4574-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:30:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-499" for this suite.

• [SLOW TEST:6.450 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":192,"skipped":3015,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:30:02.248: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4995/configmap-test-97deb79c-ea3d-43b9-bb5d-9818677275d4
STEP: Creating a pod to test consume configMaps
Jan 29 11:30:02.279: INFO: Waiting up to 5m0s for pod "pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0" in namespace "configmap-4995" to be "Succeeded or Failed"
Jan 29 11:30:02.282: INFO: Pod "pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472261ms
Jan 29 11:30:04.285: INFO: Pod "pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005896301s
STEP: Saw pod success
Jan 29 11:30:04.285: INFO: Pod "pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0" satisfied condition "Succeeded or Failed"
Jan 29 11:30:04.286: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0 container env-test: <nil>
STEP: delete the pod
Jan 29 11:30:04.299: INFO: Waiting for pod pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0 to disappear
Jan 29 11:30:04.305: INFO: Pod pod-configmaps-39e4d9e9-ba4b-4d63-a91c-f058643a71c0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:30:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4995" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3020,"failed":0}

------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:30:04.311: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:31:04.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-172" for this suite.

• [SLOW TEST:60.041 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":194,"skipped":3020,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:31:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 29 11:31:04.878: INFO: starting watch
STEP: patching
STEP: updating
Jan 29 11:31:04.885: INFO: waiting for watch events with expected annotations
Jan 29 11:31:04.885: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:31:04.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6414" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":195,"skipped":3029,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:31:04.963: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:31:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9900" for this suite.

• [SLOW TEST:11.060 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":196,"skipped":3042,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:31:16.023: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-bflj
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 11:31:16.056: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bflj" in namespace "subpath-7720" to be "Succeeded or Failed"
Jan 29 11:31:16.062: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.200898ms
Jan 29 11:31:18.065: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008783612s
Jan 29 11:31:20.068: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 4.011511255s
Jan 29 11:31:22.071: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 6.014289765s
Jan 29 11:31:24.073: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 8.016602951s
Jan 29 11:31:26.075: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 10.01924526s
Jan 29 11:31:28.078: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 12.021819536s
Jan 29 11:31:30.080: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 14.024087075s
Jan 29 11:31:32.083: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 16.027265408s
Jan 29 11:31:34.087: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 18.030329762s
Jan 29 11:31:36.089: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Running", Reason="", readiness=true. Elapsed: 20.032863089s
Jan 29 11:31:38.091: INFO: Pod "pod-subpath-test-projected-bflj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.035235672s
STEP: Saw pod success
Jan 29 11:31:38.092: INFO: Pod "pod-subpath-test-projected-bflj" satisfied condition "Succeeded or Failed"
Jan 29 11:31:38.093: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-subpath-test-projected-bflj container test-container-subpath-projected-bflj: <nil>
STEP: delete the pod
Jan 29 11:31:38.112: INFO: Waiting for pod pod-subpath-test-projected-bflj to disappear
Jan 29 11:31:38.120: INFO: Pod pod-subpath-test-projected-bflj no longer exists
STEP: Deleting pod pod-subpath-test-projected-bflj
Jan 29 11:31:38.120: INFO: Deleting pod "pod-subpath-test-projected-bflj" in namespace "subpath-7720"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:31:38.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7720" for this suite.

• [SLOW TEST:22.106 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":197,"skipped":3054,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:31:38.130: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:31:38.148: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 29 11:31:38.158: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 29 11:31:43.161: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 29 11:31:43.161: INFO: Creating deployment "test-rolling-update-deployment"
Jan 29 11:31:43.164: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 29 11:31:43.170: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 29 11:31:45.175: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 29 11:31:45.176: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jan 29 11:31:45.181: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5627 /apis/apps/v1/namespaces/deployment-5627/deployments/test-rolling-update-deployment d7543821-5694-4490-bd34-23230234f50d 21338 1 2021-01-29 11:31:43 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-01-29 11:31:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dc52b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-01-29 11:31:43 +0000 UTC,LastTransitionTime:2021-01-29 11:31:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-01-29 11:31:44 +0000 UTC,LastTransitionTime:2021-01-29 11:31:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 29 11:31:45.183: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-5627 /apis/apps/v1/namespaces/deployment-5627/replicasets/test-rolling-update-deployment-c4cb8d6d9 cfe9c9d2-9337-4c69-a2db-38e8aaff377e 21327 1 2021-01-29 11:31:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d7543821-5694-4490-bd34-23230234f50d 0xc006dc59c0 0xc006dc59c1}] []  [{kube-controller-manager Update apps/v1 2021-01-29 11:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7543821-5694-4490-bd34-23230234f50d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006dc5a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:31:45.183: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 29 11:31:45.183: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5627 /apis/apps/v1/namespaces/deployment-5627/replicasets/test-rolling-update-controller af0a5d56-4c2d-4db5-9cc6-7b9334e043a9 21337 2 2021-01-29 11:31:38 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d7543821-5694-4490-bd34-23230234f50d 0xc006dc5837 0xc006dc5838}] []  [{e2e.test Update apps/v1 2021-01-29 11:31:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-01-29 11:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7543821-5694-4490-bd34-23230234f50d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006dc5928 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 29 11:31:45.185: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-lpq9w" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-lpq9w test-rolling-update-deployment-c4cb8d6d9- deployment-5627 /api/v1/namespaces/deployment-5627/pods/test-rolling-update-deployment-c4cb8d6d9-lpq9w 28341c26-4244-4059-a766-2a80c510b0ea 21326 0 2021-01-29 11:31:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 cfe9c9d2-9337-4c69-a2db-38e8aaff377e 0xc006e20140 0xc006e20141}] []  [{kube-controller-manager Update v1 2021-01-29 11:31:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfe9c9d2-9337-4c69-a2db-38e8aaff377e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-01-29 11:31:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.24.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vrxpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vrxpl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vrxpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:c66ef63f-50cf-427f-af2a-b340450b0b53,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:31:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:31:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-01-29 11:31:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:30.0.0.11,PodIP:10.200.24.192,StartTime:2021-01-29 11:31:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-01-29 11:31:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://4b7a1333a9a6d2ac34e5d1d687987e2d59013f799add6a659dbc0dccc079f18b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.24.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:31:45.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5627" for this suite.

• [SLOW TEST:7.062 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":198,"skipped":3058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:31:45.193: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3891
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-3891
Jan 29 11:31:45.233: INFO: Found 0 stateful pods, waiting for 1
Jan 29 11:31:55.236: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 11:31:55.248: INFO: Deleting all statefulset in ns statefulset-3891
Jan 29 11:31:55.251: INFO: Scaling statefulset ss to 0
Jan 29 11:32:05.285: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:32:05.287: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:32:05.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3891" for this suite.

• [SLOW TEST:20.111 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":199,"skipped":3086,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:32:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jan 29 11:32:05.334: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 11:33:05.357: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jan 29 11:33:05.373: INFO: Created pod: pod0-sched-preemption-low-priority
Jan 29 11:33:05.394: INFO: Created pod: pod1-sched-preemption-medium-priority
Jan 29 11:33:05.415: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:33:19.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8884" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:74.170 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":200,"skipped":3095,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:33:19.476: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:33:19.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585" in namespace "projected-7347" to be "Succeeded or Failed"
Jan 29 11:33:19.516: INFO: Pod "downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585": Phase="Pending", Reason="", readiness=false. Elapsed: 3.76031ms
Jan 29 11:33:21.519: INFO: Pod "downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00665677s
STEP: Saw pod success
Jan 29 11:33:21.519: INFO: Pod "downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585" satisfied condition "Succeeded or Failed"
Jan 29 11:33:21.520: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585 container client-container: <nil>
STEP: delete the pod
Jan 29 11:33:21.541: INFO: Waiting for pod downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585 to disappear
Jan 29 11:33:21.548: INFO: Pod downwardapi-volume-84f11912-f5b7-4f95-b445-33fd3a617585 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:33:21.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7347" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":201,"skipped":3102,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:33:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:33:21.630: INFO: Waiting up to 5m0s for pod "downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10" in namespace "projected-7641" to be "Succeeded or Failed"
Jan 29 11:33:21.633: INFO: Pod "downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194014ms
Jan 29 11:33:23.636: INFO: Pod "downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005163566s
STEP: Saw pod success
Jan 29 11:33:23.636: INFO: Pod "downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10" satisfied condition "Succeeded or Failed"
Jan 29 11:33:23.638: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10 container client-container: <nil>
STEP: delete the pod
Jan 29 11:33:23.652: INFO: Waiting for pod downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10 to disappear
Jan 29 11:33:23.658: INFO: Pod downwardapi-volume-084b13a9-55ba-4ae6-a420-b73b3b399b10 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:33:23.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7641" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":202,"skipped":3108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:33:23.666: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7910
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jan 29 11:33:23.705: INFO: Found 0 stateful pods, waiting for 3
Jan 29 11:33:33.710: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:33:33.710: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:33:33.710: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Jan 29 11:33:33.730: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 29 11:33:43.755: INFO: Updating stateful set ss2
Jan 29 11:33:43.778: INFO: Waiting for Pod statefulset-7910/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
Jan 29 11:33:53.854: INFO: Found 2 stateful pods, waiting for 3
Jan 29 11:34:03.858: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:34:03.859: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:34:03.859: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 29 11:34:03.879: INFO: Updating stateful set ss2
Jan 29 11:34:03.886: INFO: Waiting for Pod statefulset-7910/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Jan 29 11:34:13.909: INFO: Updating stateful set ss2
Jan 29 11:34:13.920: INFO: Waiting for StatefulSet statefulset-7910/ss2 to complete update
Jan 29 11:34:13.920: INFO: Waiting for Pod statefulset-7910/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Jan 29 11:34:23.924: INFO: Waiting for StatefulSet statefulset-7910/ss2 to complete update
Jan 29 11:34:23.924: INFO: Waiting for Pod statefulset-7910/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 11:34:33.924: INFO: Deleting all statefulset in ns statefulset-7910
Jan 29 11:34:33.925: INFO: Scaling statefulset ss2 to 0
Jan 29 11:34:43.939: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:34:43.941: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:34:43.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7910" for this suite.

• [SLOW TEST:80.299 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":203,"skipped":3133,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:34:43.964: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jan 29 11:34:44.004: INFO: created test-pod-1
Jan 29 11:34:44.010: INFO: created test-pod-2
Jan 29 11:34:44.020: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:34:44.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8369" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":204,"skipped":3138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:34:44.111: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:34:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7993" for this suite.

• [SLOW TEST:8.037 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":205,"skipped":3174,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:34:52.149: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:34:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:34:54.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8767" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3175,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:34:54.213: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-cdd973ff-1ad4-40b5-a9a7-e35de186bf8c
STEP: Creating a pod to test consume configMaps
Jan 29 11:34:54.242: INFO: Waiting up to 5m0s for pod "pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4" in namespace "configmap-1506" to be "Succeeded or Failed"
Jan 29 11:34:54.246: INFO: Pod "pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818063ms
Jan 29 11:34:56.248: INFO: Pod "pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006407463s
STEP: Saw pod success
Jan 29 11:34:56.248: INFO: Pod "pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4" satisfied condition "Succeeded or Failed"
Jan 29 11:34:56.250: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:34:56.263: INFO: Waiting for pod pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4 to disappear
Jan 29 11:34:56.270: INFO: Pod pod-configmaps-dac6a62c-f185-4815-9d1b-f90750b221c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:34:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1506" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":207,"skipped":3180,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:34:56.277: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-8053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8053 to expose endpoints map[]
Jan 29 11:34:56.312: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 29 11:34:57.319: INFO: successfully validated that service multi-endpoint-test in namespace services-8053 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8053 to expose endpoints map[pod1:[100]]
Jan 29 11:34:59.342: INFO: successfully validated that service multi-endpoint-test in namespace services-8053 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8053 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 29 11:35:02.371: INFO: successfully validated that service multi-endpoint-test in namespace services-8053 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8053 to expose endpoints map[pod2:[101]]
Jan 29 11:35:02.391: INFO: successfully validated that service multi-endpoint-test in namespace services-8053 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8053
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8053 to expose endpoints map[]
Jan 29 11:35:03.415: INFO: successfully validated that service multi-endpoint-test in namespace services-8053 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:35:03.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8053" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.160 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":208,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:35:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:35:03.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94" in namespace "downward-api-473" to be "Succeeded or Failed"
Jan 29 11:35:03.476: INFO: Pod "downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94": Phase="Pending", Reason="", readiness=false. Elapsed: 5.827237ms
Jan 29 11:35:05.478: INFO: Pod "downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007606249s
Jan 29 11:35:07.481: INFO: Pod "downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010668802s
STEP: Saw pod success
Jan 29 11:35:07.481: INFO: Pod "downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94" satisfied condition "Succeeded or Failed"
Jan 29 11:35:07.483: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94 container client-container: <nil>
STEP: delete the pod
Jan 29 11:35:07.497: INFO: Waiting for pod downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94 to disappear
Jan 29 11:35:07.504: INFO: Pod downwardapi-volume-8e68be2d-1925-473e-8f16-07cf45810f94 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:35:07.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-473" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":209,"skipped":3214,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:35:07.511: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 11:35:07.543: INFO: Waiting up to 5m0s for pod "pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9" in namespace "emptydir-5993" to be "Succeeded or Failed"
Jan 29 11:35:07.548: INFO: Pod "pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.182611ms
Jan 29 11:35:09.550: INFO: Pod "pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00760673s
Jan 29 11:35:11.553: INFO: Pod "pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010433626s
STEP: Saw pod success
Jan 29 11:35:11.553: INFO: Pod "pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9" satisfied condition "Succeeded or Failed"
Jan 29 11:35:11.555: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9 container test-container: <nil>
STEP: delete the pod
Jan 29 11:35:11.569: INFO: Waiting for pod pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9 to disappear
Jan 29 11:35:11.575: INFO: Pod pod-f8b6e82e-23e0-4bc4-ada5-623bf254c2a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:35:11.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5993" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:35:11.585: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jan 29 11:35:11.609: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:35:14.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-492" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":211,"skipped":3281,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:35:14.737: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 29 11:35:14.775: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:35:16.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5146" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":212,"skipped":3284,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:35:16.808: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 29 11:35:16.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 11:35:16.841: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 11:35:16.845: INFO: 
Logging pods the apiserver thinks is on node 16fc05cd-9790-4b41-b13f-11160f9262cc before test
Jan 29 11:35:16.860: INFO: coredns-664d7f64c5-lwq6l from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:35:16.860: INFO: event-controller-85d6bb4d4c-qrl9c from pks-system started at 2021-01-29 10:32:14 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container event-controller ready: true, restart count 0
Jan 29 11:35:16.860: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:35:16.860: INFO: fluent-bit-82lt4 from pks-system started at 2021-01-29 10:32:33 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:35:16.860: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:35:16.860: INFO: node-exporter-hhk27 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:35:16.860: INFO: telegraf-t9hn6 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:35:16.860: INFO: wavefront-proxy-5bf7b49596-6vwv9 from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container wavefront-proxy ready: true, restart count 0
Jan 29 11:35:16.860: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:35:16.860: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:35:16.860: INFO: 
Logging pods the apiserver thinks is on node 7624137d-137a-468c-9e03-7a04d16d3fbf before test
Jan 29 11:35:16.878: INFO: coredns-664d7f64c5-8bv7k from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.878: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:35:16.878: INFO: fluent-bit-r4pxl from pks-system started at 2021-01-29 10:32:28 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.878: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:35:16.878: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:35:16.878: INFO: metric-controller-7f5cb8ff6d-cm2b2 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.878: INFO: 	Container metric-controller ready: true, restart count 0
Jan 29 11:35:16.878: INFO: node-exporter-kjb4q from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.878: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:35:16.878: INFO: observability-manager-6cf797f97-wshfj from pks-system started at 2021-01-29 10:32:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container observability-manager ready: true, restart count 0
Jan 29 11:35:16.879: INFO: telegraf-459jf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:35:16.879: INFO: telemetry-agent-8f56d9865-rt7qc from pks-system started at 2021-01-29 10:36:57 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jan 29 11:35:16.879: INFO: validator-69f557d7c6-hz8vq from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container validator ready: true, restart count 0
Jan 29 11:35:16.879: INFO: pod-release-vwzkz from replication-controller-5146 started at 2021-01-29 11:35:15 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container pod-release ready: false, restart count 0
Jan 29 11:35:16.879: INFO: sonobuoy-e2e-job-a3be688aee3e44de from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container e2e ready: true, restart count 0
Jan 29 11:35:16.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:35:16.879: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:35:16.879: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:35:16.879: INFO: 
Logging pods the apiserver thinks is on node c66ef63f-50cf-427f-af2a-b340450b0b53 before test
Jan 29 11:35:16.889: INFO: pod-init-fff9cc58-ba1e-4ebf-9a98-ac1f8f1d44cd from init-container-492 started at 2021-01-29 11:35:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container run1 ready: false, restart count 0
Jan 29 11:35:16.889: INFO: coredns-664d7f64c5-nqvrn from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:35:16.889: INFO: metrics-server-7d476fdfbd-n4b2n from kube-system started at 2021-01-29 10:32:08 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 11:35:16.889: INFO: cert-generator-e484f2435fc830429fc9747bd002fcda56dd053e-4lc5d from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container cert-generator ready: false, restart count 0
Jan 29 11:35:16.889: INFO: fluent-bit-xbbgf from pks-system started at 2021-01-29 10:32:16 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:35:16.889: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:35:16.889: INFO: node-exporter-bx9js from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:35:16.889: INFO: sink-controller-f6bc7f774-wn8vp from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container sink-controller ready: true, restart count 0
Jan 29 11:35:16.889: INFO: telegraf-w42nf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:35:16.889: INFO: wavefront-collector-7648f897c7-sb8kc from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container wavefront-collector ready: true, restart count 0
Jan 29 11:35:16.889: INFO: pod-logs-websocket-66a96d0a-b04e-4ea9-aed9-0c49c9c42d91 from pods-8767 started at 2021-01-29 11:34:52 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container main ready: true, restart count 0
Jan 29 11:35:16.889: INFO: pod-release-d29tv from replication-controller-5146 started at 2021-01-29 11:35:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container pod-release ready: true, restart count 0
Jan 29 11:35:16.889: INFO: sonobuoy from sonobuoy started at 2021-01-29 10:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 11:35:16.889: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:35:16.889: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:35:16.889: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d3fdb37c-ead0-4eff-a6ec-0bba8494a85a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-d3fdb37c-ead0-4eff-a6ec-0bba8494a85a off the node c66ef63f-50cf-427f-af2a-b340450b0b53
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d3fdb37c-ead0-4eff-a6ec-0bba8494a85a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:40:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-140" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.152 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":213,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:40:20.961: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:40:20.997: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:40:23.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:25.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:27.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:29.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:31.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:32.999: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:35.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:37.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:39.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:41.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:43.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:45.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = false)
Jan 29 11:40:47.000: INFO: The status of Pod test-webserver-47d8fc62-718f-41a3-a23c-17145d03239e is Running (Ready = true)
Jan 29 11:40:47.002: INFO: Container started at 2021-01-29 11:40:22 +0000 UTC, pod became ready at 2021-01-29 11:40:45 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:40:47.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-515" for this suite.

• [SLOW TEST:26.047 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:40:47.010: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:40:47.304: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 11:40:49.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747517247, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747517247, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747517247, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747517247, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:40:52.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:40:52.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8668" for this suite.
STEP: Destroying namespace "webhook-8668-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.375 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":215,"skipped":3337,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:40:52.385: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:40:52.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2" in namespace "downward-api-7162" to be "Succeeded or Failed"
Jan 29 11:40:52.425: INFO: Pod "downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.195928ms
Jan 29 11:40:54.428: INFO: Pod "downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00981145s
Jan 29 11:40:56.431: INFO: Pod "downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012697176s
STEP: Saw pod success
Jan 29 11:40:56.431: INFO: Pod "downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2" satisfied condition "Succeeded or Failed"
Jan 29 11:40:56.433: INFO: Trying to get logs from node 7624137d-137a-468c-9e03-7a04d16d3fbf pod downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2 container client-container: <nil>
STEP: delete the pod
Jan 29 11:40:56.460: INFO: Waiting for pod downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2 to disappear
Jan 29 11:40:56.468: INFO: Pod downwardapi-volume-baef9bff-0130-435b-821c-4004bf5030e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:40:56.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7162" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:40:56.474: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-975
STEP: creating service affinity-nodeport-transition in namespace services-975
STEP: creating replication controller affinity-nodeport-transition in namespace services-975
I0129 11:40:56.513880      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-975, replica count: 3
I0129 11:40:59.564139      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:40:59.571: INFO: Creating new exec pod
Jan 29 11:41:04.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jan 29 11:41:05.715: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 29 11:41:05.715: INFO: stdout: ""
Jan 29 11:41:05.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.170 80'
Jan 29 11:41:05.873: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.170 80\nConnection to 10.100.200.170 80 port [tcp/http] succeeded!\n"
Jan 29 11:41:05.873: INFO: stdout: ""
Jan 29 11:41:05.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30004'
Jan 29 11:41:06.031: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30004\nConnection to 30.0.0.11 30004 port [tcp/30004] succeeded!\n"
Jan 29 11:41:06.031: INFO: stdout: ""
Jan 29 11:41:06.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 30004'
Jan 29 11:41:06.178: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 30004\nConnection to 30.0.0.12 30004 port [tcp/30004] succeeded!\n"
Jan 29 11:41:06.178: INFO: stdout: ""
Jan 29 11:41:06.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30004'
Jan 29 11:41:06.358: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30004\nConnection to 30.0.0.11 30004 port [tcp/30004] succeeded!\n"
Jan 29 11:41:06.358: INFO: stdout: ""
Jan 29 11:41:06.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 30004'
Jan 29 11:41:06.508: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 30004\nConnection to 30.0.0.12 30004 port [tcp/30004] succeeded!\n"
Jan 29 11:41:06.508: INFO: stdout: ""
Jan 29 11:41:06.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.10:30004/ ; done'
Jan 29 11:41:06.762: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n"
Jan 29 11:41:06.762: INFO: stdout: "\naffinity-nodeport-transition-fhg47\naffinity-nodeport-transition-bqfq5\naffinity-nodeport-transition-fhg47\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-bqfq5\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-fhg47\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-bqfq5\naffinity-nodeport-transition-bqfq5\naffinity-nodeport-transition-fhg47\naffinity-nodeport-transition-fhg47\naffinity-nodeport-transition-bqfq5\naffinity-nodeport-transition-vjjhb"
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-fhg47
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-bqfq5
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-fhg47
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-bqfq5
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-fhg47
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-bqfq5
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-bqfq5
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-fhg47
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-fhg47
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-bqfq5
Jan 29 11:41:06.762: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-975 exec execpod-affinitybb5lm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.10:30004/ ; done'
Jan 29 11:41:07.015: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30004/\n"
Jan 29 11:41:07.015: INFO: stdout: "\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb\naffinity-nodeport-transition-vjjhb"
Jan 29 11:41:07.015: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.015: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.015: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.015: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.015: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Received response from host: affinity-nodeport-transition-vjjhb
Jan 29 11:41:07.016: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-975, will wait for the garbage collector to delete the pods
Jan 29 11:41:07.110: INFO: Deleting ReplicationController affinity-nodeport-transition took: 27.913931ms
Jan 29 11:41:07.110: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 27.401µs
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:41:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-975" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.564 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":217,"skipped":3384,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:41:19.039: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 29 11:43:19.588: INFO: Successfully updated pod "var-expansion-3c7ae502-fdac-4f2b-b680-64c220ec91d6"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 29 11:43:21.602: INFO: Deleting pod "var-expansion-3c7ae502-fdac-4f2b-b680-64c220ec91d6" in namespace "var-expansion-6531"
Jan 29 11:43:21.607: INFO: Wait up to 5m0s for pod "var-expansion-3c7ae502-fdac-4f2b-b680-64c220ec91d6" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:43:59.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6531" for this suite.

• [SLOW TEST:160.581 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":218,"skipped":3398,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:43:59.620: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 29 11:43:59.957: INFO: Pod name wrapped-volume-race-05f30031-69cf-4a78-b621-e70e2d775d79: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-05f30031-69cf-4a78-b621-e70e2d775d79 in namespace emptydir-wrapper-4383, will wait for the garbage collector to delete the pods
Jan 29 11:44:16.057: INFO: Deleting ReplicationController wrapped-volume-race-05f30031-69cf-4a78-b621-e70e2d775d79 took: 5.622635ms
Jan 29 11:44:16.557: INFO: Terminating ReplicationController wrapped-volume-race-05f30031-69cf-4a78-b621-e70e2d775d79 pods took: 500.121549ms
STEP: Creating RC which spawns configmap-volume pods
Jan 29 11:44:19.575: INFO: Pod name wrapped-volume-race-e4e12b8c-8c13-4462-8288-a5da73d2f2ad: Found 0 pods out of 5
Jan 29 11:44:24.582: INFO: Pod name wrapped-volume-race-e4e12b8c-8c13-4462-8288-a5da73d2f2ad: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e4e12b8c-8c13-4462-8288-a5da73d2f2ad in namespace emptydir-wrapper-4383, will wait for the garbage collector to delete the pods
Jan 29 11:44:34.657: INFO: Deleting ReplicationController wrapped-volume-race-e4e12b8c-8c13-4462-8288-a5da73d2f2ad took: 5.207365ms
Jan 29 11:44:34.758: INFO: Terminating ReplicationController wrapped-volume-race-e4e12b8c-8c13-4462-8288-a5da73d2f2ad pods took: 100.154515ms
STEP: Creating RC which spawns configmap-volume pods
Jan 29 11:44:49.074: INFO: Pod name wrapped-volume-race-1cc29483-dea6-4642-b279-6be372eb325e: Found 0 pods out of 5
Jan 29 11:44:54.080: INFO: Pod name wrapped-volume-race-1cc29483-dea6-4642-b279-6be372eb325e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1cc29483-dea6-4642-b279-6be372eb325e in namespace emptydir-wrapper-4383, will wait for the garbage collector to delete the pods
Jan 29 11:45:04.154: INFO: Deleting ReplicationController wrapped-volume-race-1cc29483-dea6-4642-b279-6be372eb325e took: 6.969843ms
Jan 29 11:45:04.655: INFO: Terminating ReplicationController wrapped-volume-race-1cc29483-dea6-4642-b279-6be372eb325e pods took: 500.131015ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:45:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4383" for this suite.

• [SLOW TEST:69.521 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":219,"skipped":3404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:45:09.142: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jan 29 11:45:09.164: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 29 11:45:09.174: INFO: Waiting for terminating namespaces to be deleted...
Jan 29 11:45:09.176: INFO: 
Logging pods the apiserver thinks is on node 16fc05cd-9790-4b41-b13f-11160f9262cc before test
Jan 29 11:45:09.183: INFO: coredns-664d7f64c5-lwq6l from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:45:09.183: INFO: event-controller-85d6bb4d4c-qrl9c from pks-system started at 2021-01-29 10:32:14 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container event-controller ready: true, restart count 0
Jan 29 11:45:09.183: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:45:09.183: INFO: fluent-bit-82lt4 from pks-system started at 2021-01-29 10:32:33 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:45:09.183: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:45:09.183: INFO: node-exporter-hhk27 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:45:09.183: INFO: telegraf-t9hn6 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:45:09.183: INFO: wavefront-proxy-5bf7b49596-6vwv9 from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container wavefront-proxy ready: true, restart count 0
Jan 29 11:45:09.183: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5 from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.183: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:45:09.183: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:45:09.183: INFO: 
Logging pods the apiserver thinks is on node 7624137d-137a-468c-9e03-7a04d16d3fbf before test
Jan 29 11:45:09.192: INFO: coredns-664d7f64c5-8bv7k from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:45:09.192: INFO: fluent-bit-r4pxl from pks-system started at 2021-01-29 10:32:28 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:45:09.192: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:45:09.192: INFO: metric-controller-7f5cb8ff6d-cm2b2 from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container metric-controller ready: true, restart count 0
Jan 29 11:45:09.192: INFO: node-exporter-kjb4q from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:45:09.192: INFO: observability-manager-6cf797f97-wshfj from pks-system started at 2021-01-29 10:32:11 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container observability-manager ready: true, restart count 0
Jan 29 11:45:09.192: INFO: telegraf-459jf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:45:09.192: INFO: telemetry-agent-8f56d9865-rt7qc from pks-system started at 2021-01-29 10:36:57 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container fluent-bit-telemetry ready: true, restart count 0
Jan 29 11:45:09.192: INFO: validator-69f557d7c6-hz8vq from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container validator ready: true, restart count 0
Jan 29 11:45:09.192: INFO: sonobuoy-e2e-job-a3be688aee3e44de from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container e2e ready: true, restart count 0
Jan 29 11:45:09.192: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:45:09.192: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-h5cks from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.192: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:45:09.192: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 29 11:45:09.192: INFO: 
Logging pods the apiserver thinks is on node c66ef63f-50cf-427f-af2a-b340450b0b53 before test
Jan 29 11:45:09.199: INFO: coredns-664d7f64c5-nqvrn from kube-system started at 2021-01-29 10:32:05 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container coredns ready: true, restart count 0
Jan 29 11:45:09.199: INFO: metrics-server-7d476fdfbd-n4b2n from kube-system started at 2021-01-29 10:32:08 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container metrics-server ready: true, restart count 0
Jan 29 11:45:09.199: INFO: cert-generator-e484f2435fc830429fc9747bd002fcda56dd053e-4lc5d from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container cert-generator ready: false, restart count 0
Jan 29 11:45:09.199: INFO: fluent-bit-xbbgf from pks-system started at 2021-01-29 10:32:16 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container fluent-bit ready: true, restart count 0
Jan 29 11:45:09.199: INFO: 	Container ghostunnel ready: true, restart count 0
Jan 29 11:45:09.199: INFO: node-exporter-bx9js from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Jan 29 11:45:09.199: INFO: sink-controller-f6bc7f774-wn8vp from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container sink-controller ready: true, restart count 0
Jan 29 11:45:09.199: INFO: telegraf-w42nf from pks-system started at 2021-01-29 10:32:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container telegraf ready: true, restart count 0
Jan 29 11:45:09.199: INFO: wavefront-collector-7648f897c7-sb8kc from pks-system started at 2021-01-29 10:34:14 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container wavefront-collector ready: true, restart count 0
Jan 29 11:45:09.199: INFO: sonobuoy from sonobuoy started at 2021-01-29 10:49:17 +0000 UTC (1 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 29 11:45:09.199: INFO: sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-9g2vj from sonobuoy started at 2021-01-29 10:49:22 +0000 UTC (2 container statuses recorded)
Jan 29 11:45:09.199: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 29 11:45:09.199: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-bed68fbc-e820-4b9d-a99c-44c593861873 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-bed68fbc-e820-4b9d-a99c-44c593861873 off the node c66ef63f-50cf-427f-af2a-b340450b0b53
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bed68fbc-e820-4b9d-a99c-44c593861873
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:45:17.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4639" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:8.138 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":220,"skipped":3430,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:45:17.281: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:45:17.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8569" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":221,"skipped":3435,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:45:17.365: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-9032
STEP: creating service affinity-nodeport in namespace services-9032
STEP: creating replication controller affinity-nodeport in namespace services-9032
I0129 11:45:17.406033      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-9032, replica count: 3
I0129 11:45:20.456309      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:45:20.462: INFO: Creating new exec pod
Jan 29 11:45:23.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jan 29 11:45:23.678: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 29 11:45:23.678: INFO: stdout: ""
Jan 29 11:45:23.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.185 80'
Jan 29 11:45:23.835: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.185 80\nConnection to 10.100.200.185 80 port [tcp/http] succeeded!\n"
Jan 29 11:45:23.835: INFO: stdout: ""
Jan 29 11:45:23.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 32037'
Jan 29 11:45:24.005: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 32037\nConnection to 30.0.0.10 32037 port [tcp/32037] succeeded!\n"
Jan 29 11:45:24.005: INFO: stdout: ""
Jan 29 11:45:24.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 32037'
Jan 29 11:45:24.168: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 32037\nConnection to 30.0.0.12 32037 port [tcp/32037] succeeded!\n"
Jan 29 11:45:24.168: INFO: stdout: ""
Jan 29 11:45:24.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 32037'
Jan 29 11:45:24.322: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 32037\nConnection to 30.0.0.10 32037 port [tcp/32037] succeeded!\n"
Jan 29 11:45:24.322: INFO: stdout: ""
Jan 29 11:45:24.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 32037'
Jan 29 11:45:24.476: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 32037\nConnection to 30.0.0.12 32037 port [tcp/32037] succeeded!\n"
Jan 29 11:45:24.476: INFO: stdout: ""
Jan 29 11:45:24.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-9032 exec execpod-affinity82mdq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.10:32037/ ; done'
Jan 29 11:45:24.710: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:32037/\n"
Jan 29 11:45:24.710: INFO: stdout: "\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5\naffinity-nodeport-754z5"
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Received response from host: affinity-nodeport-754z5
Jan 29 11:45:24.710: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9032, will wait for the garbage collector to delete the pods
Jan 29 11:45:24.779: INFO: Deleting ReplicationController affinity-nodeport took: 3.8593ms
Jan 29 11:45:25.279: INFO: Terminating ReplicationController affinity-nodeport pods took: 500.138962ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:45:38.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9032" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:21.633 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":222,"skipped":3447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:45:38.999: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5751, will wait for the garbage collector to delete the pods
Jan 29 11:45:43.095: INFO: Deleting Job.batch foo took: 4.279152ms
Jan 29 11:45:43.195: INFO: Terminating Job.batch foo pods took: 100.234775ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:18.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5751" for this suite.

• [SLOW TEST:39.504 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":223,"skipped":3478,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:18.504: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jan 29 11:46:18.522: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-3438 proxy --unix-socket=/tmp/kubectl-proxy-unix889971891/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:18.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3438" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":224,"skipped":3496,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:18.580: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 11:46:18.833: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 11:46:21.852: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:21.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-357" for this suite.
STEP: Destroying namespace "webhook-357-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":225,"skipped":3511,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 29 11:46:22.002: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4631 /api/v1/namespaces/watch-4631/configmaps/e2e-watch-test-resource-version 50fd582f-9e0d-4236-a56d-37fdb701a0fe 25891 0 2021-01-29 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-01-29 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 29 11:46:22.002: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4631 /api/v1/namespaces/watch-4631/configmaps/e2e-watch-test-resource-version 50fd582f-9e0d-4236-a56d-37fdb701a0fe 25892 0 2021-01-29 11:46:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-01-29 11:46:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:22.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4631" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":226,"skipped":3516,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:22.015: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 11:46:22.041: INFO: Waiting up to 5m0s for pod "pod-66e1e166-fa41-4c01-9492-8d70b29eaaee" in namespace "emptydir-4168" to be "Succeeded or Failed"
Jan 29 11:46:22.042: INFO: Pod "pod-66e1e166-fa41-4c01-9492-8d70b29eaaee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.303749ms
Jan 29 11:46:24.045: INFO: Pod "pod-66e1e166-fa41-4c01-9492-8d70b29eaaee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004101415s
STEP: Saw pod success
Jan 29 11:46:24.046: INFO: Pod "pod-66e1e166-fa41-4c01-9492-8d70b29eaaee" satisfied condition "Succeeded or Failed"
Jan 29 11:46:24.053: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-66e1e166-fa41-4c01-9492-8d70b29eaaee container test-container: <nil>
STEP: delete the pod
Jan 29 11:46:24.079: INFO: Waiting for pod pod-66e1e166-fa41-4c01-9492-8d70b29eaaee to disappear
Jan 29 11:46:24.085: INFO: Pod pod-66e1e166-fa41-4c01-9492-8d70b29eaaee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4168" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":227,"skipped":3529,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:24.092: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3007e1ac-0100-482b-88c4-ba4778ec9aee
STEP: Creating a pod to test consume secrets
Jan 29 11:46:24.166: INFO: Waiting up to 5m0s for pod "pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001" in namespace "secrets-5130" to be "Succeeded or Failed"
Jan 29 11:46:24.176: INFO: Pod "pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001": Phase="Pending", Reason="", readiness=false. Elapsed: 10.201094ms
Jan 29 11:46:26.179: INFO: Pod "pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013157803s
STEP: Saw pod success
Jan 29 11:46:26.179: INFO: Pod "pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001" satisfied condition "Succeeded or Failed"
Jan 29 11:46:26.181: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:46:26.192: INFO: Waiting for pod pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001 to disappear
Jan 29 11:46:26.200: INFO: Pod pod-secrets-62037b63-88a5-412a-9d87-e28e67e8c001 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:26.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5130" for this suite.
STEP: Destroying namespace "secret-namespace-4247" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3541,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:26.211: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:46:28.262: INFO: Waiting up to 5m0s for pod "client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b" in namespace "pods-3285" to be "Succeeded or Failed"
Jan 29 11:46:28.276: INFO: Pod "client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.766504ms
Jan 29 11:46:30.279: INFO: Pod "client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01667659s
STEP: Saw pod success
Jan 29 11:46:30.279: INFO: Pod "client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b" satisfied condition "Succeeded or Failed"
Jan 29 11:46:30.280: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b container env3cont: <nil>
STEP: delete the pod
Jan 29 11:46:30.296: INFO: Waiting for pod client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b to disappear
Jan 29 11:46:30.302: INFO: Pod client-envvars-0d57dfa3-34cf-42f8-a208-25023501ba0b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:46:30.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3285" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":229,"skipped":3554,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:46:30.307: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:48:30.351: INFO: Deleting pod "var-expansion-cc189f23-1233-4039-881d-006611d275fb" in namespace "var-expansion-8639"
Jan 29 11:48:30.354: INFO: Wait up to 5m0s for pod "var-expansion-cc189f23-1233-4039-881d-006611d275fb" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:48:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8639" for this suite.

• [SLOW TEST:130.059 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":230,"skipped":3558,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:48:40.366: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 11:48:42.403: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:48:42.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-33" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":231,"skipped":3568,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:48:42.425: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:48:44.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4008" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:48:44.489: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 29 11:48:44.533: INFO: Number of nodes with available pods: 0
Jan 29 11:48:44.534: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:48:45.539: INFO: Number of nodes with available pods: 0
Jan 29 11:48:45.539: INFO: Node 16fc05cd-9790-4b41-b13f-11160f9262cc is running more than one daemon pod
Jan 29 11:48:46.539: INFO: Number of nodes with available pods: 3
Jan 29 11:48:46.539: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 29 11:48:46.573: INFO: Number of nodes with available pods: 2
Jan 29 11:48:46.573: INFO: Node c66ef63f-50cf-427f-af2a-b340450b0b53 is running more than one daemon pod
Jan 29 11:48:47.579: INFO: Number of nodes with available pods: 2
Jan 29 11:48:47.579: INFO: Node c66ef63f-50cf-427f-af2a-b340450b0b53 is running more than one daemon pod
Jan 29 11:48:48.579: INFO: Number of nodes with available pods: 3
Jan 29 11:48:48.579: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5646, will wait for the garbage collector to delete the pods
Jan 29 11:48:48.639: INFO: Deleting DaemonSet.extensions daemon-set took: 4.552936ms
Jan 29 11:48:49.139: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.182911ms
Jan 29 11:48:58.941: INFO: Number of nodes with available pods: 0
Jan 29 11:48:58.941: INFO: Number of running nodes: 0, number of available pods: 0
Jan 29 11:48:58.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5646/daemonsets","resourceVersion":"26591"},"items":null}

Jan 29 11:48:58.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5646/pods","resourceVersion":"26591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:48:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5646" for this suite.

• [SLOW TEST:14.468 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":233,"skipped":3611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:48:58.959: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7678
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 29 11:48:58.985: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 29 11:48:59.029: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:49:01.032: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 29 11:49:03.033: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:49:05.032: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:49:07.033: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:49:09.032: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:49:11.032: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 29 11:49:13.032: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 29 11:49:13.036: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 29 11:49:15.039: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 29 11:49:17.039: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 29 11:49:19.038: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 29 11:49:19.041: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 29 11:49:23.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.248:8080/dial?request=hostname&protocol=udp&host=10.200.50.61&port=8081&tries=1'] Namespace:pod-network-test-7678 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:49:23.055: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:49:23.140: INFO: Waiting for responses: map[]
Jan 29 11:49:23.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.248:8080/dial?request=hostname&protocol=udp&host=10.200.95.76&port=8081&tries=1'] Namespace:pod-network-test-7678 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:49:23.143: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:49:23.220: INFO: Waiting for responses: map[]
Jan 29 11:49:23.222: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.24.248:8080/dial?request=hostname&protocol=udp&host=10.200.24.247&port=8081&tries=1'] Namespace:pod-network-test-7678 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:49:23.222: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:49:23.301: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7678" for this suite.

• [SLOW TEST:24.352 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":234,"skipped":3634,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:23.312: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-55d1f902-0d63-46c2-b938-05f25f574114
STEP: Creating secret with name secret-projected-all-test-volume-f2e74bee-f6c7-4cf2-8e9d-d236b3b5f63e
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 29 11:49:23.348: INFO: Waiting up to 5m0s for pod "projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782" in namespace "projected-9878" to be "Succeeded or Failed"
Jan 29 11:49:23.350: INFO: Pod "projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058964ms
Jan 29 11:49:25.353: INFO: Pod "projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005204619s
STEP: Saw pod success
Jan 29 11:49:25.353: INFO: Pod "projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782" satisfied condition "Succeeded or Failed"
Jan 29 11:49:25.355: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 29 11:49:25.369: INFO: Waiting for pod projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782 to disappear
Jan 29 11:49:25.376: INFO: Pod projected-volume-6e95fbdc-4461-4fa8-8f9d-5e112736b782 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:25.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9878" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3647,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:25.382: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5939.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5939.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5939.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5939.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5939.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5939.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:49:27.463: INFO: DNS probes using dns-5939/dns-test-ab5e48f3-4e35-4a01-a26b-13864ce30c5c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5939" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":236,"skipped":3661,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:27.528: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jan 29 11:49:27.559: INFO: created test-event-1
Jan 29 11:49:27.562: INFO: created test-event-2
Jan 29 11:49:27.564: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 29 11:49:27.566: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 29 11:49:27.588: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3540" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":237,"skipped":3662,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:27.604: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-dcb9a234-2505-4fb1-b859-63b04ca10d8b
STEP: Creating a pod to test consume configMaps
Jan 29 11:49:27.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb" in namespace "configmap-1675" to be "Succeeded or Failed"
Jan 29 11:49:27.643: INFO: Pod "pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.704847ms
Jan 29 11:49:29.646: INFO: Pod "pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005096178s
STEP: Saw pod success
Jan 29 11:49:29.646: INFO: Pod "pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb" satisfied condition "Succeeded or Failed"
Jan 29 11:49:29.648: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb container configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 11:49:29.660: INFO: Waiting for pod pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb to disappear
Jan 29 11:49:29.667: INFO: Pod pod-configmaps-b76c4f24-5bad-4c78-81a0-82fea89517bb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1675" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":238,"skipped":3666,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:29.673: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:49:29.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1" in namespace "projected-112" to be "Succeeded or Failed"
Jan 29 11:49:29.762: INFO: Pod "downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.7048ms
Jan 29 11:49:31.765: INFO: Pod "downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00724887s
STEP: Saw pod success
Jan 29 11:49:31.765: INFO: Pod "downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1" satisfied condition "Succeeded or Failed"
Jan 29 11:49:31.767: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1 container client-container: <nil>
STEP: delete the pod
Jan 29 11:49:31.778: INFO: Waiting for pod downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1 to disappear
Jan 29 11:49:31.785: INFO: Pod downwardapi-volume-52244660-ff78-49d8-970c-441cce49d8c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:31.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-112" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":239,"skipped":3681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:31.797: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 29 11:49:31.830: INFO: Waiting up to 5m0s for pod "pod-9ccfb52d-a168-41d7-81aa-a81857e5657a" in namespace "emptydir-2812" to be "Succeeded or Failed"
Jan 29 11:49:31.834: INFO: Pod "pod-9ccfb52d-a168-41d7-81aa-a81857e5657a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079876ms
Jan 29 11:49:33.837: INFO: Pod "pod-9ccfb52d-a168-41d7-81aa-a81857e5657a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006785132s
STEP: Saw pod success
Jan 29 11:49:33.837: INFO: Pod "pod-9ccfb52d-a168-41d7-81aa-a81857e5657a" satisfied condition "Succeeded or Failed"
Jan 29 11:49:33.839: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-9ccfb52d-a168-41d7-81aa-a81857e5657a container test-container: <nil>
STEP: delete the pod
Jan 29 11:49:33.850: INFO: Waiting for pod pod-9ccfb52d-a168-41d7-81aa-a81857e5657a to disappear
Jan 29 11:49:33.857: INFO: Pod pod-9ccfb52d-a168-41d7-81aa-a81857e5657a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:33.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2812" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3776,"failed":0}

------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:33.863: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 29 11:49:36.409: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb"
Jan 29 11:49:36.409: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb" in namespace "pods-6942" to be "terminated due to deadline exceeded"
Jan 29 11:49:36.411: INFO: Pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb": Phase="Running", Reason="", readiness=true. Elapsed: 1.735762ms
Jan 29 11:49:38.414: INFO: Pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004781937s
Jan 29 11:49:40.417: INFO: Pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007883965s
Jan 29 11:49:40.417: INFO: Pod "pod-update-activedeadlineseconds-ff27d261-a65d-4635-a066-7c664d29a0fb" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:49:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6942" for this suite.

• [SLOW TEST:6.560 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:49:40.424: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:49:44.473: INFO: DNS probes using dns-test-64cd1ee7-2cc8-4090-a852-d17837bc7bf4 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:49:48.512: INFO: File wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:48.515: INFO: File jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:48.515: INFO: Lookups using dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 failed for: [wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local]

Jan 29 11:49:53.518: INFO: File wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:53.521: INFO: File jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:53.521: INFO: Lookups using dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 failed for: [wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local]

Jan 29 11:49:58.518: INFO: File wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:58.520: INFO: File jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:49:58.520: INFO: Lookups using dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 failed for: [wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local]

Jan 29 11:50:03.518: INFO: File wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:50:03.521: INFO: File jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:50:03.521: INFO: Lookups using dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 failed for: [wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local]

Jan 29 11:50:08.518: INFO: File wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:50:08.521: INFO: File jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local from pod  dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 29 11:50:08.521: INFO: Lookups using dns-3830/dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 failed for: [wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local]

Jan 29 11:50:13.521: INFO: DNS probes using dns-test-75b6e40a-f4ad-4067-8cc2-b9306840fb10 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3830.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3830.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:50:15.589: INFO: DNS probes using dns-test-7d7b2540-6808-48a7-a966-8ffb85687ec0 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:50:15.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3830" for this suite.

• [SLOW TEST:35.200 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":242,"skipped":3805,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:50:15.626: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-2240
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2240 to expose endpoints map[]
Jan 29 11:50:15.677: INFO: successfully validated that service endpoint-test2 in namespace services-2240 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2240
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2240 to expose endpoints map[pod1:[80]]
Jan 29 11:50:17.703: INFO: successfully validated that service endpoint-test2 in namespace services-2240 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2240
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2240 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 29 11:50:19.725: INFO: successfully validated that service endpoint-test2 in namespace services-2240 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2240
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2240 to expose endpoints map[pod2:[80]]
Jan 29 11:50:19.751: INFO: successfully validated that service endpoint-test2 in namespace services-2240 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2240
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2240 to expose endpoints map[]
Jan 29 11:50:20.773: INFO: successfully validated that service endpoint-test2 in namespace services-2240 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:50:20.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2240" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.175 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":243,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:50:20.804: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jan 29 11:50:20.839: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 11:51:20.860: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 11:51:20.862: INFO: Starting informer...
STEP: Starting pods...
Jan 29 11:51:21.074: INFO: Pod1 is running on c66ef63f-50cf-427f-af2a-b340450b0b53. Tainting Node
Jan 29 11:51:23.288: INFO: Pod2 is running on c66ef63f-50cf-427f-af2a-b340450b0b53. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 29 11:51:38.426: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 29 11:51:50.485: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:51:50.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9825" for this suite.

• [SLOW TEST:89.725 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":244,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:51:50.531: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 29 11:51:55.616: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:51:55.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8119" for this suite.

• [SLOW TEST:5.122 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3900,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:51:55.654: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-41679f66-eba8-4c27-a186-cf528c8e5068 in namespace container-probe-6183
Jan 29 11:51:57.705: INFO: Started pod liveness-41679f66-eba8-4c27-a186-cf528c8e5068 in namespace container-probe-6183
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 11:51:57.707: INFO: Initial restart count of pod liveness-41679f66-eba8-4c27-a186-cf528c8e5068 is 0
Jan 29 11:52:15.732: INFO: Restart count of pod container-probe-6183/liveness-41679f66-eba8-4c27-a186-cf528c8e5068 is now 1 (18.024655512s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:15.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6183" for this suite.

• [SLOW TEST:20.098 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":3911,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:15.753: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0129 11:52:25.863173      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 11:52:25.863337      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 11:52:25.863376      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 11:52:25.863: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 29 11:52:25.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hp4l" in namespace "gc-5365"
Jan 29 11:52:25.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-92tx8" in namespace "gc-5365"
Jan 29 11:52:25.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6tmh" in namespace "gc-5365"
Jan 29 11:52:25.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhmp5" in namespace "gc-5365"
Jan 29 11:52:25.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmmgb" in namespace "gc-5365"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:25.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5365" for this suite.

• [SLOW TEST:10.212 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":247,"skipped":3932,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:25.965: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-a0f921a8-35f6-4014-ab89-cae25f76ae7b
STEP: Creating a pod to test consume secrets
Jan 29 11:52:26.000: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d" in namespace "projected-1445" to be "Succeeded or Failed"
Jan 29 11:52:26.002: INFO: Pod "pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368517ms
Jan 29 11:52:28.005: INFO: Pod "pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005307314s
STEP: Saw pod success
Jan 29 11:52:28.005: INFO: Pod "pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d" satisfied condition "Succeeded or Failed"
Jan 29 11:52:28.007: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:52:28.028: INFO: Waiting for pod pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d to disappear
Jan 29 11:52:28.038: INFO: Pod pod-projected-secrets-74ff6c45-b393-4391-8151-28508804ef2d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:28.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1445" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":3933,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:28.045: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 29 11:52:32.090: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.090: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.160: INFO: Exec stderr: ""
Jan 29 11:52:32.160: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.160: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.232: INFO: Exec stderr: ""
Jan 29 11:52:32.232: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.232: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.303: INFO: Exec stderr: ""
Jan 29 11:52:32.303: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.303: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.376: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 29 11:52:32.376: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.376: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.451: INFO: Exec stderr: ""
Jan 29 11:52:32.452: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.452: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.531: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 29 11:52:32.531: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.531: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.614: INFO: Exec stderr: ""
Jan 29 11:52:32.614: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.695: INFO: Exec stderr: ""
Jan 29 11:52:32.695: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.695: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.772: INFO: Exec stderr: ""
Jan 29 11:52:32.772: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8296 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:52:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
Jan 29 11:52:32.848: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:32.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8296" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":3941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:32.855: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 29 11:52:32.895: INFO: starting watch
STEP: patching
STEP: updating
Jan 29 11:52:32.900: INFO: waiting for watch events with expected annotations
Jan 29 11:52:32.900: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:32.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6256" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":250,"skipped":3969,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:45.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-642" for this suite.

• [SLOW TEST:13.082 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":251,"skipped":3970,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:46.001: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 29 11:52:46.030: INFO: Waiting up to 5m0s for pod "pod-452b1569-26d3-4fd1-880a-43f0116c9bfe" in namespace "emptydir-5018" to be "Succeeded or Failed"
Jan 29 11:52:46.036: INFO: Pod "pod-452b1569-26d3-4fd1-880a-43f0116c9bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.130605ms
Jan 29 11:52:48.039: INFO: Pod "pod-452b1569-26d3-4fd1-880a-43f0116c9bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008971489s
STEP: Saw pod success
Jan 29 11:52:48.039: INFO: Pod "pod-452b1569-26d3-4fd1-880a-43f0116c9bfe" satisfied condition "Succeeded or Failed"
Jan 29 11:52:48.041: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-452b1569-26d3-4fd1-880a-43f0116c9bfe container test-container: <nil>
STEP: delete the pod
Jan 29 11:52:48.054: INFO: Waiting for pod pod-452b1569-26d3-4fd1-880a-43f0116c9bfe to disappear
Jan 29 11:52:48.061: INFO: Pod pod-452b1569-26d3-4fd1-880a-43f0116c9bfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:48.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5018" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":252,"skipped":3975,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9993" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":253,"skipped":3977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:50.137: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jan 29 11:52:50.176: INFO: Waiting up to 5m0s for pod "var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0" in namespace "var-expansion-4669" to be "Succeeded or Failed"
Jan 29 11:52:50.186: INFO: Pod "var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.572138ms
Jan 29 11:52:52.189: INFO: Pod "var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0": Phase="Running", Reason="", readiness=true. Elapsed: 2.01253255s
Jan 29 11:52:54.191: INFO: Pod "var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014910093s
STEP: Saw pod success
Jan 29 11:52:54.191: INFO: Pod "var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0" satisfied condition "Succeeded or Failed"
Jan 29 11:52:54.193: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0 container dapi-container: <nil>
STEP: delete the pod
Jan 29 11:52:54.207: INFO: Waiting for pod var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0 to disappear
Jan 29 11:52:54.213: INFO: Pod var-expansion-6e8eb9ac-5215-4509-a09c-857076b3fba0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:52:54.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4669" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":254,"skipped":4019,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:52:54.219: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8850
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jan 29 11:52:54.266: INFO: Found 0 stateful pods, waiting for 3
Jan 29 11:53:04.269: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:53:04.269: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:53:04.269: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 29 11:53:04.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-8850 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:53:05.390: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:53:05.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:53:05.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Jan 29 11:53:15.421: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 29 11:53:25.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-8850 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:53:25.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:53:25.623: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:53:25.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:53:35.636: INFO: Waiting for StatefulSet statefulset-8850/ss2 to complete update
Jan 29 11:53:35.636: INFO: Waiting for Pod statefulset-8850/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Jan 29 11:53:45.641: INFO: Waiting for StatefulSet statefulset-8850/ss2 to complete update
Jan 29 11:53:45.641: INFO: Waiting for Pod statefulset-8850/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Rolling back to a previous revision
Jan 29 11:53:55.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-8850 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 29 11:53:55.799: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 29 11:53:55.799: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 29 11:53:55.799: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 29 11:54:05.825: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 29 11:54:15.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=statefulset-8850 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 29 11:54:15.985: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 29 11:54:15.985: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 29 11:54:15.985: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 29 11:54:25.997: INFO: Waiting for StatefulSet statefulset-8850/ss2 to complete update
Jan 29 11:54:25.997: INFO: Waiting for Pod statefulset-8850/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Jan 29 11:54:25.997: INFO: Waiting for Pod statefulset-8850/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Jan 29 11:54:25.997: INFO: Waiting for Pod statefulset-8850/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Jan 29 11:54:36.003: INFO: Waiting for StatefulSet statefulset-8850/ss2 to complete update
Jan 29 11:54:36.003: INFO: Waiting for Pod statefulset-8850/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Jan 29 11:54:36.003: INFO: Waiting for Pod statefulset-8850/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Jan 29 11:54:46.002: INFO: Waiting for StatefulSet statefulset-8850/ss2 to complete update
Jan 29 11:54:46.002: INFO: Waiting for Pod statefulset-8850/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 11:54:56.003: INFO: Deleting all statefulset in ns statefulset-8850
Jan 29 11:54:56.005: INFO: Scaling statefulset ss2 to 0
Jan 29 11:55:16.018: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 11:55:16.020: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:55:16.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8850" for this suite.

• [SLOW TEST:141.824 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":255,"skipped":4023,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:55:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 in namespace container-probe-1303
Jan 29 11:55:18.077: INFO: Started pod liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 in namespace container-probe-1303
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 11:55:18.079: INFO: Initial restart count of pod liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is 0
Jan 29 11:55:34.102: INFO: Restart count of pod container-probe-1303/liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is now 1 (16.022347862s elapsed)
Jan 29 11:55:52.125: INFO: Restart count of pod container-probe-1303/liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is now 2 (34.045674936s elapsed)
Jan 29 11:56:14.152: INFO: Restart count of pod container-probe-1303/liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is now 3 (56.072187281s elapsed)
Jan 29 11:56:34.175: INFO: Restart count of pod container-probe-1303/liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is now 4 (1m16.095737009s elapsed)
Jan 29 11:57:34.246: INFO: Restart count of pod container-probe-1303/liveness-9082bcf8-86b9-4cd8-aad8-07a6fcfba4d9 is now 5 (2m16.166100629s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:57:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1303" for this suite.

• [SLOW TEST:138.222 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4035,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:57:34.265: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 11:57:34.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e" in namespace "downward-api-4514" to be "Succeeded or Failed"
Jan 29 11:57:34.303: INFO: Pod "downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.46232ms
Jan 29 11:57:36.306: INFO: Pod "downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011149062s
STEP: Saw pod success
Jan 29 11:57:36.306: INFO: Pod "downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e" satisfied condition "Succeeded or Failed"
Jan 29 11:57:36.308: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e container client-container: <nil>
STEP: delete the pod
Jan 29 11:57:36.329: INFO: Waiting for pod downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e to disappear
Jan 29 11:57:36.335: INFO: Pod downwardapi-volume-e941941f-b85e-4ce5-9982-7b481bc8e90e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:57:36.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4514" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4037,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:57:36.342: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 29 11:57:36.375: INFO: Waiting up to 5m0s for pod "pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a" in namespace "emptydir-7025" to be "Succeeded or Failed"
Jan 29 11:57:36.379: INFO: Pod "pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.51263ms
Jan 29 11:57:38.382: INFO: Pod "pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007164625s
STEP: Saw pod success
Jan 29 11:57:38.382: INFO: Pod "pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a" satisfied condition "Succeeded or Failed"
Jan 29 11:57:38.384: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a container test-container: <nil>
STEP: delete the pod
Jan 29 11:57:38.398: INFO: Waiting for pod pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a to disappear
Jan 29 11:57:38.405: INFO: Pod pod-2f24689d-cf69-42d8-b1b5-95a13effaa3a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:57:38.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7025" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:57:38.412: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1993.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1993.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 252.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.252_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1993.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1993.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 252.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.252_udp@PTR;check="$$(dig +tcp +noall +answer +search 252.200.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.200.252_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 29 11:57:42.495: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.497: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.520: INFO: Unable to read jessie_udp@dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.522: INFO: Unable to read jessie_tcp@dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.523: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.525: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:42.536: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_udp@dns-test-service.dns-1993.svc.cluster.local jessie_tcp@dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:57:47.545: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:47.547: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:47.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:47.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:47.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:47.578: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:57:52.545: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:52.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:52.576: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:52.578: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:52.592: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:57:57.544: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:57.546: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:57.562: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:57.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:57:57.575: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:58:02.546: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:02.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:02.566: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:02.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:02.582: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:58:07.545: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:07.547: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:07.566: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:07.569: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local from pod dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679: the server could not find the requested resource (get pods dns-test-47adfe94-0099-462a-9711-005916074679)
Jan 29 11:58:07.582: INFO: Lookups using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1993.svc.cluster.local]

Jan 29 11:58:12.579: INFO: DNS probes using dns-1993/dns-test-47adfe94-0099-462a-9711-005916074679 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:12.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1993" for this suite.

• [SLOW TEST:34.248 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":259,"skipped":4078,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:12.661: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-0b345c8a-e53d-4b7a-a33f-e2465d5095ba
STEP: Creating a pod to test consume secrets
Jan 29 11:58:12.706: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a" in namespace "projected-4357" to be "Succeeded or Failed"
Jan 29 11:58:12.713: INFO: Pod "pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.53672ms
Jan 29 11:58:14.715: INFO: Pod "pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009755106s
STEP: Saw pod success
Jan 29 11:58:14.716: INFO: Pod "pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a" satisfied condition "Succeeded or Failed"
Jan 29 11:58:14.717: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 11:58:14.740: INFO: Waiting for pod pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a to disappear
Jan 29 11:58:14.742: INFO: Pod pod-projected-secrets-5e40b7e6-9a37-44bb-8480-34720df5fc5a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:14.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4357" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:14.751: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2600
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2600
I0129 11:58:14.801195      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2600, replica count: 2
I0129 11:58:17.851549      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:58:17.851: INFO: Creating new exec pod
Jan 29 11:58:20.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-2600 exec execpodjdwl5 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 29 11:58:21.031: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 29 11:58:21.031: INFO: stdout: ""
Jan 29 11:58:21.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-2600 exec execpodjdwl5 -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.118 80'
Jan 29 11:58:21.186: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.118 80\nConnection to 10.100.200.118 80 port [tcp/http] succeeded!\n"
Jan 29 11:58:21.187: INFO: stdout: ""
Jan 29 11:58:21.187: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:21.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2600" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.458 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":261,"skipped":4125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:21.209: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jan 29 11:58:21.232: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:25.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5123" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":262,"skipped":4154,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:25.134: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0129 11:58:31.193389      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0129 11:58:31.193522      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0129 11:58:31.193544      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jan 29 11:58:31.193: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:31.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-767" for this suite.

• [SLOW TEST:6.066 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":263,"skipped":4160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:31.200: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Jan 29 11:58:31.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 create -f -'
Jan 29 11:58:31.475: INFO: stderr: ""
Jan 29 11:58:31.475: INFO: stdout: "pod/pause created\n"
Jan 29 11:58:31.475: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 29 11:58:31.476: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9057" to be "running and ready"
Jan 29 11:58:31.482: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.208477ms
Jan 29 11:58:33.484: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008662393s
Jan 29 11:58:33.484: INFO: Pod "pause" satisfied condition "running and ready"
Jan 29 11:58:33.484: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 29 11:58:33.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 label pods pause testing-label=testing-label-value'
Jan 29 11:58:33.559: INFO: stderr: ""
Jan 29 11:58:33.559: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 29 11:58:33.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 get pod pause -L testing-label'
Jan 29 11:58:33.625: INFO: stderr: ""
Jan 29 11:58:33.625: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 29 11:58:33.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 label pods pause testing-label-'
Jan 29 11:58:33.695: INFO: stderr: ""
Jan 29 11:58:33.695: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 29 11:58:33.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 get pod pause -L testing-label'
Jan 29 11:58:33.763: INFO: stderr: ""
Jan 29 11:58:33.763: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Jan 29 11:58:33.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 delete --grace-period=0 --force -f -'
Jan 29 11:58:33.847: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 11:58:33.847: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 29 11:58:33.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 get rc,svc -l name=pause --no-headers'
Jan 29 11:58:33.916: INFO: stderr: "No resources found in kubectl-9057 namespace.\n"
Jan 29 11:58:33.916: INFO: stdout: ""
Jan 29 11:58:33.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9057 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 11:58:33.982: INFO: stderr: ""
Jan 29 11:58:33.983: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:33.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9057" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":264,"skipped":4183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:33.989: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 29 11:58:34.029: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 29 11:58:34.032: INFO: starting watch
STEP: patching
STEP: updating
Jan 29 11:58:34.039: INFO: waiting for watch events with expected annotations
Jan 29 11:58:34.039: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:34.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7117" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":265,"skipped":4216,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:34.073: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 29 11:58:34.109: INFO: Waiting up to 5m0s for pod "pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6" in namespace "emptydir-4687" to be "Succeeded or Failed"
Jan 29 11:58:34.119: INFO: Pod "pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.669199ms
Jan 29 11:58:36.132: INFO: Pod "pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022965882s
STEP: Saw pod success
Jan 29 11:58:36.132: INFO: Pod "pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6" satisfied condition "Succeeded or Failed"
Jan 29 11:58:36.134: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6 container test-container: <nil>
STEP: delete the pod
Jan 29 11:58:36.148: INFO: Waiting for pod pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6 to disappear
Jan 29 11:58:36.153: INFO: Pod pod-f07c56f8-2ae0-4036-9ea6-17f4ffbe91b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:58:36.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4687" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4219,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:58:36.160: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5747
Jan 29 11:58:38.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 29 11:58:38.353: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 29 11:58:38.353: INFO: stdout: "iptables"
Jan 29 11:58:38.353: INFO: proxyMode: iptables
Jan 29 11:58:38.358: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:38.360: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:40.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:40.363: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:42.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:42.363: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:44.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:44.363: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:46.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:46.363: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:48.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:48.363: INFO: Pod kube-proxy-mode-detector still exists
Jan 29 11:58:50.360: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 29 11:58:50.362: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5747
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5747
I0129 11:58:50.375272      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5747, replica count: 3
I0129 11:58:53.425492      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 11:58:53.432: INFO: Creating new exec pod
Jan 29 11:58:56.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jan 29 11:58:56.601: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 29 11:58:56.601: INFO: stdout: ""
Jan 29 11:58:56.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.171 80'
Jan 29 11:58:56.742: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.171 80\nConnection to 10.100.200.171 80 port [tcp/http] succeeded!\n"
Jan 29 11:58:56.742: INFO: stdout: ""
Jan 29 11:58:56.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30568'
Jan 29 11:58:56.896: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30568\nConnection to 30.0.0.11 30568 port [tcp/30568] succeeded!\n"
Jan 29 11:58:56.896: INFO: stdout: ""
Jan 29 11:58:56.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30568'
Jan 29 11:58:57.077: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30568\nConnection to 30.0.0.10 30568 port [tcp/30568] succeeded!\n"
Jan 29 11:58:57.077: INFO: stdout: ""
Jan 29 11:58:57.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.11 30568'
Jan 29 11:58:57.218: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.11 30568\nConnection to 30.0.0.11 30568 port [tcp/30568] succeeded!\n"
Jan 29 11:58:57.218: INFO: stdout: ""
Jan 29 11:58:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 30568'
Jan 29 11:58:57.369: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 30568\nConnection to 30.0.0.10 30568 port [tcp/30568] succeeded!\n"
Jan 29 11:58:57.369: INFO: stdout: ""
Jan 29 11:58:57.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://30.0.0.10:30568/ ; done'
Jan 29 11:58:57.609: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n+ echo\n+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n"
Jan 29 11:58:57.609: INFO: stdout: "\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn\naffinity-nodeport-timeout-2d8bn"
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Received response from host: affinity-nodeport-timeout-2d8bn
Jan 29 11:58:57.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.10:30568/'
Jan 29 11:58:57.758: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n"
Jan 29 11:58:57.758: INFO: stdout: "affinity-nodeport-timeout-2d8bn"
Jan 29 11:59:12.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5747 exec execpod-affinity8r9sx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://30.0.0.10:30568/'
Jan 29 11:59:12.911: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://30.0.0.10:30568/\n"
Jan 29 11:59:12.911: INFO: stdout: "affinity-nodeport-timeout-424l5"
Jan 29 11:59:12.911: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5747, will wait for the garbage collector to delete the pods
Jan 29 11:59:12.991: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.66723ms
Jan 29 11:59:13.491: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 500.121881ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:59:28.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5747" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:52.357 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":267,"skipped":4226,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:59:28.517: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jan 29 11:59:28.546: INFO: Waiting up to 5m0s for pod "client-containers-292baadd-afa6-485d-9c45-cf9380cd8081" in namespace "containers-5711" to be "Succeeded or Failed"
Jan 29 11:59:28.549: INFO: Pod "client-containers-292baadd-afa6-485d-9c45-cf9380cd8081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349871ms
Jan 29 11:59:30.551: INFO: Pod "client-containers-292baadd-afa6-485d-9c45-cf9380cd8081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004985688s
STEP: Saw pod success
Jan 29 11:59:30.551: INFO: Pod "client-containers-292baadd-afa6-485d-9c45-cf9380cd8081" satisfied condition "Succeeded or Failed"
Jan 29 11:59:30.553: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod client-containers-292baadd-afa6-485d-9c45-cf9380cd8081 container test-container: <nil>
STEP: delete the pod
Jan 29 11:59:30.567: INFO: Waiting for pod client-containers-292baadd-afa6-485d-9c45-cf9380cd8081 to disappear
Jan 29 11:59:30.572: INFO: Pod client-containers-292baadd-afa6-485d-9c45-cf9380cd8081 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:59:30.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5711" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":268,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:59:30.580: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:59:30.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9465" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":269,"skipped":4271,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:59:30.614: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-adfa5edb-f7ab-49ed-a7e8-5f7a996119e1
STEP: Creating a pod to test consume secrets
Jan 29 11:59:30.648: INFO: Waiting up to 5m0s for pod "pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8" in namespace "secrets-5597" to be "Succeeded or Failed"
Jan 29 11:59:30.650: INFO: Pod "pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.467779ms
Jan 29 11:59:32.652: INFO: Pod "pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004055547s
STEP: Saw pod success
Jan 29 11:59:32.652: INFO: Pod "pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8" satisfied condition "Succeeded or Failed"
Jan 29 11:59:32.654: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8 container secret-env-test: <nil>
STEP: delete the pod
Jan 29 11:59:32.666: INFO: Waiting for pod pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8 to disappear
Jan 29 11:59:32.673: INFO: Pod pod-secrets-91bdbdb5-6202-4f1f-a0da-be5a9034edf8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:59:32.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5597" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4279,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:59:32.689: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jan 29 11:59:35.240: INFO: Successfully updated pod "labelsupdatefa2a5270-1fea-4d53-b44d-038267f0fe4f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 11:59:39.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3538" for this suite.

• [SLOW TEST:6.574 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4285,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 11:59:39.264: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 29 11:59:41.299: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3413 PodName:var-expansion-1f047b77-ec2a-46d1-8546-8b008c092c2b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:59:41.299: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: test for file in mounted path
Jan 29 11:59:41.383: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3413 PodName:var-expansion-1f047b77-ec2a-46d1-8546-8b008c092c2b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 29 11:59:41.383: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: updating the annotation value
Jan 29 11:59:41.972: INFO: Successfully updated pod "var-expansion-1f047b77-ec2a-46d1-8546-8b008c092c2b"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 29 11:59:41.975: INFO: Deleting pod "var-expansion-1f047b77-ec2a-46d1-8546-8b008c092c2b" in namespace "var-expansion-3413"
Jan 29 11:59:41.978: INFO: Wait up to 5m0s for pod "var-expansion-1f047b77-ec2a-46d1-8546-8b008c092c2b" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:00:15.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3413" for this suite.

• [SLOW TEST:36.727 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":272,"skipped":4297,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:00:15.991: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jan 29 12:00:16.009: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 29 12:01:16.021: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:01:16.023: INFO: Starting informer...
STEP: Starting pod...
Jan 29 12:01:16.232: INFO: Pod is running on c66ef63f-50cf-427f-af2a-b340450b0b53. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 29 12:01:16.254: INFO: Pod wasn't evicted. Proceeding
Jan 29 12:01:16.254: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 29 12:02:31.297: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:02:31.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3893" for this suite.

• [SLOW TEST:135.315 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":273,"skipped":4301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:02:31.308: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-736f6aad-8ebd-4443-9596-2f9dbfed1bd5
STEP: Creating a pod to test consume configMaps
Jan 29 12:02:31.350: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3" in namespace "projected-4509" to be "Succeeded or Failed"
Jan 29 12:02:31.361: INFO: Pod "pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.175464ms
Jan 29 12:02:33.364: INFO: Pod "pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014069419s
STEP: Saw pod success
Jan 29 12:02:33.364: INFO: Pod "pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3" satisfied condition "Succeeded or Failed"
Jan 29 12:02:33.366: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 29 12:02:33.385: INFO: Waiting for pod pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3 to disappear
Jan 29 12:02:33.391: INFO: Pod pod-projected-configmaps-dedb119a-2666-495b-a993-58f3745e7ad3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:02:33.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4509" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:02:33.397: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-4488
STEP: creating replication controller nodeport-test in namespace services-4488
I0129 12:02:33.439252      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4488, replica count: 2
I0129 12:02:36.489560      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 29 12:02:36.489: INFO: Creating new exec pod
Jan 29 12:02:39.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jan 29 12:02:39.671: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 29 12:02:39.671: INFO: stdout: ""
Jan 29 12:02:39.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 10.100.200.163 80'
Jan 29 12:02:39.816: INFO: stderr: "+ nc -zv -t -w 2 10.100.200.163 80\nConnection to 10.100.200.163 80 port [tcp/http] succeeded!\n"
Jan 29 12:02:39.817: INFO: stdout: ""
Jan 29 12:02:39.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31745'
Jan 29 12:02:39.997: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31745\nConnection to 30.0.0.12 31745 port [tcp/31745] succeeded!\n"
Jan 29 12:02:39.997: INFO: stdout: ""
Jan 29 12:02:39.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31745'
Jan 29 12:02:40.149: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31745\nConnection to 30.0.0.10 31745 port [tcp/31745] succeeded!\n"
Jan 29 12:02:40.149: INFO: stdout: ""
Jan 29 12:02:40.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.12 31745'
Jan 29 12:02:40.293: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.12 31745\nConnection to 30.0.0.12 31745 port [tcp/31745] succeeded!\n"
Jan 29 12:02:40.293: INFO: stdout: ""
Jan 29 12:02:40.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4488 exec execpodmwxkp -- /bin/sh -x -c nc -zv -t -w 2 30.0.0.10 31745'
Jan 29 12:02:40.435: INFO: stderr: "+ nc -zv -t -w 2 30.0.0.10 31745\nConnection to 30.0.0.10 31745 port [tcp/31745] succeeded!\n"
Jan 29 12:02:40.435: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:02:40.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4488" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.045 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":275,"skipped":4360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:02:40.443: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:02:56.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1176" for this suite.

• [SLOW TEST:16.052 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":276,"skipped":4478,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:02:56.495: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 12:02:56.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0" in namespace "projected-4969" to be "Succeeded or Failed"
Jan 29 12:02:56.540: INFO: Pod "downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.413314ms
Jan 29 12:02:58.542: INFO: Pod "downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014038534s
STEP: Saw pod success
Jan 29 12:02:58.542: INFO: Pod "downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0" satisfied condition "Succeeded or Failed"
Jan 29 12:02:58.544: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0 container client-container: <nil>
STEP: delete the pod
Jan 29 12:02:58.563: INFO: Waiting for pod downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0 to disappear
Jan 29 12:02:58.570: INFO: Pod downwardapi-volume-0b3f7bdf-d51e-4c3e-bbcf-ce02dbd44ab0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:02:58.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4969" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4482,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:02:58.576: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jan 29 12:03:01.126: INFO: Successfully updated pod "annotationupdatec7ad4e49-b682-4456-827c-1b1b1467df5b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:05.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7698" for this suite.

• [SLOW TEST:6.576 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":278,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:05.153: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4c7d603b-e5ab-4709-a424-48b02effd58b
STEP: Creating a pod to test consume secrets
Jan 29 12:03:05.186: INFO: Waiting up to 5m0s for pod "pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6" in namespace "secrets-8516" to be "Succeeded or Failed"
Jan 29 12:03:05.193: INFO: Pod "pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.991105ms
Jan 29 12:03:07.196: INFO: Pod "pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010113246s
STEP: Saw pod success
Jan 29 12:03:07.196: INFO: Pod "pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6" satisfied condition "Succeeded or Failed"
Jan 29 12:03:07.198: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6 container secret-volume-test: <nil>
STEP: delete the pod
Jan 29 12:03:07.213: INFO: Waiting for pod pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6 to disappear
Jan 29 12:03:07.220: INFO: Pod pod-secrets-6496a6ed-07fc-4953-b1d5-01f12c3ee8e6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:07.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8516" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4508,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:07.226: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jan 29 12:03:07.255: INFO: Waiting up to 5m0s for pod "downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94" in namespace "downward-api-191" to be "Succeeded or Failed"
Jan 29 12:03:07.265: INFO: Pod "downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94": Phase="Pending", Reason="", readiness=false. Elapsed: 10.327424ms
Jan 29 12:03:09.268: INFO: Pod "downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013046372s
STEP: Saw pod success
Jan 29 12:03:09.268: INFO: Pod "downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94" satisfied condition "Succeeded or Failed"
Jan 29 12:03:09.269: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94 container dapi-container: <nil>
STEP: delete the pod
Jan 29 12:03:09.283: INFO: Waiting for pod downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94 to disappear
Jan 29 12:03:09.288: INFO: Pod downward-api-41482773-d6d9-4cc9-a52d-2163a4276e94 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:09.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-191" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":280,"skipped":4508,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:03:09.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 create -f -'
Jan 29 12:03:10.468: INFO: stderr: ""
Jan 29 12:03:10.468: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 29 12:03:10.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 create -f -'
Jan 29 12:03:10.643: INFO: stderr: ""
Jan 29 12:03:10.643: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 29 12:03:11.646: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:03:11.646: INFO: Found 0 / 1
Jan 29 12:03:12.646: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:03:12.646: INFO: Found 1 / 1
Jan 29 12:03:12.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 29 12:03:12.648: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:03:12.648: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 12:03:12.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 describe pod agnhost-primary-nwr4x'
Jan 29 12:03:12.730: INFO: stderr: ""
Jan 29 12:03:12.730: INFO: stdout: "Name:         agnhost-primary-nwr4x\nNamespace:    kubectl-6946\nPriority:     0\nNode:         c66ef63f-50cf-427f-af2a-b340450b0b53/30.0.0.11\nStart Time:   Fri, 29 Jan 2021 12:03:10 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.200.24.72\nIPs:\n  IP:           10.200.24.72\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e926a3200968cb3e7670b2b19f90c5485e92a8eb51fccde0e8d47a6bc264e7d2\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 29 Jan 2021 12:03:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-222h7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-222h7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-222h7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6946/agnhost-primary-nwr4x to c66ef63f-50cf-427f-af2a-b340450b0b53\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 29 12:03:12.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 describe rc agnhost-primary'
Jan 29 12:03:12.818: INFO: stderr: ""
Jan 29 12:03:12.818: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6946\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-nwr4x\n"
Jan 29 12:03:12.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 describe service agnhost-primary'
Jan 29 12:03:12.902: INFO: stderr: ""
Jan 29 12:03:12.902: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6946\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.100.200.106\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.200.24.72:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 29 12:03:12.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 describe node 16fc05cd-9790-4b41-b13f-11160f9262cc'
Jan 29 12:03:13.017: INFO: stderr: ""
Jan 29 12:03:13.017: INFO: stdout: "Name:               16fc05cd-9790-4b41-b13f-11160f9262cc\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    bosh.id=5d565aae-ebe2-45ef-9d4c-b4d4b73928ac\n                    bosh.zone=shepherd-238034-210129-011318-az\n                    failure-domain.beta.kubernetes.io/zone=shepherd-238034-210129-011318-az\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=30.0.0.10\n                    kubernetes.io/os=linux\n                    pks-system/cluster.name=9c6232a5-248a-434a-bf1e-5da8bffd3511-internal\n                    pks-system/cluster.uuid=service-instance_3411e976-fe8e-450c-bd01-b5d3b1cdf361\n                    spec.ip=30.0.0.10\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 29 Jan 2021 10:27:03 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  16fc05cd-9790-4b41-b13f-11160f9262cc\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 29 Jan 2021 12:03:05 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 29 Jan 2021 11:58:14 +0000   Fri, 29 Jan 2021 10:27:03 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 29 Jan 2021 11:58:14 +0000   Fri, 29 Jan 2021 10:27:03 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 29 Jan 2021 11:58:14 +0000   Fri, 29 Jan 2021 10:27:03 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 29 Jan 2021 11:58:14 +0000   Fri, 29 Jan 2021 10:27:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  30.0.0.10\n  InternalIP:  30.0.0.10\n  Hostname:    30.0.0.10\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      32894832Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 4039716Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    1500m\n  ephemeral-storage:      29242135298\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 2479140Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 62925436d86a248e9b533483efca593b\n  System UUID:                4220698E-02B9-1990-2D22-995BE6AD2A7B\n  Boot ID:                    fd0f5cd3-b834-4188-becd-73f16fe39b7e\n  Kernel Version:             4.15.0-132-generic\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.19.6+vmware.1\n  Kube-Proxy Version:         v1.19.6+vmware.1\nProviderID:                   vsphere://4220698e-02b9-1990-2d22-995be6ad2a7b\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-664d7f64c5-lwq6l                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (7%)     91m\n  kube-system                 coredns-664d7f64c5-mtx9d                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (7%)     11m\n  kube-system                 metrics-server-7d476fdfbd-vgd7x                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pks-system                  event-controller-85d6bb4d4c-qrl9c                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  pks-system                  fluent-bit-2rfw8                                           0 (0%)        0 (0%)      100Mi (4%)       100Mi (4%)     11m\n  pks-system                  node-exporter-hhk27                                        10m (0%)      10m (0%)    50Mi (2%)        50Mi (2%)      90m\n  pks-system                  sink-controller-f6bc7f774-td5m8                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m\n  pks-system                  telegraf-t9hn6                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  pks-system                  wavefront-proxy-5bf7b49596-6vwv9                           0 (0%)        0 (0%)      1500M (59%)      1500M (59%)    88m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9b09f49ddbef4d0b-hvjw5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests          Limits\n  --------               --------          ------\n  cpu                    210m (14%)        10m (0%)\n  memory                 1804087040 (71%)  2013802240 (79%)\n  ephemeral-storage      0 (0%)            0 (0%)\n  hugepages-1Gi          0 (0%)            0 (0%)\n  hugepages-2Mi          0 (0%)            0 (0%)\n  scheduling.k8s.io/foo  0                 0\nEvents:                  <none>\n"
Jan 29 12:03:13.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-6946 describe namespace kubectl-6946'
Jan 29 12:03:13.087: INFO: stderr: ""
Jan 29 12:03:13.087: INFO: stdout: "Name:         kubectl-6946\nLabels:       e2e-framework=kubectl\n              e2e-run=ec637448-0d54-4e57-8fe0-5c916fc150a3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:13.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6946" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":281,"skipped":4510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:13.094: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 29 12:03:13.126: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 29 12:03:13.131: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 29 12:03:13.131: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 29 12:03:13.144: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 29 12:03:13.145: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 29 12:03:13.157: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 29 12:03:13.157: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 29 12:03:20.185: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:20.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7130" for this suite.

• [SLOW TEST:7.110 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":282,"skipped":4532,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:20.204: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d
Jan 29 12:03:20.237: INFO: Pod name my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d: Found 0 pods out of 1
Jan 29 12:03:25.239: INFO: Pod name my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d: Found 1 pods out of 1
Jan 29 12:03:25.239: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d" are running
Jan 29 12:03:25.241: INFO: Pod "my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d-5sjf6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 12:03:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 12:03:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 12:03:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-01-29 12:03:20 +0000 UTC Reason: Message:}])
Jan 29 12:03:25.241: INFO: Trying to dial the pod
Jan 29 12:03:30.248: INFO: Controller my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d: Got expected result from replica 1 [my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d-5sjf6]: "my-hostname-basic-ff6101ec-4da1-4844-9be4-563be02f962d-5sjf6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:30.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4099" for this suite.

• [SLOW TEST:10.051 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":283,"skipped":4540,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:30.255: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jan 29 12:03:30.281: INFO: Waiting up to 5m0s for pod "var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3" in namespace "var-expansion-7957" to be "Succeeded or Failed"
Jan 29 12:03:30.283: INFO: Pod "var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391501ms
Jan 29 12:03:32.286: INFO: Pod "var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004337147s
STEP: Saw pod success
Jan 29 12:03:32.286: INFO: Pod "var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3" satisfied condition "Succeeded or Failed"
Jan 29 12:03:32.288: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3 container dapi-container: <nil>
STEP: delete the pod
Jan 29 12:03:32.302: INFO: Waiting for pod var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3 to disappear
Jan 29 12:03:32.307: INFO: Pod var-expansion-1d0986d3-2a3c-40e0-961c-d554b393c8d3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:32.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7957" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:32.314: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 29 12:03:32.340: INFO: Waiting up to 5m0s for pod "pod-eadeb492-010c-459d-9206-9fa4451aafa2" in namespace "emptydir-3355" to be "Succeeded or Failed"
Jan 29 12:03:32.344: INFO: Pod "pod-eadeb492-010c-459d-9206-9fa4451aafa2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194697ms
Jan 29 12:03:34.346: INFO: Pod "pod-eadeb492-010c-459d-9206-9fa4451aafa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005970496s
STEP: Saw pod success
Jan 29 12:03:34.347: INFO: Pod "pod-eadeb492-010c-459d-9206-9fa4451aafa2" satisfied condition "Succeeded or Failed"
Jan 29 12:03:34.348: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-eadeb492-010c-459d-9206-9fa4451aafa2 container test-container: <nil>
STEP: delete the pod
Jan 29 12:03:34.361: INFO: Waiting for pod pod-eadeb492-010c-459d-9206-9fa4451aafa2 to disappear
Jan 29 12:03:34.368: INFO: Pod pod-eadeb492-010c-459d-9206-9fa4451aafa2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:34.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3355" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4573,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:34.374: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 29 12:03:34.658: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 29 12:03:36.664: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518614, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518614, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518614, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518614, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 12:03:39.671: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:03:39.673: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:40.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4108" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.542 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":286,"skipped":4573,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:40.917: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-54d5acad-b0dc-4727-aa01-18bff38f7c48
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-54d5acad-b0dc-4727-aa01-18bff38f7c48
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:44.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6611" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4588,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:44.989: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jan 29 12:03:45.019: INFO: Waiting up to 5m0s for pod "downward-api-007a43b9-3760-4738-b988-abdc54b51abf" in namespace "downward-api-7657" to be "Succeeded or Failed"
Jan 29 12:03:45.030: INFO: Pod "downward-api-007a43b9-3760-4738-b988-abdc54b51abf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.855798ms
Jan 29 12:03:47.033: INFO: Pod "downward-api-007a43b9-3760-4738-b988-abdc54b51abf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013792763s
STEP: Saw pod success
Jan 29 12:03:47.033: INFO: Pod "downward-api-007a43b9-3760-4738-b988-abdc54b51abf" satisfied condition "Succeeded or Failed"
Jan 29 12:03:47.035: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downward-api-007a43b9-3760-4738-b988-abdc54b51abf container dapi-container: <nil>
STEP: delete the pod
Jan 29 12:03:47.047: INFO: Waiting for pod downward-api-007a43b9-3760-4738-b988-abdc54b51abf to disappear
Jan 29 12:03:47.054: INFO: Pod downward-api-007a43b9-3760-4738-b988-abdc54b51abf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:03:47.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7657" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":288,"skipped":4588,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:03:47.060: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7538
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7538
STEP: Creating statefulset with conflicting port in namespace statefulset-7538
STEP: Waiting until pod test-pod will start running in namespace statefulset-7538
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7538
Jan 29 12:03:51.137: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: fd1d1deb-81b4-4527-94cc-280a76a00008, status phase: Pending. Waiting for statefulset controller to delete.
Jan 29 12:03:51.710: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: fd1d1deb-81b4-4527-94cc-280a76a00008, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 12:03:51.715: INFO: Observed stateful pod in namespace: statefulset-7538, name: ss-0, uid: fd1d1deb-81b4-4527-94cc-280a76a00008, status phase: Failed. Waiting for statefulset controller to delete.
Jan 29 12:03:51.722: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7538
STEP: Removing pod with conflicting port in namespace statefulset-7538
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7538 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jan 29 12:03:55.764: INFO: Deleting all statefulset in ns statefulset-7538
Jan 29 12:03:55.767: INFO: Scaling statefulset ss to 0
Jan 29 12:04:05.779: INFO: Waiting for statefulset status.replicas updated to 0
Jan 29 12:04:05.781: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:04:05.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7538" for this suite.

• [SLOW TEST:18.743 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":289,"skipped":4606,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:04:05.805: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jan 29 12:04:05.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02" in namespace "downward-api-8716" to be "Succeeded or Failed"
Jan 29 12:04:05.890: INFO: Pod "downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02": Phase="Pending", Reason="", readiness=false. Elapsed: 6.93713ms
Jan 29 12:04:07.893: INFO: Pod "downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009520699s
Jan 29 12:04:09.895: INFO: Pod "downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012137525s
STEP: Saw pod success
Jan 29 12:04:09.895: INFO: Pod "downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02" satisfied condition "Succeeded or Failed"
Jan 29 12:04:09.897: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02 container client-container: <nil>
STEP: delete the pod
Jan 29 12:04:09.911: INFO: Waiting for pod downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02 to disappear
Jan 29 12:04:09.917: INFO: Pod downwardapi-volume-65fe5762-4fce-4328-a4fa-086496567c02 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:04:09.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8716" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4608,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:04:09.925: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-b1bea889-e794-42b4-877d-7652774d0d41
STEP: Creating configMap with name cm-test-opt-upd-01665594-fe97-4c84-84cb-16a576c1e6cf
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b1bea889-e794-42b4-877d-7652774d0d41
STEP: Updating configmap cm-test-opt-upd-01665594-fe97-4c84-84cb-16a576c1e6cf
STEP: Creating configMap with name cm-test-opt-create-cf5a9820-9bda-4de1-ae51-b7d28e87ee30
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:05:32.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4228" for this suite.

• [SLOW TEST:82.373 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:05:32.302: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:05:32.321: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Creating first CR 
Jan 29 12:05:32.879: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:32Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:05:32Z]] name:name1 resourceVersion:32595 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94b8f918-9ec8-4f19-9877-b3046b63ebe5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 29 12:05:42.883: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:05:42Z]] name:name2 resourceVersion:32647 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:124d6f3a-47f8-46bb-8e7b-aceef72dbce9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 29 12:05:52.888: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:05:52Z]] name:name1 resourceVersion:32681 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94b8f918-9ec8-4f19-9877-b3046b63ebe5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 29 12:06:02.894: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:06:02Z]] name:name2 resourceVersion:32707 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:124d6f3a-47f8-46bb-8e7b-aceef72dbce9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 29 12:06:12.899: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:32Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:05:52Z]] name:name1 resourceVersion:32733 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:94b8f918-9ec8-4f19-9877-b3046b63ebe5] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 29 12:06:22.905: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-01-29T12:05:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-01-29T12:06:02Z]] name:name2 resourceVersion:32758 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:124d6f3a-47f8-46bb-8e7b-aceef72dbce9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:06:33.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5018" for this suite.

• [SLOW TEST:61.120 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":292,"skipped":4680,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:06:33.424: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:06:33.442: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 29 12:06:36.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-583 --namespace=crd-publish-openapi-583 create -f -'
Jan 29 12:06:37.634: INFO: stderr: ""
Jan 29 12:06:37.634: INFO: stdout: "e2e-test-crd-publish-openapi-7369-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 12:06:37.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-583 --namespace=crd-publish-openapi-583 delete e2e-test-crd-publish-openapi-7369-crds test-cr'
Jan 29 12:06:37.704: INFO: stderr: ""
Jan 29 12:06:37.704: INFO: stdout: "e2e-test-crd-publish-openapi-7369-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 29 12:06:37.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-583 --namespace=crd-publish-openapi-583 apply -f -'
Jan 29 12:06:37.884: INFO: stderr: ""
Jan 29 12:06:37.884: INFO: stdout: "e2e-test-crd-publish-openapi-7369-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 29 12:06:37.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-583 --namespace=crd-publish-openapi-583 delete e2e-test-crd-publish-openapi-7369-crds test-cr'
Jan 29 12:06:37.960: INFO: stderr: ""
Jan 29 12:06:37.960: INFO: stdout: "e2e-test-crd-publish-openapi-7369-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 29 12:06:37.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=crd-publish-openapi-583 explain e2e-test-crd-publish-openapi-7369-crds'
Jan 29 12:06:38.141: INFO: stderr: ""
Jan 29 12:06:38.141: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7369-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:06:41.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-583" for this suite.

• [SLOW TEST:7.589 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":293,"skipped":4698,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:06:41.013: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jan 29 12:06:41.039: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:06:55.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-332" for this suite.

• [SLOW TEST:14.067 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":294,"skipped":4738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:06:55.080: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 12:06:55.671: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 12:06:58.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:06:58.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8171" for this suite.
STEP: Destroying namespace "webhook-8171-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":295,"skipped":4762,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:06:58.765: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jan 29 12:06:58.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 create -f -'
Jan 29 12:06:59.047: INFO: stderr: ""
Jan 29 12:06:59.047: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 29 12:06:59.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 12:06:59.116: INFO: stderr: ""
Jan 29 12:06:59.116: INFO: stdout: "update-demo-nautilus-hcwcw update-demo-nautilus-rnd8q "
Jan 29 12:06:59.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods update-demo-nautilus-hcwcw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 12:06:59.181: INFO: stderr: ""
Jan 29 12:06:59.181: INFO: stdout: ""
Jan 29 12:06:59.181: INFO: update-demo-nautilus-hcwcw is created but not running
Jan 29 12:07:04.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 29 12:07:04.257: INFO: stderr: ""
Jan 29 12:07:04.257: INFO: stdout: "update-demo-nautilus-hcwcw update-demo-nautilus-rnd8q "
Jan 29 12:07:04.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods update-demo-nautilus-hcwcw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 12:07:04.331: INFO: stderr: ""
Jan 29 12:07:04.332: INFO: stdout: "true"
Jan 29 12:07:04.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods update-demo-nautilus-hcwcw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 12:07:04.405: INFO: stderr: ""
Jan 29 12:07:04.405: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 12:07:04.405: INFO: validating pod update-demo-nautilus-hcwcw
Jan 29 12:07:04.408: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 12:07:04.408: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 12:07:04.408: INFO: update-demo-nautilus-hcwcw is verified up and running
Jan 29 12:07:04.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods update-demo-nautilus-rnd8q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 29 12:07:04.472: INFO: stderr: ""
Jan 29 12:07:04.472: INFO: stdout: "true"
Jan 29 12:07:04.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods update-demo-nautilus-rnd8q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 29 12:07:04.539: INFO: stderr: ""
Jan 29 12:07:04.539: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 29 12:07:04.539: INFO: validating pod update-demo-nautilus-rnd8q
Jan 29 12:07:04.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 29 12:07:04.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 29 12:07:04.542: INFO: update-demo-nautilus-rnd8q is verified up and running
STEP: using delete to clean up resources
Jan 29 12:07:04.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 delete --grace-period=0 --force -f -'
Jan 29 12:07:04.615: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 29 12:07:04.615: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 29 12:07:04.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get rc,svc -l name=update-demo --no-headers'
Jan 29 12:07:04.689: INFO: stderr: "No resources found in kubectl-8884 namespace.\n"
Jan 29 12:07:04.689: INFO: stdout: ""
Jan 29 12:07:04.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8884 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 29 12:07:04.765: INFO: stderr: ""
Jan 29 12:07:04.765: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:07:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8884" for this suite.

• [SLOW TEST:6.007 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":296,"skipped":4776,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:07:04.772: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5681
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5681
STEP: creating replication controller externalsvc in namespace services-5681
I0129 12:07:04.834476      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5681, replica count: 2
I0129 12:07:07.884728      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 29 12:07:07.897: INFO: Creating new exec pod
Jan 29 12:07:09.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-5681 exec execpod7gjvq -- /bin/sh -x -c nslookup nodeport-service.services-5681.svc.cluster.local'
Jan 29 12:07:10.083: INFO: stderr: "+ nslookup nodeport-service.services-5681.svc.cluster.local\n"
Jan 29 12:07:10.083: INFO: stdout: "Server:\t\t10.100.200.2\nAddress:\t10.100.200.2#53\n\nnodeport-service.services-5681.svc.cluster.local\tcanonical name = externalsvc.services-5681.svc.cluster.local.\nName:\texternalsvc.services-5681.svc.cluster.local\nAddress: 10.100.200.68\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5681, will wait for the garbage collector to delete the pods
Jan 29 12:07:10.139: INFO: Deleting ReplicationController externalsvc took: 3.786264ms
Jan 29 12:07:10.240: INFO: Terminating ReplicationController externalsvc pods took: 100.126651ms
Jan 29 12:07:18.459: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:07:18.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5681" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.700 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":297,"skipped":4795,"failed":0}
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:07:18.472: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4732
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4732
STEP: creating replication controller externalsvc in namespace services-4732
I0129 12:07:18.524094      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4732, replica count: 2
I0129 12:07:21.574372      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 29 12:07:21.585: INFO: Creating new exec pod
Jan 29 12:07:25.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=services-4732 exec execpodlwfn4 -- /bin/sh -x -c nslookup clusterip-service.services-4732.svc.cluster.local'
Jan 29 12:07:25.753: INFO: stderr: "+ nslookup clusterip-service.services-4732.svc.cluster.local\n"
Jan 29 12:07:25.753: INFO: stdout: "Server:\t\t10.100.200.2\nAddress:\t10.100.200.2#53\n\nclusterip-service.services-4732.svc.cluster.local\tcanonical name = externalsvc.services-4732.svc.cluster.local.\nName:\texternalsvc.services-4732.svc.cluster.local\nAddress: 10.100.200.246\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4732, will wait for the garbage collector to delete the pods
Jan 29 12:07:25.809: INFO: Deleting ReplicationController externalsvc took: 3.727223ms
Jan 29 12:07:25.909: INFO: Terminating ReplicationController externalsvc pods took: 100.109139ms
Jan 29 12:07:38.522: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:07:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4732" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:20.070 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":298,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:07:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 29 12:07:39.179: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 29 12:07:41.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518859, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518859, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518859, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63747518859, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 29 12:07:44.193: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jan 29 12:07:44.197: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3282-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:07:45.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7580" for this suite.
STEP: Destroying namespace "webhook-7580-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":299,"skipped":4832,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:07:45.352: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-5128261c-4177-47c2-a247-fd5577f50c7c in namespace container-probe-2015
Jan 29 12:07:47.384: INFO: Started pod test-webserver-5128261c-4177-47c2-a247-fd5577f50c7c in namespace container-probe-2015
STEP: checking the pod's current state and verifying that restartCount is present
Jan 29 12:07:47.386: INFO: Initial restart count of pod test-webserver-5128261c-4177-47c2-a247-fd5577f50c7c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:11:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2015" for this suite.

• [SLOW TEST:242.363 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4847,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:11:47.716: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-b5c4
STEP: Creating a pod to test atomic-volume-subpath
Jan 29 12:11:47.750: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-b5c4" in namespace "subpath-8108" to be "Succeeded or Failed"
Jan 29 12:11:47.752: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.898249ms
Jan 29 12:11:49.755: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004144153s
Jan 29 12:11:51.757: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.006697734s
Jan 29 12:11:53.759: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 6.008822222s
Jan 29 12:11:55.762: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 8.01121986s
Jan 29 12:11:57.764: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 10.013622772s
Jan 29 12:11:59.767: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 12.017001457s
Jan 29 12:12:01.770: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 14.019766944s
Jan 29 12:12:03.773: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 16.022250759s
Jan 29 12:12:05.775: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 18.025012145s
Jan 29 12:12:07.778: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Running", Reason="", readiness=true. Elapsed: 20.02782112s
Jan 29 12:12:09.781: INFO: Pod "pod-subpath-test-downwardapi-b5c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.030505899s
STEP: Saw pod success
Jan 29 12:12:09.781: INFO: Pod "pod-subpath-test-downwardapi-b5c4" satisfied condition "Succeeded or Failed"
Jan 29 12:12:09.783: INFO: Trying to get logs from node c66ef63f-50cf-427f-af2a-b340450b0b53 pod pod-subpath-test-downwardapi-b5c4 container test-container-subpath-downwardapi-b5c4: <nil>
STEP: delete the pod
Jan 29 12:12:09.810: INFO: Waiting for pod pod-subpath-test-downwardapi-b5c4 to disappear
Jan 29 12:12:09.817: INFO: Pod pod-subpath-test-downwardapi-b5c4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-b5c4
Jan 29 12:12:09.817: INFO: Deleting pod "pod-subpath-test-downwardapi-b5c4" in namespace "subpath-8108"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:12:09.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8108" for this suite.

• [SLOW TEST:22.108 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":301,"skipped":4858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:12:09.826: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Jan 29 12:12:09.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-3143 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jan 29 12:12:09.926: INFO: stderr: ""
Jan 29 12:12:09.926: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 29 12:12:09.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-3143 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "mirror.gcr.io/library/busybox:1.29"}]}} --dry-run=server'
Jan 29 12:12:10.166: INFO: stderr: ""
Jan 29 12:12:10.167: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
Jan 29 12:12:10.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-3143 delete pods e2e-test-httpd-pod'
Jan 29 12:12:18.415: INFO: stderr: ""
Jan 29 12:12:18.415: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:12:18.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3143" for this suite.

• [SLOW TEST:8.596 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:902
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":302,"skipped":4904,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:12:18.421: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jan 29 12:12:18.440: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:12:21.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3227" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":303,"skipped":4906,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:12:21.800: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Jan 29 12:12:21.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9465 run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine'
Jan 29 12:12:21.906: INFO: stderr: ""
Jan 29 12:12:21.906: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Jan 29 12:12:21.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-9465 delete pods e2e-test-httpd-pod'
Jan 29 12:12:28.414: INFO: stderr: ""
Jan 29 12:12:28.414: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:12:28.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9465" for this suite.

• [SLOW TEST:6.622 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":304,"skipped":4910,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jan 29 12:12:28.422: INFO: >>> kubeConfig: /tmp/kubeconfig-769685040
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jan 29 12:12:28.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8314 create -f -'
Jan 29 12:12:28.612: INFO: stderr: ""
Jan 29 12:12:28.612: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 29 12:12:29.616: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:12:29.616: INFO: Found 0 / 1
Jan 29 12:12:30.615: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:12:30.615: INFO: Found 1 / 1
Jan 29 12:12:30.615: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 29 12:12:30.616: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:12:30.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 29 12:12:30.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-769685040 --namespace=kubectl-8314 patch pod agnhost-primary-zfhnz -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 29 12:12:30.712: INFO: stderr: ""
Jan 29 12:12:30.712: INFO: stdout: "pod/agnhost-primary-zfhnz patched\n"
STEP: checking annotations
Jan 29 12:12:30.720: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 29 12:12:30.720: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jan 29 12:12:30.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8314" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":305,"skipped":4918,"failed":0}
SSSSSSSSSSSSSSSJan 29 12:12:30.726: INFO: Running AfterSuite actions on all nodes
Jan 29 12:12:30.726: INFO: Running AfterSuite actions on node 1
Jan 29 12:12:30.726: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4933,"failed":0}

Ran 305 of 5238 Specs in 4973.405 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4933 Skipped
PASS

Ginkgo ran 1 suite in 1h22m54.935282771s
Test Suite Passed
