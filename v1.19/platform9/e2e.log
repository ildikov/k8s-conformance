I0315 09:00:46.006827      20 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-016546558
I0315 09:00:46.006852      20 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0315 09:00:46.006956      20 e2e.go:129] Starting e2e run "505c0347-b7f7-4c2f-9a38-1bd921420b7e" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1615798844 - Will randomize all specs
Will run 305 of 5238 specs

Mar 15 09:00:46.019: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:00:46.021: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 15 09:00:46.035: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 15 09:00:46.058: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 15 09:00:46.058: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 15 09:00:46.058: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 15 09:00:46.064: INFO: e2e test version: v1.19.6
Mar 15 09:00:46.065: INFO: kube-apiserver version: v1.19.6
Mar 15 09:00:46.065: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:00:46.069: INFO: Cluster IP family: ipv4
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:46.069: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubelet-test
Mar 15 09:00:46.094: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:00:48.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8890" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":1,"skipped":4,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:48.151: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Mar 15 09:00:48.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 create -f -'
Mar 15 09:00:49.364: INFO: stderr: ""
Mar 15 09:00:49.364: INFO: stdout: "pod/pause created\n"
Mar 15 09:00:49.364: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 15 09:00:49.364: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8561" to be "running and ready"
Mar 15 09:00:49.373: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.272518ms
Mar 15 09:00:51.376: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011909081s
Mar 15 09:00:51.376: INFO: Pod "pause" satisfied condition "running and ready"
Mar 15 09:00:51.376: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 15 09:00:51.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 label pods pause testing-label=testing-label-value'
Mar 15 09:00:51.461: INFO: stderr: ""
Mar 15 09:00:51.461: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 15 09:00:51.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 get pod pause -L testing-label'
Mar 15 09:00:51.536: INFO: stderr: ""
Mar 15 09:00:51.536: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 15 09:00:51.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 label pods pause testing-label-'
Mar 15 09:00:51.615: INFO: stderr: ""
Mar 15 09:00:51.615: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 15 09:00:51.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 get pod pause -L testing-label'
Mar 15 09:00:51.695: INFO: stderr: ""
Mar 15 09:00:51.695: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Mar 15 09:00:51.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 delete --grace-period=0 --force -f -'
Mar 15 09:00:51.799: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:00:51.799: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 15 09:00:51.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 get rc,svc -l name=pause --no-headers'
Mar 15 09:00:51.917: INFO: stderr: "No resources found in kubectl-8561 namespace.\n"
Mar 15 09:00:51.917: INFO: stdout: ""
Mar 15 09:00:51.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8561 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 09:00:52.024: INFO: stderr: ""
Mar 15 09:00:52.024: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:00:52.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8561" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":2,"skipped":28,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:52.031: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:00:52.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3491" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":3,"skipped":33,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:52.188: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Mar 15 09:00:52.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 create -f -'
Mar 15 09:00:52.498: INFO: stderr: ""
Mar 15 09:00:52.498: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 15 09:00:52.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:00:52.598: INFO: stderr: ""
Mar 15 09:00:52.598: INFO: stdout: "update-demo-nautilus-4pq49 update-demo-nautilus-xxlr2 "
Mar 15 09:00:52.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods update-demo-nautilus-4pq49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:00:52.705: INFO: stderr: ""
Mar 15 09:00:52.705: INFO: stdout: ""
Mar 15 09:00:52.705: INFO: update-demo-nautilus-4pq49 is created but not running
Mar 15 09:00:57.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:00:57.785: INFO: stderr: ""
Mar 15 09:00:57.785: INFO: stdout: "update-demo-nautilus-4pq49 update-demo-nautilus-xxlr2 "
Mar 15 09:00:57.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods update-demo-nautilus-4pq49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:00:57.865: INFO: stderr: ""
Mar 15 09:00:57.865: INFO: stdout: "true"
Mar 15 09:00:57.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods update-demo-nautilus-4pq49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:00:57.944: INFO: stderr: ""
Mar 15 09:00:57.944: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:00:57.944: INFO: validating pod update-demo-nautilus-4pq49
Mar 15 09:00:57.947: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:00:57.947: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:00:57.947: INFO: update-demo-nautilus-4pq49 is verified up and running
Mar 15 09:00:57.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods update-demo-nautilus-xxlr2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:00:58.022: INFO: stderr: ""
Mar 15 09:00:58.022: INFO: stdout: "true"
Mar 15 09:00:58.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods update-demo-nautilus-xxlr2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:00:58.102: INFO: stderr: ""
Mar 15 09:00:58.102: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:00:58.102: INFO: validating pod update-demo-nautilus-xxlr2
Mar 15 09:00:58.107: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:00:58.107: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:00:58.107: INFO: update-demo-nautilus-xxlr2 is verified up and running
STEP: using delete to clean up resources
Mar 15 09:00:58.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 delete --grace-period=0 --force -f -'
Mar 15 09:00:58.204: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:00:58.204: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 15 09:00:58.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get rc,svc -l name=update-demo --no-headers'
Mar 15 09:00:58.283: INFO: stderr: "No resources found in kubectl-7126 namespace.\n"
Mar 15 09:00:58.283: INFO: stdout: ""
Mar 15 09:00:58.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 09:00:58.362: INFO: stderr: ""
Mar 15 09:00:58.362: INFO: stdout: "update-demo-nautilus-4pq49\nupdate-demo-nautilus-xxlr2\n"
Mar 15 09:00:58.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get rc,svc -l name=update-demo --no-headers'
Mar 15 09:00:58.944: INFO: stderr: "No resources found in kubectl-7126 namespace.\n"
Mar 15 09:00:58.944: INFO: stdout: ""
Mar 15 09:00:58.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7126 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 09:00:59.069: INFO: stderr: ""
Mar 15 09:00:59.069: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:00:59.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7126" for this suite.

• [SLOW TEST:6.889 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":4,"skipped":39,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:59.078: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:00:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:00:59.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2879" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":5,"skipped":45,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:00:59.656: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ae2d9521-27c3-46e9-8cf3-77b56d4248fa
STEP: Creating a pod to test consume secrets
Mar 15 09:00:59.692: INFO: Waiting up to 5m0s for pod "pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408" in namespace "secrets-3311" to be "Succeeded or Failed"
Mar 15 09:00:59.704: INFO: Pod "pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408": Phase="Pending", Reason="", readiness=false. Elapsed: 12.456545ms
Mar 15 09:01:01.707: INFO: Pod "pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015165542s
STEP: Saw pod success
Mar 15 09:01:01.707: INFO: Pod "pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408" satisfied condition "Succeeded or Failed"
Mar 15 09:01:01.709: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408 container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 09:01:01.721: INFO: Waiting for pod pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408 to disappear
Mar 15 09:01:01.728: INFO: Pod pod-secrets-f57e0db0-a061-416e-9cfe-09e052805408 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:01:01.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3311" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":6,"skipped":59,"failed":0}
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:01:01.738: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:01:01.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5708" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":7,"skipped":62,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:01:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:01:01.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560" in namespace "projected-4856" to be "Succeeded or Failed"
Mar 15 09:01:01.824: INFO: Pod "downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560": Phase="Pending", Reason="", readiness=false. Elapsed: 16.033774ms
Mar 15 09:01:03.827: INFO: Pod "downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019423301s
STEP: Saw pod success
Mar 15 09:01:03.827: INFO: Pod "downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560" satisfied condition "Succeeded or Failed"
Mar 15 09:01:03.829: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560 container client-container: <nil>
STEP: delete the pod
Mar 15 09:01:03.842: INFO: Waiting for pod downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560 to disappear
Mar 15 09:01:03.848: INFO: Pod downwardapi-volume-ab14b58b-bbb2-472d-860a-189982eb0560 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:01:03.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4856" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":67,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:01:03.858: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 15 09:01:05.908: INFO: &Pod{ObjectMeta:{send-events-28ea5ebd-33a5-457a-b055-9dc8d9f3f6f5  events-5369 /api/v1/namespaces/events-5369/pods/send-events-28ea5ebd-33a5-457a-b055-9dc8d9f3f6f5 9cf6fb59-a87e-4eed-96e7-4aace1fd79c2 650718 0 2021-03-15 09:01:03 +0000 UTC <nil> <nil> map[name:foo time:882460062] map[] [] []  [{e2e.test Update v1 2021-03-15 09:01:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 09:01:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xcvcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xcvcs,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xcvcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:01:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:01:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:01:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:01:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.184,StartTime:2021-03-15 09:01:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 09:01:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://3c17d074d2fc5d14ef78548ae00e18b031e596030ac89684747bcf67b1c586ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 15 09:01:07.912: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 15 09:01:09.915: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:01:09.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5369" for this suite.

• [SLOW TEST:6.085 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":9,"skipped":80,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:01:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Mar 15 09:01:09.977: INFO: Waiting up to 5m0s for pod "downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d" in namespace "downward-api-8619" to be "Succeeded or Failed"
Mar 15 09:01:09.979: INFO: Pod "downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943936ms
Mar 15 09:01:11.982: INFO: Pod "downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004884116s
STEP: Saw pod success
Mar 15 09:01:11.982: INFO: Pod "downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d" satisfied condition "Succeeded or Failed"
Mar 15 09:01:11.984: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d container dapi-container: <nil>
STEP: delete the pod
Mar 15 09:01:12.003: INFO: Waiting for pod downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d to disappear
Mar 15 09:01:12.011: INFO: Pod downward-api-b266fa29-9cf1-4fe3-8c68-f5c44c4e155d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:01:12.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8619" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":10,"skipped":87,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:01:12.017: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-88dc3659-6670-4d71-a2c8-f4f1be804f96 in namespace container-probe-271
Mar 15 09:01:14.048: INFO: Started pod busybox-88dc3659-6670-4d71-a2c8-f4f1be804f96 in namespace container-probe-271
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 09:01:14.049: INFO: Initial restart count of pod busybox-88dc3659-6670-4d71-a2c8-f4f1be804f96 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:14.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-271" for this suite.

• [SLOW TEST:242.447 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":101,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:14.465: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-3865
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3865 to expose endpoints map[]
Mar 15 09:05:14.522: INFO: successfully validated that service endpoint-test2 in namespace services-3865 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3865
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3865 to expose endpoints map[pod1:[80]]
Mar 15 09:05:17.546: INFO: successfully validated that service endpoint-test2 in namespace services-3865 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3865
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3865 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 15 09:05:20.568: INFO: successfully validated that service endpoint-test2 in namespace services-3865 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3865
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3865 to expose endpoints map[pod2:[80]]
Mar 15 09:05:20.596: INFO: successfully validated that service endpoint-test2 in namespace services-3865 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3865
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3865 to expose endpoints map[]
Mar 15 09:05:20.622: INFO: successfully validated that service endpoint-test2 in namespace services-3865 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:20.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3865" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.188 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":12,"skipped":115,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:20.653: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Mar 15 09:05:20.690: INFO: Waiting up to 5m0s for pod "downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc" in namespace "downward-api-2435" to be "Succeeded or Failed"
Mar 15 09:05:20.692: INFO: Pod "downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.69183ms
Mar 15 09:05:22.694: INFO: Pod "downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003791516s
STEP: Saw pod success
Mar 15 09:05:22.694: INFO: Pod "downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc" satisfied condition "Succeeded or Failed"
Mar 15 09:05:22.695: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc container dapi-container: <nil>
STEP: delete the pod
Mar 15 09:05:22.726: INFO: Waiting for pod downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc to disappear
Mar 15 09:05:22.733: INFO: Pod downward-api-3cde5fbd-5e20-4051-9a45-0e46c9515edc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:22.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2435" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":116,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:22.740: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-646e229c-d1b6-4f17-88e9-96e393de3a07
STEP: Creating a pod to test consume configMaps
Mar 15 09:05:22.776: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958" in namespace "projected-5935" to be "Succeeded or Failed"
Mar 15 09:05:22.779: INFO: Pod "pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.89351ms
Mar 15 09:05:24.782: INFO: Pod "pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005833488s
STEP: Saw pod success
Mar 15 09:05:24.782: INFO: Pod "pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958" satisfied condition "Succeeded or Failed"
Mar 15 09:05:24.783: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:05:24.796: INFO: Waiting for pod pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958 to disappear
Mar 15 09:05:24.804: INFO: Pod pod-projected-configmaps-9049439f-532b-41fb-bcc1-79268f723958 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:24.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5935" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":14,"skipped":126,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:24.810: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:05:24.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c" in namespace "projected-660" to be "Succeeded or Failed"
Mar 15 09:05:24.850: INFO: Pod "downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.392568ms
Mar 15 09:05:26.853: INFO: Pod "downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013363581s
STEP: Saw pod success
Mar 15 09:05:26.853: INFO: Pod "downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c" satisfied condition "Succeeded or Failed"
Mar 15 09:05:26.854: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c container client-container: <nil>
STEP: delete the pod
Mar 15 09:05:26.867: INFO: Waiting for pod downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c to disappear
Mar 15 09:05:26.875: INFO: Pod downwardapi-volume-6e308804-6b2e-410b-b6e7-b1217ef7001c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:26.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-660" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":126,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:26.882: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:05:26.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993" in namespace "projected-2166" to be "Succeeded or Failed"
Mar 15 09:05:26.910: INFO: Pod "downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.735606ms
Mar 15 09:05:28.914: INFO: Pod "downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006050589s
STEP: Saw pod success
Mar 15 09:05:28.914: INFO: Pod "downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993" satisfied condition "Succeeded or Failed"
Mar 15 09:05:28.916: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993 container client-container: <nil>
STEP: delete the pod
Mar 15 09:05:28.941: INFO: Waiting for pod downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993 to disappear
Mar 15 09:05:28.942: INFO: Pod downwardapi-volume-d7681c8e-6c06-4553-b256-b32e83fbe993 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:28.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2166" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:28.950: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 15 09:05:28.971: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 09:05:28.977: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 09:05:28.979: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-18.us-east-2.compute.internal before test
Mar 15 09:05:28.988: INFO: metrics-server-v0.3.6-58cf8c5dfb-dnzrv from kube-system started at 2021-03-12 15:30:23 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:28.988: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 09:05:28.988: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 15 09:05:28.988: INFO: dashboard-metrics-scraper-7b59f7d4df-xrqjg from kubernetes-dashboard started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.988: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 15 09:05:28.988: INFO: node-exporter-2k2xw from pf9-monitoring started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.988: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:05:28.988: INFO: prometheus-system-0 from pf9-monitoring started at 2021-03-12 15:32:05 +0000 UTC (3 container statuses recorded)
Mar 15 09:05:28.988: INFO: 	Container prometheus ready: true, restart count 1
Mar 15 09:05:28.988: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 15 09:05:28.988: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 15 09:05:28.988: INFO: packageserver-67c5dfcffc-4vmch from pf9-olm started at 2021-03-14 07:25:39 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.988: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 09:05:28.988: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:28.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:05:28.989: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 09:05:28.989: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-2-120.us-east-2.compute.internal before test
Mar 15 09:05:28.998: INFO: coredns-867bf6789f-kfcq4 from kube-system started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.998: INFO: 	Container coredns ready: true, restart count 0
Mar 15 09:05:28.998: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2021-03-14 07:25:32 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:28.998: INFO: 	Container alertmanager ready: true, restart count 0
Mar 15 09:05:28.998: INFO: 	Container config-reloader ready: true, restart count 0
Mar 15 09:05:28.999: INFO: grafana-9b947c8-gqfl8 from pf9-monitoring started at 2021-03-12 15:32:10 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container grafana ready: true, restart count 0
Mar 15 09:05:28.999: INFO: 	Container proxy ready: true, restart count 0
Mar 15 09:05:28.999: INFO: kube-state-metrics-6ff68b466d-2xpvx from pf9-monitoring started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 15 09:05:28.999: INFO: node-exporter-gs57d from pf9-monitoring started at 2021-03-12 15:30:01 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:05:28.999: INFO: catalog-operator-6656569cc-bsdrn from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 15 09:05:28.999: INFO: olm-operator-5c657955f6-jdk8b from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container olm-operator ready: true, restart count 0
Mar 15 09:05:28.999: INFO: packageserver-67c5dfcffc-vnl24 from pf9-olm started at 2021-03-14 07:25:32 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 09:05:28.999: INFO: platform9-operators-cxg4l from pf9-olm started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container registry-server ready: true, restart count 0
Mar 15 09:05:28.999: INFO: monhelper-5577f87674-m95zg from pf9-operators started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container monhelper ready: true, restart count 0
Mar 15 09:05:28.999: INFO: prometheus-operator-7b96488f76-st7bk from pf9-operators started at 2021-03-12 15:31:19 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 15 09:05:28.999: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:28.999: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:05:28.999: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 09:05:28.999: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-3-172.us-east-2.compute.internal before test
Mar 15 09:05:29.008: INFO: node-exporter-tzdsq from pf9-monitoring started at 2021-03-12 15:30:02 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:29.008: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:05:29.008: INFO: sonobuoy from sonobuoy started at 2021-03-15 09:00:40 +0000 UTC (1 container statuses recorded)
Mar 15 09:05:29.008: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 09:05:29.008: INFO: sonobuoy-e2e-job-be88518f7bd84c37 from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:29.008: INFO: 	Container e2e ready: true, restart count 0
Mar 15 09:05:29.008: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:05:29.008: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:05:29.008: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:05:29.008: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166c78bf3a4bf86c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.166c78bf3b4583a8], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:30.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2370" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":17,"skipped":173,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:30.037: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:37.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9499" for this suite.

• [SLOW TEST:7.038 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":18,"skipped":176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:37.075: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Mar 15 09:05:37.097: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3971" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":19,"skipped":201,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:05:41.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86" in namespace "downward-api-1953" to be "Succeeded or Failed"
Mar 15 09:05:41.189: INFO: Pod "downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558526ms
Mar 15 09:05:43.192: INFO: Pod "downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003998073s
STEP: Saw pod success
Mar 15 09:05:43.192: INFO: Pod "downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86" satisfied condition "Succeeded or Failed"
Mar 15 09:05:43.193: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86 container client-container: <nil>
STEP: delete the pod
Mar 15 09:05:43.210: INFO: Waiting for pod downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86 to disappear
Mar 15 09:05:43.216: INFO: Pod downwardapi-volume-92066ef2-e949-4b15-9e2b-5968cfeb9d86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:43.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1953" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":213,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:43.223: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:43.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4638" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":21,"skipped":224,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:43.278: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:05:43.960: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 15 09:05:43.961: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 15 09:05:43.961: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.961: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 15 09:05:43.961: INFO: Checking APIGroup: extensions
Mar 15 09:05:43.962: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar 15 09:05:43.962: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar 15 09:05:43.962: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar 15 09:05:43.962: INFO: Checking APIGroup: apps
Mar 15 09:05:43.963: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 15 09:05:43.963: INFO: Versions found [{apps/v1 v1}]
Mar 15 09:05:43.963: INFO: apps/v1 matches apps/v1
Mar 15 09:05:43.963: INFO: Checking APIGroup: events.k8s.io
Mar 15 09:05:43.963: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 15 09:05:43.963: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.964: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 15 09:05:43.964: INFO: Checking APIGroup: authentication.k8s.io
Mar 15 09:05:43.964: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 15 09:05:43.964: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.964: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 15 09:05:43.964: INFO: Checking APIGroup: authorization.k8s.io
Mar 15 09:05:43.965: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 15 09:05:43.965: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.965: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 15 09:05:43.965: INFO: Checking APIGroup: autoscaling
Mar 15 09:05:43.966: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar 15 09:05:43.966: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 15 09:05:43.966: INFO: autoscaling/v1 matches autoscaling/v1
Mar 15 09:05:43.966: INFO: Checking APIGroup: batch
Mar 15 09:05:43.967: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 15 09:05:43.967: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 15 09:05:43.967: INFO: batch/v1 matches batch/v1
Mar 15 09:05:43.967: INFO: Checking APIGroup: certificates.k8s.io
Mar 15 09:05:43.968: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 15 09:05:43.968: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.968: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 15 09:05:43.968: INFO: Checking APIGroup: networking.k8s.io
Mar 15 09:05:43.969: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 15 09:05:43.969: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.969: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 15 09:05:43.969: INFO: Checking APIGroup: policy
Mar 15 09:05:43.969: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Mar 15 09:05:43.969: INFO: Versions found [{policy/v1beta1 v1beta1}]
Mar 15 09:05:43.969: INFO: policy/v1beta1 matches policy/v1beta1
Mar 15 09:05:43.969: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 15 09:05:43.970: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 15 09:05:43.970: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.970: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 15 09:05:43.970: INFO: Checking APIGroup: storage.k8s.io
Mar 15 09:05:43.971: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 15 09:05:43.971: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.971: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 15 09:05:43.971: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 15 09:05:43.972: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 15 09:05:43.972: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.972: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 15 09:05:43.972: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 15 09:05:43.973: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 15 09:05:43.973: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.973: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 15 09:05:43.973: INFO: Checking APIGroup: scheduling.k8s.io
Mar 15 09:05:43.974: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 15 09:05:43.974: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
Mar 15 09:05:43.974: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 15 09:05:43.974: INFO: Checking APIGroup: coordination.k8s.io
Mar 15 09:05:43.975: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 15 09:05:43.975: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.975: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 15 09:05:43.975: INFO: Checking APIGroup: node.k8s.io
Mar 15 09:05:43.976: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Mar 15 09:05:43.976: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.976: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Mar 15 09:05:43.976: INFO: Checking APIGroup: discovery.k8s.io
Mar 15 09:05:43.977: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Mar 15 09:05:43.977: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.977: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Mar 15 09:05:43.977: INFO: Checking APIGroup: packages.operators.coreos.com
Mar 15 09:05:43.978: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Mar 15 09:05:43.978: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Mar 15 09:05:43.978: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Mar 15 09:05:43.978: INFO: Checking APIGroup: agent.pf9.io
Mar 15 09:05:43.978: INFO: PreferredVersion.GroupVersion: agent.pf9.io/v1
Mar 15 09:05:43.978: INFO: Versions found [{agent.pf9.io/v1 v1}]
Mar 15 09:05:43.978: INFO: agent.pf9.io/v1 matches agent.pf9.io/v1
Mar 15 09:05:43.978: INFO: Checking APIGroup: monitoring.coreos.com
Mar 15 09:05:43.979: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar 15 09:05:43.979: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Mar 15 09:05:43.980: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar 15 09:05:43.980: INFO: Checking APIGroup: operators.coreos.com
Mar 15 09:05:43.980: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Mar 15 09:05:43.980: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Mar 15 09:05:43.980: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Mar 15 09:05:43.980: INFO: Checking APIGroup: metrics.k8s.io
Mar 15 09:05:43.981: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 15 09:05:43.981: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 15 09:05:43.981: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:43.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-3681" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":22,"skipped":233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:43.988: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:05:44.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041" in namespace "downward-api-5913" to be "Succeeded or Failed"
Mar 15 09:05:44.028: INFO: Pod "downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041": Phase="Pending", Reason="", readiness=false. Elapsed: 11.231737ms
Mar 15 09:05:46.032: INFO: Pod "downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015395827s
Mar 15 09:05:48.036: INFO: Pod "downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018479569s
STEP: Saw pod success
Mar 15 09:05:48.036: INFO: Pod "downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041" satisfied condition "Succeeded or Failed"
Mar 15 09:05:48.037: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041 container client-container: <nil>
STEP: delete the pod
Mar 15 09:05:48.051: INFO: Waiting for pod downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041 to disappear
Mar 15 09:05:48.057: INFO: Pod downwardapi-volume-32514081-8d4b-4a8a-a17c-f060aea1a041 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:05:48.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5913" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":23,"skipped":275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:05:48.065: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:05:48.086: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Creating first CR 
Mar 15 09:05:48.632: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:48Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:05:48Z]] name:name1 resourceVersion:651893 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b288aea4-d2ef-477b-8791-27044181f254] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 15 09:05:58.637: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:05:58Z]] name:name2 resourceVersion:651943 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:135f06b7-b14e-4991-9e99-1107b0029c8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 15 09:06:08.642: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:06:08Z]] name:name1 resourceVersion:651980 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b288aea4-d2ef-477b-8791-27044181f254] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 15 09:06:18.647: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:06:18Z]] name:name2 resourceVersion:652017 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:135f06b7-b14e-4991-9e99-1107b0029c8c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 15 09:06:28.653: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:48Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:06:08Z]] name:name1 resourceVersion:652043 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:b288aea4-d2ef-477b-8791-27044181f254] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 15 09:06:38.658: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-03-15T09:05:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-03-15T09:06:18Z]] name:name2 resourceVersion:652072 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:135f06b7-b14e-4991-9e99-1107b0029c8c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:06:49.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7372" for this suite.

• [SLOW TEST:61.139 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":24,"skipped":315,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:06:49.205: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 15 09:06:53.335: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4999 PodName:pod-sharedvolume-7fbbd8be-6cbf-40cd-abbb-cc121856ea7b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:06:53.335: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:06:53.448: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:06:53.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4999" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":25,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:06:53.455: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 15 09:06:53.480: INFO: Waiting up to 5m0s for pod "pod-5c614bd1-aed5-4b40-8fd8-4205dded9145" in namespace "emptydir-5972" to be "Succeeded or Failed"
Mar 15 09:06:53.492: INFO: Pod "pod-5c614bd1-aed5-4b40-8fd8-4205dded9145": Phase="Pending", Reason="", readiness=false. Elapsed: 11.677959ms
Mar 15 09:06:55.495: INFO: Pod "pod-5c614bd1-aed5-4b40-8fd8-4205dded9145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015009855s
Mar 15 09:06:57.498: INFO: Pod "pod-5c614bd1-aed5-4b40-8fd8-4205dded9145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018155785s
STEP: Saw pod success
Mar 15 09:06:57.498: INFO: Pod "pod-5c614bd1-aed5-4b40-8fd8-4205dded9145" satisfied condition "Succeeded or Failed"
Mar 15 09:06:57.500: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-5c614bd1-aed5-4b40-8fd8-4205dded9145 container test-container: <nil>
STEP: delete the pod
Mar 15 09:06:57.512: INFO: Waiting for pod pod-5c614bd1-aed5-4b40-8fd8-4205dded9145 to disappear
Mar 15 09:06:57.520: INFO: Pod pod-5c614bd1-aed5-4b40-8fd8-4205dded9145 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:06:57.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5972" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:06:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:06:57.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025" in namespace "downward-api-4973" to be "Succeeded or Failed"
Mar 15 09:06:57.609: INFO: Pod "downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09014ms
Mar 15 09:06:59.612: INFO: Pod "downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295456s
Mar 15 09:07:01.615: INFO: Pod "downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00854927s
STEP: Saw pod success
Mar 15 09:07:01.615: INFO: Pod "downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025" satisfied condition "Succeeded or Failed"
Mar 15 09:07:01.618: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025 container client-container: <nil>
STEP: delete the pod
Mar 15 09:07:01.631: INFO: Waiting for pod downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025 to disappear
Mar 15 09:07:01.637: INFO: Pod downwardapi-volume-a0aad0ef-7e1d-48ca-9981-51a368b1a025 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:07:01.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4973" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":406,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:07:01.643: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Mar 15 09:07:01.668: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:07:24.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5567" for this suite.

• [SLOW TEST:22.594 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":28,"skipped":420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:07:24.237: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4457
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-4457
Mar 15 09:07:24.280: INFO: Found 0 stateful pods, waiting for 1
Mar 15 09:07:34.283: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 09:07:34.292: INFO: Deleting all statefulset in ns statefulset-4457
Mar 15 09:07:34.293: INFO: Scaling statefulset ss to 0
Mar 15 09:07:54.312: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:07:54.314: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:07:54.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4457" for this suite.

• [SLOW TEST:30.099 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":29,"skipped":444,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:07:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-259f4a69-4c92-49e8-873b-39b33b9ec15f
STEP: Creating a pod to test consume configMaps
Mar 15 09:07:54.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43" in namespace "configmap-6726" to be "Succeeded or Failed"
Mar 15 09:07:54.385: INFO: Pod "pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.397598ms
Mar 15 09:07:56.388: INFO: Pod "pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006470426s
Mar 15 09:07:58.391: INFO: Pod "pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009393771s
STEP: Saw pod success
Mar 15 09:07:58.391: INFO: Pod "pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43" satisfied condition "Succeeded or Failed"
Mar 15 09:07:58.393: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:07:58.406: INFO: Waiting for pod pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43 to disappear
Mar 15 09:07:58.413: INFO: Pod pod-configmaps-3c37c7be-ddaa-4a6a-8dd6-a52a41f6ec43 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:07:58.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6726" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":30,"skipped":456,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:07:58.419: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 15 09:07:58.448: INFO: Waiting up to 5m0s for pod "pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2" in namespace "emptydir-4954" to be "Succeeded or Failed"
Mar 15 09:07:58.450: INFO: Pod "pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361482ms
Mar 15 09:08:00.452: INFO: Pod "pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00428135s
STEP: Saw pod success
Mar 15 09:08:00.452: INFO: Pod "pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2" satisfied condition "Succeeded or Failed"
Mar 15 09:08:00.454: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2 container test-container: <nil>
STEP: delete the pod
Mar 15 09:08:00.465: INFO: Waiting for pod pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2 to disappear
Mar 15 09:08:00.472: INFO: Pod pod-e9d58a06-410b-4bb3-ac4d-6711dd00e8e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:00.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4954" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":474,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:00.488: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-75eb5008-f8b2-49ea-b6c8-e018e8e70db5
STEP: Creating a pod to test consume configMaps
Mar 15 09:08:00.520: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb" in namespace "projected-6838" to be "Succeeded or Failed"
Mar 15 09:08:00.531: INFO: Pod "pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.431903ms
Mar 15 09:08:02.534: INFO: Pod "pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014215298s
STEP: Saw pod success
Mar 15 09:08:02.534: INFO: Pod "pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb" satisfied condition "Succeeded or Failed"
Mar 15 09:08:02.536: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:08:02.548: INFO: Waiting for pod pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb to disappear
Mar 15 09:08:02.555: INFO: Pod pod-projected-configmaps-7590a07d-2738-47fc-af86-28545feb6ceb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:02.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6838" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":491,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:02.564: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740
Mar 15 09:08:02.646: INFO: Pod name my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740: Found 0 pods out of 1
Mar 15 09:08:07.649: INFO: Pod name my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740: Found 1 pods out of 1
Mar 15 09:08:07.649: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740" are running
Mar 15 09:08:07.651: INFO: Pod "my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740-5kgr7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 09:08:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 09:08:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 09:08:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 09:08:02 +0000 UTC Reason: Message:}])
Mar 15 09:08:07.651: INFO: Trying to dial the pod
Mar 15 09:08:12.658: INFO: Controller my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740: Got expected result from replica 1 [my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740-5kgr7]: "my-hostname-basic-447dea0a-f974-42bf-a41f-cbf1ec0be740-5kgr7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:12.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1033" for this suite.

• [SLOW TEST:10.100 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":33,"skipped":506,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:12.665: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 15 09:08:14.703: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:14.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3730" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:14.752: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:08:14.797: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 15 09:08:14.802: INFO: Number of nodes with available pods: 0
Mar 15 09:08:14.802: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 15 09:08:14.826: INFO: Number of nodes with available pods: 0
Mar 15 09:08:14.827: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:15.830: INFO: Number of nodes with available pods: 0
Mar 15 09:08:15.831: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:16.837: INFO: Number of nodes with available pods: 1
Mar 15 09:08:16.837: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 15 09:08:16.855: INFO: Number of nodes with available pods: 1
Mar 15 09:08:16.855: INFO: Number of running nodes: 0, number of available pods: 1
Mar 15 09:08:17.857: INFO: Number of nodes with available pods: 0
Mar 15 09:08:17.857: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 15 09:08:17.863: INFO: Number of nodes with available pods: 0
Mar 15 09:08:17.863: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:18.866: INFO: Number of nodes with available pods: 0
Mar 15 09:08:18.866: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:19.866: INFO: Number of nodes with available pods: 0
Mar 15 09:08:19.866: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:20.869: INFO: Number of nodes with available pods: 0
Mar 15 09:08:20.869: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:21.866: INFO: Number of nodes with available pods: 0
Mar 15 09:08:21.866: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:22.866: INFO: Number of nodes with available pods: 0
Mar 15 09:08:22.866: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:23.866: INFO: Number of nodes with available pods: 1
Mar 15 09:08:23.866: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1616, will wait for the garbage collector to delete the pods
Mar 15 09:08:23.925: INFO: Deleting DaemonSet.extensions daemon-set took: 4.061621ms
Mar 15 09:08:24.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.067224ms
Mar 15 09:08:27.028: INFO: Number of nodes with available pods: 0
Mar 15 09:08:27.028: INFO: Number of running nodes: 0, number of available pods: 0
Mar 15 09:08:27.034: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1616/daemonsets","resourceVersion":"652810"},"items":null}

Mar 15 09:08:27.036: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1616/pods","resourceVersion":"652810"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:27.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1616" for this suite.

• [SLOW TEST:12.304 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":35,"skipped":551,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:27.057: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:29.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9009" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":36,"skipped":567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:29.105: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:29.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9134" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":37,"skipped":597,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:29.173: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-1060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1060 to expose endpoints map[]
Mar 15 09:08:29.219: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 15 09:08:30.226: INFO: successfully validated that service multi-endpoint-test in namespace services-1060 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1060 to expose endpoints map[pod1:[100]]
Mar 15 09:08:32.259: INFO: successfully validated that service multi-endpoint-test in namespace services-1060 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-1060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1060 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 15 09:08:35.292: INFO: successfully validated that service multi-endpoint-test in namespace services-1060 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-1060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1060 to expose endpoints map[pod2:[101]]
Mar 15 09:08:35.314: INFO: successfully validated that service multi-endpoint-test in namespace services-1060 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-1060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1060 to expose endpoints map[]
Mar 15 09:08:36.327: INFO: successfully validated that service multi-endpoint-test in namespace services-1060 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:36.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1060" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.182 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":38,"skipped":612,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 15 09:08:36.381: INFO: Waiting up to 5m0s for pod "pod-378b20d7-64be-4d38-9767-6167dcac1de2" in namespace "emptydir-1378" to be "Succeeded or Failed"
Mar 15 09:08:36.403: INFO: Pod "pod-378b20d7-64be-4d38-9767-6167dcac1de2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.788841ms
Mar 15 09:08:38.406: INFO: Pod "pod-378b20d7-64be-4d38-9767-6167dcac1de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024813403s
STEP: Saw pod success
Mar 15 09:08:38.406: INFO: Pod "pod-378b20d7-64be-4d38-9767-6167dcac1de2" satisfied condition "Succeeded or Failed"
Mar 15 09:08:38.408: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-378b20d7-64be-4d38-9767-6167dcac1de2 container test-container: <nil>
STEP: delete the pod
Mar 15 09:08:38.419: INFO: Waiting for pod pod-378b20d7-64be-4d38-9767-6167dcac1de2 to disappear
Mar 15 09:08:38.437: INFO: Pod pod-378b20d7-64be-4d38-9767-6167dcac1de2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:38.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1378" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:38.443: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:40.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4311" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":40,"skipped":653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:40.488: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 15 09:08:40.522: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 652982 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 09:08:40.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 652983 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 09:08:40.522: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 652984 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 15 09:08:50.549: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 653060 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 09:08:50.549: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 653061 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 09:08:50.549: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-273 /api/v1/namespaces/watch-273/configmaps/e2e-watch-test-label-changed 73f930e9-6bdb-4200-9405-a15ea8af2e22 653062 0 2021-03-15 09:08:40 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-03-15 09:08:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:50.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-273" for this suite.

• [SLOW TEST:10.066 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":41,"skipped":697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-d7255753-f62d-4510-b6eb-05c50bee7c26
STEP: Creating a pod to test consume configMaps
Mar 15 09:08:50.585: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad" in namespace "projected-9911" to be "Succeeded or Failed"
Mar 15 09:08:50.592: INFO: Pod "pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962442ms
Mar 15 09:08:52.594: INFO: Pod "pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009728881s
STEP: Saw pod success
Mar 15 09:08:52.595: INFO: Pod "pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad" satisfied condition "Succeeded or Failed"
Mar 15 09:08:52.596: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:08:52.608: INFO: Waiting for pod pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad to disappear
Mar 15 09:08:52.615: INFO: Pod pod-projected-configmaps-39a57185-cbd0-4f83-a0db-8387ecd930ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:52.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9911" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":42,"skipped":745,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:52.622: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 15 09:08:52.652: INFO: Waiting up to 5m0s for pod "pod-b0b62b80-1629-4eb4-be83-881de64bea6d" in namespace "emptydir-1637" to be "Succeeded or Failed"
Mar 15 09:08:52.653: INFO: Pod "pod-b0b62b80-1629-4eb4-be83-881de64bea6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.67279ms
Mar 15 09:08:54.656: INFO: Pod "pod-b0b62b80-1629-4eb4-be83-881de64bea6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004324548s
STEP: Saw pod success
Mar 15 09:08:54.656: INFO: Pod "pod-b0b62b80-1629-4eb4-be83-881de64bea6d" satisfied condition "Succeeded or Failed"
Mar 15 09:08:54.658: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-b0b62b80-1629-4eb4-be83-881de64bea6d container test-container: <nil>
STEP: delete the pod
Mar 15 09:08:54.669: INFO: Waiting for pod pod-b0b62b80-1629-4eb4-be83-881de64bea6d to disappear
Mar 15 09:08:54.676: INFO: Pod pod-b0b62b80-1629-4eb4-be83-881de64bea6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:54.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1637" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":43,"skipped":747,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:54.682: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Mar 15 09:08:54.712: INFO: Waiting up to 5m0s for pod "client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8" in namespace "containers-5188" to be "Succeeded or Failed"
Mar 15 09:08:54.714: INFO: Pod "client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439737ms
Mar 15 09:08:56.717: INFO: Pod "client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005528983s
STEP: Saw pod success
Mar 15 09:08:56.717: INFO: Pod "client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8" satisfied condition "Succeeded or Failed"
Mar 15 09:08:56.719: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8 container test-container: <nil>
STEP: delete the pod
Mar 15 09:08:56.732: INFO: Waiting for pod client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8 to disappear
Mar 15 09:08:56.738: INFO: Pod client-containers-b43ee611-a6f0-4ced-88ac-a7030ac88ee8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:08:56.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5188" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":754,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:08:56.746: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 15 09:08:56.792: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:08:56.798: INFO: Number of nodes with available pods: 0
Mar 15 09:08:56.798: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:57.809: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:08:57.812: INFO: Number of nodes with available pods: 0
Mar 15 09:08:57.812: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:58.802: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:08:58.804: INFO: Number of nodes with available pods: 1
Mar 15 09:08:58.804: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:08:59.803: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:08:59.806: INFO: Number of nodes with available pods: 3
Mar 15 09:08:59.806: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 15 09:08:59.816: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:08:59.819: INFO: Number of nodes with available pods: 2
Mar 15 09:08:59.819: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:00.829: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:00.832: INFO: Number of nodes with available pods: 2
Mar 15 09:09:00.832: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:01.823: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:01.827: INFO: Number of nodes with available pods: 2
Mar 15 09:09:01.827: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:02.823: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:02.826: INFO: Number of nodes with available pods: 2
Mar 15 09:09:02.826: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:03.823: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:03.826: INFO: Number of nodes with available pods: 2
Mar 15 09:09:03.826: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:04.824: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:04.826: INFO: Number of nodes with available pods: 2
Mar 15 09:09:04.827: INFO: Node ip-10-0-2-120.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:09:05.823: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:09:05.825: INFO: Number of nodes with available pods: 3
Mar 15 09:09:05.826: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1721, will wait for the garbage collector to delete the pods
Mar 15 09:09:05.885: INFO: Deleting DaemonSet.extensions daemon-set took: 4.309612ms
Mar 15 09:09:06.785: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.204875ms
Mar 15 09:09:08.988: INFO: Number of nodes with available pods: 0
Mar 15 09:09:08.988: INFO: Number of running nodes: 0, number of available pods: 0
Mar 15 09:09:09.003: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1721/daemonsets","resourceVersion":"653275"},"items":null}

Mar 15 09:09:09.005: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1721/pods","resourceVersion":"653276"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:09:09.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1721" for this suite.

• [SLOW TEST:12.279 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":45,"skipped":762,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:09:09.025: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:09:09.055: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 15 09:09:12.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-9854 --namespace=crd-publish-openapi-9854 create -f -'
Mar 15 09:09:14.143: INFO: stderr: ""
Mar 15 09:09:14.143: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 15 09:09:14.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-9854 --namespace=crd-publish-openapi-9854 delete e2e-test-crd-publish-openapi-5019-crds test-cr'
Mar 15 09:09:14.228: INFO: stderr: ""
Mar 15 09:09:14.228: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 15 09:09:14.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-9854 --namespace=crd-publish-openapi-9854 apply -f -'
Mar 15 09:09:14.450: INFO: stderr: ""
Mar 15 09:09:14.450: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 15 09:09:14.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-9854 --namespace=crd-publish-openapi-9854 delete e2e-test-crd-publish-openapi-5019-crds test-cr'
Mar 15 09:09:14.537: INFO: stderr: ""
Mar 15 09:09:14.537: INFO: stdout: "e2e-test-crd-publish-openapi-5019-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 15 09:09:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-9854 explain e2e-test-crd-publish-openapi-5019-crds'
Mar 15 09:09:14.749: INFO: stderr: ""
Mar 15 09:09:14.749: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5019-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:09:18.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9854" for this suite.

• [SLOW TEST:9.475 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":46,"skipped":777,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:09:18.501: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:09:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:09:22.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3902" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:09:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1485
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 15 09:09:22.576: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 09:09:22.610: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 09:09:24.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:26.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:28.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:30.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:32.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:34.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:36.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:38.613: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:09:40.613: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 15 09:09:40.616: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 15 09:09:40.620: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 15 09:09:42.633: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.214:8080/dial?request=hostname&protocol=udp&host=10.20.54.145&port=8081&tries=1'] Namespace:pod-network-test-1485 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:09:42.633: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:09:42.743: INFO: Waiting for responses: map[]
Mar 15 09:09:42.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.214:8080/dial?request=hostname&protocol=udp&host=10.20.90.152&port=8081&tries=1'] Namespace:pod-network-test-1485 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:09:42.746: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:09:42.850: INFO: Waiting for responses: map[]
Mar 15 09:09:42.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.214:8080/dial?request=hostname&protocol=udp&host=10.20.71.213&port=8081&tries=1'] Namespace:pod-network-test-1485 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:09:42.853: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:09:42.955: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:09:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1485" for this suite.

• [SLOW TEST:20.403 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":48,"skipped":860,"failed":0}
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:09:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:09:43.006: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Pending, waiting for it to be Running (with Ready = true)
Mar 15 09:09:45.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:47.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:49.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:51.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:53.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:55.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:57.010: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:09:59.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:10:01.008: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:10:03.009: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = false)
Mar 15 09:10:05.008: INFO: The status of Pod test-webserver-ca6d1cb3-fbc4-4e26-b83d-7c2ddc7a7ddc is Running (Ready = true)
Mar 15 09:10:05.010: INFO: Container started at 2021-03-15 09:09:44 +0000 UTC, pod became ready at 2021-03-15 09:10:03 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:05.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5630" for this suite.

• [SLOW TEST:22.056 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":860,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:05.019: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Mar 15 09:10:09.598: INFO: Successfully updated pod "annotationupdate8eb85821-f6b9-48c3-bcbe-afd352a17395"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7173" for this suite.

• [SLOW TEST:6.604 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":50,"skipped":863,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:11.624: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:10:11.645: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:12.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6353" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":51,"skipped":869,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:12.757: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Mar 15 09:10:17.567: INFO: Successfully updated pod "labelsupdated415fee7-e4d5-4b2d-85dd-a49fb6005824"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:19.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4303" for this suite.

• [SLOW TEST:6.833 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":882,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:19.591: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-a950c063-5c76-4ffb-a8f9-fa340ba1de8f
STEP: Creating a pod to test consume configMaps
Mar 15 09:10:19.625: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96" in namespace "projected-8173" to be "Succeeded or Failed"
Mar 15 09:10:19.627: INFO: Pod "pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.566509ms
Mar 15 09:10:21.629: INFO: Pod "pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00388966s
STEP: Saw pod success
Mar 15 09:10:21.629: INFO: Pod "pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96" satisfied condition "Succeeded or Failed"
Mar 15 09:10:21.631: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:10:21.646: INFO: Waiting for pod pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96 to disappear
Mar 15 09:10:21.651: INFO: Pod pod-projected-configmaps-e4880550-4b28-4f56-9afb-2a27ceda7d96 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:21.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8173" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":53,"skipped":895,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:21.658: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:10:22.266: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:10:24.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396222, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396222, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396222, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396222, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:10:27.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:27.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7680" for this suite.
STEP: Destroying namespace "webhook-7680-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.720 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":54,"skipped":905,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:27.379: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-ef3d5e82-9c59-42c8-8007-9ce55121a2ae
STEP: Creating a pod to test consume configMaps
Mar 15 09:10:27.460: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c" in namespace "configmap-8545" to be "Succeeded or Failed"
Mar 15 09:10:27.468: INFO: Pod "pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019676ms
Mar 15 09:10:29.470: INFO: Pod "pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009444762s
Mar 15 09:10:31.472: INFO: Pod "pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011951465s
STEP: Saw pod success
Mar 15 09:10:31.473: INFO: Pod "pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c" satisfied condition "Succeeded or Failed"
Mar 15 09:10:31.474: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:10:31.493: INFO: Waiting for pod pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c to disappear
Mar 15 09:10:31.500: INFO: Pod pod-configmaps-9e995c64-7b8c-46fe-bfd2-4cce33a66d2c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:31.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8545" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:31.510: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 15 09:10:31.585: INFO: Waiting up to 5m0s for pod "pod-e452e671-9b08-4243-9a9f-5fb746f4a428" in namespace "emptydir-3667" to be "Succeeded or Failed"
Mar 15 09:10:31.587: INFO: Pod "pod-e452e671-9b08-4243-9a9f-5fb746f4a428": Phase="Pending", Reason="", readiness=false. Elapsed: 1.649531ms
Mar 15 09:10:33.590: INFO: Pod "pod-e452e671-9b08-4243-9a9f-5fb746f4a428": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440447s
Mar 15 09:10:35.593: INFO: Pod "pod-e452e671-9b08-4243-9a9f-5fb746f4a428": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007288381s
STEP: Saw pod success
Mar 15 09:10:35.593: INFO: Pod "pod-e452e671-9b08-4243-9a9f-5fb746f4a428" satisfied condition "Succeeded or Failed"
Mar 15 09:10:35.594: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-e452e671-9b08-4243-9a9f-5fb746f4a428 container test-container: <nil>
STEP: delete the pod
Mar 15 09:10:35.607: INFO: Waiting for pod pod-e452e671-9b08-4243-9a9f-5fb746f4a428 to disappear
Mar 15 09:10:35.613: INFO: Pod pod-e452e671-9b08-4243-9a9f-5fb746f4a428 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:35.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3667" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":989,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6676/configmap-test-236ebb0b-0d66-4050-a8b8-a9513605fe69
STEP: Creating a pod to test consume configMaps
Mar 15 09:10:35.661: INFO: Waiting up to 5m0s for pod "pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c" in namespace "configmap-6676" to be "Succeeded or Failed"
Mar 15 09:10:35.664: INFO: Pod "pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.954771ms
Mar 15 09:10:37.667: INFO: Pod "pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005977242s
Mar 15 09:10:39.670: INFO: Pod "pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009305563s
STEP: Saw pod success
Mar 15 09:10:39.670: INFO: Pod "pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c" satisfied condition "Succeeded or Failed"
Mar 15 09:10:39.672: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c container env-test: <nil>
STEP: delete the pod
Mar 15 09:10:39.685: INFO: Waiting for pod pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c to disappear
Mar 15 09:10:39.691: INFO: Pod pod-configmaps-c607cd95-1894-419e-85e7-5720d514a44c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:10:39.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6676" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":57,"skipped":1009,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:10:39.698: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Mar 15 09:10:39.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 create -f -'
Mar 15 09:10:40.000: INFO: stderr: ""
Mar 15 09:10:40.000: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 15 09:10:40.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:40.087: INFO: stderr: ""
Mar 15 09:10:40.087: INFO: stdout: "update-demo-nautilus-2h5j7 update-demo-nautilus-2wjzl "
Mar 15 09:10:40.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2h5j7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:10:40.173: INFO: stderr: ""
Mar 15 09:10:40.173: INFO: stdout: ""
Mar 15 09:10:40.173: INFO: update-demo-nautilus-2h5j7 is created but not running
Mar 15 09:10:45.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:45.251: INFO: stderr: ""
Mar 15 09:10:45.251: INFO: stdout: "update-demo-nautilus-2h5j7 update-demo-nautilus-2wjzl "
Mar 15 09:10:45.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2h5j7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:10:45.327: INFO: stderr: ""
Mar 15 09:10:45.327: INFO: stdout: "true"
Mar 15 09:10:45.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2h5j7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:10:45.401: INFO: stderr: ""
Mar 15 09:10:45.401: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:10:45.401: INFO: validating pod update-demo-nautilus-2h5j7
Mar 15 09:10:45.405: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:10:45.405: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:10:45.405: INFO: update-demo-nautilus-2h5j7 is verified up and running
Mar 15 09:10:45.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:10:45.480: INFO: stderr: ""
Mar 15 09:10:45.480: INFO: stdout: "true"
Mar 15 09:10:45.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:10:45.555: INFO: stderr: ""
Mar 15 09:10:45.555: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:10:45.555: INFO: validating pod update-demo-nautilus-2wjzl
Mar 15 09:10:45.559: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:10:45.559: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:10:45.559: INFO: update-demo-nautilus-2wjzl is verified up and running
STEP: scaling down the replication controller
Mar 15 09:10:45.561: INFO: scanned /root for discovery docs: <nil>
Mar 15 09:10:45.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 15 09:10:46.656: INFO: stderr: ""
Mar 15 09:10:46.656: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 15 09:10:46.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:46.735: INFO: stderr: ""
Mar 15 09:10:46.735: INFO: stdout: "update-demo-nautilus-2h5j7 update-demo-nautilus-2wjzl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 15 09:10:51.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:51.813: INFO: stderr: ""
Mar 15 09:10:51.813: INFO: stdout: "update-demo-nautilus-2h5j7 update-demo-nautilus-2wjzl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 15 09:10:56.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:56.891: INFO: stderr: ""
Mar 15 09:10:56.891: INFO: stdout: "update-demo-nautilus-2wjzl "
Mar 15 09:10:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:10:56.967: INFO: stderr: ""
Mar 15 09:10:56.967: INFO: stdout: "true"
Mar 15 09:10:56.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:10:57.043: INFO: stderr: ""
Mar 15 09:10:57.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:10:57.043: INFO: validating pod update-demo-nautilus-2wjzl
Mar 15 09:10:57.046: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:10:57.046: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:10:57.046: INFO: update-demo-nautilus-2wjzl is verified up and running
STEP: scaling up the replication controller
Mar 15 09:10:57.048: INFO: scanned /root for discovery docs: <nil>
Mar 15 09:10:57.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 15 09:10:58.147: INFO: stderr: ""
Mar 15 09:10:58.147: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 15 09:10:58.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:10:58.259: INFO: stderr: ""
Mar 15 09:10:58.259: INFO: stdout: "update-demo-nautilus-2mhtr update-demo-nautilus-2wjzl "
Mar 15 09:10:58.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2mhtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:10:58.339: INFO: stderr: ""
Mar 15 09:10:58.339: INFO: stdout: ""
Mar 15 09:10:58.339: INFO: update-demo-nautilus-2mhtr is created but not running
Mar 15 09:11:03.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 15 09:11:03.420: INFO: stderr: ""
Mar 15 09:11:03.420: INFO: stdout: "update-demo-nautilus-2mhtr update-demo-nautilus-2wjzl "
Mar 15 09:11:03.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2mhtr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:11:03.494: INFO: stderr: ""
Mar 15 09:11:03.494: INFO: stdout: "true"
Mar 15 09:11:03.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2mhtr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:11:03.569: INFO: stderr: ""
Mar 15 09:11:03.569: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:11:03.569: INFO: validating pod update-demo-nautilus-2mhtr
Mar 15 09:11:03.573: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:11:03.573: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:11:03.573: INFO: update-demo-nautilus-2mhtr is verified up and running
Mar 15 09:11:03.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 15 09:11:03.652: INFO: stderr: ""
Mar 15 09:11:03.652: INFO: stdout: "true"
Mar 15 09:11:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods update-demo-nautilus-2wjzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 15 09:11:03.725: INFO: stderr: ""
Mar 15 09:11:03.725: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 15 09:11:03.725: INFO: validating pod update-demo-nautilus-2wjzl
Mar 15 09:11:03.728: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 15 09:11:03.728: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 15 09:11:03.728: INFO: update-demo-nautilus-2wjzl is verified up and running
STEP: using delete to clean up resources
Mar 15 09:11:03.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 delete --grace-period=0 --force -f -'
Mar 15 09:11:03.809: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:11:03.809: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 15 09:11:03.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get rc,svc -l name=update-demo --no-headers'
Mar 15 09:11:03.891: INFO: stderr: "No resources found in kubectl-262 namespace.\n"
Mar 15 09:11:03.891: INFO: stdout: ""
Mar 15 09:11:03.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 09:11:03.970: INFO: stderr: ""
Mar 15 09:11:03.970: INFO: stdout: "update-demo-nautilus-2mhtr\nupdate-demo-nautilus-2wjzl\n"
Mar 15 09:11:04.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get rc,svc -l name=update-demo --no-headers'
Mar 15 09:11:04.552: INFO: stderr: "No resources found in kubectl-262 namespace.\n"
Mar 15 09:11:04.552: INFO: stdout: ""
Mar 15 09:11:04.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-262 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 15 09:11:04.657: INFO: stderr: ""
Mar 15 09:11:04.658: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-262" for this suite.

• [SLOW TEST:24.967 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":58,"skipped":1022,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:04.665: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:11:05.377: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:11:07.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396265, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396265, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396265, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396265, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:11:10.393: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:10.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-706" for this suite.
STEP: Destroying namespace "webhook-706-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.840 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":59,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:10.505: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-d4f30a24-6095-4bb2-9603-cd068ff8f9b4
STEP: Creating secret with name s-test-opt-upd-4cf19357-d3dd-4326-8315-76b133580d1a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d4f30a24-6095-4bb2-9603-cd068ff8f9b4
STEP: Updating secret s-test-opt-upd-4cf19357-d3dd-4326-8315-76b133580d1a
STEP: Creating secret with name s-test-opt-create-f69b2018-9b27-43ef-99ad-ac8a66c46a7d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:14.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8007" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":60,"skipped":1052,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:14.644: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:11:14.667: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-628c959d-0954-42b4-9f1e-76ed17120dca" in namespace "security-context-test-6702" to be "Succeeded or Failed"
Mar 15 09:11:14.677: INFO: Pod "busybox-readonly-false-628c959d-0954-42b4-9f1e-76ed17120dca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.427445ms
Mar 15 09:11:16.679: INFO: Pod "busybox-readonly-false-628c959d-0954-42b4-9f1e-76ed17120dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011688894s
Mar 15 09:11:16.679: INFO: Pod "busybox-readonly-false-628c959d-0954-42b4-9f1e-76ed17120dca" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6702" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":1069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:16.687: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 15 09:11:16.714: INFO: Waiting up to 5m0s for pod "pod-8acee90a-ae64-4ddf-b9c2-5757048079e0" in namespace "emptydir-5811" to be "Succeeded or Failed"
Mar 15 09:11:16.718: INFO: Pod "pod-8acee90a-ae64-4ddf-b9c2-5757048079e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139517ms
Mar 15 09:11:18.720: INFO: Pod "pod-8acee90a-ae64-4ddf-b9c2-5757048079e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006885694s
STEP: Saw pod success
Mar 15 09:11:18.721: INFO: Pod "pod-8acee90a-ae64-4ddf-b9c2-5757048079e0" satisfied condition "Succeeded or Failed"
Mar 15 09:11:18.722: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-8acee90a-ae64-4ddf-b9c2-5757048079e0 container test-container: <nil>
STEP: delete the pod
Mar 15 09:11:18.735: INFO: Waiting for pod pod-8acee90a-ae64-4ddf-b9c2-5757048079e0 to disappear
Mar 15 09:11:18.741: INFO: Pod pod-8acee90a-ae64-4ddf-b9c2-5757048079e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:18.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5811" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":1117,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:18.748: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Mar 15 09:11:18.766: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8480 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:11:18.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8480" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":63,"skipped":1134,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:11:18.921: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:11:19.010: INFO: Create a RollingUpdate DaemonSet
Mar 15 09:11:19.014: INFO: Check that daemon pods launch on every node of the cluster
Mar 15 09:11:19.017: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:19.019: INFO: Number of nodes with available pods: 0
Mar 15 09:11:19.020: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:11:20.024: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:20.027: INFO: Number of nodes with available pods: 0
Mar 15 09:11:20.027: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:11:21.036: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:21.038: INFO: Number of nodes with available pods: 2
Mar 15 09:11:21.038: INFO: Node ip-10-0-3-172.us-east-2.compute.internal is running more than one daemon pod
Mar 15 09:11:22.024: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:22.027: INFO: Number of nodes with available pods: 3
Mar 15 09:11:22.027: INFO: Number of running nodes: 3, number of available pods: 3
Mar 15 09:11:22.027: INFO: Update the DaemonSet to trigger a rollout
Mar 15 09:11:22.033: INFO: Updating DaemonSet daemon-set
Mar 15 09:11:33.047: INFO: Roll back the DaemonSet before rollout is complete
Mar 15 09:11:33.055: INFO: Updating DaemonSet daemon-set
Mar 15 09:11:33.055: INFO: Make sure DaemonSet rollback is complete
Mar 15 09:11:33.065: INFO: Wrong image for pod: daemon-set-b9h6h. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 15 09:11:33.065: INFO: Pod daemon-set-b9h6h is not available
Mar 15 09:11:33.078: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:34.096: INFO: Wrong image for pod: daemon-set-b9h6h. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 15 09:11:34.096: INFO: Pod daemon-set-b9h6h is not available
Mar 15 09:11:34.099: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 09:11:35.082: INFO: Pod daemon-set-jbzvh is not available
Mar 15 09:11:35.087: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7168, will wait for the garbage collector to delete the pods
Mar 15 09:11:35.149: INFO: Deleting DaemonSet.extensions daemon-set took: 3.824791ms
Mar 15 09:11:35.949: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.254168ms
Mar 15 09:13:12.151: INFO: Number of nodes with available pods: 0
Mar 15 09:13:12.151: INFO: Number of running nodes: 0, number of available pods: 0
Mar 15 09:13:12.153: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7168/daemonsets","resourceVersion":"654823"},"items":null}

Mar 15 09:13:12.154: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7168/pods","resourceVersion":"654823"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:13:12.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7168" for this suite.

• [SLOW TEST:113.248 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":64,"skipped":1147,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:13:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:13:12.611: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:13:14.617: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396392, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396392, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396392, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751396392, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:13:17.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:13:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7126" for this suite.
STEP: Destroying namespace "webhook-7126-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":65,"skipped":1162,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:13:17.681: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:13:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2031" for this suite.

• [SLOW TEST:11.057 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":66,"skipped":1163,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:13:28.739: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-5555e3ab-217c-49b7-acfe-dc65e1360efd in namespace container-probe-6272
Mar 15 09:13:32.830: INFO: Started pod busybox-5555e3ab-217c-49b7-acfe-dc65e1360efd in namespace container-probe-6272
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 09:13:32.832: INFO: Initial restart count of pod busybox-5555e3ab-217c-49b7-acfe-dc65e1360efd is 0
Mar 15 09:14:22.907: INFO: Restart count of pod container-probe-6272/busybox-5555e3ab-217c-49b7-acfe-dc65e1360efd is now 1 (50.074899533s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:22.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6272" for this suite.

• [SLOW TEST:54.188 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":67,"skipped":1177,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:22.926: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Mar 15 09:14:22.963: INFO: Waiting up to 5m0s for pod "client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825" in namespace "containers-1543" to be "Succeeded or Failed"
Mar 15 09:14:22.965: INFO: Pod "client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825": Phase="Pending", Reason="", readiness=false. Elapsed: 1.749384ms
Mar 15 09:14:24.968: INFO: Pod "client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004498639s
Mar 15 09:14:26.971: INFO: Pod "client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00768768s
STEP: Saw pod success
Mar 15 09:14:26.971: INFO: Pod "client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825" satisfied condition "Succeeded or Failed"
Mar 15 09:14:26.973: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825 container test-container: <nil>
STEP: delete the pod
Mar 15 09:14:27.000: INFO: Waiting for pod client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825 to disappear
Mar 15 09:14:27.009: INFO: Pod client-containers-0e8998eb-752c-45e8-b311-ddb99ece6825 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:27.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1543" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":1197,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:50.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6490" for this suite.

• [SLOW TEST:23.174 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1202,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:50.190: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-1644/secret-test-9aa93bf0-3348-45ec-9941-3db662bcda9c
STEP: Creating a pod to test consume secrets
Mar 15 09:14:50.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1" in namespace "secrets-1644" to be "Succeeded or Failed"
Mar 15 09:14:50.230: INFO: Pod "pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162604ms
Mar 15 09:14:52.232: INFO: Pod "pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004848788s
STEP: Saw pod success
Mar 15 09:14:52.232: INFO: Pod "pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1" satisfied condition "Succeeded or Failed"
Mar 15 09:14:52.234: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1 container env-test: <nil>
STEP: delete the pod
Mar 15 09:14:52.246: INFO: Waiting for pod pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1 to disappear
Mar 15 09:14:52.254: INFO: Pod pod-configmaps-f56211a2-6d1d-44be-88db-f0ad2f3ef2f1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:52.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1644" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:52.262: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:14:52.283: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:53.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8600" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":71,"skipped":1257,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:53.467: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:163
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:14:53.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1446" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":72,"skipped":1265,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:14:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0315 09:15:33.711897      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 09:15:33.711922      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 09:15:33.711929      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 15 09:15:33.711: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 15 09:15:33.711: INFO: Deleting pod "simpletest.rc-6bxt7" in namespace "gc-3690"
Mar 15 09:15:33.717: INFO: Deleting pod "simpletest.rc-7kq7n" in namespace "gc-3690"
Mar 15 09:15:33.730: INFO: Deleting pod "simpletest.rc-7xdpr" in namespace "gc-3690"
Mar 15 09:15:33.760: INFO: Deleting pod "simpletest.rc-84gkx" in namespace "gc-3690"
Mar 15 09:15:33.776: INFO: Deleting pod "simpletest.rc-cxspk" in namespace "gc-3690"
Mar 15 09:15:33.789: INFO: Deleting pod "simpletest.rc-j59kv" in namespace "gc-3690"
Mar 15 09:15:33.821: INFO: Deleting pod "simpletest.rc-lklc8" in namespace "gc-3690"
Mar 15 09:15:33.829: INFO: Deleting pod "simpletest.rc-pr2xb" in namespace "gc-3690"
Mar 15 09:15:33.857: INFO: Deleting pod "simpletest.rc-s9nlj" in namespace "gc-3690"
Mar 15 09:15:33.879: INFO: Deleting pod "simpletest.rc-z2m5x" in namespace "gc-3690"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:15:33.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3690" for this suite.

• [SLOW TEST:40.295 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":73,"skipped":1285,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:15:33.904: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-z49n
STEP: Creating a pod to test atomic-volume-subpath
Mar 15 09:15:33.938: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z49n" in namespace "subpath-6108" to be "Succeeded or Failed"
Mar 15 09:15:33.940: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.560643ms
Mar 15 09:15:35.943: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004899728s
Mar 15 09:15:37.947: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 4.009132516s
Mar 15 09:15:39.950: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 6.012238341s
Mar 15 09:15:41.953: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 8.014947819s
Mar 15 09:15:43.956: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 10.017949136s
Mar 15 09:15:45.959: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 12.020971393s
Mar 15 09:15:47.962: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 14.024405914s
Mar 15 09:15:49.965: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 16.027523973s
Mar 15 09:15:51.968: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 18.030490073s
Mar 15 09:15:53.972: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 20.033861238s
Mar 15 09:15:55.975: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Running", Reason="", readiness=true. Elapsed: 22.036804375s
Mar 15 09:15:57.978: INFO: Pod "pod-subpath-test-configmap-z49n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.040058202s
STEP: Saw pod success
Mar 15 09:15:57.978: INFO: Pod "pod-subpath-test-configmap-z49n" satisfied condition "Succeeded or Failed"
Mar 15 09:15:57.979: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-subpath-test-configmap-z49n container test-container-subpath-configmap-z49n: <nil>
STEP: delete the pod
Mar 15 09:15:57.996: INFO: Waiting for pod pod-subpath-test-configmap-z49n to disappear
Mar 15 09:15:57.999: INFO: Pod pod-subpath-test-configmap-z49n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z49n
Mar 15 09:15:57.999: INFO: Deleting pod "pod-subpath-test-configmap-z49n" in namespace "subpath-6108"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:15:58.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6108" for this suite.

• [SLOW TEST:24.104 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":74,"skipped":1306,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:15:58.008: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2649.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2649.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 09:16:02.126: INFO: DNS probes using dns-2649/dns-test-ea4fcbcd-aafe-401f-8eb3-286c394179fc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:16:02.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2649" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":75,"skipped":1307,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:16:02.148: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Mar 15 09:16:02.174: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 15 09:16:02.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:02.529: INFO: stderr: ""
Mar 15 09:16:02.529: INFO: stdout: "service/agnhost-replica created\n"
Mar 15 09:16:02.529: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 15 09:16:02.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:02.784: INFO: stderr: ""
Mar 15 09:16:02.784: INFO: stdout: "service/agnhost-primary created\n"
Mar 15 09:16:02.785: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 15 09:16:02.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:03.074: INFO: stderr: ""
Mar 15 09:16:03.076: INFO: stdout: "service/frontend created\n"
Mar 15 09:16:03.076: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 15 09:16:03.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:03.358: INFO: stderr: ""
Mar 15 09:16:03.358: INFO: stdout: "deployment.apps/frontend created\n"
Mar 15 09:16:03.358: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 15 09:16:03.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:03.577: INFO: stderr: ""
Mar 15 09:16:03.577: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 15 09:16:03.578: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 15 09:16:03.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 create -f -'
Mar 15 09:16:03.875: INFO: stderr: ""
Mar 15 09:16:03.875: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 15 09:16:03.875: INFO: Waiting for all frontend pods to be Running.
Mar 15 09:16:08.925: INFO: Waiting for frontend to serve content.
Mar 15 09:16:08.939: INFO: Trying to add a new entry to the guestbook.
Mar 15 09:16:08.951: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 15 09:16:08.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.130: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.130: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 15 09:16:09.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.264: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.264: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 15 09:16:09.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.433: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.442: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 15 09:16:09.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.560: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.560: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 15 09:16:09.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.642: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.642: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 15 09:16:09.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-5286 delete --grace-period=0 --force -f -'
Mar 15 09:16:09.749: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 15 09:16:09.749: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:16:09.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5286" for this suite.

• [SLOW TEST:7.608 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":76,"skipped":1315,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:16:09.756: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Mar 15 09:16:10.295: INFO: created pod pod-service-account-defaultsa
Mar 15 09:16:10.295: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 15 09:16:10.298: INFO: created pod pod-service-account-mountsa
Mar 15 09:16:10.299: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 15 09:16:10.305: INFO: created pod pod-service-account-nomountsa
Mar 15 09:16:10.305: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 15 09:16:10.316: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 15 09:16:10.316: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 15 09:16:10.329: INFO: created pod pod-service-account-mountsa-mountspec
Mar 15 09:16:10.329: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 15 09:16:10.338: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 15 09:16:10.338: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 15 09:16:10.347: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 15 09:16:10.347: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 15 09:16:10.367: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 15 09:16:10.367: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 15 09:16:10.378: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 15 09:16:10.378: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:16:10.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8515" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":77,"skipped":1316,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:16:10.419: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-d19c8e51-ef28-4111-baad-bae6fcf4e2da in namespace container-probe-3402
Mar 15 09:16:16.461: INFO: Started pod liveness-d19c8e51-ef28-4111-baad-bae6fcf4e2da in namespace container-probe-3402
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 09:16:16.476: INFO: Initial restart count of pod liveness-d19c8e51-ef28-4111-baad-bae6fcf4e2da is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:16.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3402" for this suite.

• [SLOW TEST:246.476 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1347,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-60016ddf-bf4d-42e8-81d9-49df3659b7d5
STEP: Creating a pod to test consume configMaps
Mar 15 09:20:16.983: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812" in namespace "projected-218" to be "Succeeded or Failed"
Mar 15 09:20:16.990: INFO: Pod "pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812": Phase="Pending", Reason="", readiness=false. Elapsed: 7.298196ms
Mar 15 09:20:18.993: INFO: Pod "pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010169385s
Mar 15 09:20:20.996: INFO: Pod "pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013119679s
STEP: Saw pod success
Mar 15 09:20:20.996: INFO: Pod "pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812" satisfied condition "Succeeded or Failed"
Mar 15 09:20:20.998: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:20:21.024: INFO: Waiting for pod pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812 to disappear
Mar 15 09:20:21.031: INFO: Pod pod-projected-configmaps-36014eb9-50a3-427c-8c38-e48ac12b9812 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:21.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-218" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":79,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:21.038: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-6dceb1c5-959a-469f-bdef-5521d4f4c9bb
STEP: Creating a pod to test consume configMaps
Mar 15 09:20:21.069: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1" in namespace "configmap-1077" to be "Succeeded or Failed"
Mar 15 09:20:21.070: INFO: Pod "pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.63516ms
Mar 15 09:20:23.073: INFO: Pod "pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004710034s
Mar 15 09:20:25.077: INFO: Pod "pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007846472s
STEP: Saw pod success
Mar 15 09:20:25.077: INFO: Pod "pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1" satisfied condition "Succeeded or Failed"
Mar 15 09:20:25.078: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:20:25.091: INFO: Waiting for pod pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1 to disappear
Mar 15 09:20:25.099: INFO: Pod pod-configmaps-5d8180cd-3068-4aa2-b551-90728555cde1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:25.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1077" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1373,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:25.106: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 15 09:20:29.649: INFO: Successfully updated pod "adopt-release-4sqhs"
STEP: Checking that the Job readopts the Pod
Mar 15 09:20:29.649: INFO: Waiting up to 15m0s for pod "adopt-release-4sqhs" in namespace "job-6292" to be "adopted"
Mar 15 09:20:29.652: INFO: Pod "adopt-release-4sqhs": Phase="Running", Reason="", readiness=true. Elapsed: 3.544344ms
Mar 15 09:20:31.655: INFO: Pod "adopt-release-4sqhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006560641s
Mar 15 09:20:31.655: INFO: Pod "adopt-release-4sqhs" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 15 09:20:32.162: INFO: Successfully updated pod "adopt-release-4sqhs"
STEP: Checking that the Job releases the Pod
Mar 15 09:20:32.162: INFO: Waiting up to 15m0s for pod "adopt-release-4sqhs" in namespace "job-6292" to be "released"
Mar 15 09:20:32.167: INFO: Pod "adopt-release-4sqhs": Phase="Running", Reason="", readiness=true. Elapsed: 4.997982ms
Mar 15 09:20:32.167: INFO: Pod "adopt-release-4sqhs" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6292" for this suite.

• [SLOW TEST:7.087 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":81,"skipped":1375,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:32.194: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8072.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8072.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8072.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8072.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 09:20:36.319: INFO: DNS probes using dns-8072/dns-test-5470e18f-5f79-4176-862a-90885ce66466 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:36.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8072" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":82,"skipped":1376,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:36.367: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 15 09:20:39.420: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:39.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6658" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":83,"skipped":1391,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:39.443: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:20:41.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-525" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":84,"skipped":1392,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:20:41.539: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-625fe4fc-8bfa-4f86-97d8-66d7b93b6d77
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-625fe4fc-8bfa-4f86-97d8-66d7b93b6d77
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:21:53.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3502" for this suite.

• [SLOW TEST:72.343 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1396,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:21:53.882: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l74cn in namespace proxy-7559
I0315 09:21:53.932407      20 runners.go:190] Created replication controller with name: proxy-service-l74cn, namespace: proxy-7559, replica count: 1
I0315 09:21:54.982901      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 09:21:55.983120      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:21:56.983375      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:21:57.983660      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:21:58.983888      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:21:59.984158      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:22:00.984395      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0315 09:22:01.984544      20 runners.go:190] proxy-service-l74cn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 09:22:01.992: INFO: setup took 8.081886514s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 15 09:22:02.015: INFO: (0) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 22.143402ms)
Mar 15 09:22:02.019: INFO: (0) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 26.278717ms)
Mar 15 09:22:02.019: INFO: (0) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 26.586511ms)
Mar 15 09:22:02.019: INFO: (0) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 26.183533ms)
Mar 15 09:22:02.019: INFO: (0) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 26.019928ms)
Mar 15 09:22:02.019: INFO: (0) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 26.128581ms)
Mar 15 09:22:02.022: INFO: (0) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 29.768787ms)
Mar 15 09:22:02.023: INFO: (0) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 29.669602ms)
Mar 15 09:22:02.024: INFO: (0) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 31.140163ms)
Mar 15 09:22:02.024: INFO: (0) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 31.050277ms)
Mar 15 09:22:02.024: INFO: (0) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 31.331564ms)
Mar 15 09:22:02.024: INFO: (0) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 31.20666ms)
Mar 15 09:22:02.024: INFO: (0) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 31.393785ms)
Mar 15 09:22:02.028: INFO: (0) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 35.459308ms)
Mar 15 09:22:02.030: INFO: (0) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 37.351123ms)
Mar 15 09:22:02.030: INFO: (0) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 37.463329ms)
Mar 15 09:22:02.038: INFO: (1) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 7.199339ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 19.569728ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 19.634781ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 19.658451ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 19.455613ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 20.005769ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 20.456067ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 20.02614ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 20.456308ms)
Mar 15 09:22:02.051: INFO: (1) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 20.312108ms)
Mar 15 09:22:02.056: INFO: (1) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 24.976232ms)
Mar 15 09:22:02.057: INFO: (1) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 25.520058ms)
Mar 15 09:22:02.057: INFO: (1) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 25.732696ms)
Mar 15 09:22:02.057: INFO: (1) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 26.088747ms)
Mar 15 09:22:02.057: INFO: (1) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 25.526314ms)
Mar 15 09:22:02.057: INFO: (1) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 26.125977ms)
Mar 15 09:22:02.075: INFO: (2) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 18.150447ms)
Mar 15 09:22:02.075: INFO: (2) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 18.050767ms)
Mar 15 09:22:02.075: INFO: (2) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 18.238609ms)
Mar 15 09:22:02.075: INFO: (2) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 18.057488ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 18.117999ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 18.179054ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 18.24128ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 18.326342ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 18.560154ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 18.110196ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 18.219705ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 18.412235ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 19.114052ms)
Mar 15 09:22:02.076: INFO: (2) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 18.967409ms)
Mar 15 09:22:02.077: INFO: (2) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 19.443635ms)
Mar 15 09:22:02.080: INFO: (2) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 23.178494ms)
Mar 15 09:22:02.091: INFO: (3) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 10.587183ms)
Mar 15 09:22:02.091: INFO: (3) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 10.099675ms)
Mar 15 09:22:02.091: INFO: (3) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 10.588285ms)
Mar 15 09:22:02.091: INFO: (3) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.251432ms)
Mar 15 09:22:02.091: INFO: (3) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 10.451907ms)
Mar 15 09:22:02.093: INFO: (3) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 11.739353ms)
Mar 15 09:22:02.093: INFO: (3) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 12.14679ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 18.007403ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 18.095037ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 18.495162ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 18.258765ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 18.340633ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 18.036132ms)
Mar 15 09:22:02.099: INFO: (3) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 18.551605ms)
Mar 15 09:22:02.103: INFO: (3) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 21.78419ms)
Mar 15 09:22:02.104: INFO: (3) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 23.042083ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 13.97139ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 14.53588ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 14.427462ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 14.422884ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 14.064433ms)
Mar 15 09:22:02.119: INFO: (4) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 14.626641ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 15.312875ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 14.463239ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 15.471478ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 14.693431ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 14.844472ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 15.192542ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 15.415ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 14.961643ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 15.375652ms)
Mar 15 09:22:02.120: INFO: (4) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 16.03915ms)
Mar 15 09:22:02.129: INFO: (5) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 8.019656ms)
Mar 15 09:22:02.129: INFO: (5) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 8.432644ms)
Mar 15 09:22:02.132: INFO: (5) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.433352ms)
Mar 15 09:22:02.132: INFO: (5) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 11.979945ms)
Mar 15 09:22:02.132: INFO: (5) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 11.6274ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 11.952718ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 12.090879ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 11.814042ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 11.910644ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 11.952312ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 12.189886ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 12.311998ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 12.137966ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 12.563063ms)
Mar 15 09:22:02.133: INFO: (5) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 12.535361ms)
Mar 15 09:22:02.134: INFO: (5) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.621208ms)
Mar 15 09:22:02.143: INFO: (6) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 8.991211ms)
Mar 15 09:22:02.144: INFO: (6) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 9.481957ms)
Mar 15 09:22:02.144: INFO: (6) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.36756ms)
Mar 15 09:22:02.144: INFO: (6) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 9.592255ms)
Mar 15 09:22:02.144: INFO: (6) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.470251ms)
Mar 15 09:22:02.144: INFO: (6) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.692681ms)
Mar 15 09:22:02.145: INFO: (6) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.095576ms)
Mar 15 09:22:02.146: INFO: (6) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 11.393038ms)
Mar 15 09:22:02.146: INFO: (6) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 11.68159ms)
Mar 15 09:22:02.146: INFO: (6) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 11.809693ms)
Mar 15 09:22:02.147: INFO: (6) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 13.044528ms)
Mar 15 09:22:02.150: INFO: (6) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 15.350258ms)
Mar 15 09:22:02.150: INFO: (6) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 15.955875ms)
Mar 15 09:22:02.151: INFO: (6) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 16.103819ms)
Mar 15 09:22:02.151: INFO: (6) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 15.817031ms)
Mar 15 09:22:02.151: INFO: (6) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 15.976835ms)
Mar 15 09:22:02.159: INFO: (7) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 8.341333ms)
Mar 15 09:22:02.159: INFO: (7) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 8.444304ms)
Mar 15 09:22:02.160: INFO: (7) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 8.549683ms)
Mar 15 09:22:02.160: INFO: (7) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 9.24328ms)
Mar 15 09:22:02.161: INFO: (7) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 9.524007ms)
Mar 15 09:22:02.163: INFO: (7) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 12.009884ms)
Mar 15 09:22:02.165: INFO: (7) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 14.052921ms)
Mar 15 09:22:02.165: INFO: (7) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 14.126537ms)
Mar 15 09:22:02.165: INFO: (7) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 13.892961ms)
Mar 15 09:22:02.165: INFO: (7) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 14.106294ms)
Mar 15 09:22:02.165: INFO: (7) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 14.429918ms)
Mar 15 09:22:02.166: INFO: (7) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 15.005431ms)
Mar 15 09:22:02.167: INFO: (7) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 15.96823ms)
Mar 15 09:22:02.167: INFO: (7) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 15.846364ms)
Mar 15 09:22:02.167: INFO: (7) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 16.17725ms)
Mar 15 09:22:02.168: INFO: (7) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 17.198302ms)
Mar 15 09:22:02.174: INFO: (8) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 5.824509ms)
Mar 15 09:22:02.177: INFO: (8) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 8.536423ms)
Mar 15 09:22:02.177: INFO: (8) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 8.361399ms)
Mar 15 09:22:02.177: INFO: (8) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 8.873714ms)
Mar 15 09:22:02.177: INFO: (8) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 8.825655ms)
Mar 15 09:22:02.177: INFO: (8) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 8.969595ms)
Mar 15 09:22:02.179: INFO: (8) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 11.187356ms)
Mar 15 09:22:02.182: INFO: (8) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 12.611249ms)
Mar 15 09:22:02.182: INFO: (8) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 12.757991ms)
Mar 15 09:22:02.182: INFO: (8) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 12.848991ms)
Mar 15 09:22:02.182: INFO: (8) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 13.350774ms)
Mar 15 09:22:02.182: INFO: (8) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 12.742548ms)
Mar 15 09:22:02.183: INFO: (8) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.89318ms)
Mar 15 09:22:02.183: INFO: (8) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 14.706597ms)
Mar 15 09:22:02.184: INFO: (8) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 15.243513ms)
Mar 15 09:22:02.184: INFO: (8) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 15.532664ms)
Mar 15 09:22:02.193: INFO: (9) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 8.921973ms)
Mar 15 09:22:02.193: INFO: (9) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 8.968205ms)
Mar 15 09:22:02.194: INFO: (9) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 9.8547ms)
Mar 15 09:22:02.194: INFO: (9) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 9.820292ms)
Mar 15 09:22:02.194: INFO: (9) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.534606ms)
Mar 15 09:22:02.194: INFO: (9) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.529012ms)
Mar 15 09:22:02.194: INFO: (9) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.949264ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 12.644483ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 12.769924ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 12.765963ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 12.722777ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.117483ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 13.093434ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 12.956266ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 13.181055ms)
Mar 15 09:22:02.197: INFO: (9) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 13.363536ms)
Mar 15 09:22:02.207: INFO: (10) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 9.66062ms)
Mar 15 09:22:02.207: INFO: (10) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.430154ms)
Mar 15 09:22:02.207: INFO: (10) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 9.853478ms)
Mar 15 09:22:02.209: INFO: (10) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 10.267366ms)
Mar 15 09:22:02.210: INFO: (10) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 11.913388ms)
Mar 15 09:22:02.210: INFO: (10) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 12.405776ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 12.823897ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 12.802704ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 12.524415ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 12.635642ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 12.86245ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 13.213387ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 13.097993ms)
Mar 15 09:22:02.211: INFO: (10) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 13.588431ms)
Mar 15 09:22:02.212: INFO: (10) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 13.562431ms)
Mar 15 09:22:02.213: INFO: (10) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 14.106161ms)
Mar 15 09:22:02.221: INFO: (11) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 7.618177ms)
Mar 15 09:22:02.224: INFO: (11) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 10.416272ms)
Mar 15 09:22:02.224: INFO: (11) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 10.818132ms)
Mar 15 09:22:02.224: INFO: (11) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 11.317027ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.728011ms)
Mar 15 09:22:02.224: INFO: (11) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 11.403364ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 11.173367ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 11.062076ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 12.034008ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 11.525421ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 11.928446ms)
Mar 15 09:22:02.225: INFO: (11) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 12.053771ms)
Mar 15 09:22:02.227: INFO: (11) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 13.549691ms)
Mar 15 09:22:02.227: INFO: (11) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 13.642543ms)
Mar 15 09:22:02.227: INFO: (11) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 14.07567ms)
Mar 15 09:22:02.228: INFO: (11) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 14.669274ms)
Mar 15 09:22:02.241: INFO: (12) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 13.244722ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 13.480825ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 13.22004ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 13.137654ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 13.353853ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.521119ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 13.156911ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 13.665564ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 13.367338ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 13.52002ms)
Mar 15 09:22:02.242: INFO: (12) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 13.754302ms)
Mar 15 09:22:02.249: INFO: (12) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 20.862346ms)
Mar 15 09:22:02.250: INFO: (12) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 21.428227ms)
Mar 15 09:22:02.250: INFO: (12) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 21.208855ms)
Mar 15 09:22:02.250: INFO: (12) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 21.840417ms)
Mar 15 09:22:02.251: INFO: (12) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 21.978147ms)
Mar 15 09:22:02.265: INFO: (13) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 14.167749ms)
Mar 15 09:22:02.265: INFO: (13) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 14.178582ms)
Mar 15 09:22:02.266: INFO: (13) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 14.909258ms)
Mar 15 09:22:02.266: INFO: (13) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 14.967497ms)
Mar 15 09:22:02.266: INFO: (13) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 15.67433ms)
Mar 15 09:22:02.266: INFO: (13) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 15.274086ms)
Mar 15 09:22:02.266: INFO: (13) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 15.849398ms)
Mar 15 09:22:02.267: INFO: (13) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 16.303321ms)
Mar 15 09:22:02.267: INFO: (13) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 16.241864ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 17.3057ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 17.318906ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 17.173564ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 17.324602ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 17.259094ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 17.260249ms)
Mar 15 09:22:02.268: INFO: (13) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 17.254898ms)
Mar 15 09:22:02.277: INFO: (14) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 8.216426ms)
Mar 15 09:22:02.277: INFO: (14) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 7.549967ms)
Mar 15 09:22:02.278: INFO: (14) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.514287ms)
Mar 15 09:22:02.280: INFO: (14) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.195753ms)
Mar 15 09:22:02.280: INFO: (14) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 11.220441ms)
Mar 15 09:22:02.281: INFO: (14) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 12.116835ms)
Mar 15 09:22:02.281: INFO: (14) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 12.274783ms)
Mar 15 09:22:02.281: INFO: (14) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 12.97417ms)
Mar 15 09:22:02.282: INFO: (14) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 12.113796ms)
Mar 15 09:22:02.282: INFO: (14) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 12.373013ms)
Mar 15 09:22:02.282: INFO: (14) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.050051ms)
Mar 15 09:22:02.282: INFO: (14) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 12.41679ms)
Mar 15 09:22:02.283: INFO: (14) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 13.644032ms)
Mar 15 09:22:02.284: INFO: (14) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 14.214424ms)
Mar 15 09:22:02.284: INFO: (14) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 14.537899ms)
Mar 15 09:22:02.284: INFO: (14) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 15.436347ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.318778ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 9.656017ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.469051ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 9.658471ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 9.543949ms)
Mar 15 09:22:02.294: INFO: (15) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 9.948388ms)
Mar 15 09:22:02.297: INFO: (15) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 12.598956ms)
Mar 15 09:22:02.297: INFO: (15) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 12.900326ms)
Mar 15 09:22:02.298: INFO: (15) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 13.500137ms)
Mar 15 09:22:02.298: INFO: (15) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 13.533902ms)
Mar 15 09:22:02.298: INFO: (15) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.823252ms)
Mar 15 09:22:02.298: INFO: (15) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 14.067275ms)
Mar 15 09:22:02.299: INFO: (15) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 14.509219ms)
Mar 15 09:22:02.300: INFO: (15) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 15.905086ms)
Mar 15 09:22:02.300: INFO: (15) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 15.955055ms)
Mar 15 09:22:02.302: INFO: (15) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 17.326576ms)
Mar 15 09:22:02.312: INFO: (16) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 10.002649ms)
Mar 15 09:22:02.312: INFO: (16) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 10.487488ms)
Mar 15 09:22:02.312: INFO: (16) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 10.147029ms)
Mar 15 09:22:02.313: INFO: (16) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 10.792362ms)
Mar 15 09:22:02.313: INFO: (16) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 10.797308ms)
Mar 15 09:22:02.313: INFO: (16) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 10.9102ms)
Mar 15 09:22:02.313: INFO: (16) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 11.217835ms)
Mar 15 09:22:02.316: INFO: (16) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 14.169044ms)
Mar 15 09:22:02.316: INFO: (16) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 14.148257ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 14.570579ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 14.54449ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 14.791093ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 15.248247ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 15.056914ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 15.116656ms)
Mar 15 09:22:02.317: INFO: (16) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 14.896313ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 12.844038ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 12.887763ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 13.037726ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 13.070883ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 13.171818ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 13.382068ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 13.61969ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 13.513844ms)
Mar 15 09:22:02.331: INFO: (17) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 13.590304ms)
Mar 15 09:22:02.333: INFO: (17) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 15.575771ms)
Mar 15 09:22:02.333: INFO: (17) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 15.636386ms)
Mar 15 09:22:02.333: INFO: (17) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 15.79987ms)
Mar 15 09:22:02.333: INFO: (17) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 15.963539ms)
Mar 15 09:22:02.334: INFO: (17) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 16.468433ms)
Mar 15 09:22:02.334: INFO: (17) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 16.784069ms)
Mar 15 09:22:02.335: INFO: (17) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 16.897408ms)
Mar 15 09:22:02.347: INFO: (18) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 11.886906ms)
Mar 15 09:22:02.347: INFO: (18) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 11.608627ms)
Mar 15 09:22:02.347: INFO: (18) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 11.911384ms)
Mar 15 09:22:02.347: INFO: (18) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 11.945306ms)
Mar 15 09:22:02.347: INFO: (18) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 12.269761ms)
Mar 15 09:22:02.351: INFO: (18) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 16.409395ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 16.498316ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 16.605169ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 16.832714ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 16.838127ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 16.588914ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 16.814101ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 16.706108ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 16.595787ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 16.938462ms)
Mar 15 09:22:02.352: INFO: (18) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 16.609918ms)
Mar 15 09:22:02.357: INFO: (19) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:160/proxy/: foo (200; 4.52252ms)
Mar 15 09:22:02.357: INFO: (19) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:443/proxy/tlsrewritem... (200; 4.87455ms)
Mar 15 09:22:02.357: INFO: (19) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:160/proxy/: foo (200; 4.952963ms)
Mar 15 09:22:02.357: INFO: (19) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:1080/proxy/rewriteme">test<... (200; 5.06241ms)
Mar 15 09:22:02.366: INFO: (19) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname1/proxy/: foo (200; 13.2812ms)
Mar 15 09:22:02.366: INFO: (19) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:1080/proxy/rewriteme">... (200; 12.798209ms)
Mar 15 09:22:02.366: INFO: (19) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname2/proxy/: tls qux (200; 13.405159ms)
Mar 15 09:22:02.366: INFO: (19) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:462/proxy/: tls qux (200; 13.281707ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/services/https:proxy-service-l74cn:tlsportname1/proxy/: tls baz (200; 13.775253ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname1/proxy/: foo (200; 14.223492ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/services/proxy-service-l74cn:portname2/proxy/: bar (200; 14.245272ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/: <a href="/api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh/proxy/rewriteme">test</a> (200; 14.449171ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/pods/proxy-service-l74cn-zcprh:162/proxy/: bar (200; 14.38552ms)
Mar 15 09:22:02.367: INFO: (19) /api/v1/namespaces/proxy-7559/pods/https:proxy-service-l74cn-zcprh:460/proxy/: tls baz (200; 14.474584ms)
Mar 15 09:22:02.368: INFO: (19) /api/v1/namespaces/proxy-7559/services/http:proxy-service-l74cn:portname2/proxy/: bar (200; 15.077472ms)
Mar 15 09:22:02.368: INFO: (19) /api/v1/namespaces/proxy-7559/pods/http:proxy-service-l74cn-zcprh:162/proxy/: bar (200; 15.204807ms)
STEP: deleting ReplicationController proxy-service-l74cn in namespace proxy-7559, will wait for the garbage collector to delete the pods
Mar 15 09:22:02.433: INFO: Deleting ReplicationController proxy-service-l74cn took: 12.710125ms
Mar 15 09:22:03.234: INFO: Terminating ReplicationController proxy-service-l74cn pods took: 800.589708ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:22:05.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7559" for this suite.

• [SLOW TEST:11.161 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":86,"skipped":1414,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:22:05.044: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Mar 15 09:22:05.118: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 09:23:05.147: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:23:05.149: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 15 09:23:09.204: INFO: found a healthy node: ip-10-0-3-172.us-east-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:23:23.276: INFO: pods created so far: [1 1 1]
Mar 15 09:23:23.276: INFO: length of pods created so far: 3
Mar 15 09:23:37.283: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:23:44.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3840" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:23:44.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3356" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:99.306 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":87,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:23:44.352: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 15 09:23:44.400: INFO: Waiting up to 5m0s for pod "pod-561b65e8-5351-4d47-8971-0846932a908c" in namespace "emptydir-8419" to be "Succeeded or Failed"
Mar 15 09:23:44.403: INFO: Pod "pod-561b65e8-5351-4d47-8971-0846932a908c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.623291ms
Mar 15 09:23:46.406: INFO: Pod "pod-561b65e8-5351-4d47-8971-0846932a908c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005702473s
STEP: Saw pod success
Mar 15 09:23:46.406: INFO: Pod "pod-561b65e8-5351-4d47-8971-0846932a908c" satisfied condition "Succeeded or Failed"
Mar 15 09:23:46.407: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-561b65e8-5351-4d47-8971-0846932a908c container test-container: <nil>
STEP: delete the pod
Mar 15 09:23:46.434: INFO: Waiting for pod pod-561b65e8-5351-4d47-8971-0846932a908c to disappear
Mar 15 09:23:46.441: INFO: Pod pod-561b65e8-5351-4d47-8971-0846932a908c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:23:46.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8419" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":88,"skipped":1449,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:23:46.448: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9895
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9895
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9895
Mar 15 09:23:46.487: INFO: Found 0 stateful pods, waiting for 1
Mar 15 09:23:56.490: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 15 09:23:56.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:23:57.583: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:23:57.583: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:23:57.583: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:23:57.586: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 15 09:24:07.590: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:24:07.590: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:24:07.601: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998602s
Mar 15 09:24:08.604: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994387166s
Mar 15 09:24:09.607: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991397411s
Mar 15 09:24:10.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987923856s
Mar 15 09:24:11.614: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984425475s
Mar 15 09:24:12.617: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981398853s
Mar 15 09:24:13.620: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977887385s
Mar 15 09:24:14.624: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974949752s
Mar 15 09:24:15.627: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971714658s
Mar 15 09:24:16.630: INFO: Verifying statefulset ss doesn't scale past 1 for another 968.416543ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9895
Mar 15 09:24:17.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:24:17.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 09:24:17.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:24:17.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:24:17.823: INFO: Found 1 stateful pods, waiting for 3
Mar 15 09:24:27.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 09:24:27.827: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 09:24:27.827: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 15 09:24:27.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:24:28.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:24:28.017: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:24:28.017: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:24:28.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:24:28.217: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:24:28.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:24:28.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:24:28.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:24:28.420: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:24:28.420: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:24:28.420: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:24:28.420: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:24:28.422: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 15 09:24:38.427: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:24:38.427: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:24:38.427: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:24:38.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999857s
Mar 15 09:24:39.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996362596s
Mar 15 09:24:40.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992934095s
Mar 15 09:24:41.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988319122s
Mar 15 09:24:42.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984328007s
Mar 15 09:24:43.456: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979438914s
Mar 15 09:24:44.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9757303s
Mar 15 09:24:45.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972115907s
Mar 15 09:24:46.468: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968115693s
Mar 15 09:24:47.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.566205ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9895
Mar 15 09:24:48.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:24:48.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 09:24:48.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:24:48.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:24:48.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:24:48.871: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 09:24:48.871: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:24:48.871: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:24:48.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9895 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:24:49.080: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 09:24:49.080: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:24:49.080: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:24:49.080: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 09:25:09.091: INFO: Deleting all statefulset in ns statefulset-9895
Mar 15 09:25:09.093: INFO: Scaling statefulset ss to 0
Mar 15 09:25:09.099: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:25:09.100: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:09.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9895" for this suite.

• [SLOW TEST:82.671 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":89,"skipped":1455,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:09.121: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 09:25:11.184: INFO: DNS probes using dns-7903/dns-test-a6c8ec57-7251-4d49-aee4-00cce150d209 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7903" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":90,"skipped":1460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:11.206: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Mar 15 09:25:11.232: INFO: created test-podtemplate-1
Mar 15 09:25:11.235: INFO: created test-podtemplate-2
Mar 15 09:25:11.242: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 15 09:25:11.245: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 15 09:25:11.254: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:11.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8576" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":91,"skipped":1571,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:11.261: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:24.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4473" for this suite.
STEP: Destroying namespace "nsdeletetest-440" for this suite.
Mar 15 09:25:24.402: INFO: Namespace nsdeletetest-440 was already deleted
STEP: Destroying namespace "nsdeletetest-3908" for this suite.

• [SLOW TEST:13.144 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":92,"skipped":1573,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:25:25.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:25:27.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397125, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397125, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397125, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397125, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:25:30.021: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:25:30.024: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7534-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:31.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-733" for this suite.
STEP: Destroying namespace "webhook-733-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.874 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":93,"skipped":1589,"failed":0}
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:31.281: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Mar 15 09:25:31.310: INFO: Waiting up to 5m0s for pod "downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db" in namespace "downward-api-7174" to be "Succeeded or Failed"
Mar 15 09:25:31.312: INFO: Pod "downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481498ms
Mar 15 09:25:33.314: INFO: Pod "downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003654661s
Mar 15 09:25:35.317: INFO: Pod "downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006696355s
STEP: Saw pod success
Mar 15 09:25:35.317: INFO: Pod "downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db" satisfied condition "Succeeded or Failed"
Mar 15 09:25:35.319: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db container dapi-container: <nil>
STEP: delete the pod
Mar 15 09:25:35.342: INFO: Waiting for pod downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db to disappear
Mar 15 09:25:35.350: INFO: Pod downward-api-23936b64-3fb3-4e2a-aa1a-917038d883db no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:35.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7174" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1589,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:35.357: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-0c742369-1e7f-4079-ae6c-c30d813c3950
STEP: Creating a pod to test consume configMaps
Mar 15 09:25:35.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b" in namespace "projected-7253" to be "Succeeded or Failed"
Mar 15 09:25:35.390: INFO: Pod "pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.898523ms
Mar 15 09:25:37.393: INFO: Pod "pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008981771s
STEP: Saw pod success
Mar 15 09:25:37.393: INFO: Pod "pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b" satisfied condition "Succeeded or Failed"
Mar 15 09:25:37.395: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 09:25:37.409: INFO: Waiting for pod pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b to disappear
Mar 15 09:25:37.414: INFO: Pod pod-projected-configmaps-00898c9f-af95-463f-a248-b1d7fe13d31b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:37.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7253" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1599,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:37.420: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:53.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4552" for this suite.

• [SLOW TEST:16.109 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":96,"skipped":1604,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:53.529: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:25:53.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2529" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1618,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:25:53.590: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:25:54.593: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 15 09:25:56.599: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397154, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397154, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397154, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397154, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:25:59.620: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:25:59.623: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:26:00.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5334" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.416 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":98,"skipped":1618,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:26:01.006: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:26:01.069: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 15 09:26:06.072: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 15 09:26:06.072: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 15 09:26:08.075: INFO: Creating deployment "test-rollover-deployment"
Mar 15 09:26:08.081: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 15 09:26:10.088: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 15 09:26:10.091: INFO: Ensure that both replica sets have 1 created replica
Mar 15 09:26:10.094: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 15 09:26:10.103: INFO: Updating deployment test-rollover-deployment
Mar 15 09:26:10.103: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 15 09:26:12.119: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 15 09:26:12.128: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 15 09:26:12.132: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:12.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397170, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:14.136: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:14.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397172, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:16.136: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:16.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397172, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:18.136: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:18.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397172, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:20.136: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:20.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397172, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:22.136: INFO: all replica sets need to contain the pod-template-hash label
Mar 15 09:26:22.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397172, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397168, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:26:24.136: INFO: 
Mar 15 09:26:24.136: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Mar 15 09:26:24.141: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6071 /apis/apps/v1/namespaces/deployment-6071/deployments/test-rollover-deployment 3fc8afd2-c975-4568-80d2-2a360be81f2c 659274 2 2021-03-15 09:26:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-15 09:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 09:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00080d3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-15 09:26:08 +0000 UTC,LastTransitionTime:2021-03-15 09:26:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-03-15 09:26:22 +0000 UTC,LastTransitionTime:2021-03-15 09:26:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 09:26:24.144: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-6071 /apis/apps/v1/namespaces/deployment-6071/replicasets/test-rollover-deployment-5797c7764 20dfe608-9e16-4226-bac4-2cf5a64a834f 659263 2 2021-03-15 09:26:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3fc8afd2-c975-4568-80d2-2a360be81f2c 0xc00080db80 0xc00080db81}] []  [{kube-controller-manager Update apps/v1 2021-03-15 09:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fc8afd2-c975-4568-80d2-2a360be81f2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00080dbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 09:26:24.144: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 15 09:26:24.144: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6071 /apis/apps/v1/namespaces/deployment-6071/replicasets/test-rollover-controller 616f97cd-bcd0-417a-9c9c-8a248aa747dc 659273 2 2021-03-15 09:26:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3fc8afd2-c975-4568-80d2-2a360be81f2c 0xc00080d9c7 0xc00080d9c8}] []  [{e2e.test Update apps/v1 2021-03-15 09:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 09:26:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fc8afd2-c975-4568-80d2-2a360be81f2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00080db18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 09:26:24.144: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6071 /apis/apps/v1/namespaces/deployment-6071/replicasets/test-rollover-deployment-78bc8b888c f1bdc3c6-895b-47a2-a97e-9a836b16872d 659210 2 2021-03-15 09:26:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3fc8afd2-c975-4568-80d2-2a360be81f2c 0xc00080dc67 0xc00080dc68}] []  [{kube-controller-manager Update apps/v1 2021-03-15 09:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fc8afd2-c975-4568-80d2-2a360be81f2c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00080dd38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 09:26:24.146: INFO: Pod "test-rollover-deployment-5797c7764-85vfc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-85vfc test-rollover-deployment-5797c7764- deployment-6071 /api/v1/namespaces/deployment-6071/pods/test-rollover-deployment-5797c7764-85vfc 282ddae4-4329-4e16-b312-f83a7405a481 659223 0 2021-03-15 09:26:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 20dfe608-9e16-4226-bac4-2cf5a64a834f 0xc00384e520 0xc00384e521}] []  [{kube-controller-manager Update v1 2021-03-15 09:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"20dfe608-9e16-4226-bac4-2cf5a64a834f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 09:26:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-snp8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-snp8n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-snp8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:26:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:26:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.28,StartTime:2021-03-15 09:26:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 09:26:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://650692c6692d235de3473c09e927bd2ca53e4ef28f930f184b896e74ef085715,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:26:24.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6071" for this suite.

• [SLOW TEST:23.146 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":99,"skipped":1624,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:26:24.152: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 15 09:26:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 15 09:26:38.824: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:26:42.590: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:26:57.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9113" for this suite.

• [SLOW TEST:33.707 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":100,"skipped":1632,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:26:57.859: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6227
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6227
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6227
Mar 15 09:26:57.905: INFO: Found 0 stateful pods, waiting for 1
Mar 15 09:27:07.908: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 15 09:27:07.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:27:08.108: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:27:08.108: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:27:08.108: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:27:08.111: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 15 09:27:18.114: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:27:18.114: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:27:18.123: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:18.123: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:18.123: INFO: 
Mar 15 09:27:18.123: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 15 09:27:19.126: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998066316s
Mar 15 09:27:20.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994550098s
Mar 15 09:27:21.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990166378s
Mar 15 09:27:22.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986382815s
Mar 15 09:27:23.142: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982353827s
Mar 15 09:27:24.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978495627s
Mar 15 09:27:25.150: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974844827s
Mar 15 09:27:26.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970941127s
Mar 15 09:27:27.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 966.950673ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6227
Mar 15 09:27:28.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:27:28.345: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 09:27:28.345: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:27:28.345: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:27:28.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:27:28.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 15 09:27:28.555: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:27:28.555: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:27:28.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:27:28.792: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 15 09:27:28.792: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 09:27:28.792: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 09:27:28.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 09:27:28.795: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 09:27:28.795: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 15 09:27:28.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:27:28.977: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:27:28.977: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:27:28.977: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:27:28.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:27:29.178: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:27:29.178: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:27:29.178: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:27:29.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 09:27:29.374: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 09:27:29.374: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 09:27:29.374: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 09:27:29.374: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:27:29.377: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 15 09:27:39.382: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:27:39.382: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:27:39.382: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 15 09:27:39.389: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:39.389: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:39.389: INFO: ss-1  ip-10-0-2-120.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:39.389: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:39.389: INFO: 
Mar 15 09:27:39.389: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 15 09:27:40.393: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:40.393: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:40.393: INFO: ss-1  ip-10-0-2-120.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:40.393: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:40.393: INFO: 
Mar 15 09:27:40.393: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 15 09:27:41.397: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:41.397: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:41.397: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:41.397: INFO: 
Mar 15 09:27:41.397: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 15 09:27:42.401: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:42.401: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:42.401: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:42.401: INFO: 
Mar 15 09:27:42.401: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 15 09:27:43.404: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:43.404: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:43.404: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:43.405: INFO: 
Mar 15 09:27:43.405: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 15 09:27:44.408: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:44.408: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:44.408: INFO: ss-2  ip-10-0-1-18.us-east-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:18 +0000 UTC  }]
Mar 15 09:27:44.408: INFO: 
Mar 15 09:27:44.408: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 15 09:27:45.412: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:45.412: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:45.412: INFO: 
Mar 15 09:27:45.412: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 15 09:27:46.415: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:46.415: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:46.415: INFO: 
Mar 15 09:27:46.415: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 15 09:27:47.418: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:47.418: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:47.418: INFO: 
Mar 15 09:27:47.418: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 15 09:27:48.422: INFO: POD   NODE                                      PHASE    GRACE  CONDITIONS
Mar 15 09:27:48.422: INFO: ss-0  ip-10-0-3-172.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:27:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-03-15 09:26:57 +0000 UTC  }]
Mar 15 09:27:48.422: INFO: 
Mar 15 09:27:48.422: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6227
Mar 15 09:27:49.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:27:49.530: INFO: rc: 1
Mar 15 09:27:49.530: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 15 09:27:59.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:27:59.606: INFO: rc: 1
Mar 15 09:27:59.606: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:09.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:28:09.681: INFO: rc: 1
Mar 15 09:28:09.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:19.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:28:19.757: INFO: rc: 1
Mar 15 09:28:19.757: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:29.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:28:29.833: INFO: rc: 1
Mar 15 09:28:29.833: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:39.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:28:39.913: INFO: rc: 1
Mar 15 09:28:39.913: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:49.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:28:49.992: INFO: rc: 1
Mar 15 09:28:49.992: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:28:59.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:00.072: INFO: rc: 1
Mar 15 09:29:00.072: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:29:10.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:10.152: INFO: rc: 1
Mar 15 09:29:10.152: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:29:20.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:20.229: INFO: rc: 1
Mar 15 09:29:20.229: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:29:30.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:30.385: INFO: rc: 1
Mar 15 09:29:30.385: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:29:40.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:40.461: INFO: rc: 1
Mar 15 09:29:40.461: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:29:50.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:29:50.539: INFO: rc: 1
Mar 15 09:29:50.539: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:00.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:00.617: INFO: rc: 1
Mar 15 09:30:00.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:10.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:10.695: INFO: rc: 1
Mar 15 09:30:10.695: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:20.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:20.773: INFO: rc: 1
Mar 15 09:30:20.773: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:30.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:30.879: INFO: rc: 1
Mar 15 09:30:30.879: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:40.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:40.960: INFO: rc: 1
Mar 15 09:30:40.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:30:50.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:30:51.037: INFO: rc: 1
Mar 15 09:30:51.037: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:01.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:01.133: INFO: rc: 1
Mar 15 09:31:01.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:11.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:11.208: INFO: rc: 1
Mar 15 09:31:11.208: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:21.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:21.285: INFO: rc: 1
Mar 15 09:31:21.285: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:31.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:31.366: INFO: rc: 1
Mar 15 09:31:31.366: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:41.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:41.473: INFO: rc: 1
Mar 15 09:31:41.473: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:31:51.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:31:51.550: INFO: rc: 1
Mar 15 09:31:51.550: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:01.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:01.626: INFO: rc: 1
Mar 15 09:32:01.626: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:11.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:11.703: INFO: rc: 1
Mar 15 09:32:11.703: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:21.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:21.781: INFO: rc: 1
Mar 15 09:32:21.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:31.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:31.859: INFO: rc: 1
Mar 15 09:32:31.859: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:41.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:41.940: INFO: rc: 1
Mar 15 09:32:41.940: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Mar 15 09:32:51.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-6227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 09:32:52.016: INFO: rc: 1
Mar 15 09:32:52.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Mar 15 09:32:52.016: INFO: Scaling statefulset ss to 0
Mar 15 09:32:52.023: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 09:32:52.024: INFO: Deleting all statefulset in ns statefulset-6227
Mar 15 09:32:52.026: INFO: Scaling statefulset ss to 0
Mar 15 09:32:52.032: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:32:52.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:32:52.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6227" for this suite.

• [SLOW TEST:354.191 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":101,"skipped":1639,"failed":0}
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:32:52.051: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:32:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9750" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":102,"skipped":1639,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:32:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-16205c88-27ce-422c-9a93-db99b6e4f637
STEP: Creating a pod to test consume secrets
Mar 15 09:32:52.175: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8" in namespace "projected-2328" to be "Succeeded or Failed"
Mar 15 09:32:52.178: INFO: Pod "pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690418ms
Mar 15 09:32:54.181: INFO: Pod "pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005736362s
Mar 15 09:32:56.184: INFO: Pod "pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008596678s
STEP: Saw pod success
Mar 15 09:32:56.184: INFO: Pod "pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8" satisfied condition "Succeeded or Failed"
Mar 15 09:32:56.185: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 15 09:32:56.212: INFO: Waiting for pod pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8 to disappear
Mar 15 09:32:56.218: INFO: Pod pod-projected-secrets-b5b43097-3a29-426c-9a5e-44e38e0dcaf8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:32:56.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2328" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":103,"skipped":1649,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:32:56.225: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-d1b136c1-590a-418e-b325-dfa52404dc57
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:32:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1718" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":104,"skipped":1664,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:32:56.260: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-e27c8a29-a875-4d36-848c-fa459fd9eb27
STEP: Creating a pod to test consume secrets
Mar 15 09:32:56.292: INFO: Waiting up to 5m0s for pod "pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e" in namespace "secrets-4811" to be "Succeeded or Failed"
Mar 15 09:32:56.294: INFO: Pod "pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822356ms
Mar 15 09:32:58.297: INFO: Pod "pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004597669s
Mar 15 09:33:00.300: INFO: Pod "pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007456043s
STEP: Saw pod success
Mar 15 09:33:00.300: INFO: Pod "pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e" satisfied condition "Succeeded or Failed"
Mar 15 09:33:00.302: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 09:33:00.316: INFO: Waiting for pod pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e to disappear
Mar 15 09:33:00.322: INFO: Pod pod-secrets-44620584-bf87-4fcd-abff-9f8d7967139e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:33:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4811" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":105,"skipped":1673,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:33:00.330: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Mar 15 09:33:00.349: INFO: PodSpec: initContainers in spec.initContainers
Mar 15 09:33:40.897: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cb7ba1bb-30d9-4867-9b4b-c8af1cf0007a", GenerateName:"", Namespace:"init-container-8005", SelfLink:"/api/v1/namespaces/init-container-8005/pods/pod-init-cb7ba1bb-30d9-4867-9b4b-c8af1cf0007a", UID:"19bcfd8c-0468-4f2d-8337-6fd960456fd0", ResourceVersion:"660843", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63751397580, loc:(*time.Location)(0x76f4840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"349296816"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002f9a500), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f9a520)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002f9a540), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002f9a560)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-dn2kb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002f14440), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dn2kb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dn2kb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dn2kb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00717d2d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-3-172.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00326e540), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00717d350)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00717d370)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00717d378), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00717d37c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002e360e0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397580, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397580, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397580, loc:(*time.Location)(0x76f4840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751397580, loc:(*time.Location)(0x76f4840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.3.172", PodIP:"10.20.71.32", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.71.32"}}, StartTime:(*v1.Time)(0xc002f9a580), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00326e620)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00326e690)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://4ecf1521d261db0a03556b33aa82b71c23f6bfe51bc1bec94bb0c9af342967f3", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f9a5c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f9a5a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00717d3ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:33:40.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8005" for this suite.

• [SLOW TEST:40.574 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":106,"skipped":1680,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:33:40.905: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:33:40.943: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 15 09:33:42.965: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:33:43.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2796" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":107,"skipped":1685,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:33:43.978: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-71499752-0b39-4794-bc11-551daa2b92bc
STEP: Creating a pod to test consume secrets
Mar 15 09:33:44.075: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71" in namespace "projected-829" to be "Succeeded or Failed"
Mar 15 09:33:44.087: INFO: Pod "pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71": Phase="Pending", Reason="", readiness=false. Elapsed: 11.933855ms
Mar 15 09:33:46.090: INFO: Pod "pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014927366s
STEP: Saw pod success
Mar 15 09:33:46.090: INFO: Pod "pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71" satisfied condition "Succeeded or Failed"
Mar 15 09:33:46.092: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 15 09:33:46.125: INFO: Waiting for pod pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71 to disappear
Mar 15 09:33:46.132: INFO: Pod pod-projected-secrets-f7103a7f-a2d9-47c1-983b-8cd5d64b7c71 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:33:46.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-829" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1690,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:33:46.139: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Mar 15 09:33:46.181: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 09:34:46.212: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Mar 15 09:34:46.225: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 15 09:34:46.243: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar 15 09:34:46.266: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:35:18.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2570" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:92.217 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":109,"skipped":1692,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:35:18.356: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Mar 15 09:35:18.394: INFO: Major version: 1
STEP: Confirm minor version
Mar 15 09:35:18.394: INFO: cleanMinorVersion: 19
Mar 15 09:35:18.394: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:35:18.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7927" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":110,"skipped":1711,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:35:18.401: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-ab0d4e7b-c39c-4d82-af2d-40ece020d3cc in namespace container-probe-9143
Mar 15 09:35:22.447: INFO: Started pod test-webserver-ab0d4e7b-c39c-4d82-af2d-40ece020d3cc in namespace container-probe-9143
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 09:35:22.449: INFO: Initial restart count of pod test-webserver-ab0d4e7b-c39c-4d82-af2d-40ece020d3cc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:39:22.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9143" for this suite.

• [SLOW TEST:244.442 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:39:22.844: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Mar 15 09:39:22.917: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:39:43.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9142" for this suite.

• [SLOW TEST:20.328 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":112,"skipped":1736,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:39:43.173: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:39:43.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a" in namespace "downward-api-9953" to be "Succeeded or Failed"
Mar 15 09:39:43.207: INFO: Pod "downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110658ms
Mar 15 09:39:45.210: INFO: Pod "downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005006005s
STEP: Saw pod success
Mar 15 09:39:45.211: INFO: Pod "downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a" satisfied condition "Succeeded or Failed"
Mar 15 09:39:45.212: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a container client-container: <nil>
STEP: delete the pod
Mar 15 09:39:45.239: INFO: Waiting for pod downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a to disappear
Mar 15 09:39:45.247: INFO: Pod downwardapi-volume-e1a2bb17-1d14-49d7-9327-33f1dd55469a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:39:45.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9953" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":113,"skipped":1770,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:39:45.253: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8406
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 15 09:39:45.274: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 09:39:45.312: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 09:39:47.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:49.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:51.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:53.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:55.315: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:57.316: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 09:39:59.315: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 15 09:39:59.319: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 09:40:01.322: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 09:40:03.321: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 09:40:05.322: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 15 09:40:05.326: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 15 09:40:09.355: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.54.157:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8406 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:40:09.355: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:40:09.465: INFO: Found all expected endpoints: [netserver-0]
Mar 15 09:40:09.468: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.90.167:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8406 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:40:09.468: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:40:09.585: INFO: Found all expected endpoints: [netserver-1]
Mar 15 09:40:09.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.71.38:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8406 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:40:09.587: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:40:09.690: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8406" for this suite.

• [SLOW TEST:24.444 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1775,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:09.697: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Mar 15 09:40:09.721: INFO: created test-pod-1
Mar 15 09:40:09.737: INFO: created test-pod-2
Mar 15 09:40:09.743: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:09.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4651" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":115,"skipped":1824,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:09.834: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1812
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1812
STEP: creating replication controller externalsvc in namespace services-1812
I0315 09:40:09.901355      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1812, replica count: 2
I0315 09:40:12.951901      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 15 09:40:12.974: INFO: Creating new exec pod
Mar 15 09:40:14.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-1812 exec execpodv74cz -- /bin/sh -x -c nslookup nodeport-service.services-1812.svc.cluster.local'
Mar 15 09:40:16.281: INFO: stderr: "+ nslookup nodeport-service.services-1812.svc.cluster.local\n"
Mar 15 09:40:16.281: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-1812.svc.cluster.local\tcanonical name = externalsvc.services-1812.svc.cluster.local.\nName:\texternalsvc.services-1812.svc.cluster.local\nAddress: 10.21.137.87\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1812, will wait for the garbage collector to delete the pods
Mar 15 09:40:16.338: INFO: Deleting ReplicationController externalsvc took: 4.339618ms
Mar 15 09:40:17.138: INFO: Terminating ReplicationController externalsvc pods took: 800.207346ms
Mar 15 09:40:32.165: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:32.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1812" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.351 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":116,"skipped":1836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:32.191: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3077
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3077
STEP: Creating statefulset with conflicting port in namespace statefulset-3077
STEP: Waiting until pod test-pod will start running in namespace statefulset-3077
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3077
Mar 15 09:40:36.255: INFO: Observed stateful pod in namespace: statefulset-3077, name: ss-0, uid: c3233add-e508-49ef-8755-8a33c7364a9d, status phase: Pending. Waiting for statefulset controller to delete.
Mar 15 09:40:36.839: INFO: Observed stateful pod in namespace: statefulset-3077, name: ss-0, uid: c3233add-e508-49ef-8755-8a33c7364a9d, status phase: Failed. Waiting for statefulset controller to delete.
Mar 15 09:40:36.847: INFO: Observed stateful pod in namespace: statefulset-3077, name: ss-0, uid: c3233add-e508-49ef-8755-8a33c7364a9d, status phase: Failed. Waiting for statefulset controller to delete.
Mar 15 09:40:36.849: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3077
STEP: Removing pod with conflicting port in namespace statefulset-3077
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3077 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 09:40:40.893: INFO: Deleting all statefulset in ns statefulset-3077
Mar 15 09:40:40.895: INFO: Scaling statefulset ss to 0
Mar 15 09:40:50.904: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 09:40:50.906: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:50.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3077" for this suite.

• [SLOW TEST:18.729 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":117,"skipped":1864,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:50.920: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:40:50.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1" in namespace "projected-8179" to be "Succeeded or Failed"
Mar 15 09:40:50.960: INFO: Pod "downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273019ms
Mar 15 09:40:52.963: INFO: Pod "downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005963621s
STEP: Saw pod success
Mar 15 09:40:52.963: INFO: Pod "downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1" satisfied condition "Succeeded or Failed"
Mar 15 09:40:52.965: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1 container client-container: <nil>
STEP: delete the pod
Mar 15 09:40:52.976: INFO: Waiting for pod downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1 to disappear
Mar 15 09:40:52.983: INFO: Pod downwardapi-volume-5eb4d2d4-a661-44d2-ab8b-0ebb4c42dbd1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:52.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8179" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1872,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:52.989: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-2afff57b-1983-47b7-9506-1ad48ba97f19
STEP: Creating a pod to test consume secrets
Mar 15 09:40:53.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3" in namespace "projected-8326" to be "Succeeded or Failed"
Mar 15 09:40:53.027: INFO: Pod "pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.63328ms
Mar 15 09:40:55.030: INFO: Pod "pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011662574s
STEP: Saw pod success
Mar 15 09:40:55.030: INFO: Pod "pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3" satisfied condition "Succeeded or Failed"
Mar 15 09:40:55.032: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 15 09:40:55.047: INFO: Waiting for pod pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3 to disappear
Mar 15 09:40:55.055: INFO: Pod pod-projected-secrets-2eeca5fa-8467-4a2d-ae19-c40a0edc75c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8326" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":119,"skipped":1875,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:55.061: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:40:55.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7959" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":120,"skipped":1882,"failed":0}
SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:40:55.097: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 15 09:41:01.138: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.138: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.243: INFO: Exec stderr: ""
Mar 15 09:41:01.243: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.243: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.367: INFO: Exec stderr: ""
Mar 15 09:41:01.367: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.367: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.463: INFO: Exec stderr: ""
Mar 15 09:41:01.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.463: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.580: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 15 09:41:01.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.580: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.695: INFO: Exec stderr: ""
Mar 15 09:41:01.695: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.695: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.803: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 15 09:41:01.803: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.803: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:01.908: INFO: Exec stderr: ""
Mar 15 09:41:01.908: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:02.020: INFO: Exec stderr: ""
Mar 15 09:41:02.020: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:02.020: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:02.137: INFO: Exec stderr: ""
Mar 15 09:41:02.138: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-217 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 09:41:02.138: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 09:41:02.239: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:41:02.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-217" for this suite.

• [SLOW TEST:7.150 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1886,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:41:02.248: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:41:02.877: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:41:04.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398062, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398062, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398062, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398062, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:41:07.892: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:41:07.895: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9831-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:41:09.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2183" for this suite.
STEP: Destroying namespace "webhook-2183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":122,"skipped":1892,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:41:09.094: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Mar 15 09:41:11.166: INFO: Pod pod-hostip-306cb98a-06ae-4953-961e-03ac2a365e22 has hostIP: 10.0.2.120
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:41:11.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-971" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":1906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:41:11.174: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:41:11.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-2697 version'
Mar 15 09:41:11.281: INFO: stderr: ""
Mar 15 09:41:11.281: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6\", GitCommit:\"fbf646b339dc52336b55d8ec85c181981b86331a\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:09:30Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.6\", GitCommit:\"fbf646b339dc52336b55d8ec85c181981b86331a\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:01:36Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:41:11.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2697" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":124,"skipped":1929,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:41:11.288: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:42:11.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1719" for this suite.

• [SLOW TEST:60.039 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":1935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:42:11.328: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-2344f6ea-4404-42b5-ab36-09c0076040ba
STEP: Creating configMap with name cm-test-opt-upd-8dc33f2a-925b-4844-b6dc-01d2fb740635
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2344f6ea-4404-42b5-ab36-09c0076040ba
STEP: Updating configmap cm-test-opt-upd-8dc33f2a-925b-4844-b6dc-01d2fb740635
STEP: Creating configMap with name cm-test-opt-create-1b7459c6-a474-46f3-ac2a-0f239be81d6c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:43:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-268" for this suite.

• [SLOW TEST:72.392 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":1959,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:43:23.720: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:43:24.363: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:43:26.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398204, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398204, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398204, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398204, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:43:29.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:43:29.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4751" for this suite.
STEP: Destroying namespace "webhook-4751-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":127,"skipped":1970,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:43:29.479: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-773775d7-e7e8-4ff5-99c2-95931bc9c953
STEP: Creating secret with name secret-projected-all-test-volume-fcba4f2b-8e5a-46fa-84b9-18e80fb5b351
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 15 09:43:29.567: INFO: Waiting up to 5m0s for pod "projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f" in namespace "projected-698" to be "Succeeded or Failed"
Mar 15 09:43:29.569: INFO: Pod "projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267536ms
Mar 15 09:43:31.572: INFO: Pod "projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005301977s
Mar 15 09:43:33.575: INFO: Pod "projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008387531s
STEP: Saw pod success
Mar 15 09:43:33.575: INFO: Pod "projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f" satisfied condition "Succeeded or Failed"
Mar 15 09:43:33.577: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 15 09:43:33.588: INFO: Waiting for pod projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f to disappear
Mar 15 09:43:33.596: INFO: Pod projected-volume-81e4b7cb-200b-4412-bc3b-5e1337abc40f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:43:33.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-698" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":128,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:43:33.605: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 15 09:43:33.645: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7855 /api/v1/namespaces/watch-7855/configmaps/e2e-watch-test-resource-version 07ee8b7d-cfc4-43ea-984b-172a4340f936 663641 0 2021-03-15 09:43:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-15 09:43:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 09:43:33.646: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7855 /api/v1/namespaces/watch-7855/configmaps/e2e-watch-test-resource-version 07ee8b7d-cfc4-43ea-984b-172a4340f936 663642 0 2021-03-15 09:43:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-03-15 09:43:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:43:33.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7855" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":129,"skipped":2008,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:43:33.652: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5212
STEP: creating service affinity-nodeport in namespace services-5212
STEP: creating replication controller affinity-nodeport in namespace services-5212
I0315 09:43:33.701574      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5212, replica count: 3
I0315 09:43:36.752212      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 09:43:36.762: INFO: Creating new exec pod
Mar 15 09:43:39.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Mar 15 09:43:40.022: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 15 09:43:40.022: INFO: stdout: ""
Mar 15 09:43:40.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 10.21.1.5 80'
Mar 15 09:43:40.241: INFO: stderr: "+ nc -zv -t -w 2 10.21.1.5 80\nConnection to 10.21.1.5 80 port [tcp/http] succeeded!\n"
Mar 15 09:43:40.241: INFO: stdout: ""
Mar 15 09:43:40.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.172 31845'
Mar 15 09:43:40.424: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.172 31845\nConnection to 10.0.3.172 31845 port [tcp/31845] succeeded!\n"
Mar 15 09:43:40.424: INFO: stdout: ""
Mar 15 09:43:40.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.18 31845'
Mar 15 09:43:40.606: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.18 31845\nConnection to 10.0.1.18 31845 port [tcp/31845] succeeded!\n"
Mar 15 09:43:40.606: INFO: stdout: ""
Mar 15 09:43:40.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 18.189.17.197 31845'
Mar 15 09:43:40.791: INFO: stderr: "+ nc -zv -t -w 2 18.189.17.197 31845\nConnection to 18.189.17.197 31845 port [tcp/31845] succeeded!\n"
Mar 15 09:43:40.791: INFO: stdout: ""
Mar 15 09:43:40.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c nc -zv -t -w 2 18.219.113.27 31845'
Mar 15 09:43:40.975: INFO: stderr: "+ nc -zv -t -w 2 18.219.113.27 31845\nConnection to 18.219.113.27 31845 port [tcp/31845] succeeded!\n"
Mar 15 09:43:40.975: INFO: stdout: ""
Mar 15 09:43:40.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5212 exec execpod-affinity22pvg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.18:31845/ ; done'
Mar 15 09:43:41.303: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31845/\n"
Mar 15 09:43:41.303: INFO: stdout: "\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f\naffinity-nodeport-knh9f"
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Received response from host: affinity-nodeport-knh9f
Mar 15 09:43:41.303: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5212, will wait for the garbage collector to delete the pods
Mar 15 09:43:41.373: INFO: Deleting ReplicationController affinity-nodeport took: 4.245305ms
Mar 15 09:43:41.473: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.125516ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:43:56.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5212" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.862 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":130,"skipped":2020,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:43:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Mar 15 09:43:56.550: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 09:44:56.578: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Mar 15 09:44:56.591: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 15 09:44:56.616: INFO: Created pod: pod1-sched-preemption-medium-priority
Mar 15 09:44:56.636: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:06.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3122" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:70.206 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":131,"skipped":2023,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:06.720: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:45:06.770: INFO: Waiting up to 5m0s for pod "downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa" in namespace "downward-api-7625" to be "Succeeded or Failed"
Mar 15 09:45:06.772: INFO: Pod "downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.749709ms
Mar 15 09:45:08.775: INFO: Pod "downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004630387s
STEP: Saw pod success
Mar 15 09:45:08.775: INFO: Pod "downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa" satisfied condition "Succeeded or Failed"
Mar 15 09:45:08.778: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa container client-container: <nil>
STEP: delete the pod
Mar 15 09:45:08.802: INFO: Waiting for pod downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa to disappear
Mar 15 09:45:08.810: INFO: Pod downwardapi-volume-778dd203-f6f3-4453-850e-7766616601fa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:08.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7625" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":132,"skipped":2027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:08.817: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:08.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6008" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":133,"skipped":2068,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:08.915: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 15 09:45:08.949: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 09:45:08.960: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 09:45:08.962: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-18.us-east-2.compute.internal before test
Mar 15 09:45:08.971: INFO: metrics-server-v0.3.6-58cf8c5dfb-dnzrv from kube-system started at 2021-03-12 15:30:23 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 09:45:08.971: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 15 09:45:08.971: INFO: dashboard-metrics-scraper-7b59f7d4df-xrqjg from kubernetes-dashboard started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 15 09:45:08.971: INFO: node-exporter-2k2xw from pf9-monitoring started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:45:08.971: INFO: prometheus-system-0 from pf9-monitoring started at 2021-03-12 15:32:05 +0000 UTC (3 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container prometheus ready: true, restart count 1
Mar 15 09:45:08.971: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 15 09:45:08.971: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 15 09:45:08.971: INFO: packageserver-67c5dfcffc-4vmch from pf9-olm started at 2021-03-14 07:25:39 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 09:45:08.971: INFO: preemptor-pod from sched-preemption-3122 started at 2021-03-15 09:45:02 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container preemptor-pod ready: true, restart count 0
Mar 15 09:45:08.971: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:45:08.971: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 09:45:08.971: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-2-120.us-east-2.compute.internal before test
Mar 15 09:45:08.981: INFO: coredns-867bf6789f-kfcq4 from kube-system started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container coredns ready: true, restart count 0
Mar 15 09:45:08.981: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2021-03-14 07:25:32 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container alertmanager ready: true, restart count 0
Mar 15 09:45:08.981: INFO: 	Container config-reloader ready: true, restart count 0
Mar 15 09:45:08.981: INFO: grafana-9b947c8-gqfl8 from pf9-monitoring started at 2021-03-12 15:32:10 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container grafana ready: true, restart count 0
Mar 15 09:45:08.981: INFO: 	Container proxy ready: true, restart count 0
Mar 15 09:45:08.981: INFO: kube-state-metrics-6ff68b466d-2xpvx from pf9-monitoring started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 15 09:45:08.981: INFO: node-exporter-gs57d from pf9-monitoring started at 2021-03-12 15:30:01 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:45:08.981: INFO: catalog-operator-6656569cc-bsdrn from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 15 09:45:08.981: INFO: olm-operator-5c657955f6-jdk8b from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container olm-operator ready: true, restart count 0
Mar 15 09:45:08.981: INFO: packageserver-67c5dfcffc-vnl24 from pf9-olm started at 2021-03-14 07:25:32 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 09:45:08.981: INFO: platform9-operators-cxg4l from pf9-olm started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container registry-server ready: true, restart count 0
Mar 15 09:45:08.981: INFO: monhelper-5577f87674-m95zg from pf9-operators started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container monhelper ready: true, restart count 0
Mar 15 09:45:08.981: INFO: prometheus-operator-7b96488f76-st7bk from pf9-operators started at 2021-03-12 15:31:19 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 15 09:45:08.981: INFO: pod1-sched-preemption-medium-priority from sched-preemption-3122 started at 2021-03-15 09:44:56 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container pod1-sched-preemption-medium-priority ready: true, restart count 0
Mar 15 09:45:08.981: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.981: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:45:08.981: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 09:45:08.981: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-3-172.us-east-2.compute.internal before test
Mar 15 09:45:08.991: INFO: node-exporter-tzdsq from pf9-monitoring started at 2021-03-12 15:30:02 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.992: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 09:45:08.992: INFO: pod2-sched-preemption-medium-priority from sched-preemption-3122 started at 2021-03-15 09:44:56 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.992: INFO: 	Container pod2-sched-preemption-medium-priority ready: true, restart count 0
Mar 15 09:45:08.992: INFO: sonobuoy from sonobuoy started at 2021-03-15 09:00:40 +0000 UTC (1 container statuses recorded)
Mar 15 09:45:08.992: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 09:45:08.992: INFO: sonobuoy-e2e-job-be88518f7bd84c37 from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.992: INFO: 	Container e2e ready: true, restart count 0
Mar 15 09:45:08.992: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:45:08.992: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 09:45:08.992: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 09:45:08.992: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-75871443-36fb-451e-b1e0-0dfdc93ded4c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-75871443-36fb-451e-b1e0-0dfdc93ded4c off the node ip-10-0-3-172.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-75871443-36fb-451e-b1e0-0dfdc93ded4c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4969" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":134,"skipped":2074,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:13.062: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:45:13.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f" in namespace "projected-3083" to be "Succeeded or Failed"
Mar 15 09:45:13.094: INFO: Pod "downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056226ms
Mar 15 09:45:15.097: INFO: Pod "downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004780334s
Mar 15 09:45:17.100: INFO: Pod "downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00791698s
STEP: Saw pod success
Mar 15 09:45:17.100: INFO: Pod "downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f" satisfied condition "Succeeded or Failed"
Mar 15 09:45:17.102: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f container client-container: <nil>
STEP: delete the pod
Mar 15 09:45:17.114: INFO: Waiting for pod downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f to disappear
Mar 15 09:45:17.120: INFO: Pod downwardapi-volume-147a28d0-36b7-41ee-9398-d2fb713dd41f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:17.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3083" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2080,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:17.126: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:45:17.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:45:20.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:20.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7050" for this suite.
STEP: Destroying namespace "webhook-7050-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":136,"skipped":2084,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:20.949: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Mar 15 09:45:20.982: INFO: namespace kubectl-3936
Mar 15 09:45:20.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3936 create -f -'
Mar 15 09:45:21.268: INFO: stderr: ""
Mar 15 09:45:21.268: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 15 09:45:22.275: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 09:45:22.275: INFO: Found 0 / 1
Mar 15 09:45:23.273: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 09:45:23.273: INFO: Found 1 / 1
Mar 15 09:45:23.273: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 15 09:45:23.275: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 09:45:23.275: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 09:45:23.275: INFO: wait on agnhost-primary startup in kubectl-3936 
Mar 15 09:45:23.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3936 logs agnhost-primary-rzv85 agnhost-primary'
Mar 15 09:45:23.369: INFO: stderr: ""
Mar 15 09:45:23.369: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 15 09:45:23.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3936 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 15 09:45:23.462: INFO: stderr: ""
Mar 15 09:45:23.462: INFO: stdout: "service/rm2 exposed\n"
Mar 15 09:45:23.468: INFO: Service rm2 in namespace kubectl-3936 found.
STEP: exposing service
Mar 15 09:45:25.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3936 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 15 09:45:25.562: INFO: stderr: ""
Mar 15 09:45:25.562: INFO: stdout: "service/rm3 exposed\n"
Mar 15 09:45:25.567: INFO: Service rm3 in namespace kubectl-3936 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:27.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3936" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":137,"skipped":2088,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:27.580: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 15 09:45:29.619: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7149" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2104,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:29.642: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-5388
STEP: creating replication controller nodeport-test in namespace services-5388
I0315 09:45:29.687015      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5388, replica count: 2
I0315 09:45:32.737537      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 09:45:32.737: INFO: Creating new exec pod
Mar 15 09:45:35.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 15 09:45:35.939: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 15 09:45:35.939: INFO: stdout: ""
Mar 15 09:45:35.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 10.21.44.70 80'
Mar 15 09:45:36.133: INFO: stderr: "+ nc -zv -t -w 2 10.21.44.70 80\nConnection to 10.21.44.70 80 port [tcp/http] succeeded!\n"
Mar 15 09:45:36.133: INFO: stdout: ""
Mar 15 09:45:36.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.120 31517'
Mar 15 09:45:36.312: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.120 31517\nConnection to 10.0.2.120 31517 port [tcp/31517] succeeded!\n"
Mar 15 09:45:36.312: INFO: stdout: ""
Mar 15 09:45:36.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.172 31517'
Mar 15 09:45:36.506: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.172 31517\nConnection to 10.0.3.172 31517 port [tcp/31517] succeeded!\n"
Mar 15 09:45:36.506: INFO: stdout: ""
Mar 15 09:45:36.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 13.59.66.200 31517'
Mar 15 09:45:36.704: INFO: stderr: "+ nc -zv -t -w 2 13.59.66.200 31517\nConnection to 13.59.66.200 31517 port [tcp/31517] succeeded!\n"
Mar 15 09:45:36.704: INFO: stdout: ""
Mar 15 09:45:36.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-5388 exec execpodvd5pw -- /bin/sh -x -c nc -zv -t -w 2 18.189.17.197 31517'
Mar 15 09:45:36.900: INFO: stderr: "+ nc -zv -t -w 2 18.189.17.197 31517\nConnection to 18.189.17.197 31517 port [tcp/31517] succeeded!\n"
Mar 15 09:45:36.900: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:36.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5388" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.266 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":139,"skipped":2108,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:36.908: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Mar 15 09:45:36.939: INFO: Waiting up to 5m0s for pod "downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943" in namespace "downward-api-1504" to be "Succeeded or Failed"
Mar 15 09:45:36.941: INFO: Pod "downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096938ms
Mar 15 09:45:38.944: INFO: Pod "downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00466963s
STEP: Saw pod success
Mar 15 09:45:38.944: INFO: Pod "downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943" satisfied condition "Succeeded or Failed"
Mar 15 09:45:38.945: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943 container dapi-container: <nil>
STEP: delete the pod
Mar 15 09:45:38.958: INFO: Waiting for pod downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943 to disappear
Mar 15 09:45:38.965: INFO: Pod downward-api-a72bfbff-5e8c-4e00-8e55-406e3f1e5943 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:38.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1504" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2110,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:38.971: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0315 09:45:49.010663      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 09:45:49.010687      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 09:45:49.010694      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 15 09:45:49.010: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:45:49.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6942" for this suite.

• [SLOW TEST:10.047 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":141,"skipped":2114,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:45:49.018: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-864c16ff-3c50-4854-9ec1-b011b819c126 in namespace container-probe-2539
Mar 15 09:45:51.068: INFO: Started pod liveness-864c16ff-3c50-4854-9ec1-b011b819c126 in namespace container-probe-2539
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 09:45:51.069: INFO: Initial restart count of pod liveness-864c16ff-3c50-4854-9ec1-b011b819c126 is 0
Mar 15 09:46:13.109: INFO: Restart count of pod container-probe-2539/liveness-864c16ff-3c50-4854-9ec1-b011b819c126 is now 1 (22.039160763s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:46:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2539" for this suite.

• [SLOW TEST:24.111 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:46:13.130: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Mar 15 09:46:13.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-6303 create -f -'
Mar 15 09:46:13.416: INFO: stderr: ""
Mar 15 09:46:13.416: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 15 09:46:13.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-6303 diff -f -'
Mar 15 09:46:13.816: INFO: rc: 1
Mar 15 09:46:13.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-6303 delete -f -'
Mar 15 09:46:13.905: INFO: stderr: ""
Mar 15 09:46:13.905: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:46:13.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6303" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":143,"skipped":2155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:46:13.927: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3072.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3072.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 200.208.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.208.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.208.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.208.200_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3072.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3072.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3072.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3072.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3072.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 200.208.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.208.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.208.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.208.200_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 09:46:18.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.056: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.061: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.077: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.082: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:18.103: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:23.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.112: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.130: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.133: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.137: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:23.151: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:28.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.110: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.113: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.134: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.139: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.142: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.144: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:28.160: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:33.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.109: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.111: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.114: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.132: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.137: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.139: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:33.153: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:38.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.109: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.112: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.114: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.130: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.134: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.136: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:38.151: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:43.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.109: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.112: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.114: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.131: INFO: Unable to read jessie_udp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.133: INFO: Unable to read jessie_tcp@dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.138: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local from pod dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9: the server could not find the requested resource (get pods dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9)
Mar 15 09:46:43.152: INFO: Lookups using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 failed for: [wheezy_udp@dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@dns-test-service.dns-3072.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_udp@dns-test-service.dns-3072.svc.cluster.local jessie_tcp@dns-test-service.dns-3072.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3072.svc.cluster.local]

Mar 15 09:46:48.152: INFO: DNS probes using dns-3072/dns-test-f1080ce3-d1de-486e-ad1f-9fbc21c6ffa9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:46:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3072" for this suite.

• [SLOW TEST:34.393 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":144,"skipped":2195,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:46:48.328: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:46:49.225: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:46:52.237: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:46:52.240: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4379-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:46:53.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9177" for this suite.
STEP: Destroying namespace "webhook-9177-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.173 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":145,"skipped":2224,"failed":0}
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:46:53.501: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:47:01.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3340" for this suite.

• [SLOW TEST:8.134 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":146,"skipped":2225,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:47:01.635: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:47:01.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8" in namespace "projected-9604" to be "Succeeded or Failed"
Mar 15 09:47:01.670: INFO: Pod "downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.011136ms
Mar 15 09:47:03.673: INFO: Pod "downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008307686s
STEP: Saw pod success
Mar 15 09:47:03.673: INFO: Pod "downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8" satisfied condition "Succeeded or Failed"
Mar 15 09:47:03.675: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8 container client-container: <nil>
STEP: delete the pod
Mar 15 09:47:03.691: INFO: Waiting for pod downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8 to disappear
Mar 15 09:47:03.699: INFO: Pod downwardapi-volume-b38d5219-e485-4d33-a68f-8700b011b7e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:47:03.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9604" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":147,"skipped":2225,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:47:03.706: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6158
STEP: creating service affinity-clusterip in namespace services-6158
STEP: creating replication controller affinity-clusterip in namespace services-6158
I0315 09:47:03.747680      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6158, replica count: 3
I0315 09:47:06.797989      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 09:47:06.821: INFO: Creating new exec pod
Mar 15 09:47:09.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6158 exec execpod-affinity5rxm2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Mar 15 09:47:10.073: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 15 09:47:10.073: INFO: stdout: ""
Mar 15 09:47:10.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6158 exec execpod-affinity5rxm2 -- /bin/sh -x -c nc -zv -t -w 2 10.21.237.142 80'
Mar 15 09:47:10.379: INFO: stderr: "+ nc -zv -t -w 2 10.21.237.142 80\nConnection to 10.21.237.142 80 port [tcp/http] succeeded!\n"
Mar 15 09:47:10.379: INFO: stdout: ""
Mar 15 09:47:10.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6158 exec execpod-affinity5rxm2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.237.142:80/ ; done'
Mar 15 09:47:10.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.237.142:80/\n"
Mar 15 09:47:10.699: INFO: stdout: "\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf\naffinity-clusterip-qp4xf"
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Received response from host: affinity-clusterip-qp4xf
Mar 15 09:47:10.699: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6158, will wait for the garbage collector to delete the pods
Mar 15 09:47:10.769: INFO: Deleting ReplicationController affinity-clusterip took: 4.185871ms
Mar 15 09:47:10.869: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.226716ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:47:22.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6158" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.489 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":148,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:47:22.196: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 15 09:47:22.227: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 09:48:22.252: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:48:22.255: INFO: Starting informer...
STEP: Starting pod...
Mar 15 09:48:22.465: INFO: Pod is running on ip-10-0-3-172.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 15 09:48:22.475: INFO: Pod wasn't evicted. Proceeding
Mar 15 09:48:22.475: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 15 09:49:37.486: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:49:37.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9716" for this suite.

• [SLOW TEST:135.299 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":149,"skipped":2258,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:49:37.495: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 09:49:37.574: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03" in namespace "downward-api-4496" to be "Succeeded or Failed"
Mar 15 09:49:37.583: INFO: Pod "downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03": Phase="Pending", Reason="", readiness=false. Elapsed: 8.71595ms
Mar 15 09:49:39.586: INFO: Pod "downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011473926s
STEP: Saw pod success
Mar 15 09:49:39.586: INFO: Pod "downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03" satisfied condition "Succeeded or Failed"
Mar 15 09:49:39.587: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03 container client-container: <nil>
STEP: delete the pod
Mar 15 09:49:39.619: INFO: Waiting for pod downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03 to disappear
Mar 15 09:49:39.621: INFO: Pod downwardapi-volume-5b15d4d3-acad-4d7a-8b85-fffd01905d03 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:49:39.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4496" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":150,"skipped":2260,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:49:39.628: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 09:49:40.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 09:49:42.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398580, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398580, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398580, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398580, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 09:49:45.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:49:45.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6831" for this suite.
STEP: Destroying namespace "webhook-6831-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.216 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":151,"skipped":2266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:49:45.845: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 09:49:45.918: INFO: Creating deployment "test-recreate-deployment"
Mar 15 09:49:45.924: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 15 09:49:45.929: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 15 09:49:47.934: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 15 09:49:47.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398585, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398585, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398585, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751398585, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 09:49:49.939: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 15 09:49:49.945: INFO: Updating deployment test-recreate-deployment
Mar 15 09:49:49.945: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Mar 15 09:49:50.071: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8844 /apis/apps/v1/namespaces/deployment-8844/deployments/test-recreate-deployment b02503db-a748-4388-899d-b6728d4b3170 666064 2 2021-03-15 09:49:45 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-15 09:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 09:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b32698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-15 09:49:50 +0000 UTC,LastTransitionTime:2021-03-15 09:49:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-03-15 09:49:50 +0000 UTC,LastTransitionTime:2021-03-15 09:49:45 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 15 09:49:50.073: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8844 /apis/apps/v1/namespaces/deployment-8844/replicasets/test-recreate-deployment-f79dd4667 22801118-10ff-4855-a27c-a0c3409117a6 666063 1 2021-03-15 09:49:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b02503db-a748-4388-899d-b6728d4b3170 0xc003b32b90 0xc003b32b91}] []  [{kube-controller-manager Update apps/v1 2021-03-15 09:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b02503db-a748-4388-899d-b6728d4b3170\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b32c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 09:49:50.073: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 15 09:49:50.073: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-8844 /apis/apps/v1/namespaces/deployment-8844/replicasets/test-recreate-deployment-c96cf48f 4e8198a9-463b-427c-93c3-b426b4a1a565 666052 2 2021-03-15 09:49:45 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b02503db-a748-4388-899d-b6728d4b3170 0xc003b32a9f 0xc003b32ab0}] []  [{kube-controller-manager Update apps/v1 2021-03-15 09:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b02503db-a748-4388-899d-b6728d4b3170\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b32b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 09:49:50.075: INFO: Pod "test-recreate-deployment-f79dd4667-88wfp" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-88wfp test-recreate-deployment-f79dd4667- deployment-8844 /api/v1/namespaces/deployment-8844/pods/test-recreate-deployment-f79dd4667-88wfp 50eb07df-39d7-4e18-92a3-be8c1cb8d2e0 666061 0 2021-03-15 09:49:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 22801118-10ff-4855-a27c-a0c3409117a6 0xc003b330d0 0xc003b330d1}] []  [{kube-controller-manager Update v1 2021-03-15 09:49:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22801118-10ff-4855-a27c-a0c3409117a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 09:49:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hb6lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hb6lk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hb6lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:49:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:49:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 09:49:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:,StartTime:2021-03-15 09:49:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:49:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8844" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":152,"skipped":2301,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:49:50.082: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-799
Mar 15 09:49:52.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 15 09:49:52.317: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 15 09:49:52.317: INFO: stdout: "ipvs"
Mar 15 09:49:52.317: INFO: proxyMode: ipvs
Mar 15 09:49:52.325: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 09:49:52.329: INFO: Pod kube-proxy-mode-detector still exists
Mar 15 09:49:54.329: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 09:49:54.332: INFO: Pod kube-proxy-mode-detector still exists
Mar 15 09:49:56.329: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 09:49:56.332: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-799
STEP: creating replication controller affinity-clusterip-timeout in namespace services-799
I0315 09:49:56.350138      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-799, replica count: 3
I0315 09:49:59.400582      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 09:49:59.404: INFO: Creating new exec pod
Mar 15 09:50:02.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Mar 15 09:50:02.607: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 15 09:50:02.607: INFO: stdout: ""
Mar 15 09:50:02.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c nc -zv -t -w 2 10.21.148.230 80'
Mar 15 09:50:02.806: INFO: stderr: "+ nc -zv -t -w 2 10.21.148.230 80\nConnection to 10.21.148.230 80 port [tcp/http] succeeded!\n"
Mar 15 09:50:02.806: INFO: stdout: ""
Mar 15 09:50:02.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.148.230:80/ ; done'
Mar 15 09:50:03.078: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:50:03.078: INFO: stdout: "\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj\naffinity-clusterip-timeout-xq8zj"
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Received response from host: affinity-clusterip-timeout-xq8zj
Mar 15 09:50:03.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.148.230:80/'
Mar 15 09:50:03.277: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:50:03.277: INFO: stdout: "affinity-clusterip-timeout-xq8zj"
Mar 15 09:52:08.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.148.230:80/'
Mar 15 09:52:09.381: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:52:09.381: INFO: stdout: "affinity-clusterip-timeout-xq8zj"
Mar 15 09:54:14.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.148.230:80/'
Mar 15 09:54:14.594: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:54:14.594: INFO: stdout: "affinity-clusterip-timeout-xq8zj"
Mar 15 09:56:19.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.148.230:80/'
Mar 15 09:56:19.801: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:56:19.801: INFO: stdout: "affinity-clusterip-timeout-xq8zj"
Mar 15 09:58:24.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-799 exec execpod-affinityk9gr9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.21.148.230:80/'
Mar 15 09:58:25.000: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.21.148.230:80/\n"
Mar 15 09:58:25.000: INFO: stdout: "affinity-clusterip-timeout-4w7j7"
Mar 15 09:58:25.000: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-799, will wait for the garbage collector to delete the pods
Mar 15 09:58:25.069: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 4.057613ms
Mar 15 09:58:25.869: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 800.227083ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 09:58:36.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-799" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:526.407 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":153,"skipped":2318,"failed":0}
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 09:58:36.490: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:00:36.533: INFO: Deleting pod "var-expansion-60d32f61-b84c-4571-85a0-d625e5008167" in namespace "var-expansion-4565"
Mar 15 10:00:36.536: INFO: Wait up to 5m0s for pod "var-expansion-60d32f61-b84c-4571-85a0-d625e5008167" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:00:40.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4565" for this suite.

• [SLOW TEST:124.056 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":154,"skipped":2318,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:00:40.548: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:00:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8407" for this suite.

• [SLOW TEST:13.087 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":155,"skipped":2320,"failed":0}
SS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:00:53.636: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:00:53.662: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-5679fd63-a63a-4374-bba4-e6a614396461" in namespace "security-context-test-8282" to be "Succeeded or Failed"
Mar 15 10:00:53.664: INFO: Pod "alpine-nnp-false-5679fd63-a63a-4374-bba4-e6a614396461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460821ms
Mar 15 10:00:55.666: INFO: Pod "alpine-nnp-false-5679fd63-a63a-4374-bba4-e6a614396461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004678284s
Mar 15 10:00:55.666: INFO: Pod "alpine-nnp-false-5679fd63-a63a-4374-bba4-e6a614396461" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:00:55.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8282" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:00:55.697: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0315 10:00:56.855402      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 10:00:56.855470      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 10:00:56.855491      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 15 10:00:56.855: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:00:56.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-995" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":157,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:00:56.862: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-1215
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 15 10:00:56.879: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 10:00:56.922: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:00:58.925: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:01:00.939: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:01:02.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:01:04.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:01:06.927: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:01:08.929: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 15 10:01:08.935: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 10:01:10.938: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 10:01:12.938: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 10:01:14.937: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 15 10:01:14.941: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 15 10:01:18.967: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.54.163 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1215 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:01:18.967: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:01:20.075: INFO: Found all expected endpoints: [netserver-0]
Mar 15 10:01:20.077: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.90.179 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1215 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:01:20.077: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:01:21.208: INFO: Found all expected endpoints: [netserver-1]
Mar 15 10:01:21.211: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.71.86 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1215 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:01:21.211: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:01:22.315: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:22.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1215" for this suite.

• [SLOW TEST:25.463 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":158,"skipped":2417,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:22.326: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 15 10:01:22.382: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 15 10:01:22.385: INFO: starting watch
STEP: patching
STEP: updating
Mar 15 10:01:22.391: INFO: waiting for watch events with expected annotations
Mar 15 10:01:22.391: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:22.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9535" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":159,"skipped":2426,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:22.427: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:01:23.310: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:01:25.316: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399283, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399283, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399283, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399283, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:01:28.326: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:38.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8051" for this suite.
STEP: Destroying namespace "webhook-8051-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.056 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":160,"skipped":2430,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:38.484: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Mar 15 10:01:38.517: INFO: Waiting up to 5m0s for pod "var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4" in namespace "var-expansion-8457" to be "Succeeded or Failed"
Mar 15 10:01:38.519: INFO: Pod "var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.200201ms
Mar 15 10:01:40.523: INFO: Pod "var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005438293s
Mar 15 10:01:42.526: INFO: Pod "var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008648124s
STEP: Saw pod success
Mar 15 10:01:42.526: INFO: Pod "var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4" satisfied condition "Succeeded or Failed"
Mar 15 10:01:42.528: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4 container dapi-container: <nil>
STEP: delete the pod
Mar 15 10:01:42.541: INFO: Waiting for pod var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4 to disappear
Mar 15 10:01:42.548: INFO: Pod var-expansion-afd39158-0140-48d1-abfb-a2cd10f49ed4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:42.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8457" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":161,"skipped":2441,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:42.554: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:46.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3325" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":162,"skipped":2446,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:46.612: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 15 10:01:51.174: INFO: Successfully updated pod "pod-update-469dd2f8-2eb5-4dfe-972b-511acc5bc4c7"
STEP: verifying the updated pod is in kubernetes
Mar 15 10:01:51.178: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6341" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:51.185: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:51.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2330" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":164,"skipped":2494,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:51.219: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 15 10:01:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Mar 15 10:01:51.855: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 15 10:01:53.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399311, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399311, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399311, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399311, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 10:01:57.248: INFO: Waited 1.339551273s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:01:58.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-731" for this suite.

• [SLOW TEST:7.077 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":165,"skipped":2508,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:01:58.297: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 15 10:01:58.334: INFO: Waiting up to 5m0s for pod "pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e" in namespace "emptydir-6253" to be "Succeeded or Failed"
Mar 15 10:01:58.336: INFO: Pod "pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499322ms
Mar 15 10:02:00.338: INFO: Pod "pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003906608s
STEP: Saw pod success
Mar 15 10:02:00.338: INFO: Pod "pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e" satisfied condition "Succeeded or Failed"
Mar 15 10:02:00.340: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e container test-container: <nil>
STEP: delete the pod
Mar 15 10:02:00.351: INFO: Waiting for pod pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e to disappear
Mar 15 10:02:00.359: INFO: Pod pod-7d7c45af-b4b0-4b1d-ac5d-195cbdace69e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:00.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6253" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2526,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:00.365: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 15 10:02:06.409: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0315 10:02:06.408982      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 10:02:06.409007      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 10:02:06.409014      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:06.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-40" for this suite.

• [SLOW TEST:6.051 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":167,"skipped":2528,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:06.415: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:02:07.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:02:09.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399327, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399327, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399327, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399327, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:02:12.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:12.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4940" for this suite.
STEP: Destroying namespace "webhook-4940-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.350 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":168,"skipped":2547,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:12.766: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:02:12.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b" in namespace "downward-api-5054" to be "Succeeded or Failed"
Mar 15 10:02:12.828: INFO: Pod "downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810851ms
Mar 15 10:02:14.831: INFO: Pod "downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007969155s
Mar 15 10:02:16.834: INFO: Pod "downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010973727s
STEP: Saw pod success
Mar 15 10:02:16.834: INFO: Pod "downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b" satisfied condition "Succeeded or Failed"
Mar 15 10:02:16.836: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b container client-container: <nil>
STEP: delete the pod
Mar 15 10:02:16.848: INFO: Waiting for pod downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b to disappear
Mar 15 10:02:16.855: INFO: Pod downwardapi-volume-ab3946d3-8e5d-4b34-9be6-718d5ef8d78b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:16.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5054" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2551,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:16.861: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 15 10:02:16.890: INFO: Waiting up to 5m0s for pod "pod-db8b8b04-066e-4dd9-977c-ad8408049043" in namespace "emptydir-897" to be "Succeeded or Failed"
Mar 15 10:02:16.900: INFO: Pod "pod-db8b8b04-066e-4dd9-977c-ad8408049043": Phase="Pending", Reason="", readiness=false. Elapsed: 9.912753ms
Mar 15 10:02:18.902: INFO: Pod "pod-db8b8b04-066e-4dd9-977c-ad8408049043": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012670498s
Mar 15 10:02:20.906: INFO: Pod "pod-db8b8b04-066e-4dd9-977c-ad8408049043": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015781635s
STEP: Saw pod success
Mar 15 10:02:20.906: INFO: Pod "pod-db8b8b04-066e-4dd9-977c-ad8408049043" satisfied condition "Succeeded or Failed"
Mar 15 10:02:20.907: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-db8b8b04-066e-4dd9-977c-ad8408049043 container test-container: <nil>
STEP: delete the pod
Mar 15 10:02:20.947: INFO: Waiting for pod pod-db8b8b04-066e-4dd9-977c-ad8408049043 to disappear
Mar 15 10:02:20.951: INFO: Pod pod-db8b8b04-066e-4dd9-977c-ad8408049043 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:20.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-897" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":170,"skipped":2552,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:20.957: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 15 10:02:29.040: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:29.042: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:31.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:31.045: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:33.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:33.045: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:35.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:35.045: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:37.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:37.045: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:39.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:39.050: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:41.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:41.046: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 15 10:02:43.042: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 15 10:02:43.048: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:02:43.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9890" for this suite.

• [SLOW TEST:22.098 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":171,"skipped":2555,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:02:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 15 10:02:43.395: INFO: Pod name wrapped-volume-race-79ee86ac-9bd7-406c-a77c-476ea38fd89c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79ee86ac-9bd7-406c-a77c-476ea38fd89c in namespace emptydir-wrapper-1596, will wait for the garbage collector to delete the pods
Mar 15 10:02:59.499: INFO: Deleting ReplicationController wrapped-volume-race-79ee86ac-9bd7-406c-a77c-476ea38fd89c took: 5.524429ms
Mar 15 10:03:00.299: INFO: Terminating ReplicationController wrapped-volume-race-79ee86ac-9bd7-406c-a77c-476ea38fd89c pods took: 800.22388ms
STEP: Creating RC which spawns configmap-volume pods
Mar 15 10:03:12.213: INFO: Pod name wrapped-volume-race-b1a7f4aa-d3a0-4e11-9800-2213602bf848: Found 0 pods out of 5
Mar 15 10:03:17.220: INFO: Pod name wrapped-volume-race-b1a7f4aa-d3a0-4e11-9800-2213602bf848: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b1a7f4aa-d3a0-4e11-9800-2213602bf848 in namespace emptydir-wrapper-1596, will wait for the garbage collector to delete the pods
Mar 15 10:03:29.313: INFO: Deleting ReplicationController wrapped-volume-race-b1a7f4aa-d3a0-4e11-9800-2213602bf848 took: 6.497812ms
Mar 15 10:03:30.213: INFO: Terminating ReplicationController wrapped-volume-race-b1a7f4aa-d3a0-4e11-9800-2213602bf848 pods took: 900.256699ms
STEP: Creating RC which spawns configmap-volume pods
Mar 15 10:03:36.529: INFO: Pod name wrapped-volume-race-9b79033f-8c34-4713-b63c-baf449cbacca: Found 0 pods out of 5
Mar 15 10:03:41.537: INFO: Pod name wrapped-volume-race-9b79033f-8c34-4713-b63c-baf449cbacca: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9b79033f-8c34-4713-b63c-baf449cbacca in namespace emptydir-wrapper-1596, will wait for the garbage collector to delete the pods
Mar 15 10:03:51.612: INFO: Deleting ReplicationController wrapped-volume-race-9b79033f-8c34-4713-b63c-baf449cbacca took: 5.091896ms
Mar 15 10:03:51.713: INFO: Terminating ReplicationController wrapped-volume-race-9b79033f-8c34-4713-b63c-baf449cbacca pods took: 100.302419ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:06.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1596" for this suite.

• [SLOW TEST:83.622 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":172,"skipped":2564,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:06.679: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-b08bb9ab-17e9-4dfa-974b-e8cdc3d30642
STEP: Creating a pod to test consume configMaps
Mar 15 10:04:06.723: INFO: Waiting up to 5m0s for pod "pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb" in namespace "configmap-9415" to be "Succeeded or Failed"
Mar 15 10:04:06.726: INFO: Pod "pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.337048ms
Mar 15 10:04:08.729: INFO: Pod "pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00598419s
STEP: Saw pod success
Mar 15 10:04:08.729: INFO: Pod "pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb" satisfied condition "Succeeded or Failed"
Mar 15 10:04:08.730: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 10:04:08.760: INFO: Waiting for pod pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb to disappear
Mar 15 10:04:08.766: INFO: Pod pod-configmaps-b27461aa-55e5-4fdf-9c60-5634504134cb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:08.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9415" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:08.772: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-0e0513b1-354e-469a-bc37-44613d9032f5
STEP: Creating configMap with name cm-test-opt-upd-7ae70f80-7d55-4695-a8be-519425b89ddf
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0e0513b1-354e-469a-bc37-44613d9032f5
STEP: Updating configmap cm-test-opt-upd-7ae70f80-7d55-4695-a8be-519425b89ddf
STEP: Creating configMap with name cm-test-opt-create-cbeee9be-3ce3-4519-8da3-bc7306329d34
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:12.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5734" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:12.873: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-005f3a37-1b36-4e54-9e4a-06bc680bb1c6
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:12.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8533" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":175,"skipped":2632,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:12.903: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2490/configmap-test-076d2026-0dda-44a3-9fa1-c0acc9c39356
STEP: Creating a pod to test consume configMaps
Mar 15 10:04:12.936: INFO: Waiting up to 5m0s for pod "pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b" in namespace "configmap-2490" to be "Succeeded or Failed"
Mar 15 10:04:12.940: INFO: Pod "pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.395037ms
Mar 15 10:04:14.943: INFO: Pod "pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007601159s
Mar 15 10:04:16.947: INFO: Pod "pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262446s
STEP: Saw pod success
Mar 15 10:04:16.947: INFO: Pod "pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b" satisfied condition "Succeeded or Failed"
Mar 15 10:04:16.953: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b container env-test: <nil>
STEP: delete the pod
Mar 15 10:04:16.967: INFO: Waiting for pod pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b to disappear
Mar 15 10:04:16.975: INFO: Pod pod-configmaps-478c6f04-2a4b-4893-b436-dea1b98ab79b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:16.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2490" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":176,"skipped":2653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:16.981: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-f196255b-9588-4a30-bba4-2fe44c6e1b0b
STEP: Creating a pod to test consume secrets
Mar 15 10:04:17.014: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0" in namespace "projected-4660" to be "Succeeded or Failed"
Mar 15 10:04:17.016: INFO: Pod "pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.89232ms
Mar 15 10:04:19.019: INFO: Pod "pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004597987s
Mar 15 10:04:21.022: INFO: Pod "pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007490679s
STEP: Saw pod success
Mar 15 10:04:21.022: INFO: Pod "pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0" satisfied condition "Succeeded or Failed"
Mar 15 10:04:21.023: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:04:21.035: INFO: Waiting for pod pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0 to disappear
Mar 15 10:04:21.042: INFO: Pod pod-projected-secrets-f26c0764-7b94-4324-b0da-ef2ea5e146e0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:21.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4660" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":177,"skipped":2687,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:21.048: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-f5tz
STEP: Creating a pod to test atomic-volume-subpath
Mar 15 10:04:21.083: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-f5tz" in namespace "subpath-5748" to be "Succeeded or Failed"
Mar 15 10:04:21.085: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927128ms
Mar 15 10:04:23.087: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004075349s
Mar 15 10:04:25.090: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 4.006669441s
Mar 15 10:04:27.092: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 6.009330168s
Mar 15 10:04:29.095: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 8.01171972s
Mar 15 10:04:31.115: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 10.03174736s
Mar 15 10:04:33.119: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 12.036128528s
Mar 15 10:04:35.122: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 14.039329536s
Mar 15 10:04:37.125: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 16.042364704s
Mar 15 10:04:39.129: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 18.045724209s
Mar 15 10:04:41.132: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 20.048896576s
Mar 15 10:04:43.135: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Running", Reason="", readiness=true. Elapsed: 22.052185004s
Mar 15 10:04:45.138: INFO: Pod "pod-subpath-test-secret-f5tz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055474864s
STEP: Saw pod success
Mar 15 10:04:45.138: INFO: Pod "pod-subpath-test-secret-f5tz" satisfied condition "Succeeded or Failed"
Mar 15 10:04:45.140: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-subpath-test-secret-f5tz container test-container-subpath-secret-f5tz: <nil>
STEP: delete the pod
Mar 15 10:04:45.152: INFO: Waiting for pod pod-subpath-test-secret-f5tz to disappear
Mar 15 10:04:45.161: INFO: Pod pod-subpath-test-secret-f5tz no longer exists
STEP: Deleting pod pod-subpath-test-secret-f5tz
Mar 15 10:04:45.161: INFO: Deleting pod "pod-subpath-test-secret-f5tz" in namespace "subpath-5748"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:45.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5748" for this suite.

• [SLOW TEST:24.121 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":178,"skipped":2688,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:45.171: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 15 10:04:45.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3772 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Mar 15 10:04:46.171: INFO: stderr: ""
Mar 15 10:04:46.171: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Mar 15 10:04:46.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-3772 delete pods e2e-test-httpd-pod'
Mar 15 10:04:52.111: INFO: stderr: ""
Mar 15 10:04:52.111: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:04:52.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3772" for this suite.

• [SLOW TEST:6.947 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":179,"skipped":2711,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:04:52.117: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0315 10:05:02.225390      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 10:05:02.225417      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 10:05:02.225424      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 15 10:05:02.225: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar 15 10:05:02.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fg9m" in namespace "gc-3942"
Mar 15 10:05:02.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-m54m6" in namespace "gc-3942"
Mar 15 10:05:02.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-p2ct7" in namespace "gc-3942"
Mar 15 10:05:02.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-qg4wx" in namespace "gc-3942"
Mar 15 10:05:02.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-r2vnj" in namespace "gc-3942"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:02.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3942" for this suite.

• [SLOW TEST:10.216 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":180,"skipped":2713,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:02.333: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 15 10:05:02.367: INFO: Waiting up to 5m0s for pod "pod-2b29ffe3-a863-4715-ab24-68a808b616a2" in namespace "emptydir-7240" to be "Succeeded or Failed"
Mar 15 10:05:02.377: INFO: Pod "pod-2b29ffe3-a863-4715-ab24-68a808b616a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057933ms
Mar 15 10:05:04.380: INFO: Pod "pod-2b29ffe3-a863-4715-ab24-68a808b616a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012987745s
STEP: Saw pod success
Mar 15 10:05:04.380: INFO: Pod "pod-2b29ffe3-a863-4715-ab24-68a808b616a2" satisfied condition "Succeeded or Failed"
Mar 15 10:05:04.382: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-2b29ffe3-a863-4715-ab24-68a808b616a2 container test-container: <nil>
STEP: delete the pod
Mar 15 10:05:04.417: INFO: Waiting for pod pod-2b29ffe3-a863-4715-ab24-68a808b616a2 to disappear
Mar 15 10:05:04.419: INFO: Pod pod-2b29ffe3-a863-4715-ab24-68a808b616a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:04.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7240" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":181,"skipped":2714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:04.425: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:04.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2549" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":182,"skipped":2737,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:04.522: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Mar 15 10:05:04.551: INFO: Waiting up to 5m0s for pod "var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf" in namespace "var-expansion-909" to be "Succeeded or Failed"
Mar 15 10:05:04.554: INFO: Pod "var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.667365ms
Mar 15 10:05:06.557: INFO: Pod "var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00568685s
STEP: Saw pod success
Mar 15 10:05:06.557: INFO: Pod "var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf" satisfied condition "Succeeded or Failed"
Mar 15 10:05:06.558: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf container dapi-container: <nil>
STEP: delete the pod
Mar 15 10:05:06.570: INFO: Waiting for pod var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf to disappear
Mar 15 10:05:06.578: INFO: Pod var-expansion-6c337774-f0d3-4b1c-9cf9-f860cddabadf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:06.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-909" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":2752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:06.585: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-39cc4fd6-7da1-4f8d-a5be-71e0f3046781
STEP: Creating a pod to test consume configMaps
Mar 15 10:05:06.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c" in namespace "configmap-8618" to be "Succeeded or Failed"
Mar 15 10:05:06.619: INFO: Pod "pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014185ms
Mar 15 10:05:08.621: INFO: Pod "pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0047072s
STEP: Saw pod success
Mar 15 10:05:08.622: INFO: Pod "pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c" satisfied condition "Succeeded or Failed"
Mar 15 10:05:08.623: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 10:05:08.635: INFO: Waiting for pod pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c to disappear
Mar 15 10:05:08.643: INFO: Pod pod-configmaps-ef84a02e-aea4-49ae-930d-030dad837c0c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:08.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8618" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":2781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:08.650: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-5fd789fe-3a19-40e0-a0b5-6754cbb6c277
STEP: Creating a pod to test consume secrets
Mar 15 10:05:08.707: INFO: Waiting up to 5m0s for pod "pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf" in namespace "secrets-2424" to be "Succeeded or Failed"
Mar 15 10:05:08.709: INFO: Pod "pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.516909ms
Mar 15 10:05:10.712: INFO: Pod "pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004335372s
STEP: Saw pod success
Mar 15 10:05:10.712: INFO: Pod "pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf" satisfied condition "Succeeded or Failed"
Mar 15 10:05:10.713: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:05:10.725: INFO: Waiting for pod pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf to disappear
Mar 15 10:05:10.732: INFO: Pod pod-secrets-8c7e01a9-5e1d-4382-b208-1d4613821baf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:10.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2424" for this suite.
STEP: Destroying namespace "secret-namespace-4537" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2804,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:10.741: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 15 10:05:13.286: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5749 pod-service-account-a1c67900-58f0-40d7-a798-3a70fd7c6509 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 15 10:05:13.480: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5749 pod-service-account-a1c67900-58f0-40d7-a798-3a70fd7c6509 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 15 10:05:13.667: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5749 pod-service-account-a1c67900-58f0-40d7-a798-3a70fd7c6509 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:13.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5749" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":186,"skipped":2875,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:13.857: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Mar 15 10:05:16.405: INFO: Successfully updated pod "labelsupdate230634bc-d7de-4f26-bedc-d193a92db1a6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2501" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":187,"skipped":2878,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:18.426: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 15 10:05:21.483: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:22.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1517" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":188,"skipped":2888,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:22.514: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:24.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8288" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":2894,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:24.567: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:29.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3445" for this suite.

• [SLOW TEST:5.406 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":190,"skipped":2902,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:29.973: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:05:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6124" for this suite.

• [SLOW TEST:16.112 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":191,"skipped":2919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:05:46.085: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 15 10:05:46.103: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 15 10:06:46.126: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:06:46.130: INFO: Starting informer...
STEP: Starting pods...
Mar 15 10:06:46.345: INFO: Pod1 is running on ip-10-0-3-172.us-east-2.compute.internal. Tainting Node
Mar 15 10:06:50.558: INFO: Pod2 is running on ip-10-0-3-172.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 15 10:07:02.113: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 15 10:07:22.112: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:22.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7558" for this suite.

• [SLOW TEST:96.043 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":192,"skipped":3001,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:22.129: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:07:22.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19" in namespace "downward-api-270" to be "Succeeded or Failed"
Mar 15 10:07:22.174: INFO: Pod "downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.844871ms
Mar 15 10:07:24.177: INFO: Pod "downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005730027s
STEP: Saw pod success
Mar 15 10:07:24.177: INFO: Pod "downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19" satisfied condition "Succeeded or Failed"
Mar 15 10:07:24.179: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19 container client-container: <nil>
STEP: delete the pod
Mar 15 10:07:24.210: INFO: Waiting for pod downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19 to disappear
Mar 15 10:07:24.216: INFO: Pod downwardapi-volume-454713d6-29be-4d39-b986-fc20b2079d19 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:24.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-270" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3014,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:24.224: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:07:24.243: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 15 10:07:27.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7635 --namespace=crd-publish-openapi-7635 create -f -'
Mar 15 10:07:29.198: INFO: stderr: ""
Mar 15 10:07:29.198: INFO: stdout: "e2e-test-crd-publish-openapi-6755-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 15 10:07:29.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7635 --namespace=crd-publish-openapi-7635 delete e2e-test-crd-publish-openapi-6755-crds test-cr'
Mar 15 10:07:29.279: INFO: stderr: ""
Mar 15 10:07:29.279: INFO: stdout: "e2e-test-crd-publish-openapi-6755-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 15 10:07:29.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7635 --namespace=crd-publish-openapi-7635 apply -f -'
Mar 15 10:07:29.495: INFO: stderr: ""
Mar 15 10:07:29.495: INFO: stdout: "e2e-test-crd-publish-openapi-6755-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 15 10:07:29.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7635 --namespace=crd-publish-openapi-7635 delete e2e-test-crd-publish-openapi-6755-crds test-cr'
Mar 15 10:07:29.574: INFO: stderr: ""
Mar 15 10:07:29.574: INFO: stdout: "e2e-test-crd-publish-openapi-6755-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 15 10:07:29.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7635 explain e2e-test-crd-publish-openapi-6755-crds'
Mar 15 10:07:29.791: INFO: stderr: ""
Mar 15 10:07:29.791: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6755-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:33.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7635" for this suite.

• [SLOW TEST:9.318 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":194,"skipped":3021,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3749
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3749
STEP: Deleting pre-stop pod
Mar 15 10:07:42.601: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:42.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3749" for this suite.

• [SLOW TEST:9.076 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":195,"skipped":3033,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:42.619: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-8cdce3bd-2fe5-45ca-a7bd-661eed5e8c87
STEP: Creating a pod to test consume configMaps
Mar 15 10:07:42.653: INFO: Waiting up to 5m0s for pod "pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7" in namespace "configmap-5867" to be "Succeeded or Failed"
Mar 15 10:07:42.658: INFO: Pod "pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289112ms
Mar 15 10:07:44.661: INFO: Pod "pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008094999s
STEP: Saw pod success
Mar 15 10:07:44.661: INFO: Pod "pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7" satisfied condition "Succeeded or Failed"
Mar 15 10:07:44.663: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 10:07:44.677: INFO: Waiting for pod pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7 to disappear
Mar 15 10:07:44.682: INFO: Pod pod-configmaps-5293866f-38d6-45e6-bec2-d327bdbddcb7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:44.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5867" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3034,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:44.689: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Mar 15 10:07:44.708: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8697 proxy --unix-socket=/tmp/kubectl-proxy-unix673239869/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:07:44.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8697" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":197,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:07:44.826: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1340 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1340;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1340 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1340;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1340.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1340.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1340.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1340.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1340.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1340.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 207.40.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.40.207_udp@PTR;check="$$(dig +tcp +noall +answer +search 207.40.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.40.207_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1340 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1340;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1340 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1340;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1340.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1340.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1340.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1340.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1340.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1340.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1340.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1340.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 207.40.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.40.207_udp@PTR;check="$$(dig +tcp +noall +answer +search 207.40.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.40.207_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 10:07:46.906: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.909: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.912: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.914: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.917: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.919: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.922: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.924: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.959: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.963: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.965: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.968: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.972: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.974: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.978: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:46.981: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:47.004: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:07:52.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.010: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.022: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.041: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.043: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.046: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.048: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.050: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.053: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.055: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.057: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:52.071: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:07:57.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.011: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.018: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.043: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.046: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.049: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.052: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.055: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.058: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.064: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:07:57.079: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:08:02.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.011: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.014: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.019: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.022: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.027: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.047: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.049: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.052: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.055: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.058: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.060: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.063: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.066: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:02.082: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:08:07.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.010: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.015: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.017: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.022: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.041: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.043: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.046: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.048: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.051: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.053: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.055: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.057: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:07.071: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:08:12.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.011: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.018: INFO: Unable to read wheezy_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.021: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.023: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.025: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.058: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.064: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.067: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.069: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.072: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.077: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.079: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.085: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:12.104: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1340 wheezy_tcp@dns-test-service.dns-1340 wheezy_udp@dns-test-service.dns-1340.svc wheezy_tcp@dns-test-service.dns-1340.svc wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:08:17.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.010: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.030: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.047: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.050: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.052: INFO: Unable to read jessie_udp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.055: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340 from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.060: INFO: Unable to read jessie_udp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.069: INFO: Unable to read jessie_tcp@dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.075: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.082: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc from pod dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae: the server could not find the requested resource (get pods dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae)
Mar 15 10:08:17.108: INFO: Lookups using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@_http._tcp.dns-test-service.dns-1340.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1340.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1340 jessie_tcp@dns-test-service.dns-1340 jessie_udp@dns-test-service.dns-1340.svc jessie_tcp@dns-test-service.dns-1340.svc jessie_udp@_http._tcp.dns-test-service.dns-1340.svc jessie_tcp@_http._tcp.dns-test-service.dns-1340.svc]

Mar 15 10:08:22.071: INFO: DNS probes using dns-1340/dns-test-c6a4cca4-f915-4396-8728-5fbb74fbe8ae succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:08:22.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1340" for this suite.

• [SLOW TEST:37.350 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":198,"skipped":3062,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:08:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:08:22.224: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:08:24.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-616" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:08:24.385: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 15 10:10:24.975: INFO: Successfully updated pod "var-expansion-39afd868-31e9-42cb-98e1-085c34c6b6c7"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 15 10:10:26.981: INFO: Deleting pod "var-expansion-39afd868-31e9-42cb-98e1-085c34c6b6c7" in namespace "var-expansion-9506"
Mar 15 10:10:26.984: INFO: Wait up to 5m0s for pod "var-expansion-39afd868-31e9-42cb-98e1-085c34c6b6c7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:11:02.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9506" for this suite.

• [SLOW TEST:158.613 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":200,"skipped":3104,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:11:02.999: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 15 10:11:07.567: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dbffd661-9721-42b9-bb0b-48ae92816b7b"
Mar 15 10:11:07.567: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dbffd661-9721-42b9-bb0b-48ae92816b7b" in namespace "pods-5430" to be "terminated due to deadline exceeded"
Mar 15 10:11:07.569: INFO: Pod "pod-update-activedeadlineseconds-dbffd661-9721-42b9-bb0b-48ae92816b7b": Phase="Running", Reason="", readiness=true. Elapsed: 1.827577ms
Mar 15 10:11:09.572: INFO: Pod "pod-update-activedeadlineseconds-dbffd661-9721-42b9-bb0b-48ae92816b7b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00496089s
Mar 15 10:11:09.572: INFO: Pod "pod-update-activedeadlineseconds-dbffd661-9721-42b9-bb0b-48ae92816b7b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:11:09.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5430" for this suite.

• [SLOW TEST:6.579 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":201,"skipped":3123,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:11:09.578: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:11:10.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:11:12.387: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399870, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399870, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399870, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751399870, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:11:15.397: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:11:15.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-904" for this suite.
STEP: Destroying namespace "webhook-904-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.912 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":202,"skipped":3135,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:11:15.490: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1172
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Mar 15 10:11:15.548: INFO: Found 0 stateful pods, waiting for 3
Mar 15 10:11:25.552: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:11:25.552: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:11:25.552: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 15 10:11:25.575: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 15 10:11:35.609: INFO: Updating stateful set ss2
Mar 15 10:11:35.615: INFO: Waiting for Pod statefulset-1172/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 15 10:11:45.663: INFO: Found 1 stateful pods, waiting for 3
Mar 15 10:11:55.667: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:11:55.667: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:11:55.667: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 15 10:11:55.687: INFO: Updating stateful set ss2
Mar 15 10:11:55.695: INFO: Waiting for Pod statefulset-1172/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 15 10:12:05.701: INFO: Waiting for Pod statefulset-1172/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 15 10:12:15.716: INFO: Updating stateful set ss2
Mar 15 10:12:15.722: INFO: Waiting for StatefulSet statefulset-1172/ss2 to complete update
Mar 15 10:12:15.722: INFO: Waiting for Pod statefulset-1172/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 10:12:25.728: INFO: Deleting all statefulset in ns statefulset-1172
Mar 15 10:12:25.730: INFO: Scaling statefulset ss2 to 0
Mar 15 10:12:45.740: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 10:12:45.742: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:12:45.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1172" for this suite.

• [SLOW TEST:90.275 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":203,"skipped":3138,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:12:45.765: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6374
STEP: creating service affinity-nodeport-transition in namespace services-6374
STEP: creating replication controller affinity-nodeport-transition in namespace services-6374
I0315 10:12:45.830467      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-6374, replica count: 3
I0315 10:12:48.880925      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:12:48.888: INFO: Creating new exec pod
Mar 15 10:12:51.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Mar 15 10:12:52.113: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 15 10:12:52.113: INFO: stdout: ""
Mar 15 10:12:52.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 10.21.193.246 80'
Mar 15 10:12:52.305: INFO: stderr: "+ nc -zv -t -w 2 10.21.193.246 80\nConnection to 10.21.193.246 80 port [tcp/http] succeeded!\n"
Mar 15 10:12:52.305: INFO: stdout: ""
Mar 15 10:12:52.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.172 31005'
Mar 15 10:12:52.493: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.172 31005\nConnection to 10.0.3.172 31005 port [tcp/31005] succeeded!\n"
Mar 15 10:12:52.493: INFO: stdout: ""
Mar 15 10:12:52.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.18 31005'
Mar 15 10:12:52.684: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.18 31005\nConnection to 10.0.1.18 31005 port [tcp/31005] succeeded!\n"
Mar 15 10:12:52.684: INFO: stdout: ""
Mar 15 10:12:52.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 18.189.17.197 31005'
Mar 15 10:12:52.874: INFO: stderr: "+ nc -zv -t -w 2 18.189.17.197 31005\nConnection to 18.189.17.197 31005 port [tcp/31005] succeeded!\n"
Mar 15 10:12:52.874: INFO: stdout: ""
Mar 15 10:12:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c nc -zv -t -w 2 18.219.113.27 31005'
Mar 15 10:12:53.058: INFO: stderr: "+ nc -zv -t -w 2 18.219.113.27 31005\nConnection to 18.219.113.27 31005 port [tcp/31005] succeeded!\n"
Mar 15 10:12:53.058: INFO: stdout: ""
Mar 15 10:12:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.18:31005/ ; done'
Mar 15 10:12:53.396: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n"
Mar 15 10:12:53.396: INFO: stdout: "\naffinity-nodeport-transition-6xlnv\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-sdsgf\naffinity-nodeport-transition-6xlnv\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-sdsgf\naffinity-nodeport-transition-6xlnv\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-sdsgf\naffinity-nodeport-transition-6xlnv\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-sdsgf\naffinity-nodeport-transition-6xlnv\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-sdsgf\naffinity-nodeport-transition-6xlnv"
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-sdsgf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-sdsgf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-sdsgf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-sdsgf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-sdsgf
Mar 15 10:12:53.396: INFO: Received response from host: affinity-nodeport-transition-6xlnv
Mar 15 10:12:53.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-6374 exec execpod-affinityzh9db -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.18:31005/ ; done'
Mar 15 10:12:53.752: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31005/\n"
Mar 15 10:12:53.752: INFO: stdout: "\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf\naffinity-nodeport-transition-9hstf"
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Received response from host: affinity-nodeport-transition-9hstf
Mar 15 10:12:53.752: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6374, will wait for the garbage collector to delete the pods
Mar 15 10:12:53.823: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.526455ms
Mar 15 10:12:54.623: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 800.222723ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:12:59.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6374" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:13.401 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":204,"skipped":3149,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:12:59.167: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:12:59.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:13:02.946: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 15 10:13:02.961: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:13:02.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6299" for this suite.
STEP: Destroying namespace "webhook-6299-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":205,"skipped":3162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:13:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:13:03.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e" in namespace "projected-4482" to be "Succeeded or Failed"
Mar 15 10:13:03.072: INFO: Pod "downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.563939ms
Mar 15 10:13:05.075: INFO: Pod "downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007139853s
Mar 15 10:13:07.078: INFO: Pod "downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009876472s
STEP: Saw pod success
Mar 15 10:13:07.078: INFO: Pod "downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e" satisfied condition "Succeeded or Failed"
Mar 15 10:13:07.080: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e container client-container: <nil>
STEP: delete the pod
Mar 15 10:13:07.101: INFO: Waiting for pod downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e to disappear
Mar 15 10:13:07.107: INFO: Pod downwardapi-volume-9e60d395-f715-4ffe-8d95-d2f7a3c46d7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:13:07.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4482" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3190,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:13:07.113: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 15 10:13:07.145: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4573 /api/v1/namespaces/watch-4573/configmaps/e2e-watch-test-watch-closed e72ce6d4-e4b6-4518-8ba5-6b36376cabf4 674075 0 2021-03-15 10:13:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-15 10:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:13:07.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4573 /api/v1/namespaces/watch-4573/configmaps/e2e-watch-test-watch-closed e72ce6d4-e4b6-4518-8ba5-6b36376cabf4 674076 0 2021-03-15 10:13:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-15 10:13:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 15 10:13:07.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4573 /api/v1/namespaces/watch-4573/configmaps/e2e-watch-test-watch-closed e72ce6d4-e4b6-4518-8ba5-6b36376cabf4 674077 0 2021-03-15 10:13:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-15 10:13:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:13:07.152: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4573 /api/v1/namespaces/watch-4573/configmaps/e2e-watch-test-watch-closed e72ce6d4-e4b6-4518-8ba5-6b36376cabf4 674078 0 2021-03-15 10:13:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-03-15 10:13:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:13:07.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4573" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":207,"skipped":3192,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:13:07.158: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1168, will wait for the garbage collector to delete the pods
Mar 15 10:13:11.242: INFO: Deleting Job.batch foo took: 4.032271ms
Mar 15 10:13:11.342: INFO: Terminating Job.batch foo pods took: 100.270647ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:13:52.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1168" for this suite.

• [SLOW TEST:44.993 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":208,"skipped":3195,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:13:52.151: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:13:53.042: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:13:55.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400033, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400033, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400033, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400033, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:13:58.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:13:58.062: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:13:59.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9552" for this suite.
STEP: Destroying namespace "webhook-9552-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.107 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":209,"skipped":3210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:13:59.259: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-t54d
STEP: Creating a pod to test atomic-volume-subpath
Mar 15 10:13:59.325: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-t54d" in namespace "subpath-5676" to be "Succeeded or Failed"
Mar 15 10:13:59.327: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993674ms
Mar 15 10:14:01.343: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017897s
Mar 15 10:14:03.347: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 4.021325716s
Mar 15 10:14:05.350: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 6.025027878s
Mar 15 10:14:07.353: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 8.027932486s
Mar 15 10:14:09.356: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 10.030855768s
Mar 15 10:14:11.359: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 12.033875935s
Mar 15 10:14:13.361: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 14.036279611s
Mar 15 10:14:15.364: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 16.038886955s
Mar 15 10:14:17.367: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 18.041289079s
Mar 15 10:14:19.369: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 20.04349032s
Mar 15 10:14:21.371: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Running", Reason="", readiness=true. Elapsed: 22.045910714s
Mar 15 10:14:23.374: INFO: Pod "pod-subpath-test-projected-t54d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048653346s
STEP: Saw pod success
Mar 15 10:14:23.374: INFO: Pod "pod-subpath-test-projected-t54d" satisfied condition "Succeeded or Failed"
Mar 15 10:14:23.376: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-subpath-test-projected-t54d container test-container-subpath-projected-t54d: <nil>
STEP: delete the pod
Mar 15 10:14:23.388: INFO: Waiting for pod pod-subpath-test-projected-t54d to disappear
Mar 15 10:14:23.396: INFO: Pod pod-subpath-test-projected-t54d no longer exists
STEP: Deleting pod pod-subpath-test-projected-t54d
Mar 15 10:14:23.397: INFO: Deleting pod "pod-subpath-test-projected-t54d" in namespace "subpath-5676"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:14:23.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5676" for this suite.

• [SLOW TEST:24.145 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":210,"skipped":3243,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:14:23.404: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 15 10:14:23.429: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674509 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:14:23.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674509 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 15 10:14:33.435: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674560 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:14:33.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674560 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 15 10:14:43.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674597 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:14:43.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674597 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 15 10:14:53.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674623 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:14:53.446: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-a f8f90a25-6508-4d74-bc71-6d1b9df6f54b 674623 0 2021-03-15 10:14:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-03-15 10:14:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 15 10:15:03.451: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-b 437873ff-9a6e-4916-b062-5069802d053c 674649 0 2021-03-15 10:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-15 10:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:15:03.451: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-b 437873ff-9a6e-4916-b062-5069802d053c 674649 0 2021-03-15 10:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-15 10:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 15 10:15:13.455: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-b 437873ff-9a6e-4916-b062-5069802d053c 674675 0 2021-03-15 10:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-15 10:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 15 10:15:13.455: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5579 /api/v1/namespaces/watch-5579/configmaps/e2e-watch-test-configmap-b 437873ff-9a6e-4916-b062-5069802d053c 674675 0 2021-03-15 10:15:03 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-03-15 10:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:15:23.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5579" for this suite.

• [SLOW TEST:60.058 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":211,"skipped":3253,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:15:23.462: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 15 10:15:23.506: INFO: Created pod &Pod{ObjectMeta:{dns-5857  dns-5857 /api/v1/namespaces/dns-5857/pods/dns-5857 eb89601b-46fb-4667-9a53-5438357a1968 674708 0 2021-03-15 10:15:23 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-03-15 10:15:23 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bbv6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bbv6x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bbv6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:15:23.509: INFO: The status of Pod dns-5857 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:15:25.511: INFO: The status of Pod dns-5857 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:15:27.512: INFO: The status of Pod dns-5857 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 15 10:15:27.512: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5857 PodName:dns-5857 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:15:27.512: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Verifying customized DNS server is configured on pod...
Mar 15 10:15:27.630: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5857 PodName:dns-5857 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:15:27.630: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:15:27.735: INFO: Deleting pod dns-5857...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:15:27.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5857" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":212,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:15:27.755: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0315 10:15:29.308185      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0315 10:15:29.308207      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0315 10:15:29.308214      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 15 10:15:29.308: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:15:29.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-436" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":213,"skipped":3290,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:15:29.315: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-4de663b4-2448-4f6a-81ac-c2a4b597474b
STEP: Creating a pod to test consume secrets
Mar 15 10:15:29.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3" in namespace "projected-5206" to be "Succeeded or Failed"
Mar 15 10:15:29.344: INFO: Pod "pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.957939ms
Mar 15 10:15:31.347: INFO: Pod "pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004851391s
Mar 15 10:15:33.350: INFO: Pod "pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748709s
STEP: Saw pod success
Mar 15 10:15:33.350: INFO: Pod "pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3" satisfied condition "Succeeded or Failed"
Mar 15 10:15:33.352: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3 container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:15:33.365: INFO: Waiting for pod pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3 to disappear
Mar 15 10:15:33.372: INFO: Pod pod-projected-secrets-9fd1521d-be05-4e30-b1f1-bf32abf0a5d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:15:33.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5206" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3307,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:15:33.377: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:17:33.410: INFO: Deleting pod "var-expansion-09b6711b-1414-4592-bf66-fadf25bec0f1" in namespace "var-expansion-7095"
Mar 15 10:17:33.414: INFO: Wait up to 5m0s for pod "var-expansion-09b6711b-1414-4592-bf66-fadf25bec0f1" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:17:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7095" for this suite.

• [SLOW TEST:122.049 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":215,"skipped":3321,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:17:35.427: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Mar 15 10:17:35.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 15 10:17:36.448: INFO: stderr: ""
Mar 15 10:17:36.448: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Mar 15 10:17:36.448: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 15 10:17:36.448: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8462" to be "running and ready, or succeeded"
Mar 15 10:17:36.454: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.46808ms
Mar 15 10:17:38.456: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007914316s
Mar 15 10:17:38.456: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 15 10:17:38.456: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 15 10:17:38.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator'
Mar 15 10:17:38.557: INFO: stderr: ""
Mar 15 10:17:38.557: INFO: stdout: "I0315 10:17:37.723709       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/xwb 262\nI0315 10:17:37.923874       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/7jv5 424\nI0315 10:17:38.123816       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/w8s 389\nI0315 10:17:38.323959       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/rqc5 488\nI0315 10:17:38.524349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gbhw 569\n"
STEP: limiting log lines
Mar 15 10:17:38.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator --tail=1'
Mar 15 10:17:38.640: INFO: stderr: ""
Mar 15 10:17:38.640: INFO: stdout: "I0315 10:17:38.524349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gbhw 569\n"
Mar 15 10:17:38.640: INFO: got output "I0315 10:17:38.524349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gbhw 569\n"
STEP: limiting log bytes
Mar 15 10:17:38.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator --limit-bytes=1'
Mar 15 10:17:38.724: INFO: stderr: ""
Mar 15 10:17:38.724: INFO: stdout: "I"
Mar 15 10:17:38.724: INFO: got output "I"
STEP: exposing timestamps
Mar 15 10:17:38.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 15 10:17:38.809: INFO: stderr: ""
Mar 15 10:17:38.809: INFO: stdout: "2021-03-15T10:17:38.723977431Z I0315 10:17:38.723895       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/rmg 518\n"
Mar 15 10:17:38.809: INFO: got output "2021-03-15T10:17:38.723977431Z I0315 10:17:38.723895       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/rmg 518\n"
STEP: restricting to a time range
Mar 15 10:17:41.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator --since=1s'
Mar 15 10:17:41.398: INFO: stderr: ""
Mar 15 10:17:41.398: INFO: stdout: "I0315 10:17:40.523891       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/6djq 291\nI0315 10:17:40.723889       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/zvbk 541\nI0315 10:17:40.923911       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/tqd6 264\nI0315 10:17:41.123863       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/xdl 353\nI0315 10:17:41.323937       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/j86 218\n"
Mar 15 10:17:41.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 logs logs-generator logs-generator --since=24h'
Mar 15 10:17:41.486: INFO: stderr: ""
Mar 15 10:17:41.486: INFO: stdout: "I0315 10:17:37.723709       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/xwb 262\nI0315 10:17:37.923874       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/7jv5 424\nI0315 10:17:38.123816       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/w8s 389\nI0315 10:17:38.323959       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/rqc5 488\nI0315 10:17:38.524349       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/gbhw 569\nI0315 10:17:38.723895       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/rmg 518\nI0315 10:17:38.923878       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/9m5 414\nI0315 10:17:39.123822       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/5d5s 454\nI0315 10:17:39.323958       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/lszw 323\nI0315 10:17:39.523973       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/bm54 272\nI0315 10:17:39.723908       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/ms8 384\nI0315 10:17:39.923896       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/8jq 412\nI0315 10:17:40.123898       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/pkw 244\nI0315 10:17:40.323905       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/vf7 542\nI0315 10:17:40.523891       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/6djq 291\nI0315 10:17:40.723889       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/zvbk 541\nI0315 10:17:40.923911       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/tqd6 264\nI0315 10:17:41.123863       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/xdl 353\nI0315 10:17:41.323937       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/j86 218\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Mar 15 10:17:41.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-8462 delete pod logs-generator'
Mar 15 10:17:52.112: INFO: stderr: ""
Mar 15 10:17:52.112: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:17:52.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8462" for this suite.

• [SLOW TEST:16.694 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":216,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:17:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4812
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 15 10:17:52.139: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 15 10:17:52.175: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:17:54.178: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 15 10:17:56.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:17:58.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:00.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:02.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:04.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:06.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:08.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:10.178: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 15 10:18:12.178: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 15 10:18:12.181: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 15 10:18:14.185: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 15 10:18:14.188: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 15 10:18:16.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.155:8080/dial?request=hostname&protocol=http&host=10.20.54.174&port=8080&tries=1'] Namespace:pod-network-test-4812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:18:16.211: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:18:16.346: INFO: Waiting for responses: map[]
Mar 15 10:18:16.348: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.155:8080/dial?request=hostname&protocol=http&host=10.20.90.200&port=8080&tries=1'] Namespace:pod-network-test-4812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:18:16.349: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:18:16.498: INFO: Waiting for responses: map[]
Mar 15 10:18:16.500: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.71.155:8080/dial?request=hostname&protocol=http&host=10.20.71.154&port=8080&tries=1'] Namespace:pod-network-test-4812 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:18:16.500: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:18:16.618: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:16.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4812" for this suite.

• [SLOW TEST:24.503 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":217,"skipped":3350,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:16.625: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 15 10:18:16.649: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 10:18:16.659: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 10:18:16.661: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-18.us-east-2.compute.internal before test
Mar 15 10:18:16.671: INFO: metrics-server-v0.3.6-58cf8c5dfb-dnzrv from kube-system started at 2021-03-12 15:30:23 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 10:18:16.671: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 15 10:18:16.671: INFO: dashboard-metrics-scraper-7b59f7d4df-xrqjg from kubernetes-dashboard started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 15 10:18:16.671: INFO: node-exporter-2k2xw from pf9-monitoring started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:18:16.671: INFO: prometheus-system-0 from pf9-monitoring started at 2021-03-12 15:32:05 +0000 UTC (3 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container prometheus ready: true, restart count 1
Mar 15 10:18:16.671: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 15 10:18:16.671: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 15 10:18:16.671: INFO: packageserver-67c5dfcffc-4vmch from pf9-olm started at 2021-03-14 07:25:39 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:18:16.671: INFO: netserver-0 from pod-network-test-4812 started at 2021-03-15 10:17:52 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container webserver ready: true, restart count 0
Mar 15 10:18:16.671: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.671: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar 15 10:18:16.671: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:18:16.671: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-2-120.us-east-2.compute.internal before test
Mar 15 10:18:16.684: INFO: coredns-867bf6789f-kfcq4 from kube-system started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container coredns ready: true, restart count 0
Mar 15 10:18:16.684: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2021-03-14 07:25:32 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container alertmanager ready: true, restart count 0
Mar 15 10:18:16.684: INFO: 	Container config-reloader ready: true, restart count 0
Mar 15 10:18:16.684: INFO: grafana-9b947c8-gqfl8 from pf9-monitoring started at 2021-03-12 15:32:10 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container grafana ready: true, restart count 0
Mar 15 10:18:16.684: INFO: 	Container proxy ready: true, restart count 0
Mar 15 10:18:16.684: INFO: kube-state-metrics-6ff68b466d-2xpvx from pf9-monitoring started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 15 10:18:16.684: INFO: node-exporter-gs57d from pf9-monitoring started at 2021-03-12 15:30:01 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:18:16.684: INFO: catalog-operator-6656569cc-bsdrn from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 15 10:18:16.684: INFO: olm-operator-5c657955f6-jdk8b from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container olm-operator ready: true, restart count 0
Mar 15 10:18:16.684: INFO: packageserver-67c5dfcffc-vnl24 from pf9-olm started at 2021-03-14 07:25:32 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:18:16.684: INFO: platform9-operators-cxg4l from pf9-olm started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container registry-server ready: true, restart count 0
Mar 15 10:18:16.684: INFO: monhelper-5577f87674-m95zg from pf9-operators started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container monhelper ready: true, restart count 0
Mar 15 10:18:16.684: INFO: prometheus-operator-7b96488f76-st7bk from pf9-operators started at 2021-03-12 15:31:19 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 15 10:18:16.684: INFO: netserver-1 from pod-network-test-4812 started at 2021-03-15 10:17:52 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container webserver ready: true, restart count 0
Mar 15 10:18:16.684: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.684: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar 15 10:18:16.684: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:18:16.684: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-3-172.us-east-2.compute.internal before test
Mar 15 10:18:16.695: INFO: node-exporter-tzdsq from pf9-monitoring started at 2021-03-12 15:30:02 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:18:16.695: INFO: netserver-2 from pod-network-test-4812 started at 2021-03-15 10:17:52 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container webserver ready: true, restart count 0
Mar 15 10:18:16.695: INFO: test-container-pod from pod-network-test-4812 started at 2021-03-15 10:18:14 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container webserver ready: true, restart count 0
Mar 15 10:18:16.695: INFO: sonobuoy from sonobuoy started at 2021-03-15 09:00:40 +0000 UTC (1 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 10:18:16.695: INFO: sonobuoy-e2e-job-be88518f7bd84c37 from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container e2e ready: true, restart count 0
Mar 15 10:18:16.695: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 10:18:16.695: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:18:16.695: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar 15 10:18:16.695: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node ip-10-0-1-18.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-2-120.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod coredns-867bf6789f-kfcq4 requesting resource cpu=100m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod metrics-server-v0.3.6-58cf8c5dfb-dnzrv requesting resource cpu=53m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod dashboard-metrics-scraper-7b59f7d4df-xrqjg requesting resource cpu=0m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod alertmanager-sysalert-0 requesting resource cpu=200m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod grafana-9b947c8-gqfl8 requesting resource cpu=100m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod kube-state-metrics-6ff68b466d-2xpvx requesting resource cpu=100m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod node-exporter-2k2xw requesting resource cpu=102m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod node-exporter-gs57d requesting resource cpu=102m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod node-exporter-tzdsq requesting resource cpu=102m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod prometheus-system-0 requesting resource cpu=700m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod catalog-operator-6656569cc-bsdrn requesting resource cpu=10m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod olm-operator-5c657955f6-jdk8b requesting resource cpu=10m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod packageserver-67c5dfcffc-4vmch requesting resource cpu=10m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod packageserver-67c5dfcffc-vnl24 requesting resource cpu=10m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod platform9-operators-cxg4l requesting resource cpu=10m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod monhelper-5577f87674-m95zg requesting resource cpu=50m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod prometheus-operator-7b96488f76-st7bk requesting resource cpu=100m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod netserver-0 requesting resource cpu=0m on Node ip-10-0-1-18.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod netserver-1 requesting resource cpu=0m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod netserver-2 requesting resource cpu=0m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod test-container-pod requesting resource cpu=0m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod sonobuoy-e2e-job-be88518f7bd84c37 requesting resource cpu=0m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd requesting resource cpu=0m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k requesting resource cpu=0m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.754: INFO: Pod sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f requesting resource cpu=0m on Node ip-10-0-1-18.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Mar 15 10:18:16.754: INFO: Creating a pod which consumes cpu=845m on Node ip-10-0-2-120.us-east-2.compute.internal
Mar 15 10:18:16.758: INFO: Creating a pod which consumes cpu=1328m on Node ip-10-0-3-172.us-east-2.compute.internal
Mar 15 10:18:16.772: INFO: Creating a pod which consumes cpu=794m on Node ip-10-0-1-18.us-east-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb.166c7cb82c05b65c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9402/filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb to ip-10-0-3-172.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb.166c7cb865755eba], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb.166c7cb86ac5e855], Reason = [Created], Message = [Created container filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb.166c7cb8754023c6], Reason = [Started], Message = [Started container filler-pod-04df00f9-36d8-4a97-821b-d534f31e5bfb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d.166c7cb82b66e933], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9402/filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d to ip-10-0-2-120.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d.166c7cb865a73887], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d.166c7cb86c191e29], Reason = [Created], Message = [Created container filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d.166c7cb8755bc22c], Reason = [Started], Message = [Started container filler-pod-52685e4b-b09e-4cb5-b7f5-28b2f769eb4d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e.166c7cb82d190ad1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9402/filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e to ip-10-0-1-18.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e.166c7cb86a50b623], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e.166c7cb870314f83], Reason = [Created], Message = [Created container filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e.166c7cb879e726f0], Reason = [Started], Message = [Started container filler-pod-b7854b75-802d-4f6b-995c-00ab173bba4e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166c7cb8a52a6381], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.166c7cb8a5ffe432], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-1-18.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-2-120.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-3-172.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:19.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9402" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":218,"skipped":3357,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:19.873: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Mar 15 10:18:19.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-6063 api-versions'
Mar 15 10:18:20.007: INFO: stderr: ""
Mar 15 10:18:20.007: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\nagent.pf9.io/v1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:20.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6063" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":219,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:20.016: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-00808383-a763-46f0-a504-b9c975cb3877
STEP: Creating a pod to test consume secrets
Mar 15 10:18:20.098: INFO: Waiting up to 5m0s for pod "pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04" in namespace "secrets-4059" to be "Succeeded or Failed"
Mar 15 10:18:20.101: INFO: Pod "pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04": Phase="Pending", Reason="", readiness=false. Elapsed: 3.560283ms
Mar 15 10:18:22.104: INFO: Pod "pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006712619s
STEP: Saw pod success
Mar 15 10:18:22.104: INFO: Pod "pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04" satisfied condition "Succeeded or Failed"
Mar 15 10:18:22.106: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04 container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:18:22.122: INFO: Waiting for pod pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04 to disappear
Mar 15 10:18:22.129: INFO: Pod pod-secrets-c044b67f-6c92-402f-8876-75155ebdbe04 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:22.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4059" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":220,"skipped":3440,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:22.135: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:18:22.164: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-57b8ffff-d65e-4958-8262-f48552c7a8a5" in namespace "security-context-test-2154" to be "Succeeded or Failed"
Mar 15 10:18:22.167: INFO: Pod "busybox-privileged-false-57b8ffff-d65e-4958-8262-f48552c7a8a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284808ms
Mar 15 10:18:24.169: INFO: Pod "busybox-privileged-false-57b8ffff-d65e-4958-8262-f48552c7a8a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005564029s
Mar 15 10:18:24.169: INFO: Pod "busybox-privileged-false-57b8ffff-d65e-4958-8262-f48552c7a8a5" satisfied condition "Succeeded or Failed"
Mar 15 10:18:24.175: INFO: Got logs for pod "busybox-privileged-false-57b8ffff-d65e-4958-8262-f48552c7a8a5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:24.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2154" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 15 10:18:24.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-4138 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 15 10:18:24.309: INFO: stderr: ""
Mar 15 10:18:24.309: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 15 10:18:29.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-4138 get pod e2e-test-httpd-pod -o json'
Mar 15 10:18:29.433: INFO: stderr: ""
Mar 15 10:18:29.433: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-03-15T10:18:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-15T10:18:24Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.20.71.159\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-03-15T10:18:26Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4138\",\n        \"resourceVersion\": \"675615\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4138/pods/e2e-test-httpd-pod\",\n        \"uid\": \"144d45ec-1ab7-4e76-b955-1f9055bd7d50\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-42w89\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-3-172.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-42w89\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-42w89\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-15T10:18:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-15T10:18:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-15T10:18:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-03-15T10:18:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c9e010d1fd2e29a5c2faa05830578b2a7361c22c240befc78b933e23e14caf05\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-03-15T10:18:25Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.3.172\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.71.159\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.71.159\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-03-15T10:18:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 15 10:18:29.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-4138 replace -f -'
Mar 15 10:18:29.710: INFO: stderr: ""
Mar 15 10:18:29.710: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Mar 15 10:18:29.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-4138 delete pods e2e-test-httpd-pod'
Mar 15 10:18:42.112: INFO: stderr: ""
Mar 15 10:18:42.112: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:42.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4138" for this suite.

• [SLOW TEST:17.941 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":222,"skipped":3464,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:42.122: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3484
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3484
STEP: creating replication controller externalsvc in namespace services-3484
I0315 10:18:42.186170      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3484, replica count: 2
I0315 10:18:45.236594      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 15 10:18:45.262: INFO: Creating new exec pod
Mar 15 10:18:49.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-3484 exec execpod6x4xf -- /bin/sh -x -c nslookup clusterip-service.services-3484.svc.cluster.local'
Mar 15 10:18:49.518: INFO: stderr: "+ nslookup clusterip-service.services-3484.svc.cluster.local\n"
Mar 15 10:18:49.518: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-3484.svc.cluster.local\tcanonical name = externalsvc.services-3484.svc.cluster.local.\nName:\texternalsvc.services-3484.svc.cluster.local\nAddress: 10.21.190.196\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3484, will wait for the garbage collector to delete the pods
Mar 15 10:18:49.575: INFO: Deleting ReplicationController externalsvc took: 4.136271ms
Mar 15 10:18:50.375: INFO: Terminating ReplicationController externalsvc pods took: 800.2418ms
Mar 15 10:18:56.491: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:18:56.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3484" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.402 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":223,"skipped":3479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:18:56.525: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:19:12.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4065" for this suite.

• [SLOW TEST:16.140 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":224,"skipped":3509,"failed":0}
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:19:12.666: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 15 10:19:12.697: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 15 10:19:12.700: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 15 10:19:12.700: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 15 10:19:12.721: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 15 10:19:12.721: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 15 10:19:12.733: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 15 10:19:12.734: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 15 10:19:19.757: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:19:19.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1206" for this suite.

• [SLOW TEST:7.108 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":225,"skipped":3509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:19:19.775: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 in namespace container-probe-5716
Mar 15 10:19:23.810: INFO: Started pod liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 in namespace container-probe-5716
STEP: checking the pod's current state and verifying that restartCount is present
Mar 15 10:19:23.812: INFO: Initial restart count of pod liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is 0
Mar 15 10:19:39.845: INFO: Restart count of pod container-probe-5716/liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is now 1 (16.032728094s elapsed)
Mar 15 10:19:59.875: INFO: Restart count of pod container-probe-5716/liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is now 2 (36.062695609s elapsed)
Mar 15 10:20:19.905: INFO: Restart count of pod container-probe-5716/liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is now 3 (56.093341439s elapsed)
Mar 15 10:20:39.938: INFO: Restart count of pod container-probe-5716/liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is now 4 (1m16.125694472s elapsed)
Mar 15 10:21:42.037: INFO: Restart count of pod container-probe-5716/liveness-0e8375b1-caac-4f0b-b1ea-bef26f9f4101 is now 5 (2m18.224395289s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:21:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5716" for this suite.

• [SLOW TEST:142.280 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":226,"skipped":3531,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:21:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Mar 15 10:21:42.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-6449 cluster-info'
Mar 15 10:21:42.204: INFO: stderr: ""
Mar 15 10:21:42.204: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:21:42.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6449" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":227,"skipped":3553,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:21:42.210: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:21:42.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 create -f -'
Mar 15 10:21:42.510: INFO: stderr: ""
Mar 15 10:21:42.510: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 15 10:21:42.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 create -f -'
Mar 15 10:21:42.753: INFO: stderr: ""
Mar 15 10:21:42.753: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 15 10:21:43.756: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:21:43.756: INFO: Found 0 / 1
Mar 15 10:21:44.756: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:21:44.757: INFO: Found 1 / 1
Mar 15 10:21:44.757: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 15 10:21:44.758: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:21:44.758: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 10:21:44.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 describe pod agnhost-primary-5z52c'
Mar 15 10:21:44.849: INFO: stderr: ""
Mar 15 10:21:44.849: INFO: stdout: "Name:         agnhost-primary-5z52c\nNamespace:    kubectl-7351\nPriority:     0\nNode:         ip-10-0-3-172.us-east-2.compute.internal/10.0.3.172\nStart Time:   Mon, 15 Mar 2021 10:21:42 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.20.71.163\nIPs:\n  IP:           10.20.71.163\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://5a80acd52d44155787e78f2f6157d9677d57e191e34f64bb5968edb42d7d76fd\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 15 Mar 2021 10:21:43 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tntnt (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tntnt:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tntnt\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7351/agnhost-primary-5z52c to ip-10-0-3-172.us-east-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 15 10:21:44.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 describe rc agnhost-primary'
Mar 15 10:21:44.946: INFO: stderr: ""
Mar 15 10:21:44.946: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7351\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5z52c\n"
Mar 15 10:21:44.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 describe service agnhost-primary'
Mar 15 10:21:45.030: INFO: stderr: ""
Mar 15 10:21:45.030: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7351\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.21.83.238\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.71.163:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 15 10:21:45.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 describe node ip-10-0-1-18.us-east-2.compute.internal'
Mar 15 10:21:45.141: INFO: stderr: ""
Mar 15 10:21:45.141: INFO: stdout: "Name:               ip-10-0-1-18.us-east-2.compute.internal\nRoles:              worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-18.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=t2.large\n                    topology.kubernetes.io/region=us-east-2\n                    topology.kubernetes.io/zone=us-east-2a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 12 Mar 2021 15:30:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-18.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 15 Mar 2021 10:21:35 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 15 Mar 2021 10:20:16 +0000   Fri, 12 Mar 2021 15:30:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 15 Mar 2021 10:20:16 +0000   Fri, 12 Mar 2021 15:30:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 15 Mar 2021 10:20:16 +0000   Fri, 12 Mar 2021 15:30:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 15 Mar 2021 10:20:16 +0000   Fri, 12 Mar 2021 15:30:10 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.18\n  ExternalIP:   18.219.113.27\n  Hostname:     ip-10-0-1-18.us-east-2.compute.internal\n  InternalDNS:  ip-10-0-1-18.us-east-2.compute.internal\n  ExternalDNS:  ec2-18-219-113-27.us-east-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           50758604Ki\n  hugepages-2Mi:               0\n  memory:                      8167024Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           46779129369\n  hugepages-2Mi:               0\n  memory:                      8064624Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 0e6dbd24119145909b241069823de054\n  System UUID:                EC2E8BA3-370B-49E7-8AF8-F0C46A3BA59B\n  Boot ID:                    c7bb4d5e-b8d9-4b67-8506-cfca9f4054be\n  Kernel Version:             4.15.0-1054-aws\n  OS Image:                   Ubuntu 18.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.11\n  Kubelet Version:            v1.19.6\n  Kube-Proxy Version:         v1.19.6\nProviderID:                   aws:///us-east-2a/i-0430b3abf88664389\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 metrics-server-v0.3.6-58cf8c5dfb-dnzrv                     53m (2%)      148m (7%)   154Mi (1%)       404Mi (5%)     2d18h\n  kubernetes-dashboard        dashboard-metrics-scraper-7b59f7d4df-xrqjg                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d18h\n  pf9-monitoring              node-exporter-2k2xw                                        102m (5%)     250m (12%)  180Mi (2%)       180Mi (2%)     2d18h\n  pf9-monitoring              prometheus-system-0                                        700m (35%)    200m (10%)  562Mi (7%)       50Mi (0%)      2d18h\n  pf9-olm                     packageserver-67c5dfcffc-4vmch                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         26h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         865m (43%)   598m (29%)\n  memory                      946Mi (12%)  634Mi (8%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:                       <none>\n"
Mar 15 10:21:45.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-7351 describe namespace kubectl-7351'
Mar 15 10:21:45.220: INFO: stderr: ""
Mar 15 10:21:45.221: INFO: stdout: "Name:         kubectl-7351\nLabels:       e2e-framework=kubectl\n              e2e-run=505c0347-b7f7-4c2f-9a38-1bd921420b7e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:21:45.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7351" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":228,"skipped":3586,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:21:45.228: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:21:45.246: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3530
I0315 10:21:45.256859      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3530, replica count: 1
I0315 10:21:46.307476      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 10:21:47.307706      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:21:47.414: INFO: Created: latency-svc-gd2pw
Mar 15 10:21:47.425: INFO: Got endpoints: latency-svc-gd2pw [17.58766ms]
Mar 15 10:21:47.438: INFO: Created: latency-svc-9jtv5
Mar 15 10:21:47.448: INFO: Got endpoints: latency-svc-9jtv5 [23.10898ms]
Mar 15 10:21:47.454: INFO: Created: latency-svc-t2twq
Mar 15 10:21:47.463: INFO: Got endpoints: latency-svc-t2twq [36.751296ms]
Mar 15 10:21:47.467: INFO: Created: latency-svc-67ddx
Mar 15 10:21:47.478: INFO: Got endpoints: latency-svc-67ddx [49.903882ms]
Mar 15 10:21:47.486: INFO: Created: latency-svc-vgtm4
Mar 15 10:21:47.497: INFO: Got endpoints: latency-svc-vgtm4 [70.835092ms]
Mar 15 10:21:47.509: INFO: Created: latency-svc-dkrbm
Mar 15 10:21:47.519: INFO: Got endpoints: latency-svc-dkrbm [92.315837ms]
Mar 15 10:21:47.524: INFO: Created: latency-svc-28ngj
Mar 15 10:21:47.528: INFO: Got endpoints: latency-svc-28ngj [101.538121ms]
Mar 15 10:21:47.542: INFO: Created: latency-svc-mx8h6
Mar 15 10:21:47.552: INFO: Got endpoints: latency-svc-mx8h6 [125.130186ms]
Mar 15 10:21:47.555: INFO: Created: latency-svc-5g66h
Mar 15 10:21:47.564: INFO: Got endpoints: latency-svc-5g66h [136.762648ms]
Mar 15 10:21:47.576: INFO: Created: latency-svc-hj69w
Mar 15 10:21:47.589: INFO: Got endpoints: latency-svc-hj69w [161.421695ms]
Mar 15 10:21:47.595: INFO: Created: latency-svc-hvhpz
Mar 15 10:21:47.606: INFO: Got endpoints: latency-svc-hvhpz [179.913955ms]
Mar 15 10:21:47.635: INFO: Created: latency-svc-4gxj9
Mar 15 10:21:47.646: INFO: Created: latency-svc-7w8jj
Mar 15 10:21:47.650: INFO: Got endpoints: latency-svc-4gxj9 [222.391033ms]
Mar 15 10:21:47.658: INFO: Got endpoints: latency-svc-7w8jj [229.452909ms]
Mar 15 10:21:47.672: INFO: Created: latency-svc-hd48q
Mar 15 10:21:47.676: INFO: Got endpoints: latency-svc-hd48q [248.702647ms]
Mar 15 10:21:47.682: INFO: Created: latency-svc-2mcm7
Mar 15 10:21:47.687: INFO: Got endpoints: latency-svc-2mcm7 [258.138094ms]
Mar 15 10:21:47.690: INFO: Created: latency-svc-tqvkv
Mar 15 10:21:47.702: INFO: Got endpoints: latency-svc-tqvkv [274.006367ms]
Mar 15 10:21:47.703: INFO: Created: latency-svc-wxqls
Mar 15 10:21:47.709: INFO: Got endpoints: latency-svc-wxqls [260.731474ms]
Mar 15 10:21:47.712: INFO: Created: latency-svc-xkhxw
Mar 15 10:21:47.721: INFO: Got endpoints: latency-svc-xkhxw [258.657323ms]
Mar 15 10:21:47.735: INFO: Created: latency-svc-wmst6
Mar 15 10:21:47.737: INFO: Got endpoints: latency-svc-wmst6 [259.057017ms]
Mar 15 10:21:47.741: INFO: Created: latency-svc-qk626
Mar 15 10:21:47.749: INFO: Got endpoints: latency-svc-qk626 [252.435007ms]
Mar 15 10:21:47.758: INFO: Created: latency-svc-w6fkj
Mar 15 10:21:47.769: INFO: Got endpoints: latency-svc-w6fkj [250.53632ms]
Mar 15 10:21:47.795: INFO: Created: latency-svc-xs492
Mar 15 10:21:47.801: INFO: Got endpoints: latency-svc-xs492 [273.320826ms]
Mar 15 10:21:47.817: INFO: Created: latency-svc-nn6nl
Mar 15 10:21:47.825: INFO: Got endpoints: latency-svc-nn6nl [272.662076ms]
Mar 15 10:21:47.829: INFO: Created: latency-svc-r9l5t
Mar 15 10:21:47.845: INFO: Got endpoints: latency-svc-r9l5t [280.886457ms]
Mar 15 10:21:47.848: INFO: Created: latency-svc-ctb58
Mar 15 10:21:47.853: INFO: Got endpoints: latency-svc-ctb58 [264.001193ms]
Mar 15 10:21:47.858: INFO: Created: latency-svc-r9wzg
Mar 15 10:21:47.866: INFO: Got endpoints: latency-svc-r9wzg [260.681145ms]
Mar 15 10:21:47.870: INFO: Created: latency-svc-9jxjk
Mar 15 10:21:47.878: INFO: Got endpoints: latency-svc-9jxjk [227.967639ms]
Mar 15 10:21:47.881: INFO: Created: latency-svc-c4qfz
Mar 15 10:21:47.893: INFO: Got endpoints: latency-svc-c4qfz [235.022802ms]
Mar 15 10:21:47.905: INFO: Created: latency-svc-zhzcn
Mar 15 10:21:47.912: INFO: Got endpoints: latency-svc-zhzcn [235.746557ms]
Mar 15 10:21:47.919: INFO: Created: latency-svc-mtnmf
Mar 15 10:21:47.926: INFO: Got endpoints: latency-svc-mtnmf [238.968396ms]
Mar 15 10:21:47.940: INFO: Created: latency-svc-2hgcq
Mar 15 10:21:47.967: INFO: Got endpoints: latency-svc-2hgcq [264.512315ms]
Mar 15 10:21:47.968: INFO: Created: latency-svc-zzgh7
Mar 15 10:21:47.976: INFO: Got endpoints: latency-svc-zzgh7 [267.104142ms]
Mar 15 10:21:47.979: INFO: Created: latency-svc-s7nr6
Mar 15 10:21:47.986: INFO: Got endpoints: latency-svc-s7nr6 [265.083307ms]
Mar 15 10:21:47.996: INFO: Created: latency-svc-bzvlp
Mar 15 10:21:48.006: INFO: Got endpoints: latency-svc-bzvlp [268.585149ms]
Mar 15 10:21:48.013: INFO: Created: latency-svc-z7dkk
Mar 15 10:21:48.022: INFO: Got endpoints: latency-svc-z7dkk [272.28596ms]
Mar 15 10:21:48.031: INFO: Created: latency-svc-7nvz9
Mar 15 10:21:48.042: INFO: Got endpoints: latency-svc-7nvz9 [272.944086ms]
Mar 15 10:21:48.045: INFO: Created: latency-svc-bk6sr
Mar 15 10:21:48.053: INFO: Got endpoints: latency-svc-bk6sr [251.503397ms]
Mar 15 10:21:48.064: INFO: Created: latency-svc-z6gvd
Mar 15 10:21:48.085: INFO: Got endpoints: latency-svc-z6gvd [259.923227ms]
Mar 15 10:21:48.091: INFO: Created: latency-svc-plx8q
Mar 15 10:21:48.100: INFO: Got endpoints: latency-svc-plx8q [46.905827ms]
Mar 15 10:21:48.112: INFO: Created: latency-svc-96jbp
Mar 15 10:21:48.112: INFO: Created: latency-svc-hk9ll
Mar 15 10:21:48.120: INFO: Got endpoints: latency-svc-hk9ll [267.648355ms]
Mar 15 10:21:48.121: INFO: Got endpoints: latency-svc-96jbp [276.045356ms]
Mar 15 10:21:48.126: INFO: Created: latency-svc-dqbfh
Mar 15 10:21:48.145: INFO: Got endpoints: latency-svc-dqbfh [278.444501ms]
Mar 15 10:21:48.146: INFO: Created: latency-svc-gkks4
Mar 15 10:21:48.156: INFO: Got endpoints: latency-svc-gkks4 [277.893776ms]
Mar 15 10:21:48.160: INFO: Created: latency-svc-j45x2
Mar 15 10:21:48.170: INFO: Got endpoints: latency-svc-j45x2 [277.106694ms]
Mar 15 10:21:48.172: INFO: Created: latency-svc-h7vs7
Mar 15 10:21:48.183: INFO: Got endpoints: latency-svc-h7vs7 [270.638098ms]
Mar 15 10:21:48.204: INFO: Created: latency-svc-57vbr
Mar 15 10:21:48.207: INFO: Created: latency-svc-lsrjd
Mar 15 10:21:48.222: INFO: Created: latency-svc-86pb9
Mar 15 10:21:48.222: INFO: Got endpoints: latency-svc-57vbr [296.289701ms]
Mar 15 10:21:48.241: INFO: Created: latency-svc-lrhvs
Mar 15 10:21:48.251: INFO: Created: latency-svc-cpbj4
Mar 15 10:21:48.263: INFO: Created: latency-svc-mzql7
Mar 15 10:21:48.273: INFO: Got endpoints: latency-svc-lsrjd [305.910074ms]
Mar 15 10:21:48.277: INFO: Created: latency-svc-qw7h6
Mar 15 10:21:48.286: INFO: Created: latency-svc-9wfk9
Mar 15 10:21:48.296: INFO: Created: latency-svc-qdf76
Mar 15 10:21:48.307: INFO: Created: latency-svc-d2rtg
Mar 15 10:21:48.319: INFO: Created: latency-svc-6fsmc
Mar 15 10:21:48.328: INFO: Got endpoints: latency-svc-86pb9 [351.302395ms]
Mar 15 10:21:48.340: INFO: Created: latency-svc-8f55d
Mar 15 10:21:48.349: INFO: Created: latency-svc-6dk5p
Mar 15 10:21:48.361: INFO: Created: latency-svc-ggctf
Mar 15 10:21:48.372: INFO: Created: latency-svc-mccvm
Mar 15 10:21:48.374: INFO: Got endpoints: latency-svc-lrhvs [387.17586ms]
Mar 15 10:21:48.391: INFO: Created: latency-svc-8kvcb
Mar 15 10:21:48.401: INFO: Created: latency-svc-hjsm9
Mar 15 10:21:48.413: INFO: Created: latency-svc-w7tcb
Mar 15 10:21:48.422: INFO: Got endpoints: latency-svc-cpbj4 [416.266624ms]
Mar 15 10:21:48.426: INFO: Created: latency-svc-9dlpv
Mar 15 10:21:48.443: INFO: Created: latency-svc-5qxp5
Mar 15 10:21:48.471: INFO: Got endpoints: latency-svc-mzql7 [448.803929ms]
Mar 15 10:21:48.476: INFO: Created: latency-svc-rsw5d
Mar 15 10:21:48.521: INFO: Got endpoints: latency-svc-qw7h6 [478.379083ms]
Mar 15 10:21:48.536: INFO: Created: latency-svc-qztdt
Mar 15 10:21:48.570: INFO: Got endpoints: latency-svc-9wfk9 [485.07749ms]
Mar 15 10:21:48.583: INFO: Created: latency-svc-lzf67
Mar 15 10:21:48.622: INFO: Got endpoints: latency-svc-qdf76 [521.731431ms]
Mar 15 10:21:48.641: INFO: Created: latency-svc-qxr29
Mar 15 10:21:48.670: INFO: Got endpoints: latency-svc-d2rtg [549.362556ms]
Mar 15 10:21:48.681: INFO: Created: latency-svc-5vf84
Mar 15 10:21:48.722: INFO: Got endpoints: latency-svc-6fsmc [600.824054ms]
Mar 15 10:21:48.735: INFO: Created: latency-svc-h6pjs
Mar 15 10:21:48.770: INFO: Got endpoints: latency-svc-8f55d [625.097151ms]
Mar 15 10:21:48.785: INFO: Created: latency-svc-h8ln8
Mar 15 10:21:48.819: INFO: Got endpoints: latency-svc-6dk5p [663.752378ms]
Mar 15 10:21:48.826: INFO: Created: latency-svc-4q8jp
Mar 15 10:21:48.869: INFO: Got endpoints: latency-svc-ggctf [698.814193ms]
Mar 15 10:21:48.878: INFO: Created: latency-svc-vt5pz
Mar 15 10:21:48.932: INFO: Got endpoints: latency-svc-mccvm [749.313434ms]
Mar 15 10:21:48.940: INFO: Created: latency-svc-mhjxz
Mar 15 10:21:48.977: INFO: Got endpoints: latency-svc-8kvcb [755.304317ms]
Mar 15 10:21:48.985: INFO: Created: latency-svc-r7htx
Mar 15 10:21:49.024: INFO: Got endpoints: latency-svc-hjsm9 [750.633317ms]
Mar 15 10:21:49.046: INFO: Created: latency-svc-w6ptg
Mar 15 10:21:49.071: INFO: Got endpoints: latency-svc-w7tcb [743.547243ms]
Mar 15 10:21:49.094: INFO: Created: latency-svc-xzv5d
Mar 15 10:21:49.119: INFO: Got endpoints: latency-svc-9dlpv [745.541067ms]
Mar 15 10:21:49.125: INFO: Created: latency-svc-kd8bl
Mar 15 10:21:49.172: INFO: Got endpoints: latency-svc-5qxp5 [750.19849ms]
Mar 15 10:21:49.184: INFO: Created: latency-svc-2l62p
Mar 15 10:21:49.220: INFO: Got endpoints: latency-svc-rsw5d [748.958976ms]
Mar 15 10:21:49.233: INFO: Created: latency-svc-h6w2c
Mar 15 10:21:49.272: INFO: Got endpoints: latency-svc-qztdt [751.124305ms]
Mar 15 10:21:49.283: INFO: Created: latency-svc-4j76g
Mar 15 10:21:49.319: INFO: Got endpoints: latency-svc-lzf67 [749.006434ms]
Mar 15 10:21:49.334: INFO: Created: latency-svc-ltdgr
Mar 15 10:21:49.372: INFO: Got endpoints: latency-svc-qxr29 [750.098094ms]
Mar 15 10:21:49.383: INFO: Created: latency-svc-rxln2
Mar 15 10:21:49.421: INFO: Got endpoints: latency-svc-5vf84 [751.235459ms]
Mar 15 10:21:49.432: INFO: Created: latency-svc-ghqn4
Mar 15 10:21:49.470: INFO: Got endpoints: latency-svc-h6pjs [747.915077ms]
Mar 15 10:21:49.485: INFO: Created: latency-svc-xfzfm
Mar 15 10:21:49.522: INFO: Got endpoints: latency-svc-h8ln8 [751.678894ms]
Mar 15 10:21:49.527: INFO: Created: latency-svc-44g9b
Mar 15 10:21:49.570: INFO: Got endpoints: latency-svc-4q8jp [750.278837ms]
Mar 15 10:21:49.582: INFO: Created: latency-svc-nvdc8
Mar 15 10:21:49.620: INFO: Got endpoints: latency-svc-vt5pz [751.686885ms]
Mar 15 10:21:49.637: INFO: Created: latency-svc-lm6rh
Mar 15 10:21:49.673: INFO: Got endpoints: latency-svc-mhjxz [740.586447ms]
Mar 15 10:21:49.684: INFO: Created: latency-svc-9bbng
Mar 15 10:21:49.720: INFO: Got endpoints: latency-svc-r7htx [743.032133ms]
Mar 15 10:21:49.746: INFO: Created: latency-svc-dqgdl
Mar 15 10:21:49.785: INFO: Got endpoints: latency-svc-w6ptg [760.93066ms]
Mar 15 10:21:49.795: INFO: Created: latency-svc-jp8ff
Mar 15 10:21:49.820: INFO: Got endpoints: latency-svc-xzv5d [748.844534ms]
Mar 15 10:21:49.844: INFO: Created: latency-svc-nq4zs
Mar 15 10:21:49.874: INFO: Got endpoints: latency-svc-kd8bl [754.482708ms]
Mar 15 10:21:49.887: INFO: Created: latency-svc-q7rbc
Mar 15 10:21:49.919: INFO: Got endpoints: latency-svc-2l62p [746.556458ms]
Mar 15 10:21:49.934: INFO: Created: latency-svc-7jnfr
Mar 15 10:21:49.970: INFO: Got endpoints: latency-svc-h6w2c [750.222458ms]
Mar 15 10:21:49.994: INFO: Created: latency-svc-6l78g
Mar 15 10:21:50.020: INFO: Got endpoints: latency-svc-4j76g [747.909735ms]
Mar 15 10:21:50.028: INFO: Created: latency-svc-mgmhx
Mar 15 10:21:50.070: INFO: Got endpoints: latency-svc-ltdgr [751.329344ms]
Mar 15 10:21:50.083: INFO: Created: latency-svc-xbsmv
Mar 15 10:21:50.121: INFO: Got endpoints: latency-svc-rxln2 [748.603393ms]
Mar 15 10:21:50.139: INFO: Created: latency-svc-49sbj
Mar 15 10:21:50.171: INFO: Got endpoints: latency-svc-ghqn4 [749.934374ms]
Mar 15 10:21:50.179: INFO: Created: latency-svc-hshsz
Mar 15 10:21:50.224: INFO: Got endpoints: latency-svc-xfzfm [754.861131ms]
Mar 15 10:21:50.259: INFO: Created: latency-svc-w5szf
Mar 15 10:21:50.270: INFO: Got endpoints: latency-svc-44g9b [748.638559ms]
Mar 15 10:21:50.287: INFO: Created: latency-svc-25fgb
Mar 15 10:21:50.336: INFO: Got endpoints: latency-svc-nvdc8 [766.175272ms]
Mar 15 10:21:50.349: INFO: Created: latency-svc-97zqw
Mar 15 10:21:50.370: INFO: Got endpoints: latency-svc-lm6rh [749.191739ms]
Mar 15 10:21:50.381: INFO: Created: latency-svc-6jmjp
Mar 15 10:21:50.436: INFO: Got endpoints: latency-svc-9bbng [763.232428ms]
Mar 15 10:21:50.447: INFO: Created: latency-svc-wdw8j
Mar 15 10:21:50.474: INFO: Got endpoints: latency-svc-dqgdl [753.28359ms]
Mar 15 10:21:50.495: INFO: Created: latency-svc-n8qf7
Mar 15 10:21:50.521: INFO: Got endpoints: latency-svc-jp8ff [736.155888ms]
Mar 15 10:21:50.535: INFO: Created: latency-svc-d4lkf
Mar 15 10:21:50.580: INFO: Got endpoints: latency-svc-nq4zs [759.523643ms]
Mar 15 10:21:50.593: INFO: Created: latency-svc-bhrwn
Mar 15 10:21:50.622: INFO: Got endpoints: latency-svc-q7rbc [747.798912ms]
Mar 15 10:21:50.631: INFO: Created: latency-svc-xzsxl
Mar 15 10:21:50.670: INFO: Got endpoints: latency-svc-7jnfr [750.733408ms]
Mar 15 10:21:50.679: INFO: Created: latency-svc-9cd98
Mar 15 10:21:50.722: INFO: Got endpoints: latency-svc-6l78g [752.239813ms]
Mar 15 10:21:50.737: INFO: Created: latency-svc-2zh2j
Mar 15 10:21:50.819: INFO: Got endpoints: latency-svc-mgmhx [799.412199ms]
Mar 15 10:21:50.836: INFO: Got endpoints: latency-svc-xbsmv [765.36947ms]
Mar 15 10:21:50.837: INFO: Created: latency-svc-87nth
Mar 15 10:21:50.849: INFO: Created: latency-svc-kwddk
Mar 15 10:21:50.870: INFO: Got endpoints: latency-svc-49sbj [749.068632ms]
Mar 15 10:21:50.887: INFO: Created: latency-svc-726k8
Mar 15 10:21:50.920: INFO: Got endpoints: latency-svc-hshsz [748.400874ms]
Mar 15 10:21:50.935: INFO: Created: latency-svc-fdncg
Mar 15 10:21:50.968: INFO: Got endpoints: latency-svc-w5szf [743.828035ms]
Mar 15 10:21:50.975: INFO: Created: latency-svc-n8j2d
Mar 15 10:21:51.021: INFO: Got endpoints: latency-svc-25fgb [750.40971ms]
Mar 15 10:21:51.033: INFO: Created: latency-svc-9fqgp
Mar 15 10:21:51.070: INFO: Got endpoints: latency-svc-97zqw [733.50246ms]
Mar 15 10:21:51.087: INFO: Created: latency-svc-x7qjx
Mar 15 10:21:51.119: INFO: Got endpoints: latency-svc-6jmjp [748.857018ms]
Mar 15 10:21:51.136: INFO: Created: latency-svc-7gmxd
Mar 15 10:21:51.228: INFO: Got endpoints: latency-svc-wdw8j [792.204187ms]
Mar 15 10:21:51.241: INFO: Created: latency-svc-c6qrj
Mar 15 10:21:51.270: INFO: Got endpoints: latency-svc-n8qf7 [795.620533ms]
Mar 15 10:21:51.282: INFO: Created: latency-svc-qfqpn
Mar 15 10:21:51.321: INFO: Got endpoints: latency-svc-d4lkf [799.65516ms]
Mar 15 10:21:51.329: INFO: Created: latency-svc-fk772
Mar 15 10:21:51.371: INFO: Got endpoints: latency-svc-bhrwn [790.525118ms]
Mar 15 10:21:51.385: INFO: Created: latency-svc-rztfb
Mar 15 10:21:51.420: INFO: Got endpoints: latency-svc-xzsxl [797.89986ms]
Mar 15 10:21:51.435: INFO: Created: latency-svc-xr22s
Mar 15 10:21:51.469: INFO: Got endpoints: latency-svc-9cd98 [799.275709ms]
Mar 15 10:21:51.484: INFO: Created: latency-svc-p6cdc
Mar 15 10:21:51.519: INFO: Got endpoints: latency-svc-2zh2j [796.75784ms]
Mar 15 10:21:51.526: INFO: Created: latency-svc-4lf7k
Mar 15 10:21:51.572: INFO: Got endpoints: latency-svc-87nth [752.703759ms]
Mar 15 10:21:51.583: INFO: Created: latency-svc-4r52w
Mar 15 10:21:51.622: INFO: Got endpoints: latency-svc-kwddk [786.192016ms]
Mar 15 10:21:51.634: INFO: Created: latency-svc-gtj7z
Mar 15 10:21:51.670: INFO: Got endpoints: latency-svc-726k8 [800.008836ms]
Mar 15 10:21:51.695: INFO: Created: latency-svc-jr9rb
Mar 15 10:21:51.720: INFO: Got endpoints: latency-svc-fdncg [800.279894ms]
Mar 15 10:21:51.734: INFO: Created: latency-svc-gk622
Mar 15 10:21:51.770: INFO: Got endpoints: latency-svc-n8j2d [801.872669ms]
Mar 15 10:21:51.790: INFO: Created: latency-svc-t28m6
Mar 15 10:21:51.819: INFO: Got endpoints: latency-svc-9fqgp [798.07387ms]
Mar 15 10:21:51.834: INFO: Created: latency-svc-nbfr2
Mar 15 10:21:51.869: INFO: Got endpoints: latency-svc-x7qjx [798.741078ms]
Mar 15 10:21:51.883: INFO: Created: latency-svc-86nnh
Mar 15 10:21:51.919: INFO: Got endpoints: latency-svc-7gmxd [800.857626ms]
Mar 15 10:21:51.935: INFO: Created: latency-svc-bwh46
Mar 15 10:21:51.969: INFO: Got endpoints: latency-svc-c6qrj [740.62054ms]
Mar 15 10:21:51.975: INFO: Created: latency-svc-82wwc
Mar 15 10:21:52.019: INFO: Got endpoints: latency-svc-qfqpn [749.54938ms]
Mar 15 10:21:52.034: INFO: Created: latency-svc-4jr6x
Mar 15 10:21:52.070: INFO: Got endpoints: latency-svc-fk772 [749.016108ms]
Mar 15 10:21:52.084: INFO: Created: latency-svc-6vl6d
Mar 15 10:21:52.120: INFO: Got endpoints: latency-svc-rztfb [748.862794ms]
Mar 15 10:21:52.130: INFO: Created: latency-svc-tb4pm
Mar 15 10:21:52.171: INFO: Got endpoints: latency-svc-xr22s [751.401371ms]
Mar 15 10:21:52.183: INFO: Created: latency-svc-7bwmw
Mar 15 10:21:52.221: INFO: Got endpoints: latency-svc-p6cdc [752.088187ms]
Mar 15 10:21:52.234: INFO: Created: latency-svc-jzzw6
Mar 15 10:21:52.272: INFO: Got endpoints: latency-svc-4lf7k [753.029662ms]
Mar 15 10:21:52.287: INFO: Created: latency-svc-jmf5f
Mar 15 10:21:52.319: INFO: Got endpoints: latency-svc-4r52w [746.92932ms]
Mar 15 10:21:52.333: INFO: Created: latency-svc-stczn
Mar 15 10:21:52.373: INFO: Got endpoints: latency-svc-gtj7z [750.615405ms]
Mar 15 10:21:52.390: INFO: Created: latency-svc-mw7r2
Mar 15 10:21:52.421: INFO: Got endpoints: latency-svc-jr9rb [751.224085ms]
Mar 15 10:21:52.434: INFO: Created: latency-svc-7v9v6
Mar 15 10:21:52.469: INFO: Got endpoints: latency-svc-gk622 [748.754172ms]
Mar 15 10:21:52.485: INFO: Created: latency-svc-kvsbd
Mar 15 10:21:52.519: INFO: Got endpoints: latency-svc-t28m6 [748.708561ms]
Mar 15 10:21:52.536: INFO: Created: latency-svc-sstlq
Mar 15 10:21:52.572: INFO: Got endpoints: latency-svc-nbfr2 [753.368675ms]
Mar 15 10:21:52.580: INFO: Created: latency-svc-rc8k6
Mar 15 10:21:52.621: INFO: Got endpoints: latency-svc-86nnh [752.594543ms]
Mar 15 10:21:52.634: INFO: Created: latency-svc-p647j
Mar 15 10:21:52.669: INFO: Got endpoints: latency-svc-bwh46 [749.201879ms]
Mar 15 10:21:52.683: INFO: Created: latency-svc-tbmln
Mar 15 10:21:52.718: INFO: Got endpoints: latency-svc-82wwc [749.209234ms]
Mar 15 10:21:52.749: INFO: Created: latency-svc-pgtf9
Mar 15 10:21:52.771: INFO: Got endpoints: latency-svc-4jr6x [751.667759ms]
Mar 15 10:21:52.786: INFO: Created: latency-svc-wbgkh
Mar 15 10:21:52.820: INFO: Got endpoints: latency-svc-6vl6d [750.148474ms]
Mar 15 10:21:52.834: INFO: Created: latency-svc-7shzz
Mar 15 10:21:52.869: INFO: Got endpoints: latency-svc-tb4pm [748.841383ms]
Mar 15 10:21:52.886: INFO: Created: latency-svc-6sd7k
Mar 15 10:21:52.922: INFO: Got endpoints: latency-svc-7bwmw [751.072648ms]
Mar 15 10:21:52.936: INFO: Created: latency-svc-pxdlh
Mar 15 10:21:52.970: INFO: Got endpoints: latency-svc-jzzw6 [749.191274ms]
Mar 15 10:21:52.984: INFO: Created: latency-svc-mkf57
Mar 15 10:21:53.019: INFO: Got endpoints: latency-svc-jmf5f [746.971035ms]
Mar 15 10:21:53.027: INFO: Created: latency-svc-zlzp7
Mar 15 10:21:53.072: INFO: Got endpoints: latency-svc-stczn [752.979096ms]
Mar 15 10:21:53.088: INFO: Created: latency-svc-gcwds
Mar 15 10:21:53.122: INFO: Got endpoints: latency-svc-mw7r2 [748.913288ms]
Mar 15 10:21:53.133: INFO: Created: latency-svc-zj7cw
Mar 15 10:21:53.171: INFO: Got endpoints: latency-svc-7v9v6 [749.314063ms]
Mar 15 10:21:53.186: INFO: Created: latency-svc-cxq4b
Mar 15 10:21:53.220: INFO: Got endpoints: latency-svc-kvsbd [750.749368ms]
Mar 15 10:21:53.226: INFO: Created: latency-svc-h6zwp
Mar 15 10:21:53.269: INFO: Got endpoints: latency-svc-sstlq [749.964133ms]
Mar 15 10:21:53.282: INFO: Created: latency-svc-4w4kp
Mar 15 10:21:53.320: INFO: Got endpoints: latency-svc-rc8k6 [747.869341ms]
Mar 15 10:21:53.338: INFO: Created: latency-svc-d22kp
Mar 15 10:21:53.369: INFO: Got endpoints: latency-svc-p647j [747.544892ms]
Mar 15 10:21:53.377: INFO: Created: latency-svc-7xbmm
Mar 15 10:21:53.422: INFO: Got endpoints: latency-svc-tbmln [753.066659ms]
Mar 15 10:21:53.432: INFO: Created: latency-svc-qvfvq
Mar 15 10:21:53.469: INFO: Got endpoints: latency-svc-pgtf9 [750.041187ms]
Mar 15 10:21:53.482: INFO: Created: latency-svc-vgsnt
Mar 15 10:21:53.523: INFO: Got endpoints: latency-svc-wbgkh [752.213774ms]
Mar 15 10:21:53.535: INFO: Created: latency-svc-mbtxg
Mar 15 10:21:53.570: INFO: Got endpoints: latency-svc-7shzz [749.584262ms]
Mar 15 10:21:53.585: INFO: Created: latency-svc-8wdk8
Mar 15 10:21:53.619: INFO: Got endpoints: latency-svc-6sd7k [750.681363ms]
Mar 15 10:21:53.629: INFO: Created: latency-svc-6lkc7
Mar 15 10:21:53.669: INFO: Got endpoints: latency-svc-pxdlh [746.972174ms]
Mar 15 10:21:53.684: INFO: Created: latency-svc-6tv8m
Mar 15 10:21:53.736: INFO: Got endpoints: latency-svc-mkf57 [765.429713ms]
Mar 15 10:21:53.767: INFO: Created: latency-svc-5tlrs
Mar 15 10:21:53.794: INFO: Got endpoints: latency-svc-zlzp7 [774.64265ms]
Mar 15 10:21:53.826: INFO: Created: latency-svc-8xt9s
Mar 15 10:21:53.827: INFO: Got endpoints: latency-svc-gcwds [754.590259ms]
Mar 15 10:21:53.847: INFO: Created: latency-svc-mntsr
Mar 15 10:21:53.885: INFO: Got endpoints: latency-svc-zj7cw [763.49222ms]
Mar 15 10:21:53.901: INFO: Created: latency-svc-d27ck
Mar 15 10:21:53.922: INFO: Got endpoints: latency-svc-cxq4b [751.514166ms]
Mar 15 10:21:53.931: INFO: Created: latency-svc-qs85j
Mar 15 10:21:53.990: INFO: Got endpoints: latency-svc-h6zwp [770.810149ms]
Mar 15 10:21:54.002: INFO: Created: latency-svc-zd6bk
Mar 15 10:21:54.027: INFO: Got endpoints: latency-svc-4w4kp [757.785848ms]
Mar 15 10:21:54.041: INFO: Created: latency-svc-g6959
Mar 15 10:21:54.144: INFO: Got endpoints: latency-svc-7xbmm [775.019917ms]
Mar 15 10:21:54.144: INFO: Got endpoints: latency-svc-d22kp [823.511774ms]
Mar 15 10:21:54.170: INFO: Created: latency-svc-rmlfk
Mar 15 10:21:54.177: INFO: Got endpoints: latency-svc-qvfvq [754.4286ms]
Mar 15 10:21:54.224: INFO: Created: latency-svc-l55x4
Mar 15 10:21:54.242: INFO: Created: latency-svc-rslk9
Mar 15 10:21:54.246: INFO: Got endpoints: latency-svc-vgsnt [776.981918ms]
Mar 15 10:21:54.273: INFO: Created: latency-svc-4v5kx
Mar 15 10:21:54.274: INFO: Got endpoints: latency-svc-mbtxg [750.487479ms]
Mar 15 10:21:54.293: INFO: Created: latency-svc-s49qj
Mar 15 10:21:54.321: INFO: Got endpoints: latency-svc-8wdk8 [751.652639ms]
Mar 15 10:21:54.331: INFO: Created: latency-svc-4kh22
Mar 15 10:21:54.386: INFO: Got endpoints: latency-svc-6lkc7 [766.263135ms]
Mar 15 10:21:54.408: INFO: Created: latency-svc-nwzdn
Mar 15 10:21:54.424: INFO: Got endpoints: latency-svc-6tv8m [753.966824ms]
Mar 15 10:21:54.444: INFO: Created: latency-svc-fxh5h
Mar 15 10:21:54.468: INFO: Got endpoints: latency-svc-5tlrs [732.256405ms]
Mar 15 10:21:54.483: INFO: Created: latency-svc-kzhxj
Mar 15 10:21:54.521: INFO: Got endpoints: latency-svc-8xt9s [726.811501ms]
Mar 15 10:21:54.529: INFO: Created: latency-svc-r86qs
Mar 15 10:21:54.570: INFO: Got endpoints: latency-svc-mntsr [743.063801ms]
Mar 15 10:21:54.581: INFO: Created: latency-svc-ntv6x
Mar 15 10:21:54.622: INFO: Got endpoints: latency-svc-d27ck [736.376298ms]
Mar 15 10:21:54.633: INFO: Created: latency-svc-5tfvl
Mar 15 10:21:54.669: INFO: Got endpoints: latency-svc-qs85j [746.556747ms]
Mar 15 10:21:54.685: INFO: Created: latency-svc-pqwwc
Mar 15 10:21:54.721: INFO: Got endpoints: latency-svc-zd6bk [730.434463ms]
Mar 15 10:21:54.734: INFO: Created: latency-svc-gth6f
Mar 15 10:21:54.770: INFO: Got endpoints: latency-svc-g6959 [743.100712ms]
Mar 15 10:21:54.778: INFO: Created: latency-svc-hf7ml
Mar 15 10:21:54.824: INFO: Got endpoints: latency-svc-rmlfk [679.532264ms]
Mar 15 10:21:54.835: INFO: Created: latency-svc-jrllr
Mar 15 10:21:54.873: INFO: Got endpoints: latency-svc-l55x4 [729.283564ms]
Mar 15 10:21:54.886: INFO: Created: latency-svc-pqqwg
Mar 15 10:21:54.920: INFO: Got endpoints: latency-svc-rslk9 [743.079065ms]
Mar 15 10:21:54.936: INFO: Created: latency-svc-gwkfh
Mar 15 10:21:54.973: INFO: Got endpoints: latency-svc-4v5kx [726.905192ms]
Mar 15 10:21:54.985: INFO: Created: latency-svc-d8rd9
Mar 15 10:21:55.021: INFO: Got endpoints: latency-svc-s49qj [747.159713ms]
Mar 15 10:21:55.033: INFO: Created: latency-svc-4f4p6
Mar 15 10:21:55.069: INFO: Got endpoints: latency-svc-4kh22 [747.300865ms]
Mar 15 10:21:55.083: INFO: Created: latency-svc-zmxsw
Mar 15 10:21:55.130: INFO: Got endpoints: latency-svc-nwzdn [744.77446ms]
Mar 15 10:21:55.145: INFO: Created: latency-svc-jm9d2
Mar 15 10:21:55.169: INFO: Got endpoints: latency-svc-fxh5h [745.334504ms]
Mar 15 10:21:55.193: INFO: Created: latency-svc-tcqgd
Mar 15 10:21:55.221: INFO: Got endpoints: latency-svc-kzhxj [752.959366ms]
Mar 15 10:21:55.264: INFO: Created: latency-svc-fkkzh
Mar 15 10:21:55.271: INFO: Got endpoints: latency-svc-r86qs [750.234745ms]
Mar 15 10:21:55.296: INFO: Created: latency-svc-bhcbk
Mar 15 10:21:55.320: INFO: Got endpoints: latency-svc-ntv6x [749.795828ms]
Mar 15 10:21:55.369: INFO: Got endpoints: latency-svc-5tfvl [747.754666ms]
Mar 15 10:21:55.420: INFO: Got endpoints: latency-svc-pqwwc [751.158818ms]
Mar 15 10:21:55.469: INFO: Got endpoints: latency-svc-gth6f [748.447595ms]
Mar 15 10:21:55.520: INFO: Got endpoints: latency-svc-hf7ml [749.365743ms]
Mar 15 10:21:55.571: INFO: Got endpoints: latency-svc-jrllr [747.598923ms]
Mar 15 10:21:55.620: INFO: Got endpoints: latency-svc-pqqwg [746.99358ms]
Mar 15 10:21:55.674: INFO: Got endpoints: latency-svc-gwkfh [754.379419ms]
Mar 15 10:21:55.724: INFO: Got endpoints: latency-svc-d8rd9 [751.439945ms]
Mar 15 10:21:55.773: INFO: Got endpoints: latency-svc-4f4p6 [751.308634ms]
Mar 15 10:21:55.823: INFO: Got endpoints: latency-svc-zmxsw [754.117746ms]
Mar 15 10:21:55.871: INFO: Got endpoints: latency-svc-jm9d2 [740.03186ms]
Mar 15 10:21:55.921: INFO: Got endpoints: latency-svc-tcqgd [751.516626ms]
Mar 15 10:21:55.970: INFO: Got endpoints: latency-svc-fkkzh [749.069897ms]
Mar 15 10:21:56.019: INFO: Got endpoints: latency-svc-bhcbk [748.013697ms]
Mar 15 10:21:56.020: INFO: Latencies: [23.10898ms 36.751296ms 46.905827ms 49.903882ms 70.835092ms 92.315837ms 101.538121ms 125.130186ms 136.762648ms 161.421695ms 179.913955ms 222.391033ms 227.967639ms 229.452909ms 235.022802ms 235.746557ms 238.968396ms 248.702647ms 250.53632ms 251.503397ms 252.435007ms 258.138094ms 258.657323ms 259.057017ms 259.923227ms 260.681145ms 260.731474ms 264.001193ms 264.512315ms 265.083307ms 267.104142ms 267.648355ms 268.585149ms 270.638098ms 272.28596ms 272.662076ms 272.944086ms 273.320826ms 274.006367ms 276.045356ms 277.106694ms 277.893776ms 278.444501ms 280.886457ms 296.289701ms 305.910074ms 351.302395ms 387.17586ms 416.266624ms 448.803929ms 478.379083ms 485.07749ms 521.731431ms 549.362556ms 600.824054ms 625.097151ms 663.752378ms 679.532264ms 698.814193ms 726.811501ms 726.905192ms 729.283564ms 730.434463ms 732.256405ms 733.50246ms 736.155888ms 736.376298ms 740.03186ms 740.586447ms 740.62054ms 743.032133ms 743.063801ms 743.079065ms 743.100712ms 743.547243ms 743.828035ms 744.77446ms 745.334504ms 745.541067ms 746.556458ms 746.556747ms 746.92932ms 746.971035ms 746.972174ms 746.99358ms 747.159713ms 747.300865ms 747.544892ms 747.598923ms 747.754666ms 747.798912ms 747.869341ms 747.909735ms 747.915077ms 748.013697ms 748.400874ms 748.447595ms 748.603393ms 748.638559ms 748.708561ms 748.754172ms 748.841383ms 748.844534ms 748.857018ms 748.862794ms 748.913288ms 748.958976ms 749.006434ms 749.016108ms 749.068632ms 749.069897ms 749.191274ms 749.191739ms 749.201879ms 749.209234ms 749.313434ms 749.314063ms 749.365743ms 749.54938ms 749.584262ms 749.795828ms 749.934374ms 749.964133ms 750.041187ms 750.098094ms 750.148474ms 750.19849ms 750.222458ms 750.234745ms 750.278837ms 750.40971ms 750.487479ms 750.615405ms 750.633317ms 750.681363ms 750.733408ms 750.749368ms 751.072648ms 751.124305ms 751.158818ms 751.224085ms 751.235459ms 751.308634ms 751.329344ms 751.401371ms 751.439945ms 751.514166ms 751.516626ms 751.652639ms 751.667759ms 751.678894ms 751.686885ms 752.088187ms 752.213774ms 752.239813ms 752.594543ms 752.703759ms 752.959366ms 752.979096ms 753.029662ms 753.066659ms 753.28359ms 753.368675ms 753.966824ms 754.117746ms 754.379419ms 754.4286ms 754.482708ms 754.590259ms 754.861131ms 755.304317ms 757.785848ms 759.523643ms 760.93066ms 763.232428ms 763.49222ms 765.36947ms 765.429713ms 766.175272ms 766.263135ms 770.810149ms 774.64265ms 775.019917ms 776.981918ms 786.192016ms 790.525118ms 792.204187ms 795.620533ms 796.75784ms 797.89986ms 798.07387ms 798.741078ms 799.275709ms 799.412199ms 799.65516ms 800.008836ms 800.279894ms 800.857626ms 801.872669ms 823.511774ms]
Mar 15 10:21:56.020: INFO: 50 %ile: 748.754172ms
Mar 15 10:21:56.020: INFO: 90 %ile: 770.810149ms
Mar 15 10:21:56.020: INFO: 99 %ile: 801.872669ms
Mar 15 10:21:56.020: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:21:56.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3530" for this suite.

• [SLOW TEST:10.801 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":229,"skipped":3596,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:21:56.029: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Mar 15 10:21:56.060: INFO: created test-event-1
Mar 15 10:21:56.073: INFO: created test-event-2
Mar 15 10:21:56.076: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 15 10:21:56.078: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 15 10:21:56.091: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:21:56.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9505" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":230,"skipped":3617,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:21:56.100: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:07.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2281" for this suite.

• [SLOW TEST:11.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":231,"skipped":3624,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:07.187: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 15 10:22:07.210: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 10:22:07.220: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 10:22:07.222: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-18.us-east-2.compute.internal before test
Mar 15 10:22:07.232: INFO: metrics-server-v0.3.6-58cf8c5dfb-dnzrv from kube-system started at 2021-03-12 15:30:23 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 10:22:07.232: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 15 10:22:07.232: INFO: dashboard-metrics-scraper-7b59f7d4df-xrqjg from kubernetes-dashboard started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 15 10:22:07.232: INFO: node-exporter-2k2xw from pf9-monitoring started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:07.232: INFO: prometheus-system-0 from pf9-monitoring started at 2021-03-12 15:32:05 +0000 UTC (3 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container prometheus ready: true, restart count 1
Mar 15 10:22:07.232: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 15 10:22:07.232: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 15 10:22:07.232: INFO: packageserver-67c5dfcffc-4vmch from pf9-olm started at 2021-03-14 07:25:39 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:22:07.232: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.232: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Mar 15 10:22:07.232: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:22:07.232: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-2-120.us-east-2.compute.internal before test
Mar 15 10:22:07.268: INFO: coredns-867bf6789f-kfcq4 from kube-system started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container coredns ready: true, restart count 0
Mar 15 10:22:07.268: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2021-03-14 07:25:32 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container alertmanager ready: true, restart count 0
Mar 15 10:22:07.268: INFO: 	Container config-reloader ready: true, restart count 0
Mar 15 10:22:07.268: INFO: grafana-9b947c8-gqfl8 from pf9-monitoring started at 2021-03-12 15:32:10 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container grafana ready: true, restart count 0
Mar 15 10:22:07.268: INFO: 	Container proxy ready: true, restart count 0
Mar 15 10:22:07.268: INFO: kube-state-metrics-6ff68b466d-2xpvx from pf9-monitoring started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 15 10:22:07.268: INFO: node-exporter-gs57d from pf9-monitoring started at 2021-03-12 15:30:01 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:07.268: INFO: catalog-operator-6656569cc-bsdrn from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 15 10:22:07.268: INFO: olm-operator-5c657955f6-jdk8b from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container olm-operator ready: true, restart count 0
Mar 15 10:22:07.268: INFO: packageserver-67c5dfcffc-vnl24 from pf9-olm started at 2021-03-14 07:25:32 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.268: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:22:07.268: INFO: platform9-operators-cxg4l from pf9-olm started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.269: INFO: 	Container registry-server ready: true, restart count 0
Mar 15 10:22:07.269: INFO: monhelper-5577f87674-m95zg from pf9-operators started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.269: INFO: 	Container monhelper ready: true, restart count 0
Mar 15 10:22:07.269: INFO: prometheus-operator-7b96488f76-st7bk from pf9-operators started at 2021-03-12 15:31:19 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.269: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 15 10:22:07.269: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.269: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Mar 15 10:22:07.269: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:22:07.269: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-3-172.us-east-2.compute.internal before test
Mar 15 10:22:07.292: INFO: node-exporter-tzdsq from pf9-monitoring started at 2021-03-12 15:30:02 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.292: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:07.292: INFO: sonobuoy from sonobuoy started at 2021-03-15 09:00:40 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.292: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 10:22:07.293: INFO: sonobuoy-e2e-job-be88518f7bd84c37 from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.293: INFO: 	Container e2e ready: true, restart count 0
Mar 15 10:22:07.293: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 10:22:07.293: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:07.293: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Mar 15 10:22:07.293: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:22:07.293: INFO: svc-latency-rc-x44jx from svc-latency-3530 started at 2021-03-15 10:21:45 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:07.293: INFO: 	Container svc-latency-rc ready: false, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-85f7ae2e-523e-462c-b6b5-0ebe61b73682 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-85f7ae2e-523e-462c-b6b5-0ebe61b73682 off the node ip-10-0-3-172.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-85f7ae2e-523e-462c-b6b5-0ebe61b73682
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:21.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3449" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:14.186 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":232,"skipped":3625,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:21.373: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Mar 15 10:22:21.403: INFO: Waiting up to 5m0s for pod "downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb" in namespace "downward-api-7249" to be "Succeeded or Failed"
Mar 15 10:22:21.406: INFO: Pod "downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06238ms
Mar 15 10:22:23.408: INFO: Pod "downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004592015s
Mar 15 10:22:25.411: INFO: Pod "downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007678685s
STEP: Saw pod success
Mar 15 10:22:25.411: INFO: Pod "downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb" satisfied condition "Succeeded or Failed"
Mar 15 10:22:25.413: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb container dapi-container: <nil>
STEP: delete the pod
Mar 15 10:22:25.449: INFO: Waiting for pod downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb to disappear
Mar 15 10:22:25.451: INFO: Pod downward-api-8ea1aab6-a9c6-46c9-ba07-40d76d8dc1fb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:25.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7249" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":233,"skipped":3644,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:25.457: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:42.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3791" for this suite.

• [SLOW TEST:17.049 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":234,"skipped":3661,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:42.507: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-cc752890-944c-4ee8-bb21-3c08bd69d620
STEP: Creating a pod to test consume secrets
Mar 15 10:22:42.586: INFO: Waiting up to 5m0s for pod "pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d" in namespace "secrets-8886" to be "Succeeded or Failed"
Mar 15 10:22:42.591: INFO: Pod "pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.656703ms
Mar 15 10:22:44.595: INFO: Pod "pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009053758s
Mar 15 10:22:46.597: INFO: Pod "pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011727256s
STEP: Saw pod success
Mar 15 10:22:46.598: INFO: Pod "pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d" satisfied condition "Succeeded or Failed"
Mar 15 10:22:46.599: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:22:46.625: INFO: Waiting for pod pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d to disappear
Mar 15 10:22:46.632: INFO: Pod pod-secrets-c8db0dd7-ce5c-4452-b4f4-d2c695c13d0d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:46.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8886" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":235,"skipped":3664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:46.639: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:22:48.692: INFO: Waiting up to 5m0s for pod "client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5" in namespace "pods-7496" to be "Succeeded or Failed"
Mar 15 10:22:48.701: INFO: Pod "client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.764112ms
Mar 15 10:22:50.704: INFO: Pod "client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011683648s
STEP: Saw pod success
Mar 15 10:22:50.704: INFO: Pod "client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5" satisfied condition "Succeeded or Failed"
Mar 15 10:22:50.706: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5 container env3cont: <nil>
STEP: delete the pod
Mar 15 10:22:50.717: INFO: Waiting for pod client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5 to disappear
Mar 15 10:22:50.725: INFO: Pod client-envvars-238c0613-b2ec-4131-afd9-5190d62471f5 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:22:50.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7496" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":3747,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:22:50.731: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 15 10:22:50.749: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 15 10:22:50.759: INFO: Waiting for terminating namespaces to be deleted...
Mar 15 10:22:50.761: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-1-18.us-east-2.compute.internal before test
Mar 15 10:22:50.770: INFO: metrics-server-v0.3.6-58cf8c5dfb-dnzrv from kube-system started at 2021-03-12 15:30:23 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container metrics-server ready: true, restart count 0
Mar 15 10:22:50.770: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 15 10:22:50.770: INFO: dashboard-metrics-scraper-7b59f7d4df-xrqjg from kubernetes-dashboard started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Mar 15 10:22:50.770: INFO: node-exporter-2k2xw from pf9-monitoring started at 2021-03-12 15:30:23 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:50.770: INFO: prometheus-system-0 from pf9-monitoring started at 2021-03-12 15:32:05 +0000 UTC (3 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container prometheus ready: true, restart count 1
Mar 15 10:22:50.770: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 15 10:22:50.770: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 15 10:22:50.770: INFO: packageserver-67c5dfcffc-4vmch from pf9-olm started at 2021-03-14 07:25:39 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:22:50.770: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-chj7f from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.770: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Mar 15 10:22:50.771: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:22:50.771: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-2-120.us-east-2.compute.internal before test
Mar 15 10:22:50.780: INFO: coredns-867bf6789f-kfcq4 from kube-system started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.780: INFO: 	Container coredns ready: true, restart count 0
Mar 15 10:22:50.780: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2021-03-14 07:25:32 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.780: INFO: 	Container alertmanager ready: true, restart count 0
Mar 15 10:22:50.780: INFO: 	Container config-reloader ready: true, restart count 0
Mar 15 10:22:50.781: INFO: grafana-9b947c8-gqfl8 from pf9-monitoring started at 2021-03-12 15:32:10 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container grafana ready: true, restart count 0
Mar 15 10:22:50.781: INFO: 	Container proxy ready: true, restart count 0
Mar 15 10:22:50.781: INFO: kube-state-metrics-6ff68b466d-2xpvx from pf9-monitoring started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 15 10:22:50.781: INFO: node-exporter-gs57d from pf9-monitoring started at 2021-03-12 15:30:01 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:50.781: INFO: catalog-operator-6656569cc-bsdrn from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 15 10:22:50.781: INFO: olm-operator-5c657955f6-jdk8b from pf9-olm started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container olm-operator ready: true, restart count 0
Mar 15 10:22:50.781: INFO: packageserver-67c5dfcffc-vnl24 from pf9-olm started at 2021-03-14 07:25:32 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container packageserver ready: true, restart count 0
Mar 15 10:22:50.781: INFO: platform9-operators-cxg4l from pf9-olm started at 2021-03-12 15:30:36 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.781: INFO: 	Container registry-server ready: true, restart count 0
Mar 15 10:22:50.781: INFO: monhelper-5577f87674-m95zg from pf9-operators started at 2021-03-14 07:25:24 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.782: INFO: 	Container monhelper ready: true, restart count 0
Mar 15 10:22:50.782: INFO: prometheus-operator-7b96488f76-st7bk from pf9-operators started at 2021-03-12 15:31:19 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.782: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 15 10:22:50.782: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7ch2k from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.782: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Mar 15 10:22:50.782: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 15 10:22:50.782: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-3-172.us-east-2.compute.internal before test
Mar 15 10:22:50.791: INFO: node-exporter-tzdsq from pf9-monitoring started at 2021-03-12 15:30:02 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.791: INFO: 	Container node-exporter ready: true, restart count 0
Mar 15 10:22:50.791: INFO: server-envvars-ab40b314-7d2b-4192-8c99-23cca2cf03c7 from pods-7496 started at 2021-03-15 10:22:46 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.791: INFO: 	Container srv ready: true, restart count 0
Mar 15 10:22:50.791: INFO: sonobuoy from sonobuoy started at 2021-03-15 09:00:40 +0000 UTC (1 container statuses recorded)
Mar 15 10:22:50.791: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 15 10:22:50.791: INFO: sonobuoy-e2e-job-be88518f7bd84c37 from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.791: INFO: 	Container e2e ready: true, restart count 0
Mar 15 10:22:50.791: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 15 10:22:50.791: INFO: sonobuoy-systemd-logs-daemon-set-69feeb62fdd54ead-7cdqd from sonobuoy started at 2021-03-15 09:00:41 +0000 UTC (2 container statuses recorded)
Mar 15 10:22:50.791: INFO: 	Container sonobuoy-worker ready: false, restart count 9
Mar 15 10:22:50.791: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-80c565d6-feac-4377-af86-4ed0a7ae0725 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-80c565d6-feac-4377-af86-4ed0a7ae0725 off the node ip-10-0-3-172.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-80c565d6-feac-4377-af86-4ed0a7ae0725
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:27:56.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-717" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.126 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":237,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:27:56.858: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 15 10:27:56.893: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 15 10:28:01.896: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:02.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9013" for this suite.

• [SLOW TEST:6.059 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":238,"skipped":3776,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:02.917: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:28:02.957: INFO: Creating ReplicaSet my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318
Mar 15 10:28:02.967: INFO: Pod name my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318: Found 0 pods out of 1
Mar 15 10:28:07.970: INFO: Pod name my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318: Found 1 pods out of 1
Mar 15 10:28:07.970: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318" is running
Mar 15 10:28:07.972: INFO: Pod "my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318-lw7lx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 10:28:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 10:28:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 10:28:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-03-15 10:28:02 +0000 UTC Reason: Message:}])
Mar 15 10:28:07.972: INFO: Trying to dial the pod
Mar 15 10:28:12.980: INFO: Controller my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318: Got expected result from replica 1 [my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318-lw7lx]: "my-hostname-basic-77bed04c-03b7-438c-a2a1-368458fce318-lw7lx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-160" for this suite.

• [SLOW TEST:10.069 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":239,"skipped":3779,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:12.987: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:28:13.006: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:19.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6704" for this suite.

• [SLOW TEST:6.561 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":240,"skipped":3797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-49bf7c0a-7303-4b20-8879-98073583782d
STEP: Creating secret with name s-test-opt-upd-081e0a49-ab6e-4836-8857-87c03e00027d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-49bf7c0a-7303-4b20-8879-98073583782d
STEP: Updating secret s-test-opt-upd-081e0a49-ab6e-4836-8857-87c03e00027d
STEP: Creating secret with name s-test-opt-create-50beef9c-856d-4adf-8ede-f776e7ffe473
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:24.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5495" for this suite.

• [SLOW TEST:5.349 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":241,"skipped":3845,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:24.897: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:28:25.059: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 15 10:28:25.065: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 15 10:28:30.068: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 15 10:28:30.068: INFO: Creating deployment "test-rolling-update-deployment"
Mar 15 10:28:30.072: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 15 10:28:30.078: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 15 10:28:32.082: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 15 10:28:32.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400910, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400910, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400910, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751400910, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 15 10:28:34.087: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Mar 15 10:28:34.092: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3246 /apis/apps/v1/namespaces/deployment-3246/deployments/test-rolling-update-deployment d45cca9b-fc52-43f7-bb4a-2a53bc13aaa8 679953 1 2021-03-15 10:28:30 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-03-15 10:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 10:28:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007973f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-03-15 10:28:30 +0000 UTC,LastTransitionTime:2021-03-15 10:28:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-03-15 10:28:32 +0000 UTC,LastTransitionTime:2021-03-15 10:28:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 15 10:28:34.094: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-3246 /apis/apps/v1/namespaces/deployment-3246/replicasets/test-rolling-update-deployment-c4cb8d6d9 df41b37f-ea4a-40fa-bb51-95708446a662 679940 1 2021-03-15 10:28:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d45cca9b-fc52-43f7-bb4a-2a53bc13aaa8 0xc003ca75e0 0xc003ca75e1}] []  [{kube-controller-manager Update apps/v1 2021-03-15 10:28:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d45cca9b-fc52-43f7-bb4a-2a53bc13aaa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ca7658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 10:28:34.094: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 15 10:28:34.094: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3246 /apis/apps/v1/namespaces/deployment-3246/replicasets/test-rolling-update-controller 1333478a-16ec-4511-9746-74944f99fd0f 679951 2 2021-03-15 10:28:25 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d45cca9b-fc52-43f7-bb4a-2a53bc13aaa8 0xc003ca74c7 0xc003ca74c8}] []  [{e2e.test Update apps/v1 2021-03-15 10:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 10:28:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d45cca9b-fc52-43f7-bb4a-2a53bc13aaa8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ca7578 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 10:28:34.096: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-ckprl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-ckprl test-rolling-update-deployment-c4cb8d6d9- deployment-3246 /api/v1/namespaces/deployment-3246/pods/test-rolling-update-deployment-c4cb8d6d9-ckprl 5c6b27e4-fb82-4d5c-a78e-cff65c298d4c 679939 0 2021-03-15 10:28:30 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 df41b37f-ea4a-40fa-bb51-95708446a662 0xc003ca7b30 0xc003ca7b31}] []  [{kube-controller-manager Update v1 2021-03-15 10:28:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df41b37f-ea4a-40fa-bb51-95708446a662\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:28:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gh48j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gh48j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gh48j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:28:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:28:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:28:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:28:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.183,StartTime:2021-03-15 10:28:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:28:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://b6f8ae71fa29a3f4bf815c68ff82c9afce67b146fc9cf875232cee36873b7d2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3246" for this suite.

• [SLOW TEST:9.206 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":242,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:34.106: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 15 10:28:35.173: INFO: starting watch
STEP: patching
STEP: updating
Mar 15 10:28:35.180: INFO: waiting for watch events with expected annotations
Mar 15 10:28:35.180: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:35.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4736" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":243,"skipped":3883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:35.246: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-f7f4752b-928c-4c7e-862c-4471b61c51ec
STEP: Creating a pod to test consume configMaps
Mar 15 10:28:35.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9" in namespace "configmap-9228" to be "Succeeded or Failed"
Mar 15 10:28:35.285: INFO: Pod "pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748257ms
Mar 15 10:28:37.288: INFO: Pod "pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003958403s
STEP: Saw pod success
Mar 15 10:28:37.288: INFO: Pod "pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9" satisfied condition "Succeeded or Failed"
Mar 15 10:28:37.289: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 10:28:37.301: INFO: Waiting for pod pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9 to disappear
Mar 15 10:28:37.309: INFO: Pod pod-configmaps-63a03e45-b944-4288-91a5-25711b7ecdc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:37.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9228" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":3932,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:37.316: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Mar 15 10:28:37.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-9428 create -f -'
Mar 15 10:28:38.581: INFO: stderr: ""
Mar 15 10:28:38.581: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 15 10:28:39.584: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:28:39.584: INFO: Found 0 / 1
Mar 15 10:28:40.584: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:28:40.585: INFO: Found 0 / 1
Mar 15 10:28:41.585: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:28:41.585: INFO: Found 1 / 1
Mar 15 10:28:41.585: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 15 10:28:41.587: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:28:41.587: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 15 10:28:41.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-9428 patch pod agnhost-primary-p6ndt -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 15 10:28:41.676: INFO: stderr: ""
Mar 15 10:28:41.676: INFO: stdout: "pod/agnhost-primary-p6ndt patched\n"
STEP: checking annotations
Mar 15 10:28:41.678: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 15 10:28:41.678: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:28:41.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9428" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":245,"skipped":3937,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:28:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8709.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8709.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8709.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 10:28:43.752: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.754: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.757: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.759: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.767: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.770: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.772: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.774: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:43.780: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:28:48.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.799: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.802: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.804: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.806: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:48.811: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:28:53.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.789: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.792: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.799: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.801: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.804: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.808: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:53.814: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:28:58.784: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.788: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.790: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.793: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.799: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.801: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.804: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.806: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:28:58.810: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:29:03.783: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.785: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.790: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.794: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.801: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.803: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.805: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.808: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:03.813: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:29:08.786: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.788: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.791: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.793: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.802: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.804: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.806: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.809: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:08.813: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local jessie_udp@dns-test-service-2.dns-8709.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:29:13.783: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:13.786: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:13.788: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:13.791: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local from pod dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df: the server could not find the requested resource (get pods dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df)
Mar 15 10:29:13.810: INFO: Lookups using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8709.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8709.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8709.svc.cluster.local]

Mar 15 10:29:18.810: INFO: DNS probes using dns-8709/dns-test-ac8a803d-2349-4de2-9462-8e7e6a50e3df succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:29:18.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8709" for this suite.

• [SLOW TEST:37.198 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":246,"skipped":3938,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:29:18.887: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:29:18.947: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5" in namespace "projected-5114" to be "Succeeded or Failed"
Mar 15 10:29:18.960: INFO: Pod "downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.657602ms
Mar 15 10:29:20.963: INFO: Pod "downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015732495s
Mar 15 10:29:22.966: INFO: Pod "downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018803738s
STEP: Saw pod success
Mar 15 10:29:22.966: INFO: Pod "downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5" satisfied condition "Succeeded or Failed"
Mar 15 10:29:22.969: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5 container client-container: <nil>
STEP: delete the pod
Mar 15 10:29:22.980: INFO: Waiting for pod downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5 to disappear
Mar 15 10:29:22.988: INFO: Pod downwardapi-volume-18b4c5dd-ae35-4fb7-9361-43cfb621dfb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:29:22.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5114" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":3945,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:29:22.995: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4911
Mar 15 10:29:25.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 15 10:29:25.299: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 15 10:29:25.299: INFO: stdout: "ipvs"
Mar 15 10:29:25.299: INFO: proxyMode: ipvs
Mar 15 10:29:25.302: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 10:29:25.305: INFO: Pod kube-proxy-mode-detector still exists
Mar 15 10:29:27.305: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 10:29:27.308: INFO: Pod kube-proxy-mode-detector still exists
Mar 15 10:29:29.305: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 15 10:29:29.307: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4911
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4911
I0315 10:29:29.333072      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4911, replica count: 3
I0315 10:29:32.383478      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:29:32.390: INFO: Creating new exec pod
Mar 15 10:29:35.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Mar 15 10:29:35.594: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 15 10:29:35.594: INFO: stdout: ""
Mar 15 10:29:35.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 10.21.178.165 80'
Mar 15 10:29:35.783: INFO: stderr: "+ nc -zv -t -w 2 10.21.178.165 80\nConnection to 10.21.178.165 80 port [tcp/http] succeeded!\n"
Mar 15 10:29:35.783: INFO: stdout: ""
Mar 15 10:29:35.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.120 31558'
Mar 15 10:29:35.961: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.120 31558\nConnection to 10.0.2.120 31558 port [tcp/31558] succeeded!\n"
Mar 15 10:29:35.961: INFO: stdout: ""
Mar 15 10:29:35.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.172 31558'
Mar 15 10:29:36.150: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.172 31558\nConnection to 10.0.3.172 31558 port [tcp/31558] succeeded!\n"
Mar 15 10:29:36.150: INFO: stdout: ""
Mar 15 10:29:36.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 13.59.66.200 31558'
Mar 15 10:29:36.333: INFO: stderr: "+ nc -zv -t -w 2 13.59.66.200 31558\nConnection to 13.59.66.200 31558 port [tcp/31558] succeeded!\n"
Mar 15 10:29:36.333: INFO: stdout: ""
Mar 15 10:29:36.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c nc -zv -t -w 2 18.189.17.197 31558'
Mar 15 10:29:36.522: INFO: stderr: "+ nc -zv -t -w 2 18.189.17.197 31558\nConnection to 18.189.17.197 31558 port [tcp/31558] succeeded!\n"
Mar 15 10:29:36.522: INFO: stdout: ""
Mar 15 10:29:36.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.1.18:31558/ ; done'
Mar 15 10:29:36.857: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n"
Mar 15 10:29:36.857: INFO: stdout: "\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6\naffinity-nodeport-timeout-8spc6"
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Received response from host: affinity-nodeport-timeout-8spc6
Mar 15 10:29:36.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.18:31558/'
Mar 15 10:29:37.053: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n"
Mar 15 10:29:37.053: INFO: stdout: "affinity-nodeport-timeout-8spc6"
Mar 15 10:31:42.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.18:31558/'
Mar 15 10:31:42.268: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n"
Mar 15 10:31:42.268: INFO: stdout: "affinity-nodeport-timeout-8spc6"
Mar 15 10:33:47.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.18:31558/'
Mar 15 10:33:47.500: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n"
Mar 15 10:33:47.500: INFO: stdout: "affinity-nodeport-timeout-8spc6"
Mar 15 10:35:52.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4911 exec execpod-affinity2gcvl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.1.18:31558/'
Mar 15 10:35:52.706: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.1.18:31558/\n"
Mar 15 10:35:52.706: INFO: stdout: "affinity-nodeport-timeout-l48qt"
Mar 15 10:35:52.706: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4911, will wait for the garbage collector to delete the pods
Mar 15 10:35:52.776: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 4.04267ms
Mar 15 10:35:55.876: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 3.10014715s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:36:06.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4911" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:403.524 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":248,"skipped":3958,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:36:06.519: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-0fa842d9-c375-4731-b607-ee60274d13c3
STEP: Creating a pod to test consume secrets
Mar 15 10:36:06.575: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6" in namespace "projected-4628" to be "Succeeded or Failed"
Mar 15 10:36:06.577: INFO: Pod "pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963118ms
Mar 15 10:36:08.580: INFO: Pod "pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004757552s
STEP: Saw pod success
Mar 15 10:36:08.580: INFO: Pod "pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6" satisfied condition "Succeeded or Failed"
Mar 15 10:36:08.582: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:36:08.605: INFO: Waiting for pod pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6 to disappear
Mar 15 10:36:08.612: INFO: Pod pod-projected-secrets-dbe5e06f-d7dd-40e9-a2ae-596a0b2291a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:36:08.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4628" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":249,"skipped":3974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:36:08.620: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:36:11.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7059" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":250,"skipped":4022,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:36:11.673: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 10:36:13.752: INFO: DNS probes using dns-test-143be8af-a4f9-45a2-a5d5-074a48ccdb94 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 10:36:15.823: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:15.826: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:15.826: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:20.830: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:20.832: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:20.833: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:25.830: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:25.832: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:25.832: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:30.830: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:30.833: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:30.833: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:35.830: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:35.832: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains '' instead of 'bar.example.com.'
Mar 15 10:36:35.832: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:40.830: INFO: File wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:40.833: INFO: File jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local from pod  dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 15 10:36:40.833: INFO: Lookups using dns-6324/dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 failed for: [wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local]

Mar 15 10:36:45.833: INFO: DNS probes using dns-test-6e977040-68cb-420d-b239-ce53b89f3b44 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6324.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6324.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 15 10:36:49.923: INFO: DNS probes using dns-test-34a4f3b0-1bc0-4a14-86d3-a88e614977ec succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:36:49.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6324" for this suite.

• [SLOW TEST:38.335 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":251,"skipped":4027,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:36:50.009: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:36:50.119: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 15 10:36:50.148: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:50.150: INFO: Number of nodes with available pods: 0
Mar 15 10:36:50.150: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:36:51.155: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:51.158: INFO: Number of nodes with available pods: 0
Mar 15 10:36:51.158: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:36:52.155: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:52.157: INFO: Number of nodes with available pods: 0
Mar 15 10:36:52.157: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:36:53.155: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:53.159: INFO: Number of nodes with available pods: 3
Mar 15 10:36:53.159: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 15 10:36:53.188: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:53.188: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:53.188: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:53.195: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:54.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:54.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:54.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:54.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:55.198: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:55.198: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:55.198: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:55.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:56.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:56.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:36:56.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:56.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:56.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:57.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:57.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:36:57.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:57.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:57.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:58.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:58.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:36:58.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:58.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:58.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:36:59.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:59.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:36:59.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:59.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:36:59.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:00.198: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:00.198: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:37:00.198: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:00.198: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:00.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:01.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:01.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:37:01.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:01.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:01.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:02.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:02.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:37:02.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:02.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:02.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:03.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:03.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:37:03.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:03.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:03.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:04.199: INFO: Wrong image for pod: daemon-set-bxfmq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:04.199: INFO: Pod daemon-set-bxfmq is not available
Mar 15 10:37:04.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:04.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:04.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:05.200: INFO: Pod daemon-set-42lgc is not available
Mar 15 10:37:05.200: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:05.200: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:05.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:06.199: INFO: Pod daemon-set-42lgc is not available
Mar 15 10:37:06.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:06.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:06.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:07.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:07.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:07.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:08.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:08.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:08.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:08.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:09.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:09.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:09.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:09.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:10.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:10.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:10.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:10.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:11.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:11.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:11.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:11.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:12.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:12.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:12.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:12.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:13.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:13.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:13.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:13.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:14.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:14.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:14.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:14.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:15.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:15.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:15.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:15.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:16.199: INFO: Wrong image for pod: daemon-set-tldfp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:16.199: INFO: Pod daemon-set-tldfp is not available
Mar 15 10:37:16.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:16.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:17.199: INFO: Pod daemon-set-7ph9l is not available
Mar 15 10:37:17.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:17.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:18.199: INFO: Pod daemon-set-7ph9l is not available
Mar 15 10:37:18.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:18.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:19.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:19.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:20.199: INFO: Wrong image for pod: daemon-set-wcrnj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Mar 15 10:37:20.199: INFO: Pod daemon-set-wcrnj is not available
Mar 15 10:37:20.203: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:21.199: INFO: Pod daemon-set-bmccf is not available
Mar 15 10:37:21.202: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 15 10:37:21.205: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:21.207: INFO: Number of nodes with available pods: 2
Mar 15 10:37:21.207: INFO: Node ip-10-0-3-172.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:37:22.213: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:22.215: INFO: Number of nodes with available pods: 2
Mar 15 10:37:22.215: INFO: Node ip-10-0-3-172.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:37:23.212: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:37:23.215: INFO: Number of nodes with available pods: 3
Mar 15 10:37:23.215: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2209, will wait for the garbage collector to delete the pods
Mar 15 10:37:23.280: INFO: Deleting DaemonSet.extensions daemon-set took: 5.060654ms
Mar 15 10:37:26.380: INFO: Terminating DaemonSet.extensions daemon-set pods took: 3.100215482s
Mar 15 10:37:34.783: INFO: Number of nodes with available pods: 0
Mar 15 10:37:34.783: INFO: Number of running nodes: 0, number of available pods: 0
Mar 15 10:37:34.784: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2209/daemonsets","resourceVersion":"682165"},"items":null}

Mar 15 10:37:34.786: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2209/pods","resourceVersion":"682165"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:37:34.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2209" for this suite.

• [SLOW TEST:44.793 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":252,"skipped":4031,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:37:34.803: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-tjvm
STEP: Creating a pod to test atomic-volume-subpath
Mar 15 10:37:34.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tjvm" in namespace "subpath-2106" to be "Succeeded or Failed"
Mar 15 10:37:34.852: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Pending", Reason="", readiness=false. Elapsed: 7.412153ms
Mar 15 10:37:36.855: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 2.009892142s
Mar 15 10:37:38.858: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 4.013121375s
Mar 15 10:37:40.861: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 6.016434327s
Mar 15 10:37:42.864: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 8.019275619s
Mar 15 10:37:44.867: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 10.022426169s
Mar 15 10:37:46.870: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 12.025433397s
Mar 15 10:37:48.873: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 14.028608607s
Mar 15 10:37:50.877: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 16.031876224s
Mar 15 10:37:52.880: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 18.034816222s
Mar 15 10:37:54.882: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 20.037432949s
Mar 15 10:37:56.885: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Running", Reason="", readiness=true. Elapsed: 22.040416331s
Mar 15 10:37:58.891: INFO: Pod "pod-subpath-test-configmap-tjvm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046266129s
STEP: Saw pod success
Mar 15 10:37:58.892: INFO: Pod "pod-subpath-test-configmap-tjvm" satisfied condition "Succeeded or Failed"
Mar 15 10:37:58.893: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-subpath-test-configmap-tjvm container test-container-subpath-configmap-tjvm: <nil>
STEP: delete the pod
Mar 15 10:37:58.923: INFO: Waiting for pod pod-subpath-test-configmap-tjvm to disappear
Mar 15 10:37:58.933: INFO: Pod pod-subpath-test-configmap-tjvm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tjvm
Mar 15 10:37:58.933: INFO: Deleting pod "pod-subpath-test-configmap-tjvm" in namespace "subpath-2106"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:37:58.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2106" for this suite.

• [SLOW TEST:24.141 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":253,"skipped":4042,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:37:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9320
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Mar 15 10:37:59.034: INFO: Found 0 stateful pods, waiting for 3
Mar 15 10:38:09.038: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:38:09.038: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:38:09.038: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 15 10:38:09.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9320 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 10:38:09.283: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 10:38:09.283: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 10:38:09.283: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 15 10:38:19.309: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 15 10:38:29.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9320 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 10:38:29.523: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 10:38:29.523: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 10:38:29.523: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 10:38:49.537: INFO: Waiting for StatefulSet statefulset-9320/ss2 to complete update
Mar 15 10:38:49.537: INFO: Waiting for Pod statefulset-9320/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar 15 10:38:59.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9320 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 15 10:39:00.650: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 15 10:39:00.650: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 15 10:39:00.650: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 15 10:39:10.676: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 15 10:39:20.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=statefulset-9320 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 15 10:39:20.888: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 15 10:39:20.888: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 15 10:39:20.888: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 15 10:39:30.903: INFO: Waiting for StatefulSet statefulset-9320/ss2 to complete update
Mar 15 10:39:30.903: INFO: Waiting for Pod statefulset-9320/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 15 10:39:30.903: INFO: Waiting for Pod statefulset-9320/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 15 10:39:30.903: INFO: Waiting for Pod statefulset-9320/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 15 10:39:40.908: INFO: Waiting for StatefulSet statefulset-9320/ss2 to complete update
Mar 15 10:39:40.908: INFO: Waiting for Pod statefulset-9320/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 15 10:39:40.908: INFO: Waiting for Pod statefulset-9320/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 15 10:39:50.908: INFO: Waiting for StatefulSet statefulset-9320/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Mar 15 10:40:00.908: INFO: Deleting all statefulset in ns statefulset-9320
Mar 15 10:40:00.910: INFO: Scaling statefulset ss2 to 0
Mar 15 10:40:20.922: INFO: Waiting for statefulset status.replicas updated to 0
Mar 15 10:40:20.924: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9320" for this suite.

• [SLOW TEST:142.012 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":254,"skipped":4050,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:20.957: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:40:20.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185" in namespace "projected-5759" to be "Succeeded or Failed"
Mar 15 10:40:20.999: INFO: Pod "downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257626ms
Mar 15 10:40:23.001: INFO: Pod "downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004904057s
STEP: Saw pod success
Mar 15 10:40:23.001: INFO: Pod "downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185" satisfied condition "Succeeded or Failed"
Mar 15 10:40:23.003: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185 container client-container: <nil>
STEP: delete the pod
Mar 15 10:40:23.032: INFO: Waiting for pod downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185 to disappear
Mar 15 10:40:23.038: INFO: Pod downwardapi-volume-1fa68e99-7859-4a14-bfc8-fc599609a185 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:23.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5759" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4065,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 15 10:40:23.076: INFO: Waiting up to 5m0s for pod "pod-484adb06-f740-4130-82bf-d5292c0100cc" in namespace "emptydir-5081" to be "Succeeded or Failed"
Mar 15 10:40:23.078: INFO: Pod "pod-484adb06-f740-4130-82bf-d5292c0100cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215092ms
Mar 15 10:40:25.081: INFO: Pod "pod-484adb06-f740-4130-82bf-d5292c0100cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004362225s
STEP: Saw pod success
Mar 15 10:40:25.081: INFO: Pod "pod-484adb06-f740-4130-82bf-d5292c0100cc" satisfied condition "Succeeded or Failed"
Mar 15 10:40:25.083: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-484adb06-f740-4130-82bf-d5292c0100cc container test-container: <nil>
STEP: delete the pod
Mar 15 10:40:25.104: INFO: Waiting for pod pod-484adb06-f740-4130-82bf-d5292c0100cc to disappear
Mar 15 10:40:25.110: INFO: Pod pod-484adb06-f740-4130-82bf-d5292c0100cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:25.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5081" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:25.118: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:40:25.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:40:27.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401625, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401625, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401625, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401625, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:40:30.576: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 15 10:40:34.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=webhook-1140 attach --namespace=webhook-1140 to-be-attached-pod -i -c=container1'
Mar 15 10:40:34.726: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:34.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1140" for this suite.
STEP: Destroying namespace "webhook-1140-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.728 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":257,"skipped":4090,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:34.846: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Mar 15 10:40:39.412: INFO: Successfully updated pod "annotationupdate4acc209e-3007-4281-b814-363b15e46807"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:41.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6046" for this suite.

• [SLOW TEST:6.585 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:41.432: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8778
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8778
I0315 10:40:41.491975      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8778, replica count: 2
Mar 15 10:40:44.542: INFO: Creating new exec pod
I0315 10:40:44.542541      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:40:47.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-8778 exec execpodjc2j4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 15 10:40:47.750: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 15 10:40:47.750: INFO: stdout: ""
Mar 15 10:40:47.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-8778 exec execpodjc2j4 -- /bin/sh -x -c nc -zv -t -w 2 10.21.75.98 80'
Mar 15 10:40:47.947: INFO: stderr: "+ nc -zv -t -w 2 10.21.75.98 80\nConnection to 10.21.75.98 80 port [tcp/http] succeeded!\n"
Mar 15 10:40:47.947: INFO: stdout: ""
Mar 15 10:40:47.947: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:47.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8778" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.549 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":259,"skipped":4163,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:47.981: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:40:48.010: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 15 10:40:52.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7983 --namespace=crd-publish-openapi-7983 create -f -'
Mar 15 10:40:53.646: INFO: stderr: ""
Mar 15 10:40:53.646: INFO: stdout: "e2e-test-crd-publish-openapi-7641-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 15 10:40:53.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7983 --namespace=crd-publish-openapi-7983 delete e2e-test-crd-publish-openapi-7641-crds test-cr'
Mar 15 10:40:53.729: INFO: stderr: ""
Mar 15 10:40:53.729: INFO: stdout: "e2e-test-crd-publish-openapi-7641-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 15 10:40:53.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7983 --namespace=crd-publish-openapi-7983 apply -f -'
Mar 15 10:40:53.942: INFO: stderr: ""
Mar 15 10:40:53.942: INFO: stdout: "e2e-test-crd-publish-openapi-7641-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 15 10:40:53.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7983 --namespace=crd-publish-openapi-7983 delete e2e-test-crd-publish-openapi-7641-crds test-cr'
Mar 15 10:40:54.029: INFO: stderr: ""
Mar 15 10:40:54.029: INFO: stdout: "e2e-test-crd-publish-openapi-7641-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 15 10:40:54.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-7983 explain e2e-test-crd-publish-openapi-7641-crds'
Mar 15 10:40:54.263: INFO: stderr: ""
Mar 15 10:40:54.263: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7641-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:40:58.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7983" for this suite.

• [SLOW TEST:10.541 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":260,"skipped":4182,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:40:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-51cf4d26-7b92-4d02-b31b-cf3f54be0a5f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-51cf4d26-7b92-4d02-b31b-cf3f54be0a5f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:02.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2616" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:02.594: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:02.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2292" for this suite.
STEP: Destroying namespace "nspatchtest-57d2178c-5128-4991-8ffc-cfb2dd2b7a41-5432" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":262,"skipped":4206,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4840
STEP: creating service affinity-clusterip-transition in namespace services-4840
STEP: creating replication controller affinity-clusterip-transition in namespace services-4840
I0315 10:41:02.740630      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4840, replica count: 3
I0315 10:41:05.791207      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:41:05.794: INFO: Creating new exec pod
Mar 15 10:41:10.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4840 exec execpod-affinitygfrxg -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Mar 15 10:41:10.994: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 15 10:41:10.994: INFO: stdout: ""
Mar 15 10:41:10.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4840 exec execpod-affinitygfrxg -- /bin/sh -x -c nc -zv -t -w 2 10.21.134.96 80'
Mar 15 10:41:11.180: INFO: stderr: "+ nc -zv -t -w 2 10.21.134.96 80\nConnection to 10.21.134.96 80 port [tcp/http] succeeded!\n"
Mar 15 10:41:11.180: INFO: stdout: ""
Mar 15 10:41:11.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4840 exec execpod-affinitygfrxg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.134.96:80/ ; done'
Mar 15 10:41:11.499: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n"
Mar 15 10:41:11.499: INFO: stdout: "\naffinity-clusterip-transition-jm5pv\naffinity-clusterip-transition-zlpzb\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-jm5pv\naffinity-clusterip-transition-zlpzb\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-jm5pv\naffinity-clusterip-transition-zlpzb\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-jm5pv\naffinity-clusterip-transition-zlpzb\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-jm5pv\naffinity-clusterip-transition-zlpzb\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-jm5pv"
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-zlpzb
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-zlpzb
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-zlpzb
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-zlpzb
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-zlpzb
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.499: INFO: Received response from host: affinity-clusterip-transition-jm5pv
Mar 15 10:41:11.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-4840 exec execpod-affinitygfrxg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.21.134.96:80/ ; done'
Mar 15 10:41:11.808: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.21.134.96:80/\n"
Mar 15 10:41:11.808: INFO: stdout: "\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm\naffinity-clusterip-transition-d75lm"
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Received response from host: affinity-clusterip-transition-d75lm
Mar 15 10:41:11.808: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4840, will wait for the garbage collector to delete the pods
Mar 15 10:41:11.878: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.234347ms
Mar 15 10:41:12.679: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 800.235514ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:26.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4840" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:23.806 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":263,"skipped":4217,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Mar 15 10:41:26.538: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3169" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":264,"skipped":4226,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:30.092: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:41:30.141: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"74407f25-a83b-40fc-b84b-bb1020b1b264", Controller:(*bool)(0xc0049b80f2), BlockOwnerDeletion:(*bool)(0xc0049b80f3)}}
Mar 15 10:41:30.145: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"861755c9-7edb-44bd-9f87-d8d41a0befc1", Controller:(*bool)(0xc0049b83ca), BlockOwnerDeletion:(*bool)(0xc0049b83cb)}}
Mar 15 10:41:30.163: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1dad67f6-8231-4f72-b7d8-5dafe860aa04", Controller:(*bool)(0xc0049b8612), BlockOwnerDeletion:(*bool)(0xc0049b8613)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:35.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5749" for this suite.

• [SLOW TEST:5.091 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":265,"skipped":4239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:35.184: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:41:36.427: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:41:38.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401696, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401696, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401696, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401696, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:41:41.463: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:53.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3906" for this suite.
STEP: Destroying namespace "webhook-3906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.414 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":266,"skipped":4275,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:53.598: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Mar 15 10:41:53.631: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:41:57.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7140" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":267,"skipped":4288,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:41:57.827: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5873" for this suite.
STEP: Destroying namespace "nsdeletetest-9325" for this suite.
Mar 15 10:42:03.924: INFO: Namespace nsdeletetest-9325 was already deleted
STEP: Destroying namespace "nsdeletetest-3314" for this suite.

• [SLOW TEST:6.100 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":268,"skipped":4296,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:03.927: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 15 10:42:03.965: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:03.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7342" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":269,"skipped":4297,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:03.986: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 15 10:42:08.092: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:08.098: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:10.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:10.100: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:12.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:12.100: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:14.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:14.101: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:16.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:16.100: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:18.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:18.100: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:20.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:20.101: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:22.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:22.101: INFO: Pod pod-with-prestop-http-hook still exists
Mar 15 10:42:24.098: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 15 10:42:24.101: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6565" for this suite.

• [SLOW TEST:20.134 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4298,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:24.121: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 15 10:42:24.155: INFO: Waiting up to 5m0s for pod "pod-88558218-9191-4216-9edd-d79f234333b7" in namespace "emptydir-3201" to be "Succeeded or Failed"
Mar 15 10:42:24.157: INFO: Pod "pod-88558218-9191-4216-9edd-d79f234333b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.745644ms
Mar 15 10:42:26.159: INFO: Pod "pod-88558218-9191-4216-9edd-d79f234333b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004098686s
Mar 15 10:42:28.163: INFO: Pod "pod-88558218-9191-4216-9edd-d79f234333b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008106084s
STEP: Saw pod success
Mar 15 10:42:28.163: INFO: Pod "pod-88558218-9191-4216-9edd-d79f234333b7" satisfied condition "Succeeded or Failed"
Mar 15 10:42:28.165: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-88558218-9191-4216-9edd-d79f234333b7 container test-container: <nil>
STEP: delete the pod
Mar 15 10:42:28.178: INFO: Waiting for pod pod-88558218-9191-4216-9edd-d79f234333b7 to disappear
Mar 15 10:42:28.184: INFO: Pod pod-88558218-9191-4216-9edd-d79f234333b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:28.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3201" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4315,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:28.191: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 15 10:42:31.231: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:31.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4929" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4335,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:31.253: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:42:31.950: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 15 10:42:33.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401751, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401751, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401752, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751401751, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:42:36.976: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:42:36.978: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:38.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2222" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.925 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":273,"skipped":4356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:38.178: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:42:38.216: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d50b957d-3199-42f4-a232-d56d6a8cd9ef" in namespace "security-context-test-4465" to be "Succeeded or Failed"
Mar 15 10:42:38.221: INFO: Pod "busybox-user-65534-d50b957d-3199-42f4-a232-d56d6a8cd9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.761756ms
Mar 15 10:42:40.224: INFO: Pod "busybox-user-65534-d50b957d-3199-42f4-a232-d56d6a8cd9ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007798942s
Mar 15 10:42:42.227: INFO: Pod "busybox-user-65534-d50b957d-3199-42f4-a232-d56d6a8cd9ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010819203s
Mar 15 10:42:42.227: INFO: Pod "busybox-user-65534-d50b957d-3199-42f4-a232-d56d6a8cd9ef" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:42.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4465" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4400,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:42.239: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 15 10:42:42.303: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:42:42.306: INFO: Number of nodes with available pods: 0
Mar 15 10:42:42.306: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:42:43.312: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:42:43.315: INFO: Number of nodes with available pods: 0
Mar 15 10:42:43.315: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:42:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:42:44.316: INFO: Number of nodes with available pods: 2
Mar 15 10:42:44.316: INFO: Node ip-10-0-1-18.us-east-2.compute.internal is running more than one daemon pod
Mar 15 10:42:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:42:45.313: INFO: Number of nodes with available pods: 3
Mar 15 10:42:45.313: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 15 10:42:45.324: INFO: DaemonSet pods can't tolerate node ip-10-0-3-107.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 15 10:42:45.327: INFO: Number of nodes with available pods: 3
Mar 15 10:42:45.327: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2320, will wait for the garbage collector to delete the pods
Mar 15 10:42:46.392: INFO: Deleting DaemonSet.extensions daemon-set took: 4.170351ms
Mar 15 10:42:46.492: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.223781ms
Mar 15 10:42:56.495: INFO: Number of nodes with available pods: 0
Mar 15 10:42:56.495: INFO: Number of running nodes: 0, number of available pods: 0
Mar 15 10:42:56.496: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2320/daemonsets","resourceVersion":"684576"},"items":null}

Mar 15 10:42:56.498: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2320/pods","resourceVersion":"684576"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:42:56.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2320" for this suite.

• [SLOW TEST:14.274 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":275,"skipped":4415,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:42:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-06cd6813-cbd4-44ce-8594-684a7581abf5
STEP: Creating a pod to test consume secrets
Mar 15 10:42:56.543: INFO: Waiting up to 5m0s for pod "pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519" in namespace "secrets-790" to be "Succeeded or Failed"
Mar 15 10:42:56.545: INFO: Pod "pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643112ms
Mar 15 10:42:58.556: INFO: Pod "pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012655866s
Mar 15 10:43:00.558: INFO: Pod "pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015483349s
STEP: Saw pod success
Mar 15 10:43:00.558: INFO: Pod "pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519" satisfied condition "Succeeded or Failed"
Mar 15 10:43:00.560: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519 container secret-env-test: <nil>
STEP: delete the pod
Mar 15 10:43:00.571: INFO: Waiting for pod pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519 to disappear
Mar 15 10:43:00.579: INFO: Pod pod-secrets-f2caafb5-1d27-453b-b350-d579e0481519 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:00.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-790" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":276,"skipped":4423,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:00.586: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-2c8c1d04-a910-49c4-a018-226156c0621a
STEP: Creating a pod to test consume secrets
Mar 15 10:43:00.624: INFO: Waiting up to 5m0s for pod "pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c" in namespace "secrets-1971" to be "Succeeded or Failed"
Mar 15 10:43:00.625: INFO: Pod "pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.780989ms
Mar 15 10:43:02.639: INFO: Pod "pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015763069s
Mar 15 10:43:04.642: INFO: Pod "pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018835234s
STEP: Saw pod success
Mar 15 10:43:04.642: INFO: Pod "pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c" satisfied condition "Succeeded or Failed"
Mar 15 10:43:04.644: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:43:04.658: INFO: Waiting for pod pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c to disappear
Mar 15 10:43:04.664: INFO: Pod pod-secrets-70c116cc-7a53-49d4-82a9-5fc635f3510c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:04.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1971" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":277,"skipped":4433,"failed":0}

------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:04.671: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:04.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4317" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":278,"skipped":4433,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 15 10:43:04.733: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:22.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6755" for this suite.

• [SLOW TEST:17.419 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:22.128: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:43:22.155: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 15 10:43:27.159: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 15 10:43:27.159: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Mar 15 10:43:27.174: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-877 /apis/apps/v1/namespaces/deployment-877/deployments/test-cleanup-deployment 24366142-c4b0-4990-9fef-41a3a94cc23a 684813 1 2021-03-15 10:43:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-03-15 10:43:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00080d458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 15 10:43:27.175: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Mar 15 10:43:27.175: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 15 10:43:27.175: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-877 /apis/apps/v1/namespaces/deployment-877/replicasets/test-cleanup-controller 7fc8c4e2-e5cf-43c4-8e3e-9686b4882399 684814 1 2021-03-15 10:43:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 24366142-c4b0-4990-9fef-41a3a94cc23a 0xc002547407 0xc002547408}] []  [{e2e.test Update apps/v1 2021-03-15 10:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 10:43:27 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"24366142-c4b0-4990-9fef-41a3a94cc23a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0025474a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 15 10:43:27.178: INFO: Pod "test-cleanup-controller-kdfhw" is available:
&Pod{ObjectMeta:{test-cleanup-controller-kdfhw test-cleanup-controller- deployment-877 /api/v1/namespaces/deployment-877/pods/test-cleanup-controller-kdfhw f07032d6-2dbe-45bc-9441-49c0272eafed 684799 0 2021-03-15 10:43:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 7fc8c4e2-e5cf-43c4-8e3e-9686b4882399 0xc002547867 0xc002547868}] []  [{kube-controller-manager Update v1 2021-03-15 10:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7fc8c4e2-e5cf-43c4-8e3e-9686b4882399\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:43:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bh6jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bh6jn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bh6jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:43:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:43:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:43:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:43:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.225,StartTime:2021-03-15 10:43:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:43:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://154ecb257b2ceb09ceb8a6f067e12c6832acd03766023f559fa62b13cb09f86d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:27.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-877" for this suite.

• [SLOW TEST:5.075 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":280,"skipped":4522,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:27.203: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-tgxk
STEP: Creating a pod to test atomic-volume-subpath
Mar 15 10:43:27.282: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tgxk" in namespace "subpath-4139" to be "Succeeded or Failed"
Mar 15 10:43:27.288: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032948ms
Mar 15 10:43:29.290: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 2.008408622s
Mar 15 10:43:31.294: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.011576118s
Mar 15 10:43:33.296: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 6.014335394s
Mar 15 10:43:35.299: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 8.017326166s
Mar 15 10:43:37.302: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 10.020411606s
Mar 15 10:43:39.306: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 12.023689952s
Mar 15 10:43:41.309: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 14.02689205s
Mar 15 10:43:43.312: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 16.030051856s
Mar 15 10:43:45.315: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 18.033386431s
Mar 15 10:43:47.319: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 20.03660663s
Mar 15 10:43:49.322: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Running", Reason="", readiness=true. Elapsed: 22.039731342s
Mar 15 10:43:51.325: INFO: Pod "pod-subpath-test-downwardapi-tgxk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042797186s
STEP: Saw pod success
Mar 15 10:43:51.325: INFO: Pod "pod-subpath-test-downwardapi-tgxk" satisfied condition "Succeeded or Failed"
Mar 15 10:43:51.327: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-subpath-test-downwardapi-tgxk container test-container-subpath-downwardapi-tgxk: <nil>
STEP: delete the pod
Mar 15 10:43:51.339: INFO: Waiting for pod pod-subpath-test-downwardapi-tgxk to disappear
Mar 15 10:43:51.346: INFO: Pod pod-subpath-test-downwardapi-tgxk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tgxk
Mar 15 10:43:51.346: INFO: Deleting pod "pod-subpath-test-downwardapi-tgxk" in namespace "subpath-4139"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:43:51.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4139" for this suite.

• [SLOW TEST:24.150 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":281,"skipped":4526,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:43:51.353: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 15 10:43:51.372: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:43:55.180: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:10.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3780" for this suite.

• [SLOW TEST:19.124 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":282,"skipped":4530,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:10.477: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 15 10:44:10.525: INFO: Waiting up to 5m0s for pod "pod-279689c0-1bcd-431f-8bc2-d37b0c9be895" in namespace "emptydir-4597" to be "Succeeded or Failed"
Mar 15 10:44:10.527: INFO: Pod "pod-279689c0-1bcd-431f-8bc2-d37b0c9be895": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968236ms
Mar 15 10:44:12.531: INFO: Pod "pod-279689c0-1bcd-431f-8bc2-d37b0c9be895": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005136805s
Mar 15 10:44:14.534: INFO: Pod "pod-279689c0-1bcd-431f-8bc2-d37b0c9be895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008099907s
STEP: Saw pod success
Mar 15 10:44:14.534: INFO: Pod "pod-279689c0-1bcd-431f-8bc2-d37b0c9be895" satisfied condition "Succeeded or Failed"
Mar 15 10:44:14.535: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-279689c0-1bcd-431f-8bc2-d37b0c9be895 container test-container: <nil>
STEP: delete the pod
Mar 15 10:44:14.547: INFO: Waiting for pod pod-279689c0-1bcd-431f-8bc2-d37b0c9be895 to disappear
Mar 15 10:44:14.554: INFO: Pod pod-279689c0-1bcd-431f-8bc2-d37b0c9be895 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:14.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4597" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4542,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:14.560: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Mar 15 10:44:14.586: INFO: Waiting up to 5m0s for pod "client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828" in namespace "containers-4357" to be "Succeeded or Failed"
Mar 15 10:44:14.588: INFO: Pod "client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828": Phase="Pending", Reason="", readiness=false. Elapsed: 1.507352ms
Mar 15 10:44:16.592: INFO: Pod "client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005997779s
STEP: Saw pod success
Mar 15 10:44:16.592: INFO: Pod "client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828" satisfied condition "Succeeded or Failed"
Mar 15 10:44:16.594: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828 container test-container: <nil>
STEP: delete the pod
Mar 15 10:44:16.627: INFO: Waiting for pod client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828 to disappear
Mar 15 10:44:16.635: INFO: Pod client-containers-5cce2a5d-884e-451b-a609-8d7d6f80b828 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:16.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4357" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":284,"skipped":4550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:16.643: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Mar 15 10:44:16.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce" in namespace "downward-api-6872" to be "Succeeded or Failed"
Mar 15 10:44:16.675: INFO: Pod "downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852621ms
Mar 15 10:44:18.678: INFO: Pod "downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004896021s
STEP: Saw pod success
Mar 15 10:44:18.678: INFO: Pod "downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce" satisfied condition "Succeeded or Failed"
Mar 15 10:44:18.680: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce container client-container: <nil>
STEP: delete the pod
Mar 15 10:44:18.692: INFO: Waiting for pod downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce to disappear
Mar 15 10:44:18.701: INFO: Pod downwardapi-volume-6f1b3e13-83cd-4d8d-89c4-a40ceaa5b6ce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:18.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6872" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4572,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:18.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8393" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":286,"skipped":4588,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:18.762: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 15 10:44:18.794: INFO: Waiting up to 5m0s for pod "pod-e1586074-33d7-48b2-b082-7cb83f2820a2" in namespace "emptydir-9101" to be "Succeeded or Failed"
Mar 15 10:44:18.796: INFO: Pod "pod-e1586074-33d7-48b2-b082-7cb83f2820a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827071ms
Mar 15 10:44:20.799: INFO: Pod "pod-e1586074-33d7-48b2-b082-7cb83f2820a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004690636s
STEP: Saw pod success
Mar 15 10:44:20.799: INFO: Pod "pod-e1586074-33d7-48b2-b082-7cb83f2820a2" satisfied condition "Succeeded or Failed"
Mar 15 10:44:20.801: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-e1586074-33d7-48b2-b082-7cb83f2820a2 container test-container: <nil>
STEP: delete the pod
Mar 15 10:44:20.814: INFO: Waiting for pod pod-e1586074-33d7-48b2-b082-7cb83f2820a2 to disappear
Mar 15 10:44:20.820: INFO: Pod pod-e1586074-33d7-48b2-b082-7cb83f2820a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:20.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9101" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":287,"skipped":4589,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 15 10:44:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-2025 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Mar 15 10:44:20.998: INFO: stderr: ""
Mar 15 10:44:20.998: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 15 10:44:20.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-2025 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Mar 15 10:44:21.293: INFO: stderr: ""
Mar 15 10:44:21.293: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Mar 15 10:44:21.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=kubectl-2025 delete pods e2e-test-httpd-pod'
Mar 15 10:44:32.125: INFO: stderr: ""
Mar 15 10:44:32.125: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:32.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2025" for this suite.

• [SLOW TEST:11.304 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:902
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":288,"skipped":4595,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:32.132: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:44:32.157: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 15 10:44:36.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 create -f -'
Mar 15 10:44:37.644: INFO: stderr: ""
Mar 15 10:44:37.644: INFO: stdout: "e2e-test-crd-publish-openapi-6291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 15 10:44:37.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 delete e2e-test-crd-publish-openapi-6291-crds test-foo'
Mar 15 10:44:37.726: INFO: stderr: ""
Mar 15 10:44:37.726: INFO: stdout: "e2e-test-crd-publish-openapi-6291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 15 10:44:37.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 apply -f -'
Mar 15 10:44:37.941: INFO: stderr: ""
Mar 15 10:44:37.941: INFO: stdout: "e2e-test-crd-publish-openapi-6291-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 15 10:44:37.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 delete e2e-test-crd-publish-openapi-6291-crds test-foo'
Mar 15 10:44:38.021: INFO: stderr: ""
Mar 15 10:44:38.021: INFO: stdout: "e2e-test-crd-publish-openapi-6291-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 15 10:44:38.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 create -f -'
Mar 15 10:44:38.223: INFO: rc: 1
Mar 15 10:44:38.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 apply -f -'
Mar 15 10:44:38.427: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 15 10:44:38.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 create -f -'
Mar 15 10:44:38.632: INFO: rc: 1
Mar 15 10:44:38.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 --namespace=crd-publish-openapi-3943 apply -f -'
Mar 15 10:44:38.823: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 15 10:44:38.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 explain e2e-test-crd-publish-openapi-6291-crds'
Mar 15 10:44:39.030: INFO: stderr: ""
Mar 15 10:44:39.030: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 15 10:44:39.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 explain e2e-test-crd-publish-openapi-6291-crds.metadata'
Mar 15 10:44:39.242: INFO: stderr: ""
Mar 15 10:44:39.242: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 15 10:44:39.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 explain e2e-test-crd-publish-openapi-6291-crds.spec'
Mar 15 10:44:39.451: INFO: stderr: ""
Mar 15 10:44:39.451: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 15 10:44:39.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 explain e2e-test-crd-publish-openapi-6291-crds.spec.bars'
Mar 15 10:44:39.660: INFO: stderr: ""
Mar 15 10:44:39.660: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6291-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 15 10:44:39.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=crd-publish-openapi-3943 explain e2e-test-crd-publish-openapi-6291-crds.spec.bars2'
Mar 15 10:44:39.861: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:43.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3943" for this suite.

• [SLOW TEST:11.492 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":289,"skipped":4598,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:43.624: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Mar 15 10:44:43.653: INFO: Waiting up to 5m0s for pod "var-expansion-39fe245f-460f-4695-b275-1be367d9a195" in namespace "var-expansion-5474" to be "Succeeded or Failed"
Mar 15 10:44:43.663: INFO: Pod "var-expansion-39fe245f-460f-4695-b275-1be367d9a195": Phase="Pending", Reason="", readiness=false. Elapsed: 9.987009ms
Mar 15 10:44:45.666: INFO: Pod "var-expansion-39fe245f-460f-4695-b275-1be367d9a195": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012968371s
STEP: Saw pod success
Mar 15 10:44:45.666: INFO: Pod "var-expansion-39fe245f-460f-4695-b275-1be367d9a195" satisfied condition "Succeeded or Failed"
Mar 15 10:44:45.668: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod var-expansion-39fe245f-460f-4695-b275-1be367d9a195 container dapi-container: <nil>
STEP: delete the pod
Mar 15 10:44:45.680: INFO: Waiting for pod var-expansion-39fe245f-460f-4695-b275-1be367d9a195 to disappear
Mar 15 10:44:45.687: INFO: Pod var-expansion-39fe245f-460f-4695-b275-1be367d9a195 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:44:45.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5474" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:44:45.694: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 15 10:44:49.750: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:49.753: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:44:51.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:51.755: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:44:53.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:53.755: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:44:55.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:55.756: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:44:57.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:57.756: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:44:59.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:44:59.756: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:45:01.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:45:01.756: INFO: Pod pod-with-poststart-http-hook still exists
Mar 15 10:45:03.753: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 15 10:45:03.756: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:03.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9599" for this suite.

• [SLOW TEST:18.068 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4655,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:03.763: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-2ebd523c-3337-47a4-8843-aeec51a88f1d
STEP: Creating a pod to test consume secrets
Mar 15 10:45:03.793: INFO: Waiting up to 5m0s for pod "pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4" in namespace "secrets-1617" to be "Succeeded or Failed"
Mar 15 10:45:03.794: INFO: Pod "pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.516594ms
Mar 15 10:45:05.798: INFO: Pod "pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004616828s
STEP: Saw pod success
Mar 15 10:45:05.798: INFO: Pod "pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4" satisfied condition "Succeeded or Failed"
Mar 15 10:45:05.799: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4 container secret-volume-test: <nil>
STEP: delete the pod
Mar 15 10:45:05.813: INFO: Waiting for pod pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4 to disappear
Mar 15 10:45:05.820: INFO: Pod pod-secrets-f2a3699f-db5c-4ce6-a6cb-d0bda876cfe4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:05.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1617" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:05.828: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Mar 15 10:45:05.846: INFO: Creating deployment "webserver-deployment"
Mar 15 10:45:05.853: INFO: Waiting for observed generation 1
Mar 15 10:45:07.865: INFO: Waiting for all required pods to come up
Mar 15 10:45:07.869: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 15 10:45:09.879: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 15 10:45:09.883: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 15 10:45:09.888: INFO: Updating deployment webserver-deployment
Mar 15 10:45:09.888: INFO: Waiting for observed generation 2
Mar 15 10:45:11.893: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 15 10:45:11.895: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 15 10:45:11.896: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 15 10:45:11.901: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 15 10:45:11.901: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 15 10:45:11.903: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 15 10:45:11.906: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 15 10:45:11.906: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 15 10:45:11.910: INFO: Updating deployment webserver-deployment
Mar 15 10:45:11.910: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 15 10:45:11.922: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 15 10:45:11.926: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Mar 15 10:45:11.983: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5520 /apis/apps/v1/namespaces/deployment-5520/deployments/webserver-deployment 0442c4ba-74f7-42b7-aa2c-2858511690dc 685706 3 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004682a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-03-15 10:45:10 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-03-15 10:45:11 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 15 10:45:12.023: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-5520 /apis/apps/v1/namespaces/deployment-5520/replicasets/webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 685700 3 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 0442c4ba-74f7-42b7-aa2c-2858511690dc 0xc004682fe7 0xc004682fe8}] []  [{kube-controller-manager Update apps/v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0442c4ba-74f7-42b7-aa2c-2858511690dc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004683078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 15 10:45:12.023: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 15 10:45:12.023: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-5520 /apis/apps/v1/namespaces/deployment-5520/replicasets/webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 685699 3 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 0442c4ba-74f7-42b7-aa2c-2858511690dc 0xc004683127 0xc004683128}] []  [{kube-controller-manager Update apps/v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0442c4ba-74f7-42b7-aa2c-2858511690dc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004683218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 15 10:45:12.071: INFO: Pod "webserver-deployment-795d758f88-5s2v9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5s2v9 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-5s2v9 7bca900a-e28c-45b5-b982-1d843da843c7 685672 0 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc004683a47 0xc004683a48}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:,StartTime:2021-03-15 10:45:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-76gtd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-76gtd webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-76gtd 9803a0fa-c0c6-437a-9a85-d68ac00a06ee 685745 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc004683cc0 0xc004683cc1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-79zh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-79zh4 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-79zh4 5d4614cd-efa9-4c63-b8c9-1999eab84314 685668 0 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc004683ec0 0xc004683ec1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:,StartTime:2021-03-15 10:45:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-82pnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-82pnj webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-82pnj ecb95cc6-2c7b-4cbd-8499-2be13db503be 685746 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b80f0 0xc0046b80f1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-8nhdc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8nhdc webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-8nhdc 85424899-b3ff-4b85-8dff-bc090ce93fe1 685740 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8270 0xc0046b8271}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-9d7mj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9d7mj webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-9d7mj 6f5095e1-daf6-4b43-baa0-c34788f30099 685747 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b83f0 0xc0046b83f1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.072: INFO: Pod "webserver-deployment-795d758f88-9f792" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9f792 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-9f792 fbda6489-bb2b-4b7c-884e-b3baca5cdf9f 685648 0 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8550 0xc0046b8551}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:,StartTime:2021-03-15 10:45:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-fn5x2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fn5x2 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-fn5x2 cab64171-ccbb-42e5-a985-ed35b10c36c7 685647 0 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8710 0xc0046b8711}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.18,PodIP:,StartTime:2021-03-15 10:45:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-grslz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-grslz webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-grslz ea48c325-5327-4226-b1bc-f7dbce571861 685717 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8960 0xc0046b8961}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-j7gj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-j7gj7 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-j7gj7 7fe4ee95-962d-45d6-9d08-41156a6320f8 685749 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8ad0 0xc0046b8ad1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-k9qp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k9qp6 webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-k9qp6 7da7fc9e-0a0f-4e99-ad4f-1e0d2540bd7d 685722 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8c70 0xc0046b8c71}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-l26fg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l26fg webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-l26fg 9004fb3b-8ca9-419a-a2d7-f1629fa642b7 685641 0 2021-03-15 10:45:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b8e70 0xc0046b8e71}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:,StartTime:2021-03-15 10:45:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-795d758f88-mnpwr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mnpwr webserver-deployment-795d758f88- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-795d758f88-mnpwr 0d04461c-d569-4513-a62e-3b22ceede4e1 685761 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c530fa6b-3b46-42c4-87d1-eb85a30c961a 0xc0046b9110 0xc0046b9111}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c530fa6b-3b46-42c4-87d1-eb85a30c961a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-dd94f59b7-4vd88" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4vd88 webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-4vd88 b11aa20c-c2ba-413b-b889-23125964d156 685566 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b9300 0xc0046b9301}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.90.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:10.20.90.215,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://90ca57209e2cb12864f2c367d02081ea6d7e55d33560928357ad86c6d0a36f84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.90.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.073: INFO: Pod "webserver-deployment-dd94f59b7-4xszq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4xszq webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-4xszq 1fd251bd-a746-46c4-872d-c6d5191442bc 685760 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b9600 0xc0046b9601}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-52qm7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-52qm7 webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-52qm7 1e59fa17-5b6e-48d3-9b28-854af3fff4c7 685573 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b97b0 0xc0046b97b1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.90.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:10.20.90.214,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ceb670637bac3b1af53015370bd6be1dfe5c123f2fd7bb833f4b49d36add1383,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.90.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-6xgdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6xgdb webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-6xgdb e0d9885f-d0eb-4479-bfba-c30ae8cbf41c 685764 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b9a10 0xc0046b9a11}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-7zvg6" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7zvg6 webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-7zvg6 17aed61b-8d2c-44f3-9321-c0713b76e620 685587 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b9b80 0xc0046b9b81}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.238,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9837f2897e6be7b7a3dbedec77c9749fda4d1b5b98bab527ecd552b6032c0d43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.238,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-82kcl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-82kcl webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-82kcl d9b22d51-c17d-464e-92d2-e0ad1c8e4984 685752 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046b9e80 0xc0046b9e81}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-8vgff" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8vgff webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-8vgff 37aa1380-5ab9-454d-bf41-31a21d282073 685742 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6090 0xc0046e6091}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-9vkct" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9vkct webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-9vkct 891587cd-1193-47fa-be67-309b42ff656a 685739 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6210 0xc0046e6211}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.074: INFO: Pod "webserver-deployment-dd94f59b7-bcxdf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bcxdf webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-bcxdf f2ddfbc4-7b6c-43c6-9fda-117599b35524 685583 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6360 0xc0046e6361}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.54.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.18,PodIP:10.20.54.184,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9dc893ce296e88f49cc037e0054473e4f6ba9adc5144d50df86360d5c9c8623c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.54.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-bq47r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bq47r webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-bq47r 42a7b704-4de5-41bc-adef-c4e0125a62c0 685748 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6570 0xc0046e6571}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:,StartTime:2021-03-15 10:45:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-jqvzq" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jqvzq webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-jqvzq 9b0ee9f8-c953-4efd-a150-26f6a5d06e9b 685570 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6730 0xc0046e6731}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.90.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:10.20.90.216,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://efed7bc4ef0d993d13c2dcefe678163d6061e831a0115563923b5ccf9c1740e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.90.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-kv84d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-kv84d webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-kv84d e8aed48f-841c-47ed-92d2-c8dafdd5aefd 685728 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6970 0xc0046e6971}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-nrczh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nrczh webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-nrczh 36194dc7-686f-4d5f-a0ec-eae21d019782 685759 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6ad0 0xc0046e6ad1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.120,PodIP:,StartTime:2021-03-15 10:45:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-pxzdw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pxzdw webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-pxzdw b53d60ba-50c1-4924-a27e-c550fd2b19b5 685580 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6cb0 0xc0046e6cb1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.54.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.18,PodIP:10.20.54.186,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f16c3ec1e3092279758b93a23e1c417082bede93f71076f6e1d5b4ba16af7f98,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.54.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-pzxbr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-pzxbr webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-pzxbr 0d35c5c4-ea8b-4b06-9559-ce3f2f7d7477 685741 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e6f50 0xc0046e6f51}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.075: INFO: Pod "webserver-deployment-dd94f59b7-skz55" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-skz55 webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-skz55 3f4ae3fc-c4b3-446d-b487-881802add1fa 685766 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e7110 0xc0046e7111}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-120.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.076: INFO: Pod "webserver-deployment-dd94f59b7-t4zlx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-t4zlx webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-t4zlx feaecc6b-8987-42d9-8c3a-b6d41b9e6325 685576 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e72b0 0xc0046e72b1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.54.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-18.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.18,PodIP:10.20.54.185,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c89471ef6aaf05be179ea0a8da023f80e4ee911b6d18503c58a0134f8f67c3bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.54.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.076: INFO: Pod "webserver-deployment-dd94f59b7-tw62d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tw62d webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-tw62d 125c669f-b0f2-44a0-aeaf-f1acdf7dcfb1 685709 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e7520 0xc0046e7521}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.076: INFO: Pod "webserver-deployment-dd94f59b7-z4h87" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z4h87 webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-z4h87 4c5cc8ce-733c-4510-a694-08b2f44fd006 685721 0 2021-03-15 10:45:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e7670 0xc0046e7671}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 15 10:45:12.076: INFO: Pod "webserver-deployment-dd94f59b7-z8b8b" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z8b8b webserver-deployment-dd94f59b7- deployment-5520 /api/v1/namespaces/deployment-5520/pods/webserver-deployment-dd94f59b7-z8b8b f24f7f4f-a388-4b65-bcfd-457aaf526bab 685597 0 2021-03-15 10:45:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 76e66d81-cdfd-4c85-82f0-29e37f0fd7c1 0xc0046e77c0 0xc0046e77c1}] []  [{kube-controller-manager Update v1 2021-03-15 10:45:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76e66d81-cdfd-4c85-82f0-29e37f0fd7c1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-03-15 10:45:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.20.71.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fkkff,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fkkff,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fkkff,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-172.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-03-15 10:45:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.172,PodIP:10.20.71.239,StartTime:2021-03-15 10:45:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-03-15 10:45:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://60efaeeff8497cc774d924ab58ce40b62099ce940acf59937fcc490b635341ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.71.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:12.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5520" for this suite.

• [SLOW TEST:6.293 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":293,"skipped":4694,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:12.122: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9628
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9628
I0315 10:45:12.248820      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9628, replica count: 2
I0315 10:45:15.299206      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0315 10:45:18.299509      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:45:21.299: INFO: Creating new exec pod
I0315 10:45:21.299884      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 15 10:45:26.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 15 10:45:26.511: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 15 10:45:26.511: INFO: stdout: ""
Mar 15 10:45:26.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 10.21.173.86 80'
Mar 15 10:45:26.744: INFO: stderr: "+ nc -zv -t -w 2 10.21.173.86 80\nConnection to 10.21.173.86 80 port [tcp/http] succeeded!\n"
Mar 15 10:45:26.744: INFO: stdout: ""
Mar 15 10:45:26.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.172 30266'
Mar 15 10:45:26.929: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.172 30266\nConnection to 10.0.3.172 30266 port [tcp/30266] succeeded!\n"
Mar 15 10:45:26.929: INFO: stdout: ""
Mar 15 10:45:26.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.120 30266'
Mar 15 10:45:27.119: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.120 30266\nConnection to 10.0.2.120 30266 port [tcp/30266] succeeded!\n"
Mar 15 10:45:27.120: INFO: stdout: ""
Mar 15 10:45:27.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 18.189.17.197 30266'
Mar 15 10:45:27.316: INFO: stderr: "+ nc -zv -t -w 2 18.189.17.197 30266\nConnection to 18.189.17.197 30266 port [tcp/30266] succeeded!\n"
Mar 15 10:45:27.316: INFO: stdout: ""
Mar 15 10:45:27.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-016546558 --namespace=services-9628 exec execpodg8rp6 -- /bin/sh -x -c nc -zv -t -w 2 13.59.66.200 30266'
Mar 15 10:45:27.530: INFO: stderr: "+ nc -zv -t -w 2 13.59.66.200 30266\nConnection to 13.59.66.200 30266 port [tcp/30266] succeeded!\n"
Mar 15 10:45:27.530: INFO: stdout: ""
Mar 15 10:45:27.530: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9628" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.455 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":294,"skipped":4806,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:27.576: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 15 10:45:31.632: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:31.636: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:33.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:33.639: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:35.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:35.639: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:37.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:37.639: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:39.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:39.639: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:41.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:41.639: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 15 10:45:43.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 15 10:45:43.639: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:43.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4948" for this suite.

• [SLOW TEST:16.076 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":295,"skipped":4809,"failed":0}
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:43.652: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Mar 15 10:45:43.687: INFO: Waiting up to 5m0s for pod "var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa" in namespace "var-expansion-6204" to be "Succeeded or Failed"
Mar 15 10:45:43.689: INFO: Pod "var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014202ms
Mar 15 10:45:45.691: INFO: Pod "var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004389569s
Mar 15 10:45:47.694: INFO: Pod "var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006859896s
STEP: Saw pod success
Mar 15 10:45:47.694: INFO: Pod "var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa" satisfied condition "Succeeded or Failed"
Mar 15 10:45:47.695: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa container dapi-container: <nil>
STEP: delete the pod
Mar 15 10:45:47.707: INFO: Waiting for pod var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa to disappear
Mar 15 10:45:47.714: INFO: Pod var-expansion-20d7a63d-e77f-4a1f-8fa0-eb252f6625aa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:47.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6204" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":296,"skipped":4809,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:47.721: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 15 10:45:47.759: INFO: starting watch
STEP: patching
STEP: updating
Mar 15 10:45:47.766: INFO: waiting for watch events with expected annotations
Mar 15 10:45:47.766: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4102" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":297,"skipped":4811,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:47.790: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:45:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6837" for this suite.

• [SLOW TEST:11.091 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":298,"skipped":4817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:45:58.882: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 15 10:45:58.911: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
Mar 15 10:46:02.762: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:46:18.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8256" for this suite.

• [SLOW TEST:19.249 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":299,"skipped":4846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:46:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-eb578caa-e33d-4a82-9e75-9381d0d3f2e8
STEP: Creating a pod to test consume configMaps
Mar 15 10:46:18.162: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f" in namespace "projected-8541" to be "Succeeded or Failed"
Mar 15 10:46:18.174: INFO: Pod "pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.539454ms
Mar 15 10:46:20.177: INFO: Pod "pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015093994s
STEP: Saw pod success
Mar 15 10:46:20.177: INFO: Pod "pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f" satisfied condition "Succeeded or Failed"
Mar 15 10:46:20.178: INFO: Trying to get logs from node ip-10-0-3-172.us-east-2.compute.internal pod pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 15 10:46:20.190: INFO: Waiting for pod pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f to disappear
Mar 15 10:46:20.198: INFO: Pod pod-projected-configmaps-f6b65e8f-0ee3-4fec-9ac5-ca71e6ab338f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:46:20.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8541" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:46:20.204: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 15 10:46:24.244: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5826 PodName:var-expansion-17456c8a-7c50-40b9-ba47-a65ff30af975 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:46:24.245: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: test for file in mounted path
Mar 15 10:46:24.358: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5826 PodName:var-expansion-17456c8a-7c50-40b9-ba47-a65ff30af975 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 15 10:46:24.358: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: updating the annotation value
Mar 15 10:46:24.979: INFO: Successfully updated pod "var-expansion-17456c8a-7c50-40b9-ba47-a65ff30af975"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 15 10:46:24.981: INFO: Deleting pod "var-expansion-17456c8a-7c50-40b9-ba47-a65ff30af975" in namespace "var-expansion-5826"
Mar 15 10:46:24.984: INFO: Wait up to 5m0s for pod "var-expansion-17456c8a-7c50-40b9-ba47-a65ff30af975" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:47:02.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5826" for this suite.

• [SLOW TEST:42.795 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":301,"skipped":4897,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:47:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 15 10:47:03.811: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 15 10:47:05.818: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751402023, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751402023, loc:(*time.Location)(0x76f4840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63751402023, loc:(*time.Location)(0x76f4840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63751402023, loc:(*time.Location)(0x76f4840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 15 10:47:08.825: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:47:08.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8933" for this suite.
STEP: Destroying namespace "webhook-8933-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.965 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":302,"skipped":4901,"failed":0}
SSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:47:08.965: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:47:09.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-675" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":303,"skipped":4907,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:47:09.091: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-7d75d094-2721-433a-97de-4730d92b1700
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:47:13.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4330" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":304,"skipped":4907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Mar 15 10:47:13.192: INFO: >>> kubeConfig: /tmp/kubeconfig-016546558
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Mar 15 10:47:13.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4350" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":305,"skipped":4933,"failed":0}
Mar 15 10:47:13.268: INFO: Running AfterSuite actions on all nodes
Mar 15 10:47:13.269: INFO: Running AfterSuite actions on node 1
Mar 15 10:47:13.269: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4933,"failed":0}

Ran 305 of 5238 Specs in 6387.252 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4933 Skipped
PASS

Ginkgo ran 1 suite in 1h46m28.7531319s
Test Suite Passed
