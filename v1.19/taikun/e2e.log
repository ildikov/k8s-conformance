I0717 11:47:05.558555      22 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-578146847
I0717 11:47:05.558580      22 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0717 11:47:05.558689      22 e2e.go:129] Starting e2e run "fb935add-64f9-4d3a-b4f3-0ea081b3fb93" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626522423 - Will randomize all specs
Will run 305 of 5484 specs

Jul 17 11:47:05.578: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 11:47:05.581: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 17 11:47:05.611: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 17 11:47:05.673: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 17 11:47:05.674: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jul 17 11:47:05.674: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 17 11:47:05.692: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 17 11:47:05.692: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Jul 17 11:47:05.692: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 17 11:47:05.692: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Jul 17 11:47:05.692: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Jul 17 11:47:05.692: INFO: e2e test version: v1.19.9
Jul 17 11:47:05.694: INFO: kube-apiserver version: v1.19.9
Jul 17 11:47:05.694: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 11:47:05.714: INFO: Cluster IP family: ipv4
SS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:47:05.714: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
Jul 17 11:47:05.755: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:47:05.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4719" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":1,"skipped":2,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:47:05.857: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-28f9d379-9397-4ece-a56c-6eb1956c8ec5
STEP: Creating configMap with name cm-test-opt-upd-fd06cc2f-0d20-43e7-a216-2e76329b9996
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-28f9d379-9397-4ece-a56c-6eb1956c8ec5
STEP: Updating configmap cm-test-opt-upd-fd06cc2f-0d20-43e7-a216-2e76329b9996
STEP: Creating configMap with name cm-test-opt-create-e8b6f5c6-cb36-4381-a881-65ce19230f79
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:47:10.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5659" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":3,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:47:10.086: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0717 11:47:11.718570      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 11:48:13.751: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:13.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8107" for this suite.

• [SLOW TEST:63.681 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":3,"skipped":10,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:13.771: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:19.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1366" for this suite.
STEP: Destroying namespace "nsdeletetest-3033" for this suite.
Jul 17 11:48:19.939: INFO: Namespace nsdeletetest-3033 was already deleted
STEP: Destroying namespace "nsdeletetest-6232" for this suite.

• [SLOW TEST:6.176 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":4,"skipped":34,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:19.950: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-07aa145a-9a28-4b01-8cd4-8ddc9d26799c
STEP: Creating a pod to test consume secrets
Jul 17 11:48:20.019: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874" in namespace "projected-2303" to be "Succeeded or Failed"
Jul 17 11:48:20.029: INFO: Pod "pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874": Phase="Pending", Reason="", readiness=false. Elapsed: 9.687478ms
Jul 17 11:48:22.034: INFO: Pod "pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015271005s
STEP: Saw pod success
Jul 17 11:48:22.034: INFO: Pod "pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874" satisfied condition "Succeeded or Failed"
Jul 17 11:48:22.037: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 17 11:48:22.078: INFO: Waiting for pod pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874 to disappear
Jul 17 11:48:22.088: INFO: Pod pod-projected-secrets-7897f082-6029-44d0-9017-60b83e843874 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:22.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2303" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":5,"skipped":38,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:22.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7843" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":6,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:22.197: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-524a5e72-d3c0-43e4-a231-d116928d52c1
STEP: Creating a pod to test consume configMaps
Jul 17 11:48:22.272: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde" in namespace "projected-5921" to be "Succeeded or Failed"
Jul 17 11:48:22.481: INFO: Pod "pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde": Phase="Pending", Reason="", readiness=false. Elapsed: 209.560584ms
Jul 17 11:48:24.487: INFO: Pod "pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.215381075s
STEP: Saw pod success
Jul 17 11:48:24.487: INFO: Pod "pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde" satisfied condition "Succeeded or Failed"
Jul 17 11:48:24.491: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 11:48:24.514: INFO: Waiting for pod pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde to disappear
Jul 17 11:48:24.517: INFO: Pod pod-projected-configmaps-c466c16d-d920-41c1-94bc-af3b97525cde no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:24.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5921" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":98,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:24.539: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 11:48:24.608: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d" in namespace "projected-6876" to be "Succeeded or Failed"
Jul 17 11:48:24.613: INFO: Pod "downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.886775ms
Jul 17 11:48:26.619: INFO: Pod "downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011727938s
Jul 17 11:48:28.624: INFO: Pod "downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016354021s
STEP: Saw pod success
Jul 17 11:48:28.625: INFO: Pod "downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d" satisfied condition "Succeeded or Failed"
Jul 17 11:48:28.629: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d container client-container: <nil>
STEP: delete the pod
Jul 17 11:48:28.660: INFO: Waiting for pod downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d to disappear
Jul 17 11:48:28.663: INFO: Pod downwardapi-volume-60348d9d-9f00-432f-8078-9c3c2f20185d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:28.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6876" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":140,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:28.682: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 11:48:29.873: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 11:48:31.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119309, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119309, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119310, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119309, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 11:48:34.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 17 11:48:36.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=webhook-852 attach --namespace=webhook-852 to-be-attached-pod -i -c=container1'
Jul 17 11:48:37.660: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:37.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-852" for this suite.
STEP: Destroying namespace "webhook-852-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.063 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":9,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1385
STEP: creating an pod
Jul 17 11:48:37.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 17 11:48:37.971: INFO: stderr: ""
Jul 17 11:48:37.972: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Jul 17 11:48:37.972: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 17 11:48:37.972: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7041" to be "running and ready, or succeeded"
Jul 17 11:48:38.008: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 35.420875ms
Jul 17 11:48:40.016: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.043458175s
Jul 17 11:48:40.016: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 17 11:48:40.016: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 17 11:48:40.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator'
Jul 17 11:48:40.157: INFO: stderr: ""
Jul 17 11:48:40.157: INFO: stdout: "I0717 11:48:39.292343       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/9qq 233\nI0717 11:48:39.492532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/297v 370\nI0717 11:48:39.692514       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/zp9 326\nI0717 11:48:39.892563       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/fbcd 282\nI0717 11:48:40.092472       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/2lm 471\n"
STEP: limiting log lines
Jul 17 11:48:40.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator --tail=1'
Jul 17 11:48:40.311: INFO: stderr: ""
Jul 17 11:48:40.311: INFO: stdout: "I0717 11:48:40.295356       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/nj6 283\n"
Jul 17 11:48:40.311: INFO: got output "I0717 11:48:40.295356       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/nj6 283\n"
STEP: limiting log bytes
Jul 17 11:48:40.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator --limit-bytes=1'
Jul 17 11:48:40.466: INFO: stderr: ""
Jul 17 11:48:40.467: INFO: stdout: "I"
Jul 17 11:48:40.467: INFO: got output "I"
STEP: exposing timestamps
Jul 17 11:48:40.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 17 11:48:40.626: INFO: stderr: ""
Jul 17 11:48:40.626: INFO: stdout: "2021-07-17T11:48:40.492647295Z I0717 11:48:40.492467       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/lgd 216\n"
Jul 17 11:48:40.626: INFO: got output "2021-07-17T11:48:40.492647295Z I0717 11:48:40.492467       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/lgd 216\n"
STEP: restricting to a time range
Jul 17 11:48:43.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator --since=1s'
Jul 17 11:48:43.523: INFO: stderr: ""
Jul 17 11:48:43.523: INFO: stdout: "I0717 11:48:42.692473       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/hr7l 528\nI0717 11:48:42.892404       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/fmq 206\nI0717 11:48:43.092550       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/lsv4 592\nI0717 11:48:43.295449       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/85d 407\nI0717 11:48:43.495347       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/7v8 436\n"
Jul 17 11:48:43.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 logs logs-generator logs-generator --since=24h'
Jul 17 11:48:43.742: INFO: stderr: ""
Jul 17 11:48:43.742: INFO: stdout: "I0717 11:48:39.292343       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/9qq 233\nI0717 11:48:39.492532       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/297v 370\nI0717 11:48:39.692514       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/zp9 326\nI0717 11:48:39.892563       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/fbcd 282\nI0717 11:48:40.092472       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/2lm 471\nI0717 11:48:40.295356       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/nj6 283\nI0717 11:48:40.492467       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/lgd 216\nI0717 11:48:40.692491       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/rx6 334\nI0717 11:48:40.892559       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/49h 480\nI0717 11:48:41.092522       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/8ctx 473\nI0717 11:48:41.292522       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/76w 564\nI0717 11:48:41.492666       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/7sw 243\nI0717 11:48:41.692646       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/ch9 401\nI0717 11:48:41.892501       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/kh9 403\nI0717 11:48:42.092548       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/88w 250\nI0717 11:48:42.292607       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/954l 486\nI0717 11:48:42.492574       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/62gg 265\nI0717 11:48:42.692473       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/hr7l 528\nI0717 11:48:42.892404       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/fmq 206\nI0717 11:48:43.092550       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/lsv4 592\nI0717 11:48:43.295449       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/85d 407\nI0717 11:48:43.495347       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/7v8 436\nI0717 11:48:43.692743       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/kfs 457\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Jul 17 11:48:43.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7041 delete pod logs-generator'
Jul 17 11:48:53.149: INFO: stderr: ""
Jul 17 11:48:53.149: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:48:53.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7041" for this suite.

• [SLOW TEST:15.413 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":10,"skipped":177,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:48:53.160: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0717 11:49:03.256739      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 11:50:05.290: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:05.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3028" for this suite.

• [SLOW TEST:72.157 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":11,"skipped":190,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:05.319: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-17b10006-1b3c-46fd-8c08-667aa8d9f3a3
STEP: Creating a pod to test consume configMaps
Jul 17 11:50:05.379: INFO: Waiting up to 5m0s for pod "pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6" in namespace "configmap-259" to be "Succeeded or Failed"
Jul 17 11:50:05.385: INFO: Pod "pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.551037ms
Jul 17 11:50:07.395: INFO: Pod "pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015686933s
STEP: Saw pod success
Jul 17 11:50:07.395: INFO: Pod "pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6" satisfied condition "Succeeded or Failed"
Jul 17 11:50:07.401: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 11:50:07.647: INFO: Waiting for pod pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6 to disappear
Jul 17 11:50:07.659: INFO: Pod pod-configmaps-1031eace-edca-438d-9a8f-d71a72c6f4a6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:07.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-259" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":12,"skipped":195,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:07.685: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 17 11:50:07.777: INFO: Waiting up to 5m0s for pod "pod-42c88301-0de8-40b9-8752-5ca66e36640e" in namespace "emptydir-3918" to be "Succeeded or Failed"
Jul 17 11:50:07.788: INFO: Pod "pod-42c88301-0de8-40b9-8752-5ca66e36640e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.631345ms
Jul 17 11:50:09.796: INFO: Pod "pod-42c88301-0de8-40b9-8752-5ca66e36640e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01915278s
STEP: Saw pod success
Jul 17 11:50:09.796: INFO: Pod "pod-42c88301-0de8-40b9-8752-5ca66e36640e" satisfied condition "Succeeded or Failed"
Jul 17 11:50:09.800: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-42c88301-0de8-40b9-8752-5ca66e36640e container test-container: <nil>
STEP: delete the pod
Jul 17 11:50:09.832: INFO: Waiting for pod pod-42c88301-0de8-40b9-8752-5ca66e36640e to disappear
Jul 17 11:50:09.835: INFO: Pod pod-42c88301-0de8-40b9-8752-5ca66e36640e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:09.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3918" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":13,"skipped":198,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:09.854: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:50:09.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 create -f -'
Jul 17 11:50:10.511: INFO: stderr: ""
Jul 17 11:50:10.511: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 17 11:50:10.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 create -f -'
Jul 17 11:50:11.028: INFO: stderr: ""
Jul 17 11:50:11.028: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 17 11:50:12.040: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:50:12.040: INFO: Found 0 / 1
Jul 17 11:50:13.031: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:50:13.031: INFO: Found 1 / 1
Jul 17 11:50:13.032: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 17 11:50:13.035: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:50:13.035: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 17 11:50:13.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 describe pod agnhost-primary-lzsmm'
Jul 17 11:50:13.188: INFO: stderr: ""
Jul 17 11:50:13.188: INFO: stdout: "Name:         agnhost-primary-lzsmm\nNamespace:    kubectl-6251\nPriority:     0\nNode:         taikun-1-1188-w-1/192.168.101.100\nStart Time:   Sat, 17 Jul 2021 11:50:10 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.233.64.144/32\n              cni.projectcalico.org/podIPs: 10.233.64.144/32\nStatus:       Running\nIP:           10.233.64.144\nIPs:\n  IP:           10.233.64.144\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a4f50e7e7e67b3495f7bacc58162cfefeec9e80864a282192d7021f954e5c573\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 17 Jul 2021 11:50:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4nx2x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-4nx2x:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4nx2x\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6251/agnhost-primary-lzsmm to taikun-1-1188-w-1\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jul 17 11:50:13.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 describe rc agnhost-primary'
Jul 17 11:50:13.346: INFO: stderr: ""
Jul 17 11:50:13.346: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6251\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-lzsmm\n"
Jul 17 11:50:13.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 describe service agnhost-primary'
Jul 17 11:50:13.498: INFO: stderr: ""
Jul 17 11:50:13.498: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6251\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.233.31.226\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.64.144:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 17 11:50:13.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 describe node taikun-1-1188-m'
Jul 17 11:50:13.739: INFO: stderr: ""
Jul 17 11:50:13.739: INFO: stdout: "Name:               taikun-1-1188-m\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=299de97c-290f-4b3a-921a-88be96ae0a72\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=pod4\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=taikun-1-1188-m\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=299de97c-290f-4b3a-921a-88be96ae0a72\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=pod4\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.101.8\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.101.8/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 17 Jul 2021 08:44:35 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  taikun-1-1188-m\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 17 Jul 2021 11:50:11 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 17 Jul 2021 08:46:26 +0000   Sat, 17 Jul 2021 08:46:26 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 17 Jul 2021 11:50:10 +0000   Sat, 17 Jul 2021 08:44:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 17 Jul 2021 11:50:10 +0000   Sat, 17 Jul 2021 08:44:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 17 Jul 2021 11:50:10 +0000   Sat, 17 Jul 2021 08:44:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 17 Jul 2021 11:50:10 +0000   Sat, 17 Jul 2021 08:46:30 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.101.8\n  Hostname:    taikun-1-1188-m\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30308240Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4030132Ki\n  pods:               110\nAllocatable:\n  cpu:                1800m\n  ephemeral-storage:  27932073938\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3403444Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 bfd6d08d7bc4496a9514c7cc576c35f2\n  System UUID:                efaaadfa-d71b-4414-90f6-659ba7be494d\n  Boot ID:                    aff10db6-a5bb-4bbb-aa79-9fb5074c0ca9\n  Kernel Version:             5.8.0-43-generic\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.9\n  Kubelet Version:            v1.19.9\n  Kube-Proxy Version:         v1.19.9\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   openstack:///efaaadfa-d71b-4414-90f6-659ba7be494d\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-7fbf9b4bbb-mgqsk                   30m (1%)      100m (5%)   64M (1%)         256M (7%)      3h3m\n  kube-system                 calico-node-ltchj                                          150m (8%)     300m (16%)  64M (1%)         500M (14%)     3h3m\n  kube-system                 coredns-7677f9bb54-rd9h2                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     3h2m\n  kube-system                 dns-autoscaler-5b7b5c9b6f-k85tj                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         3h2m\n  kube-system                 kube-apiserver-taikun-1-1188-m                             250m (13%)    0 (0%)      0 (0%)           0 (0%)         3h5m\n  kube-system                 kube-controller-manager-taikun-1-1188-m                    200m (11%)    0 (0%)      0 (0%)           0 (0%)         3h5m\n  kube-system                 kube-proxy-dsrpz                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h4m\n  kube-system                 kube-scheduler-taikun-1-1188-m                             100m (5%)     0 (0%)      0 (0%)           0 (0%)         3h5m\n  kube-system                 metrics-server-747c56cf5f-sgz5z                            48m (2%)      143m (7%)   105Mi (3%)       355Mi (10%)    3h2m\n  kube-system                 nodelocaldns-f88l6                                         100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     3h2m\n  kube-system                 openstack-cloud-controller-manager-jc2gn                   200m (11%)    0 (0%)      0 (0%)           0 (0%)         3h3m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-wsmbf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m19s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1198m (66%)      543m (30%)\n  memory             395386880 (11%)  1484760320 (42%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:              <none>\n"
Jul 17 11:50:13.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6251 describe namespace kubectl-6251'
Jul 17 11:50:13.939: INFO: stderr: ""
Jul 17 11:50:13.939: INFO: stdout: "Name:         kubectl-6251\nLabels:       e2e-framework=kubectl\n              e2e-run=fb935add-64f9-4d3a-b4f3-0ea081b3fb93\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:13.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6251" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":14,"skipped":217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:13.954: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-rhlv
STEP: Creating a pod to test atomic-volume-subpath
Jul 17 11:50:14.075: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rhlv" in namespace "subpath-7677" to be "Succeeded or Failed"
Jul 17 11:50:14.080: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208291ms
Jul 17 11:50:16.092: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016629392s
Jul 17 11:50:18.099: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 4.023278303s
Jul 17 11:50:20.104: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 6.028754356s
Jul 17 11:50:22.109: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 8.034154115s
Jul 17 11:50:24.114: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 10.038731602s
Jul 17 11:50:26.119: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 12.043256507s
Jul 17 11:50:28.127: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 14.051257233s
Jul 17 11:50:30.133: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 16.057491179s
Jul 17 11:50:32.140: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 18.064883794s
Jul 17 11:50:34.147: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 20.071607377s
Jul 17 11:50:36.152: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Running", Reason="", readiness=true. Elapsed: 22.076678703s
Jul 17 11:50:38.157: INFO: Pod "pod-subpath-test-projected-rhlv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.081924311s
STEP: Saw pod success
Jul 17 11:50:38.157: INFO: Pod "pod-subpath-test-projected-rhlv" satisfied condition "Succeeded or Failed"
Jul 17 11:50:38.162: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-subpath-test-projected-rhlv container test-container-subpath-projected-rhlv: <nil>
STEP: delete the pod
Jul 17 11:50:38.192: INFO: Waiting for pod pod-subpath-test-projected-rhlv to disappear
Jul 17 11:50:38.196: INFO: Pod pod-subpath-test-projected-rhlv no longer exists
STEP: Deleting pod pod-subpath-test-projected-rhlv
Jul 17 11:50:38.196: INFO: Deleting pod "pod-subpath-test-projected-rhlv" in namespace "subpath-7677"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:38.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7677" for this suite.

• [SLOW TEST:24.257 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":15,"skipped":242,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:38.217: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:50:38.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6529" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:50:38.353: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:01.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8380" for this suite.

• [SLOW TEST:23.456 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":17,"skipped":280,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:01.810: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6062.svc.taikun-1-1188)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6062.svc.taikun-1-1188;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6062.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6062.svc.taikun-1-1188)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6062.svc.taikun-1-1188;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6062.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 11:51:05.967: INFO: DNS probes using dns-6062/dns-test-0a9ea2f5-cba3-4eac-ba65-d0e1d9687517 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:06.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6062" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":18,"skipped":293,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:06.075: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-5143
STEP: creating service affinity-nodeport in namespace services-5143
STEP: creating replication controller affinity-nodeport in namespace services-5143
I0717 11:51:06.194925      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-5143, replica count: 3
I0717 11:51:09.245719      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 11:51:09.257: INFO: Creating new exec pod
Jul 17 11:51:12.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-5143 exec execpod-affinitywr5q7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jul 17 11:51:12.559: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 17 11:51:12.559: INFO: stdout: ""
Jul 17 11:51:12.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-5143 exec execpod-affinitywr5q7 -- /bin/sh -x -c nc -zv -t -w 2 10.233.11.102 80'
Jul 17 11:51:12.829: INFO: stderr: "+ nc -zv -t -w 2 10.233.11.102 80\nConnection to 10.233.11.102 80 port [tcp/http] succeeded!\n"
Jul 17 11:51:12.829: INFO: stdout: ""
Jul 17 11:51:12.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-5143 exec execpod-affinitywr5q7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.100 32139'
Jul 17 11:51:13.093: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.100 32139\nConnection to 192.168.101.100 32139 port [tcp/32139] succeeded!\n"
Jul 17 11:51:13.093: INFO: stdout: ""
Jul 17 11:51:13.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-5143 exec execpod-affinitywr5q7 -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.67 32139'
Jul 17 11:51:13.356: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.67 32139\nConnection to 192.168.101.67 32139 port [tcp/32139] succeeded!\n"
Jul 17 11:51:13.356: INFO: stdout: ""
Jul 17 11:51:13.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-5143 exec execpod-affinitywr5q7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.100:32139/ ; done'
Jul 17 11:51:13.718: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:32139/\n"
Jul 17 11:51:13.719: INFO: stdout: "\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7\naffinity-nodeport-gw7z7"
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Received response from host: affinity-nodeport-gw7z7
Jul 17 11:51:13.719: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5143, will wait for the garbage collector to delete the pods
Jul 17 11:51:13.840: INFO: Deleting ReplicationController affinity-nodeport took: 10.995584ms
Jul 17 11:51:13.942: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.514304ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:23.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5143" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.215 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":19,"skipped":306,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:23.292: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 17 11:51:25.884: INFO: Successfully updated pod "labelsupdate8d5107ba-e32c-49d4-9b95-ee421b05ef86"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:29.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6163" for this suite.

• [SLOW TEST:6.645 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":20,"skipped":320,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:29.938: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 17 11:51:32.538: INFO: Successfully updated pod "pod-update-2ecd71af-f411-4877-b8b4-86e1413a6edb"
STEP: verifying the updated pod is in kubernetes
Jul 17 11:51:32.566: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:32.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2476" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:32.583: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 17 11:51:32.617: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 17 11:51:32.626: INFO: Waiting for terminating namespaces to be deleted...
Jul 17 11:51:32.629: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-1 before test
Jul 17 11:51:32.645: INFO: labelsupdate8d5107ba-e32c-49d4-9b95-ee421b05ef86 from downward-api-6163 started at 2021-07-17 11:51:23 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.646: INFO: 	Container client-container ready: true, restart count 0
Jul 17 11:51:32.646: INFO: calico-node-ggcth from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.646: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 11:51:32.647: INFO: csi-cinder-nodeplugin-hww5z from kube-system started at 2021-07-17 10:41:19 +0000 UTC (2 container statuses recorded)
Jul 17 11:51:32.648: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 11:51:32.648: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 11:51:32.649: INFO: kube-proxy-rwc5f from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.649: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 11:51:32.649: INFO: nginx-proxy-taikun-1-1188-w-1 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.650: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 11:51:32.650: INFO: nodelocaldns-dsbzl from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.650: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 11:51:32.650: INFO: pod-update-2ecd71af-f411-4877-b8b4-86e1413a6edb from pods-2476 started at 2021-07-17 11:51:30 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.651: INFO: 	Container nginx ready: true, restart count 0
Jul 17 11:51:32.651: INFO: sonobuoy from sonobuoy started at 2021-07-17 11:46:52 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.651: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 17 11:51:32.651: INFO: sonobuoy-e2e-job-bd3d388fed13450e from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 11:51:32.652: INFO: 	Container e2e ready: true, restart count 0
Jul 17 11:51:32.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 11:51:32.652: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 11:51:32.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 11:51:32.652: INFO: 	Container systemd-logs ready: false, restart count 5
Jul 17 11:51:32.653: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-2 before test
Jul 17 11:51:32.663: INFO: calico-node-zsmxq from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.664: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 11:51:32.664: INFO: coredns-7677f9bb54-c5jf9 from kube-system started at 2021-07-17 08:47:31 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.664: INFO: 	Container coredns ready: true, restart count 0
Jul 17 11:51:32.664: INFO: csi-cinder-controllerplugin-694ccb959f-kxjgc from kube-system started at 2021-07-17 09:09:41 +0000 UTC (5 container statuses recorded)
Jul 17 11:51:32.664: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 11:51:32.664: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 17 11:51:32.665: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 17 11:51:32.665: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 17 11:51:32.665: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 17 11:51:32.665: INFO: csi-cinder-nodeplugin-4v7m8 from kube-system started at 2021-07-17 08:48:19 +0000 UTC (2 container statuses recorded)
Jul 17 11:51:32.666: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 11:51:32.666: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 11:51:32.666: INFO: kube-proxy-jvfsd from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.667: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 11:51:32.667: INFO: nginx-proxy-taikun-1-1188-w-2 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.668: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 11:51:32.668: INFO: nodelocaldns-pr4zp from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.668: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 11:51:32.668: INFO: snapshot-controller-0 from kube-system started at 2021-07-17 08:48:27 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.669: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul 17 11:51:32.669: INFO: event-exporter-6664b95b84-szltf from monitoring started at 2021-07-17 08:49:00 +0000 UTC (1 container statuses recorded)
Jul 17 11:51:32.670: INFO: 	Container event-exporter ready: true, restart count 0
Jul 17 11:51:32.670: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 11:51:32.670: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 11:51:32.670: INFO: 	Container systemd-logs ready: false, restart count 5
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6890eac0-6f10-426c-87ab-64282aa6b89e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-6890eac0-6f10-426c-87ab-64282aa6b89e off the node taikun-1-1188-w-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6890eac0-6f10-426c-87ab-64282aa6b89e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:42.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9117" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.330 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":22,"skipped":350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:42.917: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:43.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4467" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":23,"skipped":381,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 17 11:51:43.138: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:46.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-907" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":24,"skipped":396,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:46.876: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 17 11:51:46.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7391 create -f -'
Jul 17 11:51:47.366: INFO: stderr: ""
Jul 17 11:51:47.366: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 17 11:51:48.387: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:51:48.387: INFO: Found 0 / 1
Jul 17 11:51:49.372: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:51:49.372: INFO: Found 1 / 1
Jul 17 11:51:49.372: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 17 11:51:49.375: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:51:49.375: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 17 11:51:49.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7391 patch pod agnhost-primary-vmfzl -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 17 11:51:49.598: INFO: stderr: ""
Jul 17 11:51:49.598: INFO: stdout: "pod/agnhost-primary-vmfzl patched\n"
STEP: checking annotations
Jul 17 11:51:49.604: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:51:49.604: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:51:49.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7391" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":25,"skipped":398,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:51:49.618: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-abd73fe7-ae19-4505-9ccd-ccecd10ee3ba in namespace container-probe-285
Jul 17 11:51:53.899: INFO: Started pod busybox-abd73fe7-ae19-4505-9ccd-ccecd10ee3ba in namespace container-probe-285
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 11:51:53.907: INFO: Initial restart count of pod busybox-abd73fe7-ae19-4505-9ccd-ccecd10ee3ba is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:55:54.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-285" for this suite.

• [SLOW TEST:245.237 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":401,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:55:54.872: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:55:54.934: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:55:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9822" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":27,"skipped":412,"failed":0}

------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:55:57.274: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9657.svc.taikun-1-1188)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9657.svc.taikun-1-1188;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9657.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9657.svc.taikun-1-1188)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9657.svc.taikun-1-1188;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9657.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 11:56:01.379: INFO: DNS probes using dns-9657/dns-test-24e9c6b6-135a-4279-be33-7a2d7421cc0f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:01.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9657" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":28,"skipped":412,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Jul 17 11:56:01.519: INFO: namespace kubectl-9712
Jul 17 11:56:01.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9712 create -f -'
Jul 17 11:56:02.222: INFO: stderr: ""
Jul 17 11:56:02.222: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 17 11:56:03.248: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:56:03.249: INFO: Found 0 / 1
Jul 17 11:56:04.228: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:56:04.228: INFO: Found 1 / 1
Jul 17 11:56:04.228: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 17 11:56:04.232: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 17 11:56:04.232: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 17 11:56:04.232: INFO: wait on agnhost-primary startup in kubectl-9712 
Jul 17 11:56:04.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9712 logs agnhost-primary-smw9k agnhost-primary'
Jul 17 11:56:04.381: INFO: stderr: ""
Jul 17 11:56:04.381: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 17 11:56:04.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9712 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 17 11:56:04.546: INFO: stderr: ""
Jul 17 11:56:04.546: INFO: stdout: "service/rm2 exposed\n"
Jul 17 11:56:04.559: INFO: Service rm2 in namespace kubectl-9712 found.
STEP: exposing service
Jul 17 11:56:06.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9712 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 17 11:56:06.754: INFO: stderr: ""
Jul 17 11:56:06.754: INFO: stdout: "service/rm3 exposed\n"
Jul 17 11:56:06.767: INFO: Service rm3 in namespace kubectl-9712 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:08.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9712" for this suite.

• [SLOW TEST:7.354 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1222
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":29,"skipped":430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:08.795: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 17 11:56:08.886: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:12.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4613" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":30,"skipped":459,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:12.834: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 17 11:56:12.922: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:12.929: INFO: Number of nodes with available pods: 0
Jul 17 11:56:12.929: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 11:56:13.962: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:13.982: INFO: Number of nodes with available pods: 0
Jul 17 11:56:13.982: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 11:56:14.939: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:14.945: INFO: Number of nodes with available pods: 2
Jul 17 11:56:14.945: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 17 11:56:14.975: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:14.988: INFO: Number of nodes with available pods: 1
Jul 17 11:56:14.988: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 11:56:15.995: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:15.999: INFO: Number of nodes with available pods: 1
Jul 17 11:56:15.999: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 11:56:16.994: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:56:16.999: INFO: Number of nodes with available pods: 2
Jul 17 11:56:16.999: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8072, will wait for the garbage collector to delete the pods
Jul 17 11:56:17.072: INFO: Deleting DaemonSet.extensions daemon-set took: 10.942331ms
Jul 17 11:56:17.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.52504ms
Jul 17 11:56:23.177: INFO: Number of nodes with available pods: 0
Jul 17 11:56:23.177: INFO: Number of running nodes: 0, number of available pods: 0
Jul 17 11:56:23.183: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8072/daemonsets","resourceVersion":"65771"},"items":null}

Jul 17 11:56:23.186: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8072/pods","resourceVersion":"65771"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:23.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8072" for this suite.

• [SLOW TEST:10.387 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":31,"skipped":461,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Jul 17 11:56:23.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-249 create -f -'
Jul 17 11:56:23.693: INFO: stderr: ""
Jul 17 11:56:23.693: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 17 11:56:23.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-249 diff -f -'
Jul 17 11:56:24.540: INFO: rc: 1
Jul 17 11:56:24.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-249 delete -f -'
Jul 17 11:56:24.712: INFO: stderr: ""
Jul 17 11:56:24.712: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:24.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-249" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":32,"skipped":479,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:24.727: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-c09dde87-9ed9-4992-87e8-ed195b8b3cf5
STEP: Creating secret with name secret-projected-all-test-volume-12436f51-6e1b-4e3e-bd27-e0fecd30f905
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 17 11:56:24.830: INFO: Waiting up to 5m0s for pod "projected-volume-177b0b7b-3544-4361-b853-0569c7947f13" in namespace "projected-6569" to be "Succeeded or Failed"
Jul 17 11:56:24.840: INFO: Pod "projected-volume-177b0b7b-3544-4361-b853-0569c7947f13": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468245ms
Jul 17 11:56:26.847: INFO: Pod "projected-volume-177b0b7b-3544-4361-b853-0569c7947f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017048501s
Jul 17 11:56:28.855: INFO: Pod "projected-volume-177b0b7b-3544-4361-b853-0569c7947f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024460135s
STEP: Saw pod success
Jul 17 11:56:28.856: INFO: Pod "projected-volume-177b0b7b-3544-4361-b853-0569c7947f13" satisfied condition "Succeeded or Failed"
Jul 17 11:56:28.861: INFO: Trying to get logs from node taikun-1-1188-w-1 pod projected-volume-177b0b7b-3544-4361-b853-0569c7947f13 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 17 11:56:28.886: INFO: Waiting for pod projected-volume-177b0b7b-3544-4361-b853-0569c7947f13 to disappear
Jul 17 11:56:28.890: INFO: Pod projected-volume-177b0b7b-3544-4361-b853-0569c7947f13 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:28.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6569" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":33,"skipped":503,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:28.909: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 17 11:56:33.083: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:33.089: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:35.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:35.102: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:37.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:37.095: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:39.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:39.104: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:41.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:41.104: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:43.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:43.097: INFO: Pod pod-with-prestop-http-hook still exists
Jul 17 11:56:45.090: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 17 11:56:45.095: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:45.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8539" for this suite.

• [SLOW TEST:16.267 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":34,"skipped":510,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:45.177: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:56:45.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-998" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":35,"skipped":513,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:56:45.324: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:01.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8913" for this suite.

• [SLOW TEST:16.220 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":36,"skipped":522,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:12.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4820" for this suite.

• [SLOW TEST:11.138 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":37,"skipped":532,"failed":0}
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:12.686: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:57:12.803: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bd7c113e-d504-4157-9b9a-a2c981991bd5", Controller:(*bool)(0xc002b25d6a), BlockOwnerDeletion:(*bool)(0xc002b25d6b)}}
Jul 17 11:57:12.826: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c2e6527d-c7d3-46aa-a040-f328a2c5fff3", Controller:(*bool)(0xc0028d357a), BlockOwnerDeletion:(*bool)(0xc0028d357b)}}
Jul 17 11:57:12.835: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3d88e5bd-0f02-4470-9b5d-b2087bfb0e66", Controller:(*bool)(0xc002b25fb6), BlockOwnerDeletion:(*bool)(0xc002b25fb7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:17.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7803" for this suite.

• [SLOW TEST:5.178 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":38,"skipped":532,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:17.864: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-9995
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 17 11:57:17.926: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 17 11:57:17.968: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 11:57:19.973: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 11:57:21.974: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 11:57:23.975: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 11:57:25.979: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 11:57:27.976: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 11:57:29.975: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 17 11:57:29.986: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 17 11:57:31.990: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 17 11:57:36.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.173:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9995 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 11:57:36.055: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 11:57:36.169: INFO: Found all expected endpoints: [netserver-0]
Jul 17 11:57:36.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.69.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9995 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 11:57:36.175: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 11:57:36.295: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:36.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9995" for this suite.

• [SLOW TEST:18.451 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":39,"skipped":539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:36.317: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:57:37.598: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 17 11:57:37.599: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 17 11:57:37.599: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.599: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 17 11:57:37.599: INFO: Checking APIGroup: extensions
Jul 17 11:57:37.600: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 17 11:57:37.600: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 17 11:57:37.600: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 17 11:57:37.600: INFO: Checking APIGroup: apps
Jul 17 11:57:37.601: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 17 11:57:37.601: INFO: Versions found [{apps/v1 v1}]
Jul 17 11:57:37.601: INFO: apps/v1 matches apps/v1
Jul 17 11:57:37.601: INFO: Checking APIGroup: events.k8s.io
Jul 17 11:57:37.603: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 17 11:57:37.603: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.603: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 17 11:57:37.603: INFO: Checking APIGroup: authentication.k8s.io
Jul 17 11:57:37.604: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 17 11:57:37.604: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.604: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 17 11:57:37.604: INFO: Checking APIGroup: authorization.k8s.io
Jul 17 11:57:37.606: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 17 11:57:37.606: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.606: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 17 11:57:37.606: INFO: Checking APIGroup: autoscaling
Jul 17 11:57:37.607: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 17 11:57:37.607: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 17 11:57:37.607: INFO: autoscaling/v1 matches autoscaling/v1
Jul 17 11:57:37.607: INFO: Checking APIGroup: batch
Jul 17 11:57:37.609: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 17 11:57:37.609: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 17 11:57:37.609: INFO: batch/v1 matches batch/v1
Jul 17 11:57:37.609: INFO: Checking APIGroup: certificates.k8s.io
Jul 17 11:57:37.611: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 17 11:57:37.611: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.611: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 17 11:57:37.612: INFO: Checking APIGroup: networking.k8s.io
Jul 17 11:57:37.613: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 17 11:57:37.613: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.613: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 17 11:57:37.613: INFO: Checking APIGroup: policy
Jul 17 11:57:37.614: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jul 17 11:57:37.614: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jul 17 11:57:37.614: INFO: policy/v1beta1 matches policy/v1beta1
Jul 17 11:57:37.614: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 17 11:57:37.615: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 17 11:57:37.616: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.616: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 17 11:57:37.616: INFO: Checking APIGroup: storage.k8s.io
Jul 17 11:57:37.617: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 17 11:57:37.618: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.618: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 17 11:57:37.618: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 17 11:57:37.620: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 17 11:57:37.620: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.620: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 17 11:57:37.620: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 17 11:57:37.621: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 17 11:57:37.621: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.621: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 17 11:57:37.622: INFO: Checking APIGroup: scheduling.k8s.io
Jul 17 11:57:37.623: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 17 11:57:37.623: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.623: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 17 11:57:37.623: INFO: Checking APIGroup: coordination.k8s.io
Jul 17 11:57:37.624: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 17 11:57:37.624: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.625: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 17 11:57:37.625: INFO: Checking APIGroup: node.k8s.io
Jul 17 11:57:37.626: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Jul 17 11:57:37.626: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.626: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Jul 17 11:57:37.626: INFO: Checking APIGroup: discovery.k8s.io
Jul 17 11:57:37.628: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jul 17 11:57:37.628: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.628: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jul 17 11:57:37.628: INFO: Checking APIGroup: crd.projectcalico.org
Jul 17 11:57:37.630: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul 17 11:57:37.630: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul 17 11:57:37.630: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul 17 11:57:37.630: INFO: Checking APIGroup: monitoring.coreos.com
Jul 17 11:57:37.632: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jul 17 11:57:37.632: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jul 17 11:57:37.632: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jul 17 11:57:37.632: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jul 17 11:57:37.633: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Jul 17 11:57:37.633: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.633: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Jul 17 11:57:37.633: INFO: Checking APIGroup: metrics.k8s.io
Jul 17 11:57:37.634: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 17 11:57:37.634: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 17 11:57:37.634: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:37.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7116" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":40,"skipped":577,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6517" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":41,"skipped":578,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qm22r in namespace proxy-4736
I0717 11:57:37.837877      22 runners.go:190] Created replication controller with name: proxy-service-qm22r, namespace: proxy-4736, replica count: 1
I0717 11:57:38.889382      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0717 11:57:39.890628      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:40.891716      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:41.897089      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:42.897629      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:43.898331      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:44.898950      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:45.899469      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:46.899938      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0717 11:57:47.900847      22 runners.go:190] proxy-service-qm22r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 11:57:47.906: INFO: setup took 10.12613141s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 17 11:57:47.923: INFO: (0) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 16.171411ms)
Jul 17 11:57:47.924: INFO: (0) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 17.047959ms)
Jul 17 11:57:47.924: INFO: (0) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 16.797063ms)
Jul 17 11:57:47.929: INFO: (0) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 22.060402ms)
Jul 17 11:57:47.929: INFO: (0) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 21.976157ms)
Jul 17 11:57:47.934: INFO: (0) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 26.438327ms)
Jul 17 11:57:47.934: INFO: (0) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 26.604721ms)
Jul 17 11:57:47.934: INFO: (0) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 26.664674ms)
Jul 17 11:57:47.938: INFO: (0) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 31.548498ms)
Jul 17 11:57:47.939: INFO: (0) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 31.790213ms)
Jul 17 11:57:47.939: INFO: (0) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 31.349128ms)
Jul 17 11:57:47.947: INFO: (0) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 39.791056ms)
Jul 17 11:57:47.947: INFO: (0) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 39.54856ms)
Jul 17 11:57:47.948: INFO: (0) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 40.259264ms)
Jul 17 11:57:47.949: INFO: (0) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 41.265466ms)
Jul 17 11:57:47.952: INFO: (0) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 44.536831ms)
Jul 17 11:57:47.976: INFO: (1) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 22.813948ms)
Jul 17 11:57:47.976: INFO: (1) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 22.668167ms)
Jul 17 11:57:47.977: INFO: (1) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 23.69657ms)
Jul 17 11:57:47.981: INFO: (1) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 27.812654ms)
Jul 17 11:57:47.985: INFO: (1) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 32.38798ms)
Jul 17 11:57:47.985: INFO: (1) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 32.345813ms)
Jul 17 11:57:47.986: INFO: (1) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 32.780672ms)
Jul 17 11:57:47.986: INFO: (1) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 32.471684ms)
Jul 17 11:57:47.986: INFO: (1) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 33.183728ms)
Jul 17 11:57:47.986: INFO: (1) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 33.642115ms)
Jul 17 11:57:47.992: INFO: (1) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 39.709801ms)
Jul 17 11:57:47.993: INFO: (1) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 39.848147ms)
Jul 17 11:57:47.993: INFO: (1) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 39.498065ms)
Jul 17 11:57:47.993: INFO: (1) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 40.641997ms)
Jul 17 11:57:47.993: INFO: (1) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 40.111781ms)
Jul 17 11:57:47.993: INFO: (1) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 39.910384ms)
Jul 17 11:57:48.002: INFO: (2) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 7.776931ms)
Jul 17 11:57:48.002: INFO: (2) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 7.432095ms)
Jul 17 11:57:48.002: INFO: (2) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 8.024456ms)
Jul 17 11:57:48.002: INFO: (2) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 8.552168ms)
Jul 17 11:57:48.003: INFO: (2) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 8.466163ms)
Jul 17 11:57:48.005: INFO: (2) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 11.628868ms)
Jul 17 11:57:48.009: INFO: (2) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 14.840667ms)
Jul 17 11:57:48.010: INFO: (2) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 15.261691ms)
Jul 17 11:57:48.011: INFO: (2) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 16.370123ms)
Jul 17 11:57:48.012: INFO: (2) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 17.470906ms)
Jul 17 11:57:48.013: INFO: (2) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 18.566636ms)
Jul 17 11:57:48.016: INFO: (2) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 21.006972ms)
Jul 17 11:57:48.017: INFO: (2) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 21.747588ms)
Jul 17 11:57:48.018: INFO: (2) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 22.990471ms)
Jul 17 11:57:48.018: INFO: (2) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 23.378243ms)
Jul 17 11:57:48.021: INFO: (2) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 26.250295ms)
Jul 17 11:57:48.034: INFO: (3) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 10.34671ms)
Jul 17 11:57:48.034: INFO: (3) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 11.886604ms)
Jul 17 11:57:48.040: INFO: (3) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 17.250463ms)
Jul 17 11:57:48.040: INFO: (3) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 16.513452ms)
Jul 17 11:57:48.040: INFO: (3) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 17.025277ms)
Jul 17 11:57:48.041: INFO: (3) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 17.996167ms)
Jul 17 11:57:48.043: INFO: (3) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 19.765926ms)
Jul 17 11:57:48.043: INFO: (3) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 20.114554ms)
Jul 17 11:57:48.044: INFO: (3) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 20.302401ms)
Jul 17 11:57:48.045: INFO: (3) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 22.093563ms)
Jul 17 11:57:48.046: INFO: (3) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 22.345036ms)
Jul 17 11:57:48.046: INFO: (3) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 23.280882ms)
Jul 17 11:57:48.047: INFO: (3) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 24.470037ms)
Jul 17 11:57:48.048: INFO: (3) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 24.247923ms)
Jul 17 11:57:48.048: INFO: (3) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 25.08845ms)
Jul 17 11:57:48.049: INFO: (3) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 26.903427ms)
Jul 17 11:57:48.060: INFO: (4) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 11.217826ms)
Jul 17 11:57:48.061: INFO: (4) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 11.754777ms)
Jul 17 11:57:48.061: INFO: (4) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 11.363869ms)
Jul 17 11:57:48.061: INFO: (4) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 11.67253ms)
Jul 17 11:57:48.062: INFO: (4) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 12.380062ms)
Jul 17 11:57:48.062: INFO: (4) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 12.663095ms)
Jul 17 11:57:48.063: INFO: (4) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 12.462639ms)
Jul 17 11:57:48.063: INFO: (4) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 12.56033ms)
Jul 17 11:57:48.063: INFO: (4) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 13.584253ms)
Jul 17 11:57:48.069: INFO: (4) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 18.811508ms)
Jul 17 11:57:48.069: INFO: (4) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 19.730111ms)
Jul 17 11:57:48.070: INFO: (4) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 19.699541ms)
Jul 17 11:57:48.070: INFO: (4) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 20.079175ms)
Jul 17 11:57:48.071: INFO: (4) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 21.112452ms)
Jul 17 11:57:48.071: INFO: (4) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 21.189075ms)
Jul 17 11:57:48.072: INFO: (4) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 22.882218ms)
Jul 17 11:57:48.086: INFO: (5) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 10.900103ms)
Jul 17 11:57:48.087: INFO: (5) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 13.191131ms)
Jul 17 11:57:48.087: INFO: (5) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 12.120569ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 12.782276ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 12.870629ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 13.541227ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 14.519414ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 14.304397ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 13.419883ms)
Jul 17 11:57:48.088: INFO: (5) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.378256ms)
Jul 17 11:57:48.098: INFO: (5) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 24.6132ms)
Jul 17 11:57:48.098: INFO: (5) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 22.855859ms)
Jul 17 11:57:48.098: INFO: (5) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 25.246588ms)
Jul 17 11:57:48.099: INFO: (5) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 23.626333ms)
Jul 17 11:57:48.099: INFO: (5) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 24.196289ms)
Jul 17 11:57:48.099: INFO: (5) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 26.014294ms)
Jul 17 11:57:48.107: INFO: (6) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 7.041577ms)
Jul 17 11:57:48.108: INFO: (6) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 8.299094ms)
Jul 17 11:57:48.108: INFO: (6) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 8.363324ms)
Jul 17 11:57:48.108: INFO: (6) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 8.4608ms)
Jul 17 11:57:48.118: INFO: (6) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 18.13162ms)
Jul 17 11:57:48.120: INFO: (6) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 20.037846ms)
Jul 17 11:57:48.121: INFO: (6) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 20.246507ms)
Jul 17 11:57:48.121: INFO: (6) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 19.932636ms)
Jul 17 11:57:48.121: INFO: (6) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 20.355809ms)
Jul 17 11:57:48.121: INFO: (6) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 20.279476ms)
Jul 17 11:57:48.122: INFO: (6) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 20.702591ms)
Jul 17 11:57:48.122: INFO: (6) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 21.852778ms)
Jul 17 11:57:48.122: INFO: (6) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 21.67606ms)
Jul 17 11:57:48.133: INFO: (6) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 31.767876ms)
Jul 17 11:57:48.136: INFO: (6) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 34.998989ms)
Jul 17 11:57:48.136: INFO: (6) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 35.183285ms)
Jul 17 11:57:48.143: INFO: (7) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 6.498137ms)
Jul 17 11:57:48.151: INFO: (7) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 12.451985ms)
Jul 17 11:57:48.151: INFO: (7) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.370494ms)
Jul 17 11:57:48.151: INFO: (7) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 13.512729ms)
Jul 17 11:57:48.152: INFO: (7) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 13.902186ms)
Jul 17 11:57:48.152: INFO: (7) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 14.047661ms)
Jul 17 11:57:48.153: INFO: (7) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 14.632284ms)
Jul 17 11:57:48.153: INFO: (7) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 14.801749ms)
Jul 17 11:57:48.153: INFO: (7) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 14.611286ms)
Jul 17 11:57:48.154: INFO: (7) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 16.631589ms)
Jul 17 11:57:48.154: INFO: (7) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 16.604456ms)
Jul 17 11:57:48.155: INFO: (7) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 18.25691ms)
Jul 17 11:57:48.156: INFO: (7) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 17.49591ms)
Jul 17 11:57:48.156: INFO: (7) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 16.385748ms)
Jul 17 11:57:48.156: INFO: (7) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 18.230088ms)
Jul 17 11:57:48.157: INFO: (7) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 20.125268ms)
Jul 17 11:57:48.168: INFO: (8) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 10.106441ms)
Jul 17 11:57:48.171: INFO: (8) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 13.030754ms)
Jul 17 11:57:48.173: INFO: (8) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 15.203305ms)
Jul 17 11:57:48.173: INFO: (8) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 15.279689ms)
Jul 17 11:57:48.174: INFO: (8) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 15.824725ms)
Jul 17 11:57:48.174: INFO: (8) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 15.426654ms)
Jul 17 11:57:48.177: INFO: (8) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 19.019637ms)
Jul 17 11:57:48.178: INFO: (8) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 18.909418ms)
Jul 17 11:57:48.178: INFO: (8) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 19.138806ms)
Jul 17 11:57:48.179: INFO: (8) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 19.736379ms)
Jul 17 11:57:48.180: INFO: (8) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 20.913985ms)
Jul 17 11:57:48.180: INFO: (8) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 20.95228ms)
Jul 17 11:57:48.181: INFO: (8) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 21.603712ms)
Jul 17 11:57:48.181: INFO: (8) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 21.992013ms)
Jul 17 11:57:48.182: INFO: (8) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 23.78241ms)
Jul 17 11:57:48.183: INFO: (8) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 24.186638ms)
Jul 17 11:57:48.194: INFO: (9) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 10.431591ms)
Jul 17 11:57:48.213: INFO: (9) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 29.5064ms)
Jul 17 11:57:48.213: INFO: (9) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 29.872898ms)
Jul 17 11:57:48.214: INFO: (9) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 31.101635ms)
Jul 17 11:57:48.214: INFO: (9) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 31.066835ms)
Jul 17 11:57:48.215: INFO: (9) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 31.93227ms)
Jul 17 11:57:48.216: INFO: (9) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 31.897245ms)
Jul 17 11:57:48.216: INFO: (9) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 32.114044ms)
Jul 17 11:57:48.216: INFO: (9) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 32.36431ms)
Jul 17 11:57:48.216: INFO: (9) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 32.218077ms)
Jul 17 11:57:48.216: INFO: (9) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 32.311749ms)
Jul 17 11:57:48.219: INFO: (9) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 35.499244ms)
Jul 17 11:57:48.219: INFO: (9) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 35.934196ms)
Jul 17 11:57:48.219: INFO: (9) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 36.037937ms)
Jul 17 11:57:48.220: INFO: (9) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 36.04142ms)
Jul 17 11:57:48.220: INFO: (9) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 36.11202ms)
Jul 17 11:57:48.229: INFO: (10) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 9.0359ms)
Jul 17 11:57:48.253: INFO: (10) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 32.671055ms)
Jul 17 11:57:48.255: INFO: (10) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 34.54435ms)
Jul 17 11:57:48.255: INFO: (10) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 35.169241ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 35.405494ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 35.791319ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 35.585458ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 35.505108ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 35.620746ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 35.707289ms)
Jul 17 11:57:48.256: INFO: (10) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 35.759661ms)
Jul 17 11:57:48.257: INFO: (10) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 36.80528ms)
Jul 17 11:57:48.258: INFO: (10) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 37.334674ms)
Jul 17 11:57:48.258: INFO: (10) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 37.529161ms)
Jul 17 11:57:48.258: INFO: (10) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 37.655319ms)
Jul 17 11:57:48.258: INFO: (10) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 37.740373ms)
Jul 17 11:57:48.270: INFO: (11) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 11.487252ms)
Jul 17 11:57:48.272: INFO: (11) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 13.305032ms)
Jul 17 11:57:48.272: INFO: (11) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 12.917304ms)
Jul 17 11:57:48.272: INFO: (11) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 12.837386ms)
Jul 17 11:57:48.275: INFO: (11) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 15.984797ms)
Jul 17 11:57:48.276: INFO: (11) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 16.474846ms)
Jul 17 11:57:48.277: INFO: (11) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 18.068268ms)
Jul 17 11:57:48.278: INFO: (11) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 19.117331ms)
Jul 17 11:57:48.278: INFO: (11) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 18.775101ms)
Jul 17 11:57:48.279: INFO: (11) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 20.011832ms)
Jul 17 11:57:48.279: INFO: (11) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 20.273782ms)
Jul 17 11:57:48.282: INFO: (11) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 22.894851ms)
Jul 17 11:57:48.283: INFO: (11) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 23.60816ms)
Jul 17 11:57:48.286: INFO: (11) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 26.25751ms)
Jul 17 11:57:48.286: INFO: (11) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 26.777348ms)
Jul 17 11:57:48.286: INFO: (11) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 26.897935ms)
Jul 17 11:57:48.302: INFO: (12) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 15.057472ms)
Jul 17 11:57:48.303: INFO: (12) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 15.468447ms)
Jul 17 11:57:48.303: INFO: (12) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 15.636138ms)
Jul 17 11:57:48.303: INFO: (12) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 15.652735ms)
Jul 17 11:57:48.304: INFO: (12) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 16.958663ms)
Jul 17 11:57:48.304: INFO: (12) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 16.280315ms)
Jul 17 11:57:48.304: INFO: (12) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 16.987423ms)
Jul 17 11:57:48.304: INFO: (12) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 16.333175ms)
Jul 17 11:57:48.304: INFO: (12) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 17.209537ms)
Jul 17 11:57:48.305: INFO: (12) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 17.573829ms)
Jul 17 11:57:48.310: INFO: (12) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 22.523869ms)
Jul 17 11:57:48.310: INFO: (12) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 23.22947ms)
Jul 17 11:57:48.311: INFO: (12) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 22.975998ms)
Jul 17 11:57:48.311: INFO: (12) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 22.829825ms)
Jul 17 11:57:48.311: INFO: (12) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 23.776343ms)
Jul 17 11:57:48.311: INFO: (12) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 23.410906ms)
Jul 17 11:57:48.317: INFO: (13) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 4.896429ms)
Jul 17 11:57:48.317: INFO: (13) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 5.178488ms)
Jul 17 11:57:48.325: INFO: (13) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 12.350044ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 13.171348ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.205724ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 13.763703ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 13.46279ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 13.881423ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 13.820168ms)
Jul 17 11:57:48.326: INFO: (13) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 14.794064ms)
Jul 17 11:57:48.334: INFO: (13) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 21.479915ms)
Jul 17 11:57:48.338: INFO: (13) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 26.537268ms)
Jul 17 11:57:48.340: INFO: (13) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 27.788524ms)
Jul 17 11:57:48.340: INFO: (13) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 27.88483ms)
Jul 17 11:57:48.340: INFO: (13) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 28.72455ms)
Jul 17 11:57:48.342: INFO: (13) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 29.747997ms)
Jul 17 11:57:48.357: INFO: (14) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 14.163384ms)
Jul 17 11:57:48.358: INFO: (14) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.853063ms)
Jul 17 11:57:48.358: INFO: (14) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 14.273315ms)
Jul 17 11:57:48.358: INFO: (14) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 14.30284ms)
Jul 17 11:57:48.358: INFO: (14) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 14.901583ms)
Jul 17 11:57:48.379: INFO: (14) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 35.733731ms)
Jul 17 11:57:48.379: INFO: (14) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 36.026004ms)
Jul 17 11:57:48.381: INFO: (14) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 36.961437ms)
Jul 17 11:57:48.381: INFO: (14) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 38.206444ms)
Jul 17 11:57:48.381: INFO: (14) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 38.462388ms)
Jul 17 11:57:48.385: INFO: (14) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 41.925757ms)
Jul 17 11:57:48.386: INFO: (14) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 43.623706ms)
Jul 17 11:57:48.386: INFO: (14) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 42.711228ms)
Jul 17 11:57:48.387: INFO: (14) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 43.186911ms)
Jul 17 11:57:48.387: INFO: (14) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 43.159571ms)
Jul 17 11:57:48.387: INFO: (14) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 44.02576ms)
Jul 17 11:57:48.395: INFO: (15) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 8.189747ms)
Jul 17 11:57:48.396: INFO: (15) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 7.385459ms)
Jul 17 11:57:48.401: INFO: (15) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 12.600018ms)
Jul 17 11:57:48.403: INFO: (15) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 13.970614ms)
Jul 17 11:57:48.403: INFO: (15) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 14.160023ms)
Jul 17 11:57:48.404: INFO: (15) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 15.283999ms)
Jul 17 11:57:48.405: INFO: (15) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 16.973432ms)
Jul 17 11:57:48.405: INFO: (15) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 16.154153ms)
Jul 17 11:57:48.405: INFO: (15) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 18.012897ms)
Jul 17 11:57:48.405: INFO: (15) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 17.473708ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 17.620277ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 17.489493ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 17.857253ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 18.121028ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 16.870307ms)
Jul 17 11:57:48.406: INFO: (15) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 18.322399ms)
Jul 17 11:57:48.417: INFO: (16) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 8.935117ms)
Jul 17 11:57:48.418: INFO: (16) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 9.658503ms)
Jul 17 11:57:48.419: INFO: (16) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 10.901377ms)
Jul 17 11:57:48.423: INFO: (16) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 15.142197ms)
Jul 17 11:57:48.425: INFO: (16) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 15.970244ms)
Jul 17 11:57:48.425: INFO: (16) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 16.446793ms)
Jul 17 11:57:48.428: INFO: (16) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 19.177561ms)
Jul 17 11:57:48.428: INFO: (16) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 19.373999ms)
Jul 17 11:57:48.428: INFO: (16) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 19.429497ms)
Jul 17 11:57:48.430: INFO: (16) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 21.874693ms)
Jul 17 11:57:48.430: INFO: (16) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 22.462697ms)
Jul 17 11:57:48.431: INFO: (16) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 22.181302ms)
Jul 17 11:57:48.431: INFO: (16) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 22.435708ms)
Jul 17 11:57:48.431: INFO: (16) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 22.495751ms)
Jul 17 11:57:48.431: INFO: (16) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 22.695232ms)
Jul 17 11:57:48.432: INFO: (16) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 22.982431ms)
Jul 17 11:57:48.446: INFO: (17) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 13.586146ms)
Jul 17 11:57:48.446: INFO: (17) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.377283ms)
Jul 17 11:57:48.446: INFO: (17) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 13.582708ms)
Jul 17 11:57:48.447: INFO: (17) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 14.792847ms)
Jul 17 11:57:48.448: INFO: (17) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 15.226857ms)
Jul 17 11:57:48.448: INFO: (17) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 16.180493ms)
Jul 17 11:57:48.450: INFO: (17) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 17.774367ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 17.831918ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 17.90595ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 18.957977ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 18.804429ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 18.73093ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 18.224299ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 18.356257ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 18.317021ms)
Jul 17 11:57:48.451: INFO: (17) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 18.556638ms)
Jul 17 11:57:48.461: INFO: (18) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 7.499381ms)
Jul 17 11:57:48.461: INFO: (18) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 7.551685ms)
Jul 17 11:57:48.469: INFO: (18) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 14.084411ms)
Jul 17 11:57:48.469: INFO: (18) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 15.339999ms)
Jul 17 11:57:48.469: INFO: (18) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 14.440535ms)
Jul 17 11:57:48.470: INFO: (18) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 16.379891ms)
Jul 17 11:57:48.472: INFO: (18) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 18.904726ms)
Jul 17 11:57:48.472: INFO: (18) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 16.985441ms)
Jul 17 11:57:48.473: INFO: (18) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 18.14412ms)
Jul 17 11:57:48.474: INFO: (18) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 19.341972ms)
Jul 17 11:57:48.474: INFO: (18) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 19.712533ms)
Jul 17 11:57:48.474: INFO: (18) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 19.171239ms)
Jul 17 11:57:48.474: INFO: (18) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 19.409427ms)
Jul 17 11:57:48.476: INFO: (18) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 22.000532ms)
Jul 17 11:57:48.476: INFO: (18) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 22.319783ms)
Jul 17 11:57:48.476: INFO: (18) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 21.850557ms)
Jul 17 11:57:48.492: INFO: (19) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:162/proxy/: bar (200; 15.326323ms)
Jul 17 11:57:48.493: INFO: (19) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:1080/proxy/rewriteme">... (200; 15.89103ms)
Jul 17 11:57:48.493: INFO: (19) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:160/proxy/: foo (200; 16.496084ms)
Jul 17 11:57:48.493: INFO: (19) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:462/proxy/: tls qux (200; 15.952321ms)
Jul 17 11:57:48.497: INFO: (19) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:443/proxy/tlsrewritem... (200; 19.344145ms)
Jul 17 11:57:48.497: INFO: (19) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w:1080/proxy/rewriteme">test<... (200; 20.097282ms)
Jul 17 11:57:48.497: INFO: (19) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:160/proxy/: foo (200; 20.587568ms)
Jul 17 11:57:48.497: INFO: (19) /api/v1/namespaces/proxy-4736/pods/http:proxy-service-qm22r-r968w:162/proxy/: bar (200; 20.151379ms)
Jul 17 11:57:48.497: INFO: (19) /api/v1/namespaces/proxy-4736/pods/https:proxy-service-qm22r-r968w:460/proxy/: tls baz (200; 20.52702ms)
Jul 17 11:57:48.498: INFO: (19) /api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/: <a href="/api/v1/namespaces/proxy-4736/pods/proxy-service-qm22r-r968w/proxy/rewriteme">test</a> (200; 21.401398ms)
Jul 17 11:57:48.501: INFO: (19) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname1/proxy/: foo (200; 24.044186ms)
Jul 17 11:57:48.501: INFO: (19) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname2/proxy/: bar (200; 25.102733ms)
Jul 17 11:57:48.501: INFO: (19) /api/v1/namespaces/proxy-4736/services/http:proxy-service-qm22r:portname2/proxy/: bar (200; 24.174455ms)
Jul 17 11:57:48.502: INFO: (19) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname2/proxy/: tls qux (200; 25.196204ms)
Jul 17 11:57:48.503: INFO: (19) /api/v1/namespaces/proxy-4736/services/proxy-service-qm22r:portname1/proxy/: foo (200; 26.13373ms)
Jul 17 11:57:48.507: INFO: (19) /api/v1/namespaces/proxy-4736/services/https:proxy-service-qm22r:tlsportname1/proxy/: tls baz (200; 30.787022ms)
STEP: deleting ReplicationController proxy-service-qm22r in namespace proxy-4736, will wait for the garbage collector to delete the pods
Jul 17 11:57:48.573: INFO: Deleting ReplicationController proxy-service-qm22r took: 8.504573ms
Jul 17 11:57:49.273: INFO: Terminating ReplicationController proxy-service-qm22r pods took: 700.514051ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:57:51.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4736" for this suite.

• [SLOW TEST:13.549 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":42,"skipped":580,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:57:51.295: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:57:51.383: INFO: Create a RollingUpdate DaemonSet
Jul 17 11:57:51.396: INFO: Check that daemon pods launch on every node of the cluster
Jul 17 11:57:51.408: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:57:51.424: INFO: Number of nodes with available pods: 0
Jul 17 11:57:51.424: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 11:57:52.430: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:57:52.444: INFO: Number of nodes with available pods: 0
Jul 17 11:57:52.444: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 11:57:53.433: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:57:53.438: INFO: Number of nodes with available pods: 2
Jul 17 11:57:53.438: INFO: Number of running nodes: 2, number of available pods: 2
Jul 17 11:57:53.439: INFO: Update the DaemonSet to trigger a rollout
Jul 17 11:57:53.446: INFO: Updating DaemonSet daemon-set
Jul 17 11:58:03.470: INFO: Roll back the DaemonSet before rollout is complete
Jul 17 11:58:03.480: INFO: Updating DaemonSet daemon-set
Jul 17 11:58:03.480: INFO: Make sure DaemonSet rollback is complete
Jul 17 11:58:03.490: INFO: Wrong image for pod: daemon-set-8xfwm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 17 11:58:03.500: INFO: Pod daemon-set-8xfwm is not available
Jul 17 11:58:03.508: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:58:04.514: INFO: Wrong image for pod: daemon-set-8xfwm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 17 11:58:04.515: INFO: Pod daemon-set-8xfwm is not available
Jul 17 11:58:04.521: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:58:05.514: INFO: Wrong image for pod: daemon-set-8xfwm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 17 11:58:05.514: INFO: Pod daemon-set-8xfwm is not available
Jul 17 11:58:05.525: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 11:58:06.514: INFO: Pod daemon-set-t9h9t is not available
Jul 17 11:58:06.521: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-67, will wait for the garbage collector to delete the pods
Jul 17 11:58:06.597: INFO: Deleting DaemonSet.extensions daemon-set took: 10.502156ms
Jul 17 11:58:07.298: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.502816ms
Jul 17 11:58:13.205: INFO: Number of nodes with available pods: 0
Jul 17 11:58:13.205: INFO: Number of running nodes: 0, number of available pods: 0
Jul 17 11:58:13.209: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-67/daemonsets","resourceVersion":"66776"},"items":null}

Jul 17 11:58:13.213: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-67/pods","resourceVersion":"66777"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:58:13.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-67" for this suite.

• [SLOW TEST:21.959 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":43,"skipped":582,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:58:13.258: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-eea1cf8c-b587-4e2c-926b-87cd3961373c
STEP: Creating a pod to test consume configMaps
Jul 17 11:58:13.326: INFO: Waiting up to 5m0s for pod "pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b" in namespace "configmap-932" to be "Succeeded or Failed"
Jul 17 11:58:13.330: INFO: Pod "pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.743659ms
Jul 17 11:58:15.342: INFO: Pod "pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015357613s
STEP: Saw pod success
Jul 17 11:58:15.342: INFO: Pod "pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b" satisfied condition "Succeeded or Failed"
Jul 17 11:58:15.363: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 11:58:15.430: INFO: Waiting for pod pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b to disappear
Jul 17 11:58:15.435: INFO: Pod pod-configmaps-e7dbec34-62a6-4e41-b37f-93b2c6dbc99b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:58:15.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-932" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":44,"skipped":590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:58:15.465: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 11:58:16.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 11:58:18.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119896, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119896, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119896, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762119896, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 11:58:21.788: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:58:21.798: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8622-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:58:23.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9242" for this suite.
STEP: Destroying namespace "webhook-9242-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.878 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":45,"skipped":613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:58:23.352: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-46827707-128a-4dbb-a089-dadfe691391c in namespace container-probe-4948
Jul 17 11:58:25.500: INFO: Started pod busybox-46827707-128a-4dbb-a089-dadfe691391c in namespace container-probe-4948
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 11:58:25.502: INFO: Initial restart count of pod busybox-46827707-128a-4dbb-a089-dadfe691391c is 0
Jul 17 11:59:13.656: INFO: Restart count of pod container-probe-4948/busybox-46827707-128a-4dbb-a089-dadfe691391c is now 1 (48.153420027s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:13.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4948" for this suite.

• [SLOW TEST:50.359 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":46,"skipped":638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:13.725: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 17 11:59:13.788: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 17 11:59:13.796: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 17 11:59:13.796: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 17 11:59:13.810: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 17 11:59:13.810: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 17 11:59:13.826: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 17 11:59:13.826: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 17 11:59:20.885: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:20.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6268" for this suite.

• [SLOW TEST:7.269 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":47,"skipped":674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:21.002: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:59:21.064: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3627
I0717 11:59:21.089852      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3627, replica count: 1
I0717 11:59:22.140789      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0717 11:59:23.141477      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 11:59:23.259: INFO: Created: latency-svc-scq7w
Jul 17 11:59:23.279: INFO: Got endpoints: latency-svc-scq7w [36.922641ms]
Jul 17 11:59:23.311: INFO: Created: latency-svc-v4vn9
Jul 17 11:59:23.336: INFO: Got endpoints: latency-svc-v4vn9 [56.881549ms]
Jul 17 11:59:23.337: INFO: Created: latency-svc-7cllv
Jul 17 11:59:23.348: INFO: Created: latency-svc-m5lr2
Jul 17 11:59:23.364: INFO: Created: latency-svc-z4mpg
Jul 17 11:59:23.365: INFO: Got endpoints: latency-svc-m5lr2 [84.457716ms]
Jul 17 11:59:23.365: INFO: Got endpoints: latency-svc-7cllv [85.460318ms]
Jul 17 11:59:23.367: INFO: Created: latency-svc-hslgl
Jul 17 11:59:23.381: INFO: Got endpoints: latency-svc-z4mpg [101.042062ms]
Jul 17 11:59:23.390: INFO: Got endpoints: latency-svc-hslgl [110.36788ms]
Jul 17 11:59:23.391: INFO: Created: latency-svc-k5hvl
Jul 17 11:59:23.393: INFO: Created: latency-svc-42x7p
Jul 17 11:59:23.403: INFO: Created: latency-svc-r6slg
Jul 17 11:59:23.417: INFO: Created: latency-svc-qtztm
Jul 17 11:59:23.427: INFO: Got endpoints: latency-svc-k5hvl [146.377599ms]
Jul 17 11:59:23.427: INFO: Got endpoints: latency-svc-r6slg [145.802165ms]
Jul 17 11:59:23.428: INFO: Got endpoints: latency-svc-42x7p [61.844864ms]
Jul 17 11:59:23.435: INFO: Created: latency-svc-59n95
Jul 17 11:59:23.456: INFO: Got endpoints: latency-svc-59n95 [175.291535ms]
Jul 17 11:59:23.456: INFO: Got endpoints: latency-svc-qtztm [175.9752ms]
Jul 17 11:59:23.459: INFO: Created: latency-svc-867m7
Jul 17 11:59:23.464: INFO: Created: latency-svc-qdpmb
Jul 17 11:59:23.472: INFO: Got endpoints: latency-svc-867m7 [191.549495ms]
Jul 17 11:59:23.483: INFO: Created: latency-svc-4l9mc
Jul 17 11:59:23.489: INFO: Got endpoints: latency-svc-qdpmb [208.227295ms]
Jul 17 11:59:23.496: INFO: Got endpoints: latency-svc-4l9mc [215.531052ms]
Jul 17 11:59:23.497: INFO: Created: latency-svc-54jc2
Jul 17 11:59:23.511: INFO: Created: latency-svc-s2sdg
Jul 17 11:59:23.516: INFO: Created: latency-svc-snd8s
Jul 17 11:59:23.519: INFO: Got endpoints: latency-svc-54jc2 [237.836369ms]
Jul 17 11:59:23.520: INFO: Got endpoints: latency-svc-s2sdg [238.92301ms]
Jul 17 11:59:23.526: INFO: Got endpoints: latency-svc-snd8s [245.166685ms]
Jul 17 11:59:23.532: INFO: Created: latency-svc-s2snv
Jul 17 11:59:23.540: INFO: Got endpoints: latency-svc-s2snv [204.139125ms]
Jul 17 11:59:23.547: INFO: Created: latency-svc-g9jwt
Jul 17 11:59:23.566: INFO: Created: latency-svc-ssf56
Jul 17 11:59:23.573: INFO: Created: latency-svc-xhwwd
Jul 17 11:59:23.593: INFO: Got endpoints: latency-svc-g9jwt [227.695129ms]
Jul 17 11:59:23.611: INFO: Got endpoints: latency-svc-ssf56 [230.064649ms]
Jul 17 11:59:23.613: INFO: Got endpoints: latency-svc-xhwwd [222.549287ms]
Jul 17 11:59:23.615: INFO: Created: latency-svc-8f4bw
Jul 17 11:59:23.625: INFO: Got endpoints: latency-svc-8f4bw [198.366198ms]
Jul 17 11:59:23.632: INFO: Created: latency-svc-djgpf
Jul 17 11:59:23.639: INFO: Got endpoints: latency-svc-djgpf [211.875932ms]
Jul 17 11:59:23.649: INFO: Created: latency-svc-49xdq
Jul 17 11:59:23.650: INFO: Created: latency-svc-hdk45
Jul 17 11:59:23.669: INFO: Got endpoints: latency-svc-49xdq [239.636666ms]
Jul 17 11:59:23.670: INFO: Got endpoints: latency-svc-hdk45 [212.71483ms]
Jul 17 11:59:23.680: INFO: Created: latency-svc-6255l
Jul 17 11:59:23.687: INFO: Got endpoints: latency-svc-6255l [230.569682ms]
Jul 17 11:59:23.692: INFO: Created: latency-svc-xmtjz
Jul 17 11:59:23.693: INFO: Got endpoints: latency-svc-xmtjz [220.841737ms]
Jul 17 11:59:23.692: INFO: Created: latency-svc-nmn4w
Jul 17 11:59:23.702: INFO: Created: latency-svc-2hgnf
Jul 17 11:59:23.710: INFO: Got endpoints: latency-svc-nmn4w [220.015963ms]
Jul 17 11:59:23.716: INFO: Got endpoints: latency-svc-2hgnf [219.208248ms]
Jul 17 11:59:23.725: INFO: Created: latency-svc-rngz4
Jul 17 11:59:23.729: INFO: Created: latency-svc-skn6j
Jul 17 11:59:23.743: INFO: Created: latency-svc-2zq4r
Jul 17 11:59:23.748: INFO: Got endpoints: latency-svc-rngz4 [227.88895ms]
Jul 17 11:59:23.751: INFO: Created: latency-svc-jhg8t
Jul 17 11:59:23.766: INFO: Got endpoints: latency-svc-skn6j [246.841249ms]
Jul 17 11:59:23.777: INFO: Created: latency-svc-75q6c
Jul 17 11:59:23.780: INFO: Got endpoints: latency-svc-2zq4r [253.022701ms]
Jul 17 11:59:23.780: INFO: Got endpoints: latency-svc-jhg8t [238.819848ms]
Jul 17 11:59:23.803: INFO: Created: latency-svc-pwjgx
Jul 17 11:59:23.803: INFO: Got endpoints: latency-svc-75q6c [210.13935ms]
Jul 17 11:59:23.818: INFO: Created: latency-svc-bzmkw
Jul 17 11:59:23.819: INFO: Got endpoints: latency-svc-pwjgx [207.32767ms]
Jul 17 11:59:23.832: INFO: Created: latency-svc-kkslg
Jul 17 11:59:23.838: INFO: Got endpoints: latency-svc-bzmkw [224.33517ms]
Jul 17 11:59:23.844: INFO: Got endpoints: latency-svc-kkslg [218.480819ms]
Jul 17 11:59:23.846: INFO: Created: latency-svc-b9xqm
Jul 17 11:59:23.858: INFO: Created: latency-svc-lrwh2
Jul 17 11:59:23.872: INFO: Created: latency-svc-g5s22
Jul 17 11:59:23.874: INFO: Got endpoints: latency-svc-b9xqm [235.397123ms]
Jul 17 11:59:23.895: INFO: Got endpoints: latency-svc-lrwh2 [224.546126ms]
Jul 17 11:59:23.899: INFO: Created: latency-svc-ct4mm
Jul 17 11:59:23.905: INFO: Created: latency-svc-7m2jz
Jul 17 11:59:23.909: INFO: Got endpoints: latency-svc-g5s22 [237.200276ms]
Jul 17 11:59:23.934: INFO: Got endpoints: latency-svc-ct4mm [240.858837ms]
Jul 17 11:59:23.936: INFO: Created: latency-svc-rwfzd
Jul 17 11:59:23.936: INFO: Created: latency-svc-8twm2
Jul 17 11:59:23.937: INFO: Got endpoints: latency-svc-7m2jz [244.147756ms]
Jul 17 11:59:23.947: INFO: Created: latency-svc-ztlnx
Jul 17 11:59:23.949: INFO: Got endpoints: latency-svc-8twm2 [238.853685ms]
Jul 17 11:59:23.954: INFO: Created: latency-svc-ms9bk
Jul 17 11:59:23.960: INFO: Created: latency-svc-w75g9
Jul 17 11:59:23.963: INFO: Created: latency-svc-wnm8v
Jul 17 11:59:23.975: INFO: Created: latency-svc-qdpbx
Jul 17 11:59:23.980: INFO: Created: latency-svc-lftnz
Jul 17 11:59:23.983: INFO: Got endpoints: latency-svc-rwfzd [267.066758ms]
Jul 17 11:59:24.001: INFO: Created: latency-svc-dn5p9
Jul 17 11:59:24.001: INFO: Created: latency-svc-qw6z2
Jul 17 11:59:24.009: INFO: Created: latency-svc-t2jrf
Jul 17 11:59:24.030: INFO: Created: latency-svc-5jkjl
Jul 17 11:59:24.031: INFO: Created: latency-svc-qq2m5
Jul 17 11:59:24.031: INFO: Got endpoints: latency-svc-ztlnx [282.574338ms]
Jul 17 11:59:24.034: INFO: Created: latency-svc-pxnd9
Jul 17 11:59:24.040: INFO: Created: latency-svc-26fhr
Jul 17 11:59:24.063: INFO: Created: latency-svc-sf9fd
Jul 17 11:59:24.079: INFO: Got endpoints: latency-svc-ms9bk [313.339444ms]
Jul 17 11:59:24.080: INFO: Created: latency-svc-42ng9
Jul 17 11:59:24.101: INFO: Created: latency-svc-ct8zw
Jul 17 11:59:24.109: INFO: Created: latency-svc-rqxlk
Jul 17 11:59:24.129: INFO: Got endpoints: latency-svc-w75g9 [348.58145ms]
Jul 17 11:59:24.140: INFO: Created: latency-svc-6d786
Jul 17 11:59:24.178: INFO: Got endpoints: latency-svc-wnm8v [397.445575ms]
Jul 17 11:59:24.202: INFO: Created: latency-svc-pgm8f
Jul 17 11:59:24.228: INFO: Got endpoints: latency-svc-qdpbx [423.50068ms]
Jul 17 11:59:24.241: INFO: Created: latency-svc-zsjh6
Jul 17 11:59:24.271: INFO: Got endpoints: latency-svc-lftnz [451.39169ms]
Jul 17 11:59:24.286: INFO: Created: latency-svc-8c9bp
Jul 17 11:59:24.331: INFO: Got endpoints: latency-svc-dn5p9 [493.029336ms]
Jul 17 11:59:24.379: INFO: Created: latency-svc-pp9lm
Jul 17 11:59:24.387: INFO: Got endpoints: latency-svc-qw6z2 [542.557586ms]
Jul 17 11:59:24.398: INFO: Created: latency-svc-n4wvt
Jul 17 11:59:24.424: INFO: Got endpoints: latency-svc-t2jrf [549.596997ms]
Jul 17 11:59:24.442: INFO: Created: latency-svc-zhgmt
Jul 17 11:59:24.477: INFO: Got endpoints: latency-svc-5jkjl [582.142915ms]
Jul 17 11:59:24.495: INFO: Created: latency-svc-94xrr
Jul 17 11:59:24.521: INFO: Got endpoints: latency-svc-qq2m5 [612.434617ms]
Jul 17 11:59:24.534: INFO: Created: latency-svc-kpwsv
Jul 17 11:59:24.574: INFO: Got endpoints: latency-svc-pxnd9 [639.73077ms]
Jul 17 11:59:24.588: INFO: Created: latency-svc-xzglb
Jul 17 11:59:24.623: INFO: Got endpoints: latency-svc-26fhr [685.70415ms]
Jul 17 11:59:24.630: INFO: Created: latency-svc-c85xf
Jul 17 11:59:24.672: INFO: Got endpoints: latency-svc-sf9fd [722.132279ms]
Jul 17 11:59:24.695: INFO: Created: latency-svc-zx6vs
Jul 17 11:59:24.736: INFO: Got endpoints: latency-svc-42ng9 [752.37588ms]
Jul 17 11:59:24.760: INFO: Created: latency-svc-fmkzr
Jul 17 11:59:24.777: INFO: Got endpoints: latency-svc-ct8zw [745.177137ms]
Jul 17 11:59:24.794: INFO: Created: latency-svc-9t8cd
Jul 17 11:59:24.823: INFO: Got endpoints: latency-svc-rqxlk [743.935419ms]
Jul 17 11:59:24.833: INFO: Created: latency-svc-nwq7l
Jul 17 11:59:24.878: INFO: Got endpoints: latency-svc-6d786 [748.681425ms]
Jul 17 11:59:24.894: INFO: Created: latency-svc-lqs79
Jul 17 11:59:24.927: INFO: Got endpoints: latency-svc-pgm8f [748.537275ms]
Jul 17 11:59:24.943: INFO: Created: latency-svc-rscn5
Jul 17 11:59:24.981: INFO: Got endpoints: latency-svc-zsjh6 [753.810946ms]
Jul 17 11:59:24.997: INFO: Created: latency-svc-dzhxk
Jul 17 11:59:25.027: INFO: Got endpoints: latency-svc-8c9bp [756.286481ms]
Jul 17 11:59:25.058: INFO: Created: latency-svc-9ptkw
Jul 17 11:59:25.073: INFO: Got endpoints: latency-svc-pp9lm [739.946824ms]
Jul 17 11:59:25.092: INFO: Created: latency-svc-9mgrx
Jul 17 11:59:25.123: INFO: Got endpoints: latency-svc-n4wvt [736.090717ms]
Jul 17 11:59:25.143: INFO: Created: latency-svc-fw7dh
Jul 17 11:59:25.171: INFO: Got endpoints: latency-svc-zhgmt [746.367169ms]
Jul 17 11:59:25.189: INFO: Created: latency-svc-jl59x
Jul 17 11:59:25.224: INFO: Got endpoints: latency-svc-94xrr [746.43003ms]
Jul 17 11:59:25.237: INFO: Created: latency-svc-7qpqs
Jul 17 11:59:25.273: INFO: Got endpoints: latency-svc-kpwsv [751.11556ms]
Jul 17 11:59:25.288: INFO: Created: latency-svc-kvxlj
Jul 17 11:59:25.325: INFO: Got endpoints: latency-svc-xzglb [750.364392ms]
Jul 17 11:59:25.344: INFO: Created: latency-svc-wzd5q
Jul 17 11:59:25.379: INFO: Got endpoints: latency-svc-c85xf [755.72486ms]
Jul 17 11:59:25.411: INFO: Created: latency-svc-p6b69
Jul 17 11:59:25.424: INFO: Got endpoints: latency-svc-zx6vs [750.856635ms]
Jul 17 11:59:25.436: INFO: Created: latency-svc-xh8ls
Jul 17 11:59:25.471: INFO: Got endpoints: latency-svc-fmkzr [734.886211ms]
Jul 17 11:59:25.485: INFO: Created: latency-svc-q2lxl
Jul 17 11:59:25.522: INFO: Got endpoints: latency-svc-9t8cd [745.199073ms]
Jul 17 11:59:25.530: INFO: Created: latency-svc-5ql4w
Jul 17 11:59:25.571: INFO: Got endpoints: latency-svc-nwq7l [747.67959ms]
Jul 17 11:59:25.590: INFO: Created: latency-svc-5mghw
Jul 17 11:59:25.623: INFO: Got endpoints: latency-svc-lqs79 [744.733022ms]
Jul 17 11:59:25.641: INFO: Created: latency-svc-lxhnr
Jul 17 11:59:25.674: INFO: Got endpoints: latency-svc-rscn5 [746.31479ms]
Jul 17 11:59:25.683: INFO: Created: latency-svc-54lc5
Jul 17 11:59:25.730: INFO: Got endpoints: latency-svc-dzhxk [747.949232ms]
Jul 17 11:59:25.745: INFO: Created: latency-svc-gfgkx
Jul 17 11:59:25.775: INFO: Got endpoints: latency-svc-9ptkw [747.62826ms]
Jul 17 11:59:25.795: INFO: Created: latency-svc-88msw
Jul 17 11:59:25.822: INFO: Got endpoints: latency-svc-9mgrx [748.884609ms]
Jul 17 11:59:25.840: INFO: Created: latency-svc-lwwk6
Jul 17 11:59:25.895: INFO: Got endpoints: latency-svc-fw7dh [771.845329ms]
Jul 17 11:59:25.933: INFO: Got endpoints: latency-svc-jl59x [761.625499ms]
Jul 17 11:59:25.947: INFO: Created: latency-svc-4nbbx
Jul 17 11:59:25.963: INFO: Created: latency-svc-n577p
Jul 17 11:59:25.974: INFO: Got endpoints: latency-svc-7qpqs [749.87424ms]
Jul 17 11:59:25.994: INFO: Created: latency-svc-bvx29
Jul 17 11:59:26.023: INFO: Got endpoints: latency-svc-kvxlj [747.809078ms]
Jul 17 11:59:26.042: INFO: Created: latency-svc-4v8pz
Jul 17 11:59:26.072: INFO: Got endpoints: latency-svc-wzd5q [747.030373ms]
Jul 17 11:59:26.084: INFO: Created: latency-svc-7lpz4
Jul 17 11:59:26.124: INFO: Got endpoints: latency-svc-p6b69 [745.526537ms]
Jul 17 11:59:26.151: INFO: Created: latency-svc-wqbsv
Jul 17 11:59:26.170: INFO: Got endpoints: latency-svc-xh8ls [745.94204ms]
Jul 17 11:59:26.183: INFO: Created: latency-svc-lt5p2
Jul 17 11:59:26.221: INFO: Got endpoints: latency-svc-q2lxl [749.633021ms]
Jul 17 11:59:26.242: INFO: Created: latency-svc-lj7pr
Jul 17 11:59:26.296: INFO: Got endpoints: latency-svc-5ql4w [772.847162ms]
Jul 17 11:59:26.313: INFO: Created: latency-svc-gjq7h
Jul 17 11:59:26.335: INFO: Got endpoints: latency-svc-5mghw [763.311541ms]
Jul 17 11:59:26.350: INFO: Created: latency-svc-jjcxg
Jul 17 11:59:26.379: INFO: Got endpoints: latency-svc-lxhnr [755.758209ms]
Jul 17 11:59:26.405: INFO: Created: latency-svc-hj2wl
Jul 17 11:59:26.423: INFO: Got endpoints: latency-svc-54lc5 [749.154467ms]
Jul 17 11:59:26.444: INFO: Created: latency-svc-qxpqt
Jul 17 11:59:26.478: INFO: Got endpoints: latency-svc-gfgkx [747.966664ms]
Jul 17 11:59:26.495: INFO: Created: latency-svc-vgzhk
Jul 17 11:59:26.523: INFO: Got endpoints: latency-svc-88msw [748.071661ms]
Jul 17 11:59:26.544: INFO: Created: latency-svc-6wgq9
Jul 17 11:59:26.574: INFO: Got endpoints: latency-svc-lwwk6 [751.373651ms]
Jul 17 11:59:26.590: INFO: Created: latency-svc-c8jjn
Jul 17 11:59:26.622: INFO: Got endpoints: latency-svc-4nbbx [725.390484ms]
Jul 17 11:59:26.634: INFO: Created: latency-svc-6bbmb
Jul 17 11:59:26.677: INFO: Got endpoints: latency-svc-n577p [744.464534ms]
Jul 17 11:59:26.697: INFO: Created: latency-svc-4pw8z
Jul 17 11:59:26.729: INFO: Got endpoints: latency-svc-bvx29 [754.914774ms]
Jul 17 11:59:26.743: INFO: Created: latency-svc-z4kgl
Jul 17 11:59:26.772: INFO: Got endpoints: latency-svc-4v8pz [749.4814ms]
Jul 17 11:59:26.788: INFO: Created: latency-svc-ldw22
Jul 17 11:59:26.822: INFO: Got endpoints: latency-svc-7lpz4 [749.256151ms]
Jul 17 11:59:26.838: INFO: Created: latency-svc-fptgn
Jul 17 11:59:26.875: INFO: Got endpoints: latency-svc-wqbsv [750.523712ms]
Jul 17 11:59:26.929: INFO: Got endpoints: latency-svc-lt5p2 [757.763514ms]
Jul 17 11:59:26.930: INFO: Created: latency-svc-qtpr7
Jul 17 11:59:26.943: INFO: Created: latency-svc-t45dn
Jul 17 11:59:26.971: INFO: Got endpoints: latency-svc-lj7pr [749.83164ms]
Jul 17 11:59:27.008: INFO: Created: latency-svc-8lqjz
Jul 17 11:59:27.029: INFO: Got endpoints: latency-svc-gjq7h [732.806797ms]
Jul 17 11:59:27.048: INFO: Created: latency-svc-cj8vm
Jul 17 11:59:27.074: INFO: Got endpoints: latency-svc-jjcxg [739.511392ms]
Jul 17 11:59:27.096: INFO: Created: latency-svc-qssp7
Jul 17 11:59:27.139: INFO: Got endpoints: latency-svc-hj2wl [758.845372ms]
Jul 17 11:59:27.159: INFO: Created: latency-svc-5hrx8
Jul 17 11:59:27.170: INFO: Got endpoints: latency-svc-qxpqt [747.108691ms]
Jul 17 11:59:27.186: INFO: Created: latency-svc-vwk7w
Jul 17 11:59:27.227: INFO: Got endpoints: latency-svc-vgzhk [749.060061ms]
Jul 17 11:59:27.256: INFO: Created: latency-svc-4gkw4
Jul 17 11:59:27.276: INFO: Got endpoints: latency-svc-6wgq9 [752.16314ms]
Jul 17 11:59:27.293: INFO: Created: latency-svc-xc8s8
Jul 17 11:59:27.323: INFO: Got endpoints: latency-svc-c8jjn [748.70205ms]
Jul 17 11:59:27.359: INFO: Created: latency-svc-kt296
Jul 17 11:59:27.375: INFO: Got endpoints: latency-svc-6bbmb [753.893729ms]
Jul 17 11:59:27.388: INFO: Created: latency-svc-ntvtt
Jul 17 11:59:27.425: INFO: Got endpoints: latency-svc-4pw8z [747.797403ms]
Jul 17 11:59:27.437: INFO: Created: latency-svc-h9vr8
Jul 17 11:59:27.474: INFO: Got endpoints: latency-svc-z4kgl [744.718127ms]
Jul 17 11:59:27.485: INFO: Created: latency-svc-n2k5g
Jul 17 11:59:27.525: INFO: Got endpoints: latency-svc-ldw22 [752.629083ms]
Jul 17 11:59:27.541: INFO: Created: latency-svc-4xz4n
Jul 17 11:59:27.577: INFO: Got endpoints: latency-svc-fptgn [755.265122ms]
Jul 17 11:59:27.592: INFO: Created: latency-svc-7qbd2
Jul 17 11:59:27.620: INFO: Got endpoints: latency-svc-qtpr7 [745.37563ms]
Jul 17 11:59:27.631: INFO: Created: latency-svc-wl8mb
Jul 17 11:59:27.682: INFO: Got endpoints: latency-svc-t45dn [752.930538ms]
Jul 17 11:59:27.697: INFO: Created: latency-svc-brw4j
Jul 17 11:59:27.729: INFO: Got endpoints: latency-svc-8lqjz [757.434777ms]
Jul 17 11:59:27.748: INFO: Created: latency-svc-h7r9q
Jul 17 11:59:27.773: INFO: Got endpoints: latency-svc-cj8vm [743.998907ms]
Jul 17 11:59:27.794: INFO: Created: latency-svc-j6lg9
Jul 17 11:59:27.827: INFO: Got endpoints: latency-svc-qssp7 [753.160934ms]
Jul 17 11:59:27.842: INFO: Created: latency-svc-zdtz7
Jul 17 11:59:27.876: INFO: Got endpoints: latency-svc-5hrx8 [737.114061ms]
Jul 17 11:59:27.895: INFO: Created: latency-svc-p6rcv
Jul 17 11:59:27.923: INFO: Got endpoints: latency-svc-vwk7w [751.628517ms]
Jul 17 11:59:27.935: INFO: Created: latency-svc-q2dwq
Jul 17 11:59:27.975: INFO: Got endpoints: latency-svc-4gkw4 [746.721276ms]
Jul 17 11:59:27.986: INFO: Created: latency-svc-kq55g
Jul 17 11:59:28.027: INFO: Got endpoints: latency-svc-xc8s8 [750.898209ms]
Jul 17 11:59:28.038: INFO: Created: latency-svc-vj8s9
Jul 17 11:59:28.072: INFO: Got endpoints: latency-svc-kt296 [748.762405ms]
Jul 17 11:59:28.084: INFO: Created: latency-svc-x9r7t
Jul 17 11:59:28.129: INFO: Got endpoints: latency-svc-ntvtt [753.713413ms]
Jul 17 11:59:28.144: INFO: Created: latency-svc-27jrf
Jul 17 11:59:28.173: INFO: Got endpoints: latency-svc-h9vr8 [748.014611ms]
Jul 17 11:59:28.186: INFO: Created: latency-svc-pr22w
Jul 17 11:59:28.223: INFO: Got endpoints: latency-svc-n2k5g [749.364111ms]
Jul 17 11:59:28.236: INFO: Created: latency-svc-f9prf
Jul 17 11:59:28.273: INFO: Got endpoints: latency-svc-4xz4n [747.901336ms]
Jul 17 11:59:28.287: INFO: Created: latency-svc-q568z
Jul 17 11:59:28.322: INFO: Got endpoints: latency-svc-7qbd2 [745.049027ms]
Jul 17 11:59:28.336: INFO: Created: latency-svc-wjtvs
Jul 17 11:59:28.372: INFO: Got endpoints: latency-svc-wl8mb [751.495574ms]
Jul 17 11:59:28.385: INFO: Created: latency-svc-zmgg2
Jul 17 11:59:28.428: INFO: Got endpoints: latency-svc-brw4j [745.932876ms]
Jul 17 11:59:28.441: INFO: Created: latency-svc-r4b9n
Jul 17 11:59:28.473: INFO: Got endpoints: latency-svc-h7r9q [744.294418ms]
Jul 17 11:59:28.492: INFO: Created: latency-svc-94st2
Jul 17 11:59:28.525: INFO: Got endpoints: latency-svc-j6lg9 [751.94371ms]
Jul 17 11:59:28.543: INFO: Created: latency-svc-jv8k9
Jul 17 11:59:28.572: INFO: Got endpoints: latency-svc-zdtz7 [744.139841ms]
Jul 17 11:59:28.585: INFO: Created: latency-svc-bvvbv
Jul 17 11:59:28.623: INFO: Got endpoints: latency-svc-p6rcv [747.497612ms]
Jul 17 11:59:28.650: INFO: Created: latency-svc-gngmf
Jul 17 11:59:28.674: INFO: Got endpoints: latency-svc-q2dwq [751.151364ms]
Jul 17 11:59:28.682: INFO: Created: latency-svc-2vzjc
Jul 17 11:59:28.746: INFO: Got endpoints: latency-svc-kq55g [770.758129ms]
Jul 17 11:59:28.754: INFO: Created: latency-svc-cxqh8
Jul 17 11:59:28.773: INFO: Got endpoints: latency-svc-vj8s9 [745.730189ms]
Jul 17 11:59:28.784: INFO: Created: latency-svc-x7d9z
Jul 17 11:59:28.821: INFO: Got endpoints: latency-svc-x9r7t [748.90025ms]
Jul 17 11:59:28.834: INFO: Created: latency-svc-wxckv
Jul 17 11:59:28.871: INFO: Got endpoints: latency-svc-27jrf [741.435058ms]
Jul 17 11:59:28.884: INFO: Created: latency-svc-dsvn4
Jul 17 11:59:28.922: INFO: Got endpoints: latency-svc-pr22w [748.240504ms]
Jul 17 11:59:28.934: INFO: Created: latency-svc-wzct5
Jul 17 11:59:28.972: INFO: Got endpoints: latency-svc-f9prf [748.083865ms]
Jul 17 11:59:28.985: INFO: Created: latency-svc-h5mhf
Jul 17 11:59:29.028: INFO: Got endpoints: latency-svc-q568z [753.601725ms]
Jul 17 11:59:29.037: INFO: Created: latency-svc-pnvws
Jul 17 11:59:29.072: INFO: Got endpoints: latency-svc-wjtvs [749.347236ms]
Jul 17 11:59:29.088: INFO: Created: latency-svc-hd9pl
Jul 17 11:59:29.122: INFO: Got endpoints: latency-svc-zmgg2 [749.436489ms]
Jul 17 11:59:29.138: INFO: Created: latency-svc-zpxft
Jul 17 11:59:29.172: INFO: Got endpoints: latency-svc-r4b9n [744.177583ms]
Jul 17 11:59:29.184: INFO: Created: latency-svc-l7g2b
Jul 17 11:59:29.221: INFO: Got endpoints: latency-svc-94st2 [747.639492ms]
Jul 17 11:59:29.234: INFO: Created: latency-svc-npss5
Jul 17 11:59:29.291: INFO: Got endpoints: latency-svc-jv8k9 [765.078788ms]
Jul 17 11:59:29.311: INFO: Created: latency-svc-wwf74
Jul 17 11:59:29.330: INFO: Got endpoints: latency-svc-bvvbv [758.130725ms]
Jul 17 11:59:29.338: INFO: Created: latency-svc-7w862
Jul 17 11:59:29.381: INFO: Got endpoints: latency-svc-gngmf [757.570582ms]
Jul 17 11:59:29.394: INFO: Created: latency-svc-p2ts2
Jul 17 11:59:29.427: INFO: Got endpoints: latency-svc-2vzjc [752.924834ms]
Jul 17 11:59:29.440: INFO: Created: latency-svc-rwmdq
Jul 17 11:59:29.475: INFO: Got endpoints: latency-svc-cxqh8 [728.402987ms]
Jul 17 11:59:29.487: INFO: Created: latency-svc-824jc
Jul 17 11:59:29.520: INFO: Got endpoints: latency-svc-x7d9z [747.052259ms]
Jul 17 11:59:29.535: INFO: Created: latency-svc-mvlg7
Jul 17 11:59:29.570: INFO: Got endpoints: latency-svc-wxckv [748.795871ms]
Jul 17 11:59:29.582: INFO: Created: latency-svc-x8jph
Jul 17 11:59:29.626: INFO: Got endpoints: latency-svc-dsvn4 [754.184929ms]
Jul 17 11:59:29.637: INFO: Created: latency-svc-mmbtq
Jul 17 11:59:29.673: INFO: Got endpoints: latency-svc-wzct5 [751.321611ms]
Jul 17 11:59:29.687: INFO: Created: latency-svc-v4vqc
Jul 17 11:59:29.728: INFO: Got endpoints: latency-svc-h5mhf [755.608762ms]
Jul 17 11:59:29.739: INFO: Created: latency-svc-c5xgt
Jul 17 11:59:29.782: INFO: Got endpoints: latency-svc-pnvws [754.330069ms]
Jul 17 11:59:29.798: INFO: Created: latency-svc-b9xp7
Jul 17 11:59:29.825: INFO: Got endpoints: latency-svc-hd9pl [753.102499ms]
Jul 17 11:59:29.846: INFO: Created: latency-svc-bvkrb
Jul 17 11:59:29.872: INFO: Got endpoints: latency-svc-zpxft [749.107789ms]
Jul 17 11:59:29.888: INFO: Created: latency-svc-8vrlg
Jul 17 11:59:29.922: INFO: Got endpoints: latency-svc-l7g2b [749.58819ms]
Jul 17 11:59:29.940: INFO: Created: latency-svc-qbz8r
Jul 17 11:59:29.972: INFO: Got endpoints: latency-svc-npss5 [750.72972ms]
Jul 17 11:59:30.003: INFO: Created: latency-svc-9trx8
Jul 17 11:59:30.036: INFO: Got endpoints: latency-svc-wwf74 [744.78036ms]
Jul 17 11:59:30.056: INFO: Created: latency-svc-lc7rc
Jul 17 11:59:30.077: INFO: Got endpoints: latency-svc-7w862 [746.180301ms]
Jul 17 11:59:30.093: INFO: Created: latency-svc-7vrfn
Jul 17 11:59:30.127: INFO: Got endpoints: latency-svc-p2ts2 [745.117404ms]
Jul 17 11:59:30.187: INFO: Created: latency-svc-w6wxp
Jul 17 11:59:30.193: INFO: Got endpoints: latency-svc-rwmdq [765.729338ms]
Jul 17 11:59:30.212: INFO: Created: latency-svc-bkwt5
Jul 17 11:59:30.238: INFO: Got endpoints: latency-svc-824jc [763.236215ms]
Jul 17 11:59:30.255: INFO: Created: latency-svc-xq9tz
Jul 17 11:59:30.287: INFO: Got endpoints: latency-svc-mvlg7 [766.324946ms]
Jul 17 11:59:30.303: INFO: Created: latency-svc-lqm4x
Jul 17 11:59:30.324: INFO: Got endpoints: latency-svc-x8jph [753.550204ms]
Jul 17 11:59:30.348: INFO: Created: latency-svc-mjf7j
Jul 17 11:59:30.384: INFO: Got endpoints: latency-svc-mmbtq [757.893148ms]
Jul 17 11:59:30.401: INFO: Created: latency-svc-ntnrg
Jul 17 11:59:30.422: INFO: Got endpoints: latency-svc-v4vqc [747.724913ms]
Jul 17 11:59:30.441: INFO: Created: latency-svc-8bpnv
Jul 17 11:59:30.472: INFO: Got endpoints: latency-svc-c5xgt [744.262666ms]
Jul 17 11:59:30.486: INFO: Created: latency-svc-dl8sz
Jul 17 11:59:30.527: INFO: Got endpoints: latency-svc-b9xp7 [745.218512ms]
Jul 17 11:59:30.538: INFO: Created: latency-svc-dfpz4
Jul 17 11:59:30.573: INFO: Got endpoints: latency-svc-bvkrb [747.550333ms]
Jul 17 11:59:30.587: INFO: Created: latency-svc-2mrrw
Jul 17 11:59:30.633: INFO: Got endpoints: latency-svc-8vrlg [760.386904ms]
Jul 17 11:59:30.653: INFO: Created: latency-svc-8tc72
Jul 17 11:59:30.677: INFO: Got endpoints: latency-svc-qbz8r [755.161598ms]
Jul 17 11:59:30.690: INFO: Created: latency-svc-2bzxg
Jul 17 11:59:30.736: INFO: Got endpoints: latency-svc-9trx8 [763.0685ms]
Jul 17 11:59:30.755: INFO: Created: latency-svc-dsw5h
Jul 17 11:59:30.774: INFO: Got endpoints: latency-svc-lc7rc [738.080215ms]
Jul 17 11:59:30.799: INFO: Created: latency-svc-5hlq2
Jul 17 11:59:30.827: INFO: Got endpoints: latency-svc-7vrfn [750.348396ms]
Jul 17 11:59:30.837: INFO: Created: latency-svc-7xsml
Jul 17 11:59:30.886: INFO: Got endpoints: latency-svc-w6wxp [758.920353ms]
Jul 17 11:59:30.901: INFO: Created: latency-svc-drxpq
Jul 17 11:59:30.923: INFO: Got endpoints: latency-svc-bkwt5 [729.669159ms]
Jul 17 11:59:30.940: INFO: Created: latency-svc-kgc42
Jul 17 11:59:30.975: INFO: Got endpoints: latency-svc-xq9tz [736.594474ms]
Jul 17 11:59:30.996: INFO: Created: latency-svc-cqn9w
Jul 17 11:59:31.024: INFO: Got endpoints: latency-svc-lqm4x [735.424454ms]
Jul 17 11:59:31.047: INFO: Created: latency-svc-wpbn5
Jul 17 11:59:31.078: INFO: Got endpoints: latency-svc-mjf7j [754.249415ms]
Jul 17 11:59:31.091: INFO: Created: latency-svc-74kjw
Jul 17 11:59:31.130: INFO: Got endpoints: latency-svc-ntnrg [746.377406ms]
Jul 17 11:59:31.177: INFO: Got endpoints: latency-svc-8bpnv [754.400769ms]
Jul 17 11:59:31.226: INFO: Got endpoints: latency-svc-dl8sz [754.29885ms]
Jul 17 11:59:31.281: INFO: Got endpoints: latency-svc-dfpz4 [753.211232ms]
Jul 17 11:59:31.321: INFO: Got endpoints: latency-svc-2mrrw [748.211365ms]
Jul 17 11:59:31.377: INFO: Got endpoints: latency-svc-8tc72 [744.381884ms]
Jul 17 11:59:31.439: INFO: Got endpoints: latency-svc-2bzxg [761.694078ms]
Jul 17 11:59:31.474: INFO: Got endpoints: latency-svc-dsw5h [737.800311ms]
Jul 17 11:59:31.527: INFO: Got endpoints: latency-svc-5hlq2 [753.049926ms]
Jul 17 11:59:31.575: INFO: Got endpoints: latency-svc-7xsml [747.614493ms]
Jul 17 11:59:31.625: INFO: Got endpoints: latency-svc-drxpq [738.251226ms]
Jul 17 11:59:31.679: INFO: Got endpoints: latency-svc-kgc42 [756.071978ms]
Jul 17 11:59:31.730: INFO: Got endpoints: latency-svc-cqn9w [754.847595ms]
Jul 17 11:59:31.780: INFO: Got endpoints: latency-svc-wpbn5 [755.503718ms]
Jul 17 11:59:31.826: INFO: Got endpoints: latency-svc-74kjw [747.48584ms]
Jul 17 11:59:31.826: INFO: Latencies: [56.881549ms 61.844864ms 84.457716ms 85.460318ms 101.042062ms 110.36788ms 145.802165ms 146.377599ms 175.291535ms 175.9752ms 191.549495ms 198.366198ms 204.139125ms 207.32767ms 208.227295ms 210.13935ms 211.875932ms 212.71483ms 215.531052ms 218.480819ms 219.208248ms 220.015963ms 220.841737ms 222.549287ms 224.33517ms 224.546126ms 227.695129ms 227.88895ms 230.064649ms 230.569682ms 235.397123ms 237.200276ms 237.836369ms 238.819848ms 238.853685ms 238.92301ms 239.636666ms 240.858837ms 244.147756ms 245.166685ms 246.841249ms 253.022701ms 267.066758ms 282.574338ms 313.339444ms 348.58145ms 397.445575ms 423.50068ms 451.39169ms 493.029336ms 542.557586ms 549.596997ms 582.142915ms 612.434617ms 639.73077ms 685.70415ms 722.132279ms 725.390484ms 728.402987ms 729.669159ms 732.806797ms 734.886211ms 735.424454ms 736.090717ms 736.594474ms 737.114061ms 737.800311ms 738.080215ms 738.251226ms 739.511392ms 739.946824ms 741.435058ms 743.935419ms 743.998907ms 744.139841ms 744.177583ms 744.262666ms 744.294418ms 744.381884ms 744.464534ms 744.718127ms 744.733022ms 744.78036ms 745.049027ms 745.117404ms 745.177137ms 745.199073ms 745.218512ms 745.37563ms 745.526537ms 745.730189ms 745.932876ms 745.94204ms 746.180301ms 746.31479ms 746.367169ms 746.377406ms 746.43003ms 746.721276ms 747.030373ms 747.052259ms 747.108691ms 747.48584ms 747.497612ms 747.550333ms 747.614493ms 747.62826ms 747.639492ms 747.67959ms 747.724913ms 747.797403ms 747.809078ms 747.901336ms 747.949232ms 747.966664ms 748.014611ms 748.071661ms 748.083865ms 748.211365ms 748.240504ms 748.537275ms 748.681425ms 748.70205ms 748.762405ms 748.795871ms 748.884609ms 748.90025ms 749.060061ms 749.107789ms 749.154467ms 749.256151ms 749.347236ms 749.364111ms 749.436489ms 749.4814ms 749.58819ms 749.633021ms 749.83164ms 749.87424ms 750.348396ms 750.364392ms 750.523712ms 750.72972ms 750.856635ms 750.898209ms 751.11556ms 751.151364ms 751.321611ms 751.373651ms 751.495574ms 751.628517ms 751.94371ms 752.16314ms 752.37588ms 752.629083ms 752.924834ms 752.930538ms 753.049926ms 753.102499ms 753.160934ms 753.211232ms 753.550204ms 753.601725ms 753.713413ms 753.810946ms 753.893729ms 754.184929ms 754.249415ms 754.29885ms 754.330069ms 754.400769ms 754.847595ms 754.914774ms 755.161598ms 755.265122ms 755.503718ms 755.608762ms 755.72486ms 755.758209ms 756.071978ms 756.286481ms 757.434777ms 757.570582ms 757.763514ms 757.893148ms 758.130725ms 758.845372ms 758.920353ms 760.386904ms 761.625499ms 761.694078ms 763.0685ms 763.236215ms 763.311541ms 765.078788ms 765.729338ms 766.324946ms 770.758129ms 771.845329ms 772.847162ms]
Jul 17 11:59:31.827: INFO: 50 %ile: 747.052259ms
Jul 17 11:59:31.828: INFO: 90 %ile: 756.286481ms
Jul 17 11:59:31.828: INFO: 99 %ile: 771.845329ms
Jul 17 11:59:31.828: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:31.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3627" for this suite.

• [SLOW TEST:10.876 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":48,"skipped":700,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:31.879: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 17 11:59:34.512: INFO: Successfully updated pod "annotationupdate21c02fd6-8a69-4b2b-b603-4629559a4001"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:36.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1756" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Jul 17 11:59:36.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-4483 api-versions'
Jul 17 11:59:36.746: INFO: stderr: ""
Jul 17 11:59:36.746: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:36.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4483" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":50,"skipped":756,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 11:59:36.837: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49" in namespace "projected-7229" to be "Succeeded or Failed"
Jul 17 11:59:36.846: INFO: Pod "downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49": Phase="Pending", Reason="", readiness=false. Elapsed: 8.770217ms
Jul 17 11:59:38.852: INFO: Pod "downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014283827s
STEP: Saw pod success
Jul 17 11:59:38.852: INFO: Pod "downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49" satisfied condition "Succeeded or Failed"
Jul 17 11:59:38.861: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49 container client-container: <nil>
STEP: delete the pod
Jul 17 11:59:38.961: INFO: Waiting for pod downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49 to disappear
Jul 17 11:59:38.969: INFO: Pod downwardapi-volume-8924f422-2d4d-448d-a888-a36f80e4dc49 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:38.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7229" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":51,"skipped":757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:39.013: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 17 11:59:39.140: INFO: Waiting up to 5m0s for pod "pod-5d6741d6-4a33-4a50-a053-9a95aa90392e" in namespace "emptydir-6795" to be "Succeeded or Failed"
Jul 17 11:59:39.162: INFO: Pod "pod-5d6741d6-4a33-4a50-a053-9a95aa90392e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.025132ms
Jul 17 11:59:41.173: INFO: Pod "pod-5d6741d6-4a33-4a50-a053-9a95aa90392e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031608244s
STEP: Saw pod success
Jul 17 11:59:41.173: INFO: Pod "pod-5d6741d6-4a33-4a50-a053-9a95aa90392e" satisfied condition "Succeeded or Failed"
Jul 17 11:59:41.179: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-5d6741d6-4a33-4a50-a053-9a95aa90392e container test-container: <nil>
STEP: delete the pod
Jul 17 11:59:41.214: INFO: Waiting for pod pod-5d6741d6-4a33-4a50-a053-9a95aa90392e to disappear
Jul 17 11:59:41.223: INFO: Pod pod-5d6741d6-4a33-4a50-a053-9a95aa90392e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:41.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6795" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":805,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:41.244: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:41.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8898" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":53,"skipped":820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:41.439: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 11:59:41.511: INFO: Creating deployment "webserver-deployment"
Jul 17 11:59:41.520: INFO: Waiting for observed generation 1
Jul 17 11:59:43.604: INFO: Waiting for all required pods to come up
Jul 17 11:59:43.654: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 17 11:59:47.782: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 17 11:59:47.795: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 17 11:59:47.810: INFO: Updating deployment webserver-deployment
Jul 17 11:59:47.810: INFO: Waiting for observed generation 2
Jul 17 11:59:49.822: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 17 11:59:49.825: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 17 11:59:49.827: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 17 11:59:49.839: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 17 11:59:49.839: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 17 11:59:49.841: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 17 11:59:49.847: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 17 11:59:49.847: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 17 11:59:49.857: INFO: Updating deployment webserver-deployment
Jul 17 11:59:49.857: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 17 11:59:49.869: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 17 11:59:51.899: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 17 11:59:51.918: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7406 /apis/apps/v1/namespaces/deployment-7406/deployments/webserver-deployment a2dd653a-bc1c-4fcb-b1e7-3c46163fb764 69682 3 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d735a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-17 11:59:49 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-17 11:59:50 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 17 11:59:51.941: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7406 /apis/apps/v1/namespaces/deployment-7406/replicasets/webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 69667 3 2021-07-17 11:59:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a2dd653a-bc1c-4fcb-b1e7-3c46163fb764 0xc003cfe1f7 0xc003cfe1f8}] []  [{kube-controller-manager Update apps/v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2dd653a-bc1c-4fcb-b1e7-3c46163fb764\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfe278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 11:59:51.941: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 17 11:59:51.942: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-7406 /apis/apps/v1/namespaces/deployment-7406/replicasets/webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 69672 3 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a2dd653a-bc1c-4fcb-b1e7-3c46163fb764 0xc003cfe2d7 0xc003cfe2d8}] []  [{kube-controller-manager Update apps/v1 2021-07-17 11:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2dd653a-bc1c-4fcb-b1e7-3c46163fb764\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cfe348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 17 11:59:51.960: INFO: Pod "webserver-deployment-795d758f88-2nsw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2nsw5 webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-2nsw5 b3555626-4aa8-4b70-b97f-6902fc715fa3 69697 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003d73a17 0xc003d73a18}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.961: INFO: Pod "webserver-deployment-795d758f88-4rzzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4rzzk webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-4rzzk 33641f35-6772-4677-bb4d-563b5ffb15ff 69570 0 2021-07-17 11:59:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.69.72/32 cni.projectcalico.org/podIPs:10.233.69.72/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003d73bd7 0xc003d73bd8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.961: INFO: Pod "webserver-deployment-795d758f88-7452p" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7452p webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-7452p d7ce7c29-0e46-4ef6-b416-05cf77dbd8dc 69666 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003d73d87 0xc003d73d88}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.961: INFO: Pod "webserver-deployment-795d758f88-7jllj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7jllj webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-7jllj a5a9ee6e-a20d-4d53-87c8-352e7649450f 69653 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003d73eb0 0xc003d73eb1}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.961: INFO: Pod "webserver-deployment-795d758f88-ggwjm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ggwjm webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-ggwjm d0391e80-0525-4c21-a0e6-a3705feb35d3 69590 0 2021-07-17 11:59:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.64.195/32 cni.projectcalico.org/podIPs:10.233.64.195/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6067 0xc003ea6068}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.962: INFO: Pod "webserver-deployment-795d758f88-przgr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-przgr webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-przgr 17fc4767-c24a-43d4-86e0-39574c8fa4c6 69691 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6217 0xc003ea6218}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.962: INFO: Pod "webserver-deployment-795d758f88-rsvzr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rsvzr webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-rsvzr 5d8187dd-9f0e-4b76-bd48-f11d7210b6a9 69735 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.64.197/32 cni.projectcalico.org/podIPs:10.233.64.197/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea63d7 0xc003ea63d8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.962: INFO: Pod "webserver-deployment-795d758f88-tnxsb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tnxsb webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-tnxsb 5909df5e-3d4f-4b4d-bb09-e787f15eb83f 69576 0 2021-07-17 11:59:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.64.194/32 cni.projectcalico.org/podIPs:10.233.64.194/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea65a7 0xc003ea65a8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.967: INFO: Pod "webserver-deployment-795d758f88-twvrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-twvrp webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-twvrp bc33be2b-264d-44d0-a0c7-026ecbbed599 69699 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6757 0xc003ea6758}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.967: INFO: Pod "webserver-deployment-795d758f88-vxd9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vxd9q webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-vxd9q 6fad41ee-67b2-40ee-bb2c-318490f80d6a 69585 0 2021-07-17 11:59:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.69.73/32 cni.projectcalico.org/podIPs:10.233.69.73/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6917 0xc003ea6918}] []  [{calico Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.968: INFO: Pod "webserver-deployment-795d758f88-wrxtf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wrxtf webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-wrxtf 461b4f19-07dd-4c8d-a7d0-ebb4a5509602 69705 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6ac7 0xc003ea6ac8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.968: INFO: Pod "webserver-deployment-795d758f88-wxpm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wxpm4 webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-wxpm4 56d30235-b330-4799-82eb-d1027069e396 69713 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6c67 0xc003ea6c68}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.968: INFO: Pod "webserver-deployment-795d758f88-xwlb8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xwlb8 webserver-deployment-795d758f88- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-795d758f88-xwlb8 ff4ec92e-aed5-48c2-91d6-ca654b223dcf 69587 0 2021-07-17 11:59:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.233.64.193/32 cni.projectcalico.org/podIPs:10.233.64.193/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 56296218-08e5-4b60-9dfc-40d2227b6edb 0xc003ea6e27 0xc003ea6e28}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56296218-08e5-4b60-9dfc-40d2227b6edb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.968: INFO: Pod "webserver-deployment-dd94f59b7-62zft" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-62zft webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-62zft e24c7094-b5ec-488f-9782-bfedcc47358a 69141 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.69/32 cni.projectcalico.org/podIPs:10.233.69.69/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea6ff7 0xc003ea6ff8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:10.233.69.69,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://5019b5150d93b09c46b69de254def7520e27f5a0156391a96e93ea107b9695d8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.969: INFO: Pod "webserver-deployment-dd94f59b7-9pspx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9pspx webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-9pspx 481dade0-2564-4573-af7d-d62af5df77ae 69272 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.64.191/32 cni.projectcalico.org/podIPs:10.233.64.191/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea71c7 0xc003ea71c8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.191,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8dbda82e7b81d749977a189258f4ac068895d3d44e358d7d595b5e7c0ac51d3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.969: INFO: Pod "webserver-deployment-dd94f59b7-cf2lj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cf2lj webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-cf2lj c3092e71-2825-475b-8a69-75eccdd07ee7 69288 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.64.188/32 cni.projectcalico.org/podIPs:10.233.64.188/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7397 0xc003ea7398}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.188,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://f8ff387a9b1f32c86acb18717ed260e206e50467276e6fc092bf83a64631be3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.969: INFO: Pod "webserver-deployment-dd94f59b7-cgrwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cgrwf webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-cgrwf 52eb436d-c24b-49be-9988-5e40bde8aa3f 69745 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7547 0xc003ea7548}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.970: INFO: Pod "webserver-deployment-dd94f59b7-ct84d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ct84d webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-ct84d a653cd24-ea38-4f17-9621-e27a643ea055 69695 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea76c7 0xc003ea76c8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.970: INFO: Pod "webserver-deployment-dd94f59b7-fjpjc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fjpjc webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-fjpjc 484f013c-dfb7-4d73-8a7d-41874c79ab77 69693 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7847 0xc003ea7848}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.970: INFO: Pod "webserver-deployment-dd94f59b7-hrv4w" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-hrv4w webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-hrv4w 070f282d-8230-46db-b103-d6ee22ecb291 69684 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea79c7 0xc003ea79c8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.970: INFO: Pod "webserver-deployment-dd94f59b7-ksm9s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ksm9s webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-ksm9s 29403925-b563-40da-ae77-ca28b3660dd6 69723 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7b47 0xc003ea7b48}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.971: INFO: Pod "webserver-deployment-dd94f59b7-mxz6n" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mxz6n webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-mxz6n fb674af6-7331-45f2-bcea-12a01a68601c 69277 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.64.192/32 cni.projectcalico.org/podIPs:10.233.64.192/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7ce7 0xc003ea7ce8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.192,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://fce20e2029d0c16d5e7929645e336601f6416d7bd8dc544ed101826e46eb601a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.972: INFO: Pod "webserver-deployment-dd94f59b7-n626t" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n626t webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-n626t ffb2aeae-0da7-422c-a1db-c18dc9519d2f 69712 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.64.196/32 cni.projectcalico.org/podIPs:10.233.64.196/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ea7eb7 0xc003ea7eb8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.973: INFO: Pod "webserver-deployment-dd94f59b7-ndwpf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ndwpf webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-ndwpf 3c412808-834c-43cc-b5e7-64ebc7f650be 69241 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.71/32 cni.projectcalico.org/podIPs:10.233.69.71/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2067 0xc003ed2068}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:10.233.69.71,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://6643a072acd190b4df4d2f7fe90337227567a6847c6e611697ef775950b050ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.973: INFO: Pod "webserver-deployment-dd94f59b7-qwwcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qwwcs webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-qwwcs 638e5482-1386-43f9-ba28-71f007edda19 69734 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2217 0xc003ed2218}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.976: INFO: Pod "webserver-deployment-dd94f59b7-r5c2m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r5c2m webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-r5c2m fe576308-e6d4-4e92-948e-4c643a25ff5c 69133 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.68/32 cni.projectcalico.org/podIPs:10.233.69.68/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed23b7 0xc003ed23b8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:10.233.69.68,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://bbf1285bd4bd5bf794ff56dbeac384037b9f2ce1c266ddbda57a0756a5c814c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.981: INFO: Pod "webserver-deployment-dd94f59b7-r9czr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-r9czr webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-r9czr bfbfca80-f431-4710-af9c-fb0357fc5052 69236 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.70/32 cni.projectcalico.org/podIPs:10.233.69.70/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2587 0xc003ed2588}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:10.233.69.70,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://81bc633cf678634afc6e8c3b85b92ee57e8df2be745c5a9911c3fe3fe1c981c0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.981: INFO: Pod "webserver-deployment-dd94f59b7-rcwt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rcwt4 webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-rcwt4 960e0f4e-f085-4779-83db-ac87c6fa4f28 69737 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.75/32 cni.projectcalico.org/podIPs:10.233.69.75/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2757 0xc003ed2758}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.986: INFO: Pod "webserver-deployment-dd94f59b7-rktz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rktz9 webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-rktz9 e22ab16e-c940-4b04-a593-9cc9afa3a024 69690 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed28e7 0xc003ed28e8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:51.991: INFO: Pod "webserver-deployment-dd94f59b7-rxcx4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rxcx4 webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-rxcx4 af0f650a-71c9-4a55-86e2-44f1ae6ee9e2 69738 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2a67 0xc003ed2a68}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:52.002: INFO: Pod "webserver-deployment-dd94f59b7-tvpfd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tvpfd webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-tvpfd 249a6ce2-3b2c-48ad-b0b6-c128d6cf1d8e 69177 0 2021-07-17 11:59:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.64.185/32 cni.projectcalico.org/podIPs:10.233.64.185/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2c07 0xc003ed2c08}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 11:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 11:59:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.185,StartTime:2021-07-17 11:59:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 11:59:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://1216bd9c8a62fd0ea083e7e97fd5ee52fbfecd8b2935f556da5a1d118fca5cb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.185,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:52.002: INFO: Pod "webserver-deployment-dd94f59b7-w9c4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w9c4s webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-w9c4s 2ea7087e-b4a2-4da1-aaa4-3acd60660858 69715 0 2021-07-17 11:59:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2db7 0xc003ed2db8}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 11:59:52.003: INFO: Pod "webserver-deployment-dd94f59b7-zcfkf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zcfkf webserver-deployment-dd94f59b7- deployment-7406 /api/v1/namespaces/deployment-7406/pods/webserver-deployment-dd94f59b7-zcfkf 4899265f-52da-4e45-8d68-c968e48835a0 69728 0 2021-07-17 11:59:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.233.69.74/32 cni.projectcalico.org/podIPs:10.233.69.74/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 ea998692-1148-4184-ae35-b7eaf5babf98 0xc003ed2f57 0xc003ed2f58}] []  [{kube-controller-manager Update v1 2021-07-17 11:59:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea998692-1148-4184-ae35-b7eaf5babf98\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 11:59:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-17 11:59:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tzdv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tzdv6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tzdv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 11:59:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.67,PodIP:,StartTime:2021-07-17 11:59:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:52.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7406" for this suite.

• [SLOW TEST:10.607 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":54,"skipped":847,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:52.048: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 17 11:59:52.702: INFO: Waiting up to 5m0s for pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921" in namespace "emptydir-3343" to be "Succeeded or Failed"
Jul 17 11:59:52.774: INFO: Pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921": Phase="Pending", Reason="", readiness=false. Elapsed: 72.205358ms
Jul 17 11:59:54.780: INFO: Pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07837747s
Jul 17 11:59:56.783: INFO: Pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081738644s
Jul 17 11:59:58.798: INFO: Pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096324759s
STEP: Saw pod success
Jul 17 11:59:58.798: INFO: Pod "pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921" satisfied condition "Succeeded or Failed"
Jul 17 11:59:58.812: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921 container test-container: <nil>
STEP: delete the pod
Jul 17 11:59:58.847: INFO: Waiting for pod pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921 to disappear
Jul 17 11:59:58.860: INFO: Pod pod-3c1841aa-016b-4d9a-a03a-d5eb19cf0921 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:58.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3343" for this suite.

• [SLOW TEST:6.878 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":55,"skipped":850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-0e60a4d6-eac1-4754-a5e4-66bdd9862665
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 11:59:59.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5545" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":56,"skipped":909,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 11:59:59.052: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:00:12.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6487" for this suite.

• [SLOW TEST:13.166 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":57,"skipped":910,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:00:12.220: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 17 12:00:12.477: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70347 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:00:12.478: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70347 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 17 12:00:22.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70394 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:00:22.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70394 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 17 12:00:32.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70432 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:00:32.508: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70432 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 17 12:00:42.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70470 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:00:42.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-a e775b3ea-da06-4ab7-b49f-701414b797df 70470 0 2021-07-17 12:00:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 17 12:00:52.550: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-b cbac9f1c-7329-4516-9443-1eec556c2002 70508 0 2021-07-17 12:00:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:00:52.550: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-b cbac9f1c-7329-4516-9443-1eec556c2002 70508 0 2021-07-17 12:00:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 17 12:01:02.568: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-b cbac9f1c-7329-4516-9443-1eec556c2002 70544 0 2021-07-17 12:00:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:01:02.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3262 /api/v1/namespaces/watch-3262/configmaps/e2e-watch-test-configmap-b cbac9f1c-7329-4516-9443-1eec556c2002 70544 0 2021-07-17 12:00:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-17 12:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3262" for this suite.

• [SLOW TEST:60.379 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":58,"skipped":933,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:12.600: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 17 12:01:12.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 create -f -'
Jul 17 12:01:13.689: INFO: stderr: ""
Jul 17 12:01:13.689: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 17 12:01:13.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:01:13.845: INFO: stderr: ""
Jul 17 12:01:13.845: INFO: stdout: "update-demo-nautilus-549vc update-demo-nautilus-rfqbd "
Jul 17 12:01:13.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods update-demo-nautilus-549vc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:01:14.014: INFO: stderr: ""
Jul 17 12:01:14.014: INFO: stdout: ""
Jul 17 12:01:14.014: INFO: update-demo-nautilus-549vc is created but not running
Jul 17 12:01:19.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:01:19.180: INFO: stderr: ""
Jul 17 12:01:19.180: INFO: stdout: "update-demo-nautilus-549vc update-demo-nautilus-rfqbd "
Jul 17 12:01:19.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods update-demo-nautilus-549vc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:01:19.308: INFO: stderr: ""
Jul 17 12:01:19.308: INFO: stdout: "true"
Jul 17 12:01:19.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods update-demo-nautilus-549vc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:01:19.443: INFO: stderr: ""
Jul 17 12:01:19.443: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:01:19.443: INFO: validating pod update-demo-nautilus-549vc
Jul 17 12:01:19.451: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:01:19.451: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:01:19.451: INFO: update-demo-nautilus-549vc is verified up and running
Jul 17 12:01:19.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods update-demo-nautilus-rfqbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:01:19.572: INFO: stderr: ""
Jul 17 12:01:19.572: INFO: stdout: "true"
Jul 17 12:01:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods update-demo-nautilus-rfqbd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:01:19.698: INFO: stderr: ""
Jul 17 12:01:19.698: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:01:19.698: INFO: validating pod update-demo-nautilus-rfqbd
Jul 17 12:01:19.706: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:01:19.706: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:01:19.706: INFO: update-demo-nautilus-rfqbd is verified up and running
STEP: using delete to clean up resources
Jul 17 12:01:19.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 delete --grace-period=0 --force -f -'
Jul 17 12:01:19.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:01:19.827: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 17 12:01:19.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get rc,svc -l name=update-demo --no-headers'
Jul 17 12:01:19.952: INFO: stderr: "No resources found in kubectl-5827 namespace.\n"
Jul 17 12:01:19.952: INFO: stdout: ""
Jul 17 12:01:19.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 17 12:01:20.098: INFO: stderr: ""
Jul 17 12:01:20.098: INFO: stdout: "update-demo-nautilus-549vc\nupdate-demo-nautilus-rfqbd\n"
Jul 17 12:01:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get rc,svc -l name=update-demo --no-headers'
Jul 17 12:01:20.783: INFO: stderr: "No resources found in kubectl-5827 namespace.\n"
Jul 17 12:01:20.783: INFO: stdout: ""
Jul 17 12:01:20.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-5827 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 17 12:01:20.998: INFO: stderr: ""
Jul 17 12:01:20.998: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:20.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5827" for this suite.

• [SLOW TEST:8.416 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":59,"skipped":937,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:21.017: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:34.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5245" for this suite.
STEP: Destroying namespace "nsdeletetest-5237" for this suite.
Jul 17 12:01:34.199: INFO: Namespace nsdeletetest-5237 was already deleted
STEP: Destroying namespace "nsdeletetest-7465" for this suite.

• [SLOW TEST:13.187 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":60,"skipped":943,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:34.206: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:01:34.267: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 12:01:36.272: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 12:01:38.273: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:40.274: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:42.273: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:44.274: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:46.274: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:48.275: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:50.275: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:52.289: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = false)
Jul 17 12:01:54.272: INFO: The status of Pod test-webserver-f6bd321d-32da-4839-a12f-4f51305f9989 is Running (Ready = true)
Jul 17 12:01:54.275: INFO: Container started at 2021-07-17 12:01:35 +0000 UTC, pod became ready at 2021-07-17 12:01:52 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:54.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4669" for this suite.

• [SLOW TEST:20.084 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":61,"skipped":945,"failed":0}
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:54.295: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:54.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-523" for this suite.
STEP: Destroying namespace "nspatchtest-cb99ca42-7482-461d-89ae-849b2a324d32-6150" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":62,"skipped":945,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:54.424: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:01:54.499: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc" in namespace "downward-api-2220" to be "Succeeded or Failed"
Jul 17 12:01:54.508: INFO: Pod "downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922489ms
Jul 17 12:01:56.514: INFO: Pod "downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015010022s
STEP: Saw pod success
Jul 17 12:01:56.514: INFO: Pod "downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc" satisfied condition "Succeeded or Failed"
Jul 17 12:01:56.520: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc container client-container: <nil>
STEP: delete the pod
Jul 17 12:01:56.558: INFO: Waiting for pod downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc to disappear
Jul 17 12:01:56.561: INFO: Pod downwardapi-volume-b037c904-571e-457e-95f8-d583692e1ccc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:56.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2220" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":948,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Jul 17 12:01:56.660: INFO: Waiting up to 5m0s for pod "client-containers-dc71a116-d63e-43cc-8448-6577f9684a39" in namespace "containers-7861" to be "Succeeded or Failed"
Jul 17 12:01:56.665: INFO: Pod "client-containers-dc71a116-d63e-43cc-8448-6577f9684a39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218035ms
Jul 17 12:01:58.671: INFO: Pod "client-containers-dc71a116-d63e-43cc-8448-6577f9684a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009481424s
STEP: Saw pod success
Jul 17 12:01:58.671: INFO: Pod "client-containers-dc71a116-d63e-43cc-8448-6577f9684a39" satisfied condition "Succeeded or Failed"
Jul 17 12:01:58.675: INFO: Trying to get logs from node taikun-1-1188-w-1 pod client-containers-dc71a116-d63e-43cc-8448-6577f9684a39 container test-container: <nil>
STEP: delete the pod
Jul 17 12:01:58.710: INFO: Waiting for pod client-containers-dc71a116-d63e-43cc-8448-6577f9684a39 to disappear
Jul 17 12:01:58.719: INFO: Pod client-containers-dc71a116-d63e-43cc-8448-6577f9684a39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:01:58.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7861" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":964,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:01:58.733: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:09.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5376" for this suite.

• [SLOW TEST:11.251 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":65,"skipped":966,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 17 12:02:10.047: INFO: Waiting up to 5m0s for pod "pod-f87de831-81f5-4d42-a878-9d2db21a1b6e" in namespace "emptydir-9962" to be "Succeeded or Failed"
Jul 17 12:02:10.057: INFO: Pod "pod-f87de831-81f5-4d42-a878-9d2db21a1b6e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.021364ms
Jul 17 12:02:12.064: INFO: Pod "pod-f87de831-81f5-4d42-a878-9d2db21a1b6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016304287s
STEP: Saw pod success
Jul 17 12:02:12.064: INFO: Pod "pod-f87de831-81f5-4d42-a878-9d2db21a1b6e" satisfied condition "Succeeded or Failed"
Jul 17 12:02:12.068: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-f87de831-81f5-4d42-a878-9d2db21a1b6e container test-container: <nil>
STEP: delete the pod
Jul 17 12:02:12.096: INFO: Waiting for pod pod-f87de831-81f5-4d42-a878-9d2db21a1b6e to disappear
Jul 17 12:02:12.103: INFO: Pod pod-f87de831-81f5-4d42-a878-9d2db21a1b6e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:12.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9962" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":969,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:12.124: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Jul 17 12:02:12.170: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 17 12:02:12.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:12.789: INFO: stderr: ""
Jul 17 12:02:12.789: INFO: stdout: "service/agnhost-replica created\n"
Jul 17 12:02:12.789: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 17 12:02:12.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:13.275: INFO: stderr: ""
Jul 17 12:02:13.275: INFO: stdout: "service/agnhost-primary created\n"
Jul 17 12:02:13.275: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 17 12:02:13.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:13.752: INFO: stderr: ""
Jul 17 12:02:13.752: INFO: stdout: "service/frontend created\n"
Jul 17 12:02:13.752: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 17 12:02:13.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:14.282: INFO: stderr: ""
Jul 17 12:02:14.282: INFO: stdout: "deployment.apps/frontend created\n"
Jul 17 12:02:14.283: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 17 12:02:14.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:14.844: INFO: stderr: ""
Jul 17 12:02:14.844: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 17 12:02:14.844: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 17 12:02:14.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 create -f -'
Jul 17 12:02:15.531: INFO: stderr: ""
Jul 17 12:02:15.531: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 17 12:02:15.531: INFO: Waiting for all frontend pods to be Running.
Jul 17 12:02:20.582: INFO: Waiting for frontend to serve content.
Jul 17 12:02:20.601: INFO: Trying to add a new entry to the guestbook.
Jul 17 12:02:20.621: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 17 12:02:20.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:20.801: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:20.801: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 17 12:02:20.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:20.998: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:20.998: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 17 12:02:20.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:21.169: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:21.170: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 17 12:02:21.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:21.326: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:21.326: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 17 12:02:21.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:21.520: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:21.520: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 17 12:02:21.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2580 delete --grace-period=0 --force -f -'
Jul 17 12:02:21.829: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:21.829: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:21.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2580" for this suite.

• [SLOW TEST:9.728 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":67,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:21.852: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-68552f74-872a-49c4-b5be-1a189aab2a5d
STEP: Creating a pod to test consume secrets
Jul 17 12:02:21.985: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386" in namespace "projected-6928" to be "Succeeded or Failed"
Jul 17 12:02:22.008: INFO: Pod "pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386": Phase="Pending", Reason="", readiness=false. Elapsed: 22.981252ms
Jul 17 12:02:24.014: INFO: Pod "pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029261634s
Jul 17 12:02:26.018: INFO: Pod "pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033455233s
STEP: Saw pod success
Jul 17 12:02:26.018: INFO: Pod "pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386" satisfied condition "Succeeded or Failed"
Jul 17 12:02:26.022: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:02:26.050: INFO: Waiting for pod pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386 to disappear
Jul 17 12:02:26.054: INFO: Pod pod-projected-secrets-e2e53450-bfe7-4ba0-91d7-35aa1c8a6386 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:26.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6928" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":68,"skipped":997,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:26.074: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-8088
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8088 to expose endpoints map[]
Jul 17 12:02:26.137: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 17 12:02:27.159: INFO: successfully validated that service endpoint-test2 in namespace services-8088 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8088
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8088 to expose endpoints map[pod1:[80]]
Jul 17 12:02:29.199: INFO: successfully validated that service endpoint-test2 in namespace services-8088 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-8088
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8088 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 17 12:02:31.243: INFO: successfully validated that service endpoint-test2 in namespace services-8088 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-8088
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8088 to expose endpoints map[pod2:[80]]
Jul 17 12:02:31.401: INFO: successfully validated that service endpoint-test2 in namespace services-8088 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-8088
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8088 to expose endpoints map[]
Jul 17 12:02:31.485: INFO: successfully validated that service endpoint-test2 in namespace services-8088 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:31.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8088" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.470 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":69,"skipped":1014,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:31.544: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 17 12:02:34.665: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:34.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3056" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":70,"skipped":1016,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:02:34.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2790 version'
Jul 17 12:02:34.924: INFO: stderr: ""
Jul 17 12:02:34.924: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9\", GitCommit:\"9dd794e454ac32d97cde41ae10be801ae98f75df\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:09:28Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.9\", GitCommit:\"9dd794e454ac32d97cde41ae10be801ae98f75df\", GitTreeState:\"clean\", BuildDate:\"2021-03-18T01:00:06Z\", GoVersion:\"go1.15.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:34.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2790" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":71,"skipped":1036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:34.947: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Jul 17 12:02:35.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 create -f -'
Jul 17 12:02:35.609: INFO: stderr: ""
Jul 17 12:02:35.609: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 17 12:02:35.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:35.879: INFO: stderr: ""
Jul 17 12:02:35.879: INFO: stdout: "update-demo-nautilus-rwvs8 update-demo-nautilus-xlnj2 "
Jul 17 12:02:35.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:36.081: INFO: stderr: ""
Jul 17 12:02:36.081: INFO: stdout: ""
Jul 17 12:02:36.081: INFO: update-demo-nautilus-rwvs8 is created but not running
Jul 17 12:02:41.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:41.252: INFO: stderr: ""
Jul 17 12:02:41.252: INFO: stdout: "update-demo-nautilus-rwvs8 update-demo-nautilus-xlnj2 "
Jul 17 12:02:41.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:41.380: INFO: stderr: ""
Jul 17 12:02:41.380: INFO: stdout: "true"
Jul 17 12:02:41.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:02:41.520: INFO: stderr: ""
Jul 17 12:02:41.520: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:02:41.520: INFO: validating pod update-demo-nautilus-rwvs8
Jul 17 12:02:41.529: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:02:41.529: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:02:41.529: INFO: update-demo-nautilus-rwvs8 is verified up and running
Jul 17 12:02:41.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-xlnj2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:41.655: INFO: stderr: ""
Jul 17 12:02:41.655: INFO: stdout: "true"
Jul 17 12:02:41.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-xlnj2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:02:41.774: INFO: stderr: ""
Jul 17 12:02:41.774: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:02:41.774: INFO: validating pod update-demo-nautilus-xlnj2
Jul 17 12:02:41.780: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:02:41.780: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:02:41.780: INFO: update-demo-nautilus-xlnj2 is verified up and running
STEP: scaling down the replication controller
Jul 17 12:02:41.784: INFO: scanned /root for discovery docs: <nil>
Jul 17 12:02:41.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 17 12:02:42.942: INFO: stderr: ""
Jul 17 12:02:42.942: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 17 12:02:42.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:43.061: INFO: stderr: ""
Jul 17 12:02:43.061: INFO: stdout: "update-demo-nautilus-rwvs8 update-demo-nautilus-xlnj2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 17 12:02:48.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:48.197: INFO: stderr: ""
Jul 17 12:02:48.197: INFO: stdout: "update-demo-nautilus-rwvs8 "
Jul 17 12:02:48.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:48.322: INFO: stderr: ""
Jul 17 12:02:48.322: INFO: stdout: "true"
Jul 17 12:02:48.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:02:48.470: INFO: stderr: ""
Jul 17 12:02:48.470: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:02:48.470: INFO: validating pod update-demo-nautilus-rwvs8
Jul 17 12:02:48.476: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:02:48.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:02:48.476: INFO: update-demo-nautilus-rwvs8 is verified up and running
STEP: scaling up the replication controller
Jul 17 12:02:48.481: INFO: scanned /root for discovery docs: <nil>
Jul 17 12:02:48.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 17 12:02:49.644: INFO: stderr: ""
Jul 17 12:02:49.644: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 17 12:02:49.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:49.840: INFO: stderr: ""
Jul 17 12:02:49.840: INFO: stdout: "update-demo-nautilus-ddkz2 update-demo-nautilus-rwvs8 "
Jul 17 12:02:49.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-ddkz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:49.963: INFO: stderr: ""
Jul 17 12:02:49.963: INFO: stdout: ""
Jul 17 12:02:49.963: INFO: update-demo-nautilus-ddkz2 is created but not running
Jul 17 12:02:54.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 17 12:02:55.145: INFO: stderr: ""
Jul 17 12:02:55.145: INFO: stdout: "update-demo-nautilus-ddkz2 update-demo-nautilus-rwvs8 "
Jul 17 12:02:55.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-ddkz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:55.295: INFO: stderr: ""
Jul 17 12:02:55.295: INFO: stdout: "true"
Jul 17 12:02:55.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-ddkz2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:02:55.424: INFO: stderr: ""
Jul 17 12:02:55.424: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:02:55.424: INFO: validating pod update-demo-nautilus-ddkz2
Jul 17 12:02:55.430: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:02:55.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:02:55.431: INFO: update-demo-nautilus-ddkz2 is verified up and running
Jul 17 12:02:55.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 17 12:02:55.550: INFO: stderr: ""
Jul 17 12:02:55.550: INFO: stdout: "true"
Jul 17 12:02:55.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods update-demo-nautilus-rwvs8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 17 12:02:55.671: INFO: stderr: ""
Jul 17 12:02:55.671: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 17 12:02:55.671: INFO: validating pod update-demo-nautilus-rwvs8
Jul 17 12:02:55.677: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 17 12:02:55.677: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 17 12:02:55.677: INFO: update-demo-nautilus-rwvs8 is verified up and running
STEP: using delete to clean up resources
Jul 17 12:02:55.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 delete --grace-period=0 --force -f -'
Jul 17 12:02:55.801: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:02:55.801: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 17 12:02:55.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get rc,svc -l name=update-demo --no-headers'
Jul 17 12:02:56.026: INFO: stderr: "No resources found in kubectl-9191 namespace.\n"
Jul 17 12:02:56.026: INFO: stdout: ""
Jul 17 12:02:56.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-9191 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 17 12:02:56.257: INFO: stderr: ""
Jul 17 12:02:56.257: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:02:56.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9191" for this suite.

• [SLOW TEST:21.339 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":72,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:02:56.287: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:02:57.368: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 12:02:59.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120177, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120177, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120177, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120177, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:03:02.408: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:02.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7967" for this suite.
STEP: Destroying namespace "webhook-7967-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":73,"skipped":1078,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:02.783: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:02.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8089" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":74,"skipped":1083,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:02.904: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-j544
STEP: Creating a pod to test atomic-volume-subpath
Jul 17 12:03:03.022: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j544" in namespace "subpath-5844" to be "Succeeded or Failed"
Jul 17 12:03:03.033: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Pending", Reason="", readiness=false. Elapsed: 10.563861ms
Jul 17 12:03:05.041: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 2.018701741s
Jul 17 12:03:07.055: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 4.033303686s
Jul 17 12:03:09.061: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 6.038561356s
Jul 17 12:03:11.066: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 8.044131931s
Jul 17 12:03:13.071: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 10.049488121s
Jul 17 12:03:15.076: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 12.054139853s
Jul 17 12:03:17.098: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 14.075908898s
Jul 17 12:03:19.103: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 16.080570276s
Jul 17 12:03:21.108: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 18.085871014s
Jul 17 12:03:23.114: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Running", Reason="", readiness=true. Elapsed: 20.091725121s
Jul 17 12:03:25.128: INFO: Pod "pod-subpath-test-configmap-j544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.105612517s
STEP: Saw pod success
Jul 17 12:03:25.128: INFO: Pod "pod-subpath-test-configmap-j544" satisfied condition "Succeeded or Failed"
Jul 17 12:03:25.135: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-subpath-test-configmap-j544 container test-container-subpath-configmap-j544: <nil>
STEP: delete the pod
Jul 17 12:03:25.169: INFO: Waiting for pod pod-subpath-test-configmap-j544 to disappear
Jul 17 12:03:25.174: INFO: Pod pod-subpath-test-configmap-j544 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j544
Jul 17 12:03:25.174: INFO: Deleting pod "pod-subpath-test-configmap-j544" in namespace "subpath-5844"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:25.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5844" for this suite.

• [SLOW TEST:22.295 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":75,"skipped":1089,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:25.201: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:25.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7988" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":76,"skipped":1100,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:25.328: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7525
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7525
STEP: creating replication controller externalsvc in namespace services-7525
I0717 12:03:25.426725      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7525, replica count: 2
I0717 12:03:28.477616      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 17 12:03:28.747: INFO: Creating new exec pod
Jul 17 12:03:30.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-7525 exec execpod8mwp7 -- /bin/sh -x -c nslookup nodeport-service.services-7525.svc.taikun-1-1188'
Jul 17 12:03:31.109: INFO: stderr: "+ nslookup nodeport-service.services-7525.svc.taikun-1-1188\n"
Jul 17 12:03:31.109: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-7525.svc.taikun-1-1188\tcanonical name = externalsvc.services-7525.svc.taikun-1-1188.\nName:\texternalsvc.services-7525.svc.taikun-1-1188\nAddress: 10.233.19.233\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7525, will wait for the garbage collector to delete the pods
Jul 17 12:03:31.181: INFO: Deleting ReplicationController externalsvc took: 7.374257ms
Jul 17 12:03:31.881: INFO: Terminating ReplicationController externalsvc pods took: 700.283131ms
Jul 17 12:03:43.209: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:43.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7525" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.924 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":77,"skipped":1108,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:43.253: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 17 12:03:43.574: INFO: Waiting up to 5m0s for pod "pod-53d061be-9d5a-4c7c-a5ae-edab0764af63" in namespace "emptydir-3290" to be "Succeeded or Failed"
Jul 17 12:03:43.588: INFO: Pod "pod-53d061be-9d5a-4c7c-a5ae-edab0764af63": Phase="Pending", Reason="", readiness=false. Elapsed: 13.190083ms
Jul 17 12:03:45.596: INFO: Pod "pod-53d061be-9d5a-4c7c-a5ae-edab0764af63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021021696s
STEP: Saw pod success
Jul 17 12:03:45.596: INFO: Pod "pod-53d061be-9d5a-4c7c-a5ae-edab0764af63" satisfied condition "Succeeded or Failed"
Jul 17 12:03:45.600: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-53d061be-9d5a-4c7c-a5ae-edab0764af63 container test-container: <nil>
STEP: delete the pod
Jul 17 12:03:45.631: INFO: Waiting for pod pod-53d061be-9d5a-4c7c-a5ae-edab0764af63 to disappear
Jul 17 12:03:45.636: INFO: Pod pod-53d061be-9d5a-4c7c-a5ae-edab0764af63 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:45.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3290" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":78,"skipped":1110,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:45.651: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-950" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":79,"skipped":1123,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:45.748: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 17 12:03:45.798: INFO: Waiting up to 5m0s for pod "pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783" in namespace "emptydir-3858" to be "Succeeded or Failed"
Jul 17 12:03:45.813: INFO: Pod "pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783": Phase="Pending", Reason="", readiness=false. Elapsed: 15.388915ms
Jul 17 12:03:47.819: INFO: Pod "pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020989823s
Jul 17 12:03:49.826: INFO: Pod "pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027659602s
STEP: Saw pod success
Jul 17 12:03:49.826: INFO: Pod "pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783" satisfied condition "Succeeded or Failed"
Jul 17 12:03:49.829: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783 container test-container: <nil>
STEP: delete the pod
Jul 17 12:03:49.858: INFO: Waiting for pod pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783 to disappear
Jul 17 12:03:49.862: INFO: Pod pod-5818ecc2-f868-4f6c-bfd2-c297c45c1783 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:49.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3858" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:49.899: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Jul 17 12:03:49.956: INFO: Waiting up to 5m0s for pod "var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca" in namespace "var-expansion-2221" to be "Succeeded or Failed"
Jul 17 12:03:49.963: INFO: Pod "var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.105201ms
Jul 17 12:03:51.969: INFO: Pod "var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012991117s
Jul 17 12:03:53.977: INFO: Pod "var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020721087s
STEP: Saw pod success
Jul 17 12:03:53.977: INFO: Pod "var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca" satisfied condition "Succeeded or Failed"
Jul 17 12:03:53.983: INFO: Trying to get logs from node taikun-1-1188-w-1 pod var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca container dapi-container: <nil>
STEP: delete the pod
Jul 17 12:03:54.008: INFO: Waiting for pod var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca to disappear
Jul 17 12:03:54.013: INFO: Pod var-expansion-984ce243-51ba-46d6-b254-b0ac30c78dca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:54.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2221" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:54.032: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:03:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3335" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":82,"skipped":1241,"failed":0}

------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:03:54.087: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 17 12:03:54.170: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 17 12:04:54.224: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:04:54.232: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 17 12:04:56.321: INFO: found a healthy node: taikun-1-1188-w-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:05:02.471: INFO: pods created so far: [1 1 1]
Jul 17 12:05:02.471: INFO: length of pods created so far: 3
Jul 17 12:05:18.491: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:05:25.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5102" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:05:25.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9890" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:91.540 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":83,"skipped":1241,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:05:25.635: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 17 12:07:26.286: INFO: Successfully updated pod "var-expansion-4a79ae0f-7c6f-4f99-aae3-86eb3062c81f"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 17 12:07:28.304: INFO: Deleting pod "var-expansion-4a79ae0f-7c6f-4f99-aae3-86eb3062c81f" in namespace "var-expansion-4877"
Jul 17 12:07:28.318: INFO: Wait up to 5m0s for pod "var-expansion-4a79ae0f-7c6f-4f99-aae3-86eb3062c81f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:04.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4877" for this suite.

• [SLOW TEST:158.715 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":84,"skipped":1250,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:04.359: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 17 12:08:04.446: INFO: Waiting up to 5m0s for pod "downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c" in namespace "downward-api-6389" to be "Succeeded or Failed"
Jul 17 12:08:04.454: INFO: Pod "downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033516ms
Jul 17 12:08:06.462: INFO: Pod "downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015414101s
STEP: Saw pod success
Jul 17 12:08:06.462: INFO: Pod "downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c" satisfied condition "Succeeded or Failed"
Jul 17 12:08:06.465: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c container dapi-container: <nil>
STEP: delete the pod
Jul 17 12:08:06.517: INFO: Waiting for pod downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c to disappear
Jul 17 12:08:06.526: INFO: Pod downward-api-92536a6f-c80c-4b1e-ab56-bd60a4ab853c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:06.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6389" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":85,"skipped":1256,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:06.543: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:08:06.613: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f" in namespace "projected-3213" to be "Succeeded or Failed"
Jul 17 12:08:06.654: INFO: Pod "downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 41.472782ms
Jul 17 12:08:08.663: INFO: Pod "downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050026067s
Jul 17 12:08:10.670: INFO: Pod "downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057399501s
STEP: Saw pod success
Jul 17 12:08:10.670: INFO: Pod "downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f" satisfied condition "Succeeded or Failed"
Jul 17 12:08:10.676: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f container client-container: <nil>
STEP: delete the pod
Jul 17 12:08:10.712: INFO: Waiting for pod downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f to disappear
Jul 17 12:08:10.723: INFO: Pod downwardapi-volume-b3224c4c-bd39-4d4b-a206-00ef96849a1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:10.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3213" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":86,"skipped":1264,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:10.736: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:08:11.786: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 12:08:13.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120491, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120491, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120491, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762120491, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:08:16.827: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:08:16.830: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5256-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:17.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6348" for this suite.
STEP: Destroying namespace "webhook-6348-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":87,"skipped":1280,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:18.125: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6509
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6509
STEP: Creating statefulset with conflicting port in namespace statefulset-6509
STEP: Waiting until pod test-pod will start running in namespace statefulset-6509
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6509
Jul 17 12:08:22.478: INFO: Observed stateful pod in namespace: statefulset-6509, name: ss-0, uid: 71120989-08c3-48e8-859c-d40ecb48f234, status phase: Pending. Waiting for statefulset controller to delete.
Jul 17 12:08:22.669: INFO: Observed stateful pod in namespace: statefulset-6509, name: ss-0, uid: 71120989-08c3-48e8-859c-d40ecb48f234, status phase: Failed. Waiting for statefulset controller to delete.
Jul 17 12:08:22.681: INFO: Observed stateful pod in namespace: statefulset-6509, name: ss-0, uid: 71120989-08c3-48e8-859c-d40ecb48f234, status phase: Failed. Waiting for statefulset controller to delete.
Jul 17 12:08:22.697: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6509
STEP: Removing pod with conflicting port in namespace statefulset-6509
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6509 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 12:08:26.810: INFO: Deleting all statefulset in ns statefulset-6509
Jul 17 12:08:26.815: INFO: Scaling statefulset ss to 0
Jul 17 12:08:36.847: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:08:36.851: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:36.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6509" for this suite.

• [SLOW TEST:18.773 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":88,"skipped":1298,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:36.898: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:08:36.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29" in namespace "projected-8874" to be "Succeeded or Failed"
Jul 17 12:08:36.979: INFO: Pod "downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.810713ms
Jul 17 12:08:38.984: INFO: Pod "downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009937697s
Jul 17 12:08:40.988: INFO: Pod "downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014064907s
STEP: Saw pod success
Jul 17 12:08:40.988: INFO: Pod "downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29" satisfied condition "Succeeded or Failed"
Jul 17 12:08:40.993: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29 container client-container: <nil>
STEP: delete the pod
Jul 17 12:08:41.027: INFO: Waiting for pod downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29 to disappear
Jul 17 12:08:41.032: INFO: Pod downwardapi-volume-46d5164e-dfd1-42e0-bd43-90c329c87c29 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:08:41.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8874" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1300,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:08:41.051: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-8590
STEP: creating service affinity-clusterip-transition in namespace services-8590
STEP: creating replication controller affinity-clusterip-transition in namespace services-8590
I0717 12:08:41.226388      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-8590, replica count: 3
I0717 12:08:44.279995      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:08:44.287: INFO: Creating new exec pod
Jul 17 12:08:47.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-8590 exec execpod-affinity58rsn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jul 17 12:08:47.592: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 17 12:08:47.592: INFO: stdout: ""
Jul 17 12:08:47.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-8590 exec execpod-affinity58rsn -- /bin/sh -x -c nc -zv -t -w 2 10.233.30.243 80'
Jul 17 12:08:47.883: INFO: stderr: "+ nc -zv -t -w 2 10.233.30.243 80\nConnection to 10.233.30.243 80 port [tcp/http] succeeded!\n"
Jul 17 12:08:47.883: INFO: stdout: ""
Jul 17 12:08:47.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-8590 exec execpod-affinity58rsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.30.243:80/ ; done'
Jul 17 12:08:48.307: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n"
Jul 17 12:08:48.307: INFO: stdout: "\naffinity-clusterip-transition-pzdqm\naffinity-clusterip-transition-fkzwg\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-pzdqm\naffinity-clusterip-transition-fkzwg\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-pzdqm\naffinity-clusterip-transition-fkzwg\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-pzdqm\naffinity-clusterip-transition-fkzwg\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-pzdqm\naffinity-clusterip-transition-fkzwg\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-pzdqm"
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-fkzwg
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-fkzwg
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-fkzwg
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-fkzwg
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-fkzwg
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.307: INFO: Received response from host: affinity-clusterip-transition-pzdqm
Jul 17 12:08:48.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-8590 exec execpod-affinity58rsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.30.243:80/ ; done'
Jul 17 12:08:48.706: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.243:80/\n"
Jul 17 12:08:48.706: INFO: stdout: "\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs\naffinity-clusterip-transition-kcqvs"
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Received response from host: affinity-clusterip-transition-kcqvs
Jul 17 12:08:48.706: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8590, will wait for the garbage collector to delete the pods
Jul 17 12:08:48.805: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.447741ms
Jul 17 12:08:48.905: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.570616ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:09:03.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8590" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:22.205 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":90,"skipped":1309,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:09:03.263: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:03.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3015" for this suite.

• [SLOW TEST:60.092 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1323,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:03.356: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 CNAME > /results/wheezy_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 CNAME > /results/jessie_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:10:07.450: INFO: DNS probes using dns-test-13620075-0118-4ccd-90c0-02a428bc61e7 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 CNAME > /results/wheezy_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 CNAME > /results/jessie_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:10:09.801: INFO: File wheezy_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188 from pod  dns-7841/dns-test-5226e4c1-bf4c-4fa6-bc1c-fde7eb7da40e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 17 12:10:09.805: INFO: File jessie_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188 from pod  dns-7841/dns-test-5226e4c1-bf4c-4fa6-bc1c-fde7eb7da40e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 17 12:10:09.805: INFO: Lookups using dns-7841/dns-test-5226e4c1-bf4c-4fa6-bc1c-fde7eb7da40e failed for: [wheezy_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188 jessie_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188]

Jul 17 12:10:14.816: INFO: DNS probes using dns-test-5226e4c1-bf4c-4fa6-bc1c-fde7eb7da40e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 A > /results/wheezy_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7841.svc.taikun-1-1188 A > /results/jessie_udp@dns-test-service-3.dns-7841.svc.taikun-1-1188; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:10:18.973: INFO: DNS probes using dns-test-b24065f0-4cc7-48e8-8073-6c661899c667 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:19.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7841" for this suite.

• [SLOW TEST:15.729 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":92,"skipped":1324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:19.097: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 17 12:10:19.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74668 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:10:19.200: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74669 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:10:19.200: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74670 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 17 12:10:29.265: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74755 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:10:29.266: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74756 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:10:29.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5620 /api/v1/namespaces/watch-5620/configmaps/e2e-watch-test-label-changed 242310ed-c024-4592-a354-243b1f8fe196 74757 0 2021-07-17 12:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-17 12:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5620" for this suite.

• [SLOW TEST:10.182 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":93,"skipped":1358,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:29.280: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 17 12:10:29.420: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:29.427: INFO: Number of nodes with available pods: 0
Jul 17 12:10:29.427: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:30.442: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:30.446: INFO: Number of nodes with available pods: 0
Jul 17 12:10:30.446: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:31.433: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:31.437: INFO: Number of nodes with available pods: 1
Jul 17 12:10:31.438: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:32.436: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:32.440: INFO: Number of nodes with available pods: 2
Jul 17 12:10:32.441: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 17 12:10:32.462: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:32.468: INFO: Number of nodes with available pods: 1
Jul 17 12:10:32.468: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:33.476: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:33.481: INFO: Number of nodes with available pods: 1
Jul 17 12:10:33.481: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:34.475: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:34.481: INFO: Number of nodes with available pods: 1
Jul 17 12:10:34.481: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:35.479: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:35.483: INFO: Number of nodes with available pods: 1
Jul 17 12:10:35.483: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:36.579: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:36.587: INFO: Number of nodes with available pods: 1
Jul 17 12:10:36.587: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:37.482: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:37.488: INFO: Number of nodes with available pods: 1
Jul 17 12:10:37.489: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:10:38.477: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:10:38.482: INFO: Number of nodes with available pods: 2
Jul 17 12:10:38.482: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2056, will wait for the garbage collector to delete the pods
Jul 17 12:10:38.552: INFO: Deleting DaemonSet.extensions daemon-set took: 13.049955ms
Jul 17 12:10:39.253: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.26123ms
Jul 17 12:10:43.156: INFO: Number of nodes with available pods: 0
Jul 17 12:10:43.156: INFO: Number of running nodes: 0, number of available pods: 0
Jul 17 12:10:43.159: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2056/daemonsets","resourceVersion":"74888"},"items":null}

Jul 17 12:10:43.163: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2056/pods","resourceVersion":"74888"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2056" for this suite.

• [SLOW TEST:13.903 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":94,"skipped":1381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:43.186: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Jul 17 12:10:43.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-4334 cluster-info'
Jul 17 12:10:43.445: INFO: stderr: ""
Jul 17 12:10:43.445: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:43.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4334" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":95,"skipped":1468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:43.457: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 17 12:10:47.947: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 17 12:10:47.952: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 17 12:10:49.952: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 17 12:10:49.958: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 17 12:10:51.952: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 17 12:10:51.959: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:10:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7059" for this suite.

• [SLOW TEST:8.517 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1496,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:10:51.975: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0717 12:11:32.088268      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 12:12:34.117: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jul 17 12:12:34.117: INFO: Deleting pod "simpletest.rc-2xk9s" in namespace "gc-8914"
Jul 17 12:12:34.141: INFO: Deleting pod "simpletest.rc-58hqc" in namespace "gc-8914"
Jul 17 12:12:34.190: INFO: Deleting pod "simpletest.rc-5rdzd" in namespace "gc-8914"
Jul 17 12:12:34.203: INFO: Deleting pod "simpletest.rc-6vgvh" in namespace "gc-8914"
Jul 17 12:12:34.247: INFO: Deleting pod "simpletest.rc-8fvh4" in namespace "gc-8914"
Jul 17 12:12:34.283: INFO: Deleting pod "simpletest.rc-cl4z9" in namespace "gc-8914"
Jul 17 12:12:34.311: INFO: Deleting pod "simpletest.rc-dj2wt" in namespace "gc-8914"
Jul 17 12:12:34.335: INFO: Deleting pod "simpletest.rc-s9jf2" in namespace "gc-8914"
Jul 17 12:12:34.367: INFO: Deleting pod "simpletest.rc-wsl8p" in namespace "gc-8914"
Jul 17 12:12:34.388: INFO: Deleting pod "simpletest.rc-x6dfg" in namespace "gc-8914"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:12:34.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8914" for this suite.

• [SLOW TEST:102.514 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":97,"skipped":1522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:12:34.505: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 17 12:12:34.693: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:13:03.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2" for this suite.

• [SLOW TEST:28.849 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":98,"skipped":1569,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:13:03.362: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:13:03.409: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:13:11.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2996" for this suite.

• [SLOW TEST:8.191 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":99,"skipped":1585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:13:11.558: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:15:11.644: INFO: Deleting pod "var-expansion-0f6691a8-c1f5-4788-9f5b-d16dc15efc83" in namespace "var-expansion-1612"
Jul 17 12:15:11.664: INFO: Wait up to 5m0s for pod "var-expansion-0f6691a8-c1f5-4788-9f5b-d16dc15efc83" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:15:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1612" for this suite.

• [SLOW TEST:122.126 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":100,"skipped":1634,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:15:13.687: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:15:13.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624" in namespace "projected-1387" to be "Succeeded or Failed"
Jul 17 12:15:13.755: INFO: Pod "downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624": Phase="Pending", Reason="", readiness=false. Elapsed: 5.980742ms
Jul 17 12:15:15.760: INFO: Pod "downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010971086s
STEP: Saw pod success
Jul 17 12:15:15.760: INFO: Pod "downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624" satisfied condition "Succeeded or Failed"
Jul 17 12:15:15.764: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624 container client-container: <nil>
STEP: delete the pod
Jul 17 12:15:15.816: INFO: Waiting for pod downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624 to disappear
Jul 17 12:15:15.820: INFO: Pod downwardapi-volume-30ebdf03-5025-4cc6-853f-e7e4331e7624 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:15:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1387" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":101,"skipped":1655,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:15:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:15:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Creating first CR 
Jul 17 12:15:16.487: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:16Z]] name:name1 resourceVersion:76452 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:641194e4-67f1-467d-ab33-7e88f39610b1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 17 12:15:26.498: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:26Z]] name:name2 resourceVersion:76506 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e2229fc7-d4b3-49d2-8022-ebb28d27dac9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 17 12:15:36.513: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:36Z]] name:name1 resourceVersion:76542 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:641194e4-67f1-467d-ab33-7e88f39610b1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 17 12:15:46.524: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:46Z]] name:name2 resourceVersion:76578 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e2229fc7-d4b3-49d2-8022-ebb28d27dac9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 17 12:15:56.539: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:36Z]] name:name1 resourceVersion:76616 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:641194e4-67f1-467d-ab33-7e88f39610b1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 17 12:16:06.551: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-17T12:15:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-17T12:15:46Z]] name:name2 resourceVersion:76654 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:e2229fc7-d4b3-49d2-8022-ebb28d27dac9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:16:17.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3" for this suite.

• [SLOW TEST:61.262 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":102,"skipped":1662,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:16:17.104: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:16:17.195: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 17 12:16:17.216: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:17.221: INFO: Number of nodes with available pods: 0
Jul 17 12:16:17.222: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:16:18.278: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:18.283: INFO: Number of nodes with available pods: 0
Jul 17 12:16:18.284: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:16:19.229: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:19.234: INFO: Number of nodes with available pods: 2
Jul 17 12:16:19.235: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 17 12:16:19.274: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:19.274: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:19.292: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:20.300: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:20.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:20.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:21.299: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:21.299: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:21.305: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:22.300: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:22.300: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:22.301: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:22.308: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:23.299: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:23.300: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:23.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:23.305: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:24.300: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:24.300: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:24.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:24.306: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:25.300: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:25.300: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:25.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:25.309: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:26.299: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:26.299: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:26.299: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:26.306: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:27.297: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:27.297: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:27.298: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:27.302: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:28.302: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:28.302: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:28.303: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:28.312: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:29.301: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:29.301: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:29.302: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:29.310: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:30.301: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:30.301: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:30.301: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:30.308: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:31.301: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:31.301: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:31.301: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:31.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:32.297: INFO: Wrong image for pod: daemon-set-lgvtp. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:32.297: INFO: Pod daemon-set-lgvtp is not available
Jul 17 12:16:32.297: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:32.303: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:33.299: INFO: Pod daemon-set-8dhjq is not available
Jul 17 12:16:33.299: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:33.304: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:34.300: INFO: Pod daemon-set-8dhjq is not available
Jul 17 12:16:34.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:34.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:35.304: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:35.310: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:36.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:36.300: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:36.305: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:37.299: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:37.299: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:37.304: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:38.298: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:38.299: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:38.304: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:39.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:39.301: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:39.310: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:40.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:40.301: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:40.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:41.300: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:41.300: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:41.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:42.301: INFO: Wrong image for pod: daemon-set-n7ghq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Jul 17 12:16:42.301: INFO: Pod daemon-set-n7ghq is not available
Jul 17 12:16:42.307: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:43.300: INFO: Pod daemon-set-pbckz is not available
Jul 17 12:16:43.306: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 17 12:16:43.311: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:43.315: INFO: Number of nodes with available pods: 1
Jul 17 12:16:43.316: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:16:44.322: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:44.325: INFO: Number of nodes with available pods: 1
Jul 17 12:16:44.325: INFO: Node taikun-1-1188-w-1 is running more than one daemon pod
Jul 17 12:16:45.325: INFO: DaemonSet pods can't tolerate node taikun-1-1188-m with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 17 12:16:45.331: INFO: Number of nodes with available pods: 2
Jul 17 12:16:45.331: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5671, will wait for the garbage collector to delete the pods
Jul 17 12:16:45.415: INFO: Deleting DaemonSet.extensions daemon-set took: 7.655196ms
Jul 17 12:16:46.116: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.64879ms
Jul 17 12:16:53.221: INFO: Number of nodes with available pods: 0
Jul 17 12:16:53.221: INFO: Number of running nodes: 0, number of available pods: 0
Jul 17 12:16:53.225: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5671/daemonsets","resourceVersion":"76943"},"items":null}

Jul 17 12:16:53.228: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5671/pods","resourceVersion":"76943"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:16:53.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5671" for this suite.

• [SLOW TEST:36.163 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":103,"skipped":1673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:16:53.268: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1512
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 17 12:16:53.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2731 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jul 17 12:16:54.211: INFO: stderr: ""
Jul 17 12:16:54.211: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Jul 17 12:16:54.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-2731 delete pods e2e-test-httpd-pod'
Jul 17 12:17:03.136: INFO: stderr: ""
Jul 17 12:17:03.136: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:17:03.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2731" for this suite.

• [SLOW TEST:9.882 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1509
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":104,"skipped":1696,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:17:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Jul 17 12:17:03.217: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-1318 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:17:03.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1318" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":105,"skipped":1708,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:17:03.371: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:17:04.126: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 17 12:17:06.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121024, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121024, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121024, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121024, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:17:09.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:17:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9165" for this suite.
STEP: Destroying namespace "webhook-9165-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":106,"skipped":1710,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:17:09.353: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7693.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7693.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7693.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:17:13.495: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.502: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.508: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.514: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.530: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.534: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.540: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.546: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188 from pod dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7: the server could not find the requested resource (get pods dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7)
Jul 17 12:17:13.555: INFO: Lookups using dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 wheezy_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188 wheezy_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188 jessie_udp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 jessie_tcp@dns-querier-2.dns-test-service-2.dns-7693.svc.taikun-1-1188 jessie_udp@dns-test-service-2.dns-7693.svc.taikun-1-1188 jessie_tcp@dns-test-service-2.dns-7693.svc.taikun-1-1188]

Jul 17 12:17:18.621: INFO: DNS probes using dns-7693/dns-test-799d7655-df64-4e3b-a92a-7a61a1cabfd7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:17:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7693" for this suite.

• [SLOW TEST:9.352 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":107,"skipped":1711,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:17:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-65b1af33-b5a8-4ec8-84d9-623ad9f2ae00
STEP: Creating configMap with name cm-test-opt-upd-8c60c96f-48ee-4621-b4b6-989bb33a0eec
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-65b1af33-b5a8-4ec8-84d9-623ad9f2ae00
STEP: Updating configmap cm-test-opt-upd-8c60c96f-48ee-4621-b4b6-989bb33a0eec
STEP: Creating configMap with name cm-test-opt-create-792b5357-41c8-4754-ad8c-f80ccec0ef59
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:18:49.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5853" for this suite.

• [SLOW TEST:90.999 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1714,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:18:49.713: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-d2c760d4-e3cb-41d7-9e07-68fcac430078 in namespace container-probe-5269
Jul 17 12:18:51.801: INFO: Started pod test-webserver-d2c760d4-e3cb-41d7-9e07-68fcac430078 in namespace container-probe-5269
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 12:18:51.806: INFO: Initial restart count of pod test-webserver-d2c760d4-e3cb-41d7-9e07-68fcac430078 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:22:52.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5269" for this suite.

• [SLOW TEST:242.888 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:22:52.605: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-0997acbb-29ee-4dc4-8663-38704dffdae3
STEP: Creating a pod to test consume configMaps
Jul 17 12:22:52.663: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e" in namespace "projected-8105" to be "Succeeded or Failed"
Jul 17 12:22:52.678: INFO: Pod "pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.844231ms
Jul 17 12:22:54.688: INFO: Pod "pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023430873s
STEP: Saw pod success
Jul 17 12:22:54.689: INFO: Pod "pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e" satisfied condition "Succeeded or Failed"
Jul 17 12:22:54.693: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 12:22:54.762: INFO: Waiting for pod pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e to disappear
Jul 17 12:22:54.771: INFO: Pod pod-projected-configmaps-f18afddf-3ec5-4870-bad7-5f0e360fe12e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:22:54.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8105" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":110,"skipped":1750,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:22:54.786: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 17 12:22:57.872: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:22:57.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4544" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":111,"skipped":1758,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:22:57.966: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 17 12:23:00.069: INFO: &Pod{ObjectMeta:{send-events-4c36a23b-1211-4646-a5d8-5e8e96c40576  events-951 /api/v1/namespaces/events-951/pods/send-events-4c36a23b-1211-4646-a5d8-5e8e96c40576 561c3b23-263e-4622-8861-cfb2fb356a0c 78682 0 2021-07-17 12:22:58 +0000 UTC <nil> <nil> map[name:foo time:24784711] map[cni.projectcalico.org/podIP:10.233.64.14/32 cni.projectcalico.org/podIPs:10.233.64.14/32] [] []  [{calico Update v1 2021-07-17 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {e2e.test Update v1 2021-07-17 12:22:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 12:22:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wlzll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wlzll,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wlzll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:22:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:22:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.14,StartTime:2021-07-17 12:22:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 12:22:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://e01517becac03bdd1333e928cbcc8ab14ac9ecf679ead20a637cb8852dab9411,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 17 12:23:02.080: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 17 12:23:04.085: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:04.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-951" for this suite.

• [SLOW TEST:6.159 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":112,"skipped":1770,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:04.133: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:23:04.809: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:23:07.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:23:07.842: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:09.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7887" for this suite.
STEP: Destroying namespace "webhook-7887-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.531 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":113,"skipped":1781,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 17 12:23:09.788: INFO: Waiting up to 5m0s for pod "pod-16a87b99-84ed-42cf-859c-5722037ef4b1" in namespace "emptydir-4032" to be "Succeeded or Failed"
Jul 17 12:23:09.806: INFO: Pod "pod-16a87b99-84ed-42cf-859c-5722037ef4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.038292ms
Jul 17 12:23:11.811: INFO: Pod "pod-16a87b99-84ed-42cf-859c-5722037ef4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022880333s
STEP: Saw pod success
Jul 17 12:23:11.811: INFO: Pod "pod-16a87b99-84ed-42cf-859c-5722037ef4b1" satisfied condition "Succeeded or Failed"
Jul 17 12:23:11.821: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-16a87b99-84ed-42cf-859c-5722037ef4b1 container test-container: <nil>
STEP: delete the pod
Jul 17 12:23:11.854: INFO: Waiting for pod pod-16a87b99-84ed-42cf-859c-5722037ef4b1 to disappear
Jul 17 12:23:11.857: INFO: Pod pod-16a87b99-84ed-42cf-859c-5722037ef4b1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:11.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4032" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":114,"skipped":1894,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:11.869: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 17 12:23:11.931: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:23:17.746: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:38.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7806" for this suite.

• [SLOW TEST:26.470 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":115,"skipped":1904,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3900
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3900
STEP: creating replication controller externalsvc in namespace services-3900
I0717 12:23:38.438122      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3900, replica count: 2
I0717 12:23:41.489048      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 17 12:23:41.514: INFO: Creating new exec pod
Jul 17 12:23:43.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3900 exec execpodv74v4 -- /bin/sh -x -c nslookup clusterip-service.services-3900.svc.taikun-1-1188'
Jul 17 12:23:43.917: INFO: stderr: "+ nslookup clusterip-service.services-3900.svc.taikun-1-1188\n"
Jul 17 12:23:43.917: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-3900.svc.taikun-1-1188\tcanonical name = externalsvc.services-3900.svc.taikun-1-1188.\nName:\texternalsvc.services-3900.svc.taikun-1-1188\nAddress: 10.233.56.79\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3900, will wait for the garbage collector to delete the pods
Jul 17 12:23:43.982: INFO: Deleting ReplicationController externalsvc took: 9.52391ms
Jul 17 12:23:44.682: INFO: Terminating ReplicationController externalsvc pods took: 700.37642ms
Jul 17 12:23:53.213: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:53.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3900" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:14.914 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":116,"skipped":1906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:53.265: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:23:58.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-484" for this suite.

• [SLOW TEST:5.152 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":117,"skipped":1949,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:23:58.416: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-2b85ebe2-9ef8-464b-a80f-21ed1a7f6d34
STEP: Creating a pod to test consume secrets
Jul 17 12:23:58.526: INFO: Waiting up to 5m0s for pod "pod-secrets-2a5abf38-792a-4870-a869-360773abba38" in namespace "secrets-2997" to be "Succeeded or Failed"
Jul 17 12:23:58.536: INFO: Pod "pod-secrets-2a5abf38-792a-4870-a869-360773abba38": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06869ms
Jul 17 12:24:00.550: INFO: Pod "pod-secrets-2a5abf38-792a-4870-a869-360773abba38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023436178s
STEP: Saw pod success
Jul 17 12:24:00.550: INFO: Pod "pod-secrets-2a5abf38-792a-4870-a869-360773abba38" satisfied condition "Succeeded or Failed"
Jul 17 12:24:00.554: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-2a5abf38-792a-4870-a869-360773abba38 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:24:00.597: INFO: Waiting for pod pod-secrets-2a5abf38-792a-4870-a869-360773abba38 to disappear
Jul 17 12:24:00.602: INFO: Pod pod-secrets-2a5abf38-792a-4870-a869-360773abba38 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:00.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2997" for this suite.
STEP: Destroying namespace "secret-namespace-7376" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:00.623: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:02.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5837" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":119,"skipped":1978,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:02.790: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 17 12:24:02.859: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:24:08.151: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:28.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2715" for this suite.

• [SLOW TEST:25.867 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":120,"skipped":1988,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:28.661: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-045be0c0-084a-4a3d-a9d6-3462de02fd79
STEP: Creating a pod to test consume secrets
Jul 17 12:24:28.729: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66" in namespace "projected-4545" to be "Succeeded or Failed"
Jul 17 12:24:28.733: INFO: Pod "pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.987907ms
Jul 17 12:24:30.738: INFO: Pod "pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00931645s
STEP: Saw pod success
Jul 17 12:24:30.739: INFO: Pod "pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66" satisfied condition "Succeeded or Failed"
Jul 17 12:24:30.743: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:24:30.762: INFO: Waiting for pod pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66 to disappear
Jul 17 12:24:30.767: INFO: Pod pod-projected-secrets-496da3bc-f6e7-4303-bcda-095ac751ba66 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:30.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4545" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":121,"skipped":1989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:30.778: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 17 12:24:33.400: INFO: Successfully updated pod "labelsupdate81594b77-b808-4a43-907d-e735844a828e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6990" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":122,"skipped":2033,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:35.433: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 17 12:24:35.486: INFO: Waiting up to 5m0s for pod "pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec" in namespace "emptydir-9072" to be "Succeeded or Failed"
Jul 17 12:24:35.501: INFO: Pod "pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec": Phase="Pending", Reason="", readiness=false. Elapsed: 14.462783ms
Jul 17 12:24:37.510: INFO: Pod "pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02359946s
Jul 17 12:24:39.515: INFO: Pod "pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028569807s
STEP: Saw pod success
Jul 17 12:24:39.515: INFO: Pod "pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec" satisfied condition "Succeeded or Failed"
Jul 17 12:24:39.518: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec container test-container: <nil>
STEP: delete the pod
Jul 17 12:24:39.761: INFO: Waiting for pod pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec to disappear
Jul 17 12:24:39.765: INFO: Pod pod-03cb5d20-e654-4be5-99b7-c641d25ab6ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:39.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9072" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":123,"skipped":2038,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:39.779: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:24:40.683: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 12:24:42.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121480, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121480, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121480, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121480, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:24:45.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:24:45.724: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2037-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:24:47.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3314" for this suite.
STEP: Destroying namespace "webhook-3314-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.432 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":124,"skipped":2047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:24:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-ea14b5b3-22e5-45bb-a48a-16f84e24adc6
STEP: Creating secret with name s-test-opt-upd-eab13006-228d-4433-9717-59e5e9868783
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ea14b5b3-22e5-45bb-a48a-16f84e24adc6
STEP: Updating secret s-test-opt-upd-eab13006-228d-4433-9717-59e5e9868783
STEP: Creating secret with name s-test-opt-create-b99db6b9-937f-4a84-8067-b037efe27ade
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:07.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1211" for this suite.

• [SLOW TEST:80.724 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2157,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:07.963: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:26:08.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235" in namespace "projected-4600" to be "Succeeded or Failed"
Jul 17 12:26:08.025: INFO: Pod "downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235": Phase="Pending", Reason="", readiness=false. Elapsed: 5.621622ms
Jul 17 12:26:10.034: INFO: Pod "downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014304571s
STEP: Saw pod success
Jul 17 12:26:10.034: INFO: Pod "downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235" satisfied condition "Succeeded or Failed"
Jul 17 12:26:10.038: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235 container client-container: <nil>
STEP: delete the pod
Jul 17 12:26:10.059: INFO: Waiting for pod downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235 to disappear
Jul 17 12:26:10.063: INFO: Pod downwardapi-volume-8281313b-4efa-4416-96dc-234b24599235 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:10.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4600" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2158,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:10.077: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 17 12:26:14.227: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:14.238: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 17 12:26:16.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:16.245: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 17 12:26:18.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:18.248: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 17 12:26:20.239: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:20.243: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 17 12:26:22.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:22.243: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 17 12:26:24.238: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 17 12:26:24.244: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:24.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7099" for this suite.

• [SLOW TEST:14.199 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":127,"skipped":2184,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:24.279: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Jul 17 12:26:24.333: INFO: created test-event-1
Jul 17 12:26:24.339: INFO: created test-event-2
Jul 17 12:26:24.344: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 17 12:26:24.348: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 17 12:26:24.366: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:24.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8874" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":128,"skipped":2189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:24.397: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1546
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 17 12:26:24.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7382 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jul 17 12:26:24.897: INFO: stderr: ""
Jul 17 12:26:24.897: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 17 12:26:29.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7382 get pod e2e-test-httpd-pod -o json'
Jul 17 12:26:30.085: INFO: stderr: ""
Jul 17 12:26:30.085: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.233.64.32/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.64.32/32\"\n        },\n        \"creationTimestamp\": \"2021-07-17T12:26:24Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-17T12:26:24Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-17T12:26:25Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.233.64.32\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-07-17T12:26:26Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7382\",\n        \"resourceVersion\": \"80310\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7382/pods/e2e-test-httpd-pod\",\n        \"uid\": \"bd3e72d3-a78f-486c-8176-481bdfcfc2a6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-z5nhx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"taikun-1-1188-w-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-z5nhx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-z5nhx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-17T12:26:24Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-17T12:26:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-17T12:26:26Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-17T12:26:24Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://440a0b310306c1cc40b9902a290720aff4b69193f362581f7c6754680d345d1a\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-17T12:26:26Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.101.100\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.64.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.64.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-17T12:26:24Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 17 12:26:30.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7382 replace -f -'
Jul 17 12:26:30.744: INFO: stderr: ""
Jul 17 12:26:30.744: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Jul 17 12:26:30.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-7382 delete pods e2e-test-httpd-pod'
Jul 17 12:26:43.131: INFO: stderr: ""
Jul 17 12:26:43.132: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:43.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7382" for this suite.

• [SLOW TEST:18.748 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":129,"skipped":2211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:43.154: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:26:45.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5758" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":2237,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:26:45.254: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3517
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 17 12:26:45.300: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 17 12:26:45.337: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 12:26:47.353: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:49.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:51.343: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:53.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:55.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:57.343: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:26:59.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:27:01.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:27:03.344: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 17 12:27:03.352: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 17 12:27:05.419: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.0 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3517 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:27:05.419: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:27:06.543: INFO: Found all expected endpoints: [netserver-0]
Jul 17 12:27:06.547: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.69.98 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3517 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:27:06.547: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:27:07.672: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:07.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3517" for this suite.

• [SLOW TEST:22.447 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":131,"skipped":2251,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:07.708: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-wxrb
STEP: Creating a pod to test atomic-volume-subpath
Jul 17 12:27:07.768: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wxrb" in namespace "subpath-4550" to be "Succeeded or Failed"
Jul 17 12:27:07.773: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.324053ms
Jul 17 12:27:09.779: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010904863s
Jul 17 12:27:11.785: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 4.016887605s
Jul 17 12:27:13.790: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 6.022591736s
Jul 17 12:27:15.795: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 8.027213243s
Jul 17 12:27:17.800: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 10.032622317s
Jul 17 12:27:19.809: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 12.040890649s
Jul 17 12:27:21.825: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 14.057052556s
Jul 17 12:27:23.830: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 16.062512291s
Jul 17 12:27:25.836: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 18.068324771s
Jul 17 12:27:27.844: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 20.075823408s
Jul 17 12:27:29.849: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Running", Reason="", readiness=true. Elapsed: 22.081458169s
Jul 17 12:27:31.858: INFO: Pod "pod-subpath-test-secret-wxrb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.090075764s
STEP: Saw pod success
Jul 17 12:27:31.858: INFO: Pod "pod-subpath-test-secret-wxrb" satisfied condition "Succeeded or Failed"
Jul 17 12:27:31.863: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-subpath-test-secret-wxrb container test-container-subpath-secret-wxrb: <nil>
STEP: delete the pod
Jul 17 12:27:31.891: INFO: Waiting for pod pod-subpath-test-secret-wxrb to disappear
Jul 17 12:27:31.898: INFO: Pod pod-subpath-test-secret-wxrb no longer exists
STEP: Deleting pod pod-subpath-test-secret-wxrb
Jul 17 12:27:31.899: INFO: Deleting pod "pod-subpath-test-secret-wxrb" in namespace "subpath-4550"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:31.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4550" for this suite.

• [SLOW TEST:24.204 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":132,"skipped":2264,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:31.912: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:27:31.967: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-294ef931-3dbd-492f-9f6f-f2130ee60efb" in namespace "security-context-test-5968" to be "Succeeded or Failed"
Jul 17 12:27:31.970: INFO: Pod "alpine-nnp-false-294ef931-3dbd-492f-9f6f-f2130ee60efb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086822ms
Jul 17 12:27:33.977: INFO: Pod "alpine-nnp-false-294ef931-3dbd-492f-9f6f-f2130ee60efb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00958748s
Jul 17 12:27:35.985: INFO: Pod "alpine-nnp-false-294ef931-3dbd-492f-9f6f-f2130ee60efb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017997433s
Jul 17 12:27:35.985: INFO: Pod "alpine-nnp-false-294ef931-3dbd-492f-9f6f-f2130ee60efb" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5968" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":133,"skipped":2269,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:36.005: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 17 12:27:36.053: INFO: Waiting up to 5m0s for pod "downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9" in namespace "downward-api-8654" to be "Succeeded or Failed"
Jul 17 12:27:36.060: INFO: Pod "downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.69251ms
Jul 17 12:27:38.067: INFO: Pod "downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012917517s
STEP: Saw pod success
Jul 17 12:27:38.068: INFO: Pod "downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9" satisfied condition "Succeeded or Failed"
Jul 17 12:27:38.072: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9 container dapi-container: <nil>
STEP: delete the pod
Jul 17 12:27:38.097: INFO: Waiting for pod downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9 to disappear
Jul 17 12:27:38.101: INFO: Pod downward-api-8c7a47d6-f9b0-48a7-8230-63aff611f1a9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:38.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8654" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":134,"skipped":2283,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:38.118: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-77e678e1-c0b5-4093-8b1a-8d99daffdde9
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:42.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-249" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":135,"skipped":2284,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6304
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6304
I0717 12:27:42.372348      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6304, replica count: 2
I0717 12:27:45.424338      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:27:45.425: INFO: Creating new exec pod
Jul 17 12:27:48.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-6304 exec execpodnsw69 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 17 12:27:49.377: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 17 12:27:49.377: INFO: stdout: ""
Jul 17 12:27:49.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-6304 exec execpodnsw69 -- /bin/sh -x -c nc -zv -t -w 2 10.233.51.201 80'
Jul 17 12:27:49.640: INFO: stderr: "+ nc -zv -t -w 2 10.233.51.201 80\nConnection to 10.233.51.201 80 port [tcp/http] succeeded!\n"
Jul 17 12:27:49.641: INFO: stdout: ""
Jul 17 12:27:49.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-6304 exec execpodnsw69 -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.100 32303'
Jul 17 12:27:49.911: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.100 32303\nConnection to 192.168.101.100 32303 port [tcp/32303] succeeded!\n"
Jul 17 12:27:49.911: INFO: stdout: ""
Jul 17 12:27:49.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-6304 exec execpodnsw69 -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.67 32303'
Jul 17 12:27:50.177: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.67 32303\nConnection to 192.168.101.67 32303 port [tcp/32303] succeeded!\n"
Jul 17 12:27:50.177: INFO: stdout: ""
Jul 17 12:27:50.177: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:50.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6304" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.987 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":136,"skipped":2284,"failed":0}
SSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:50.228: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 17 12:27:50.305: INFO: starting watch
STEP: patching
STEP: updating
Jul 17 12:27:50.326: INFO: waiting for watch events with expected annotations
Jul 17 12:27:50.327: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:50.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4838" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":137,"skipped":2289,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-6665abb2-98ec-4979-b1cc-fb3a60fe100c
STEP: Creating a pod to test consume secrets
Jul 17 12:27:50.432: INFO: Waiting up to 5m0s for pod "pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377" in namespace "secrets-9006" to be "Succeeded or Failed"
Jul 17 12:27:50.437: INFO: Pod "pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828042ms
Jul 17 12:27:52.444: INFO: Pod "pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041027s
Jul 17 12:27:54.451: INFO: Pod "pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018439752s
STEP: Saw pod success
Jul 17 12:27:54.451: INFO: Pod "pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377" satisfied condition "Succeeded or Failed"
Jul 17 12:27:54.455: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:27:54.480: INFO: Waiting for pod pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377 to disappear
Jul 17 12:27:54.484: INFO: Pod pod-secrets-19b32aa7-41f4-4e57-8834-af8e9af7e377 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:54.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9006" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:54.505: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:56.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3849" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":139,"skipped":2354,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:56.613: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 17 12:27:56.687: INFO: Waiting up to 5m0s for pod "pod-2c72a48a-6f4d-44db-a040-ff84bff6a083" in namespace "emptydir-1096" to be "Succeeded or Failed"
Jul 17 12:27:56.693: INFO: Pod "pod-2c72a48a-6f4d-44db-a040-ff84bff6a083": Phase="Pending", Reason="", readiness=false. Elapsed: 5.629613ms
Jul 17 12:27:58.700: INFO: Pod "pod-2c72a48a-6f4d-44db-a040-ff84bff6a083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012724008s
STEP: Saw pod success
Jul 17 12:27:58.700: INFO: Pod "pod-2c72a48a-6f4d-44db-a040-ff84bff6a083" satisfied condition "Succeeded or Failed"
Jul 17 12:27:58.703: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-2c72a48a-6f4d-44db-a040-ff84bff6a083 container test-container: <nil>
STEP: delete the pod
Jul 17 12:27:58.728: INFO: Waiting for pod pod-2c72a48a-6f4d-44db-a040-ff84bff6a083 to disappear
Jul 17 12:27:58.734: INFO: Pod pod-2c72a48a-6f4d-44db-a040-ff84bff6a083 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:27:58.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1096" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":140,"skipped":2364,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:27:58.757: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-c12971cc-e01c-4b2f-aed4-9625bd213de2
STEP: Creating a pod to test consume secrets
Jul 17 12:27:58.807: INFO: Waiting up to 5m0s for pod "pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610" in namespace "secrets-8559" to be "Succeeded or Failed"
Jul 17 12:27:58.816: INFO: Pod "pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610": Phase="Pending", Reason="", readiness=false. Elapsed: 9.456826ms
Jul 17 12:28:00.820: INFO: Pod "pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013089147s
STEP: Saw pod success
Jul 17 12:28:00.820: INFO: Pod "pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610" satisfied condition "Succeeded or Failed"
Jul 17 12:28:00.824: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:28:00.846: INFO: Waiting for pod pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610 to disappear
Jul 17 12:28:00.850: INFO: Pod pod-secrets-d7df0d3e-4071-4039-a962-4f5c15a9b610 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:28:00.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8559" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:28:00.876: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7400
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 17 12:28:01.017: INFO: Found 0 stateful pods, waiting for 3
Jul 17 12:28:11.025: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:28:11.025: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:28:11.025: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:28:11.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:28:11.323: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:28:11.323: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:28:11.323: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 17 12:28:21.367: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 17 12:28:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:28:31.703: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:28:31.703: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:28:31.703: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:28:41.743: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:28:41.743: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 12:28:41.743: INFO: Waiting for Pod statefulset-7400/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 12:28:41.743: INFO: Waiting for Pod statefulset-7400/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 12:28:51.753: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:28:51.753: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 12:29:01.756: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:29:01.756: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jul 17 12:29:11.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:29:12.041: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:29:12.041: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:29:12.041: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:29:22.085: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 17 12:29:32.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-7400 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:29:32.407: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:29:32.407: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:29:32.407: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:29:42.454: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:29:42.454: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 17 12:29:42.454: INFO: Waiting for Pod statefulset-7400/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 17 12:29:42.454: INFO: Waiting for Pod statefulset-7400/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 17 12:29:52.465: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:29:52.465: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 17 12:29:52.465: INFO: Waiting for Pod statefulset-7400/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jul 17 12:30:02.471: INFO: Waiting for StatefulSet statefulset-7400/ss2 to complete update
Jul 17 12:30:02.471: INFO: Waiting for Pod statefulset-7400/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 12:30:12.462: INFO: Deleting all statefulset in ns statefulset-7400
Jul 17 12:30:12.465: INFO: Scaling statefulset ss2 to 0
Jul 17 12:30:42.506: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:30:42.514: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:30:42.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7400" for this suite.

• [SLOW TEST:161.679 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":142,"skipped":2400,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:30:42.558: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-46db2a5f-2eba-4b82-9c6e-e00f80b4777b
STEP: Creating a pod to test consume secrets
Jul 17 12:30:42.612: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d" in namespace "projected-6590" to be "Succeeded or Failed"
Jul 17 12:30:42.616: INFO: Pod "pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709742ms
Jul 17 12:30:44.621: INFO: Pod "pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008368258s
STEP: Saw pod success
Jul 17 12:30:44.621: INFO: Pod "pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d" satisfied condition "Succeeded or Failed"
Jul 17 12:30:44.624: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:30:44.665: INFO: Waiting for pod pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d to disappear
Jul 17 12:30:44.671: INFO: Pod pod-projected-secrets-344b8652-1567-48cc-bbbf-95a432e6c14d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:30:44.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6590" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2411,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:30:44.699: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Jul 17 12:30:44.820: INFO: Waiting up to 5m0s for pod "var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc" in namespace "var-expansion-5480" to be "Succeeded or Failed"
Jul 17 12:30:44.823: INFO: Pod "var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.970204ms
Jul 17 12:30:46.829: INFO: Pod "var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008384013s
STEP: Saw pod success
Jul 17 12:30:46.829: INFO: Pod "var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc" satisfied condition "Succeeded or Failed"
Jul 17 12:30:46.833: INFO: Trying to get logs from node taikun-1-1188-w-1 pod var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc container dapi-container: <nil>
STEP: delete the pod
Jul 17 12:30:46.857: INFO: Waiting for pod var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc to disappear
Jul 17 12:30:46.862: INFO: Pod var-expansion-f34154e2-d2df-47f0-9bc0-16e3c27ad5bc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:30:46.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5480" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2415,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:30:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:30:46.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1" in namespace "downward-api-5322" to be "Succeeded or Failed"
Jul 17 12:30:46.944: INFO: Pod "downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.902045ms
Jul 17 12:30:48.952: INFO: Pod "downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012586576s
STEP: Saw pod success
Jul 17 12:30:48.952: INFO: Pod "downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1" satisfied condition "Succeeded or Failed"
Jul 17 12:30:48.956: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1 container client-container: <nil>
STEP: delete the pod
Jul 17 12:30:48.983: INFO: Waiting for pod downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1 to disappear
Jul 17 12:30:48.993: INFO: Pod downwardapi-volume-5ce2fd69-2730-476b-ae24-62b4cadffba1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:30:48.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5322" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2421,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:30:49.020: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:30:49.070: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 17 12:30:54.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 create -f -'
Jul 17 12:30:55.643: INFO: stderr: ""
Jul 17 12:30:55.643: INFO: stdout: "e2e-test-crd-publish-openapi-5280-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 17 12:30:55.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 delete e2e-test-crd-publish-openapi-5280-crds test-foo'
Jul 17 12:30:55.812: INFO: stderr: ""
Jul 17 12:30:55.812: INFO: stdout: "e2e-test-crd-publish-openapi-5280-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 17 12:30:55.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 apply -f -'
Jul 17 12:30:56.229: INFO: stderr: ""
Jul 17 12:30:56.229: INFO: stdout: "e2e-test-crd-publish-openapi-5280-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 17 12:30:56.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 delete e2e-test-crd-publish-openapi-5280-crds test-foo'
Jul 17 12:30:56.361: INFO: stderr: ""
Jul 17 12:30:56.361: INFO: stdout: "e2e-test-crd-publish-openapi-5280-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 17 12:30:56.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 create -f -'
Jul 17 12:30:56.782: INFO: rc: 1
Jul 17 12:30:56.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 apply -f -'
Jul 17 12:30:57.227: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 17 12:30:57.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 create -f -'
Jul 17 12:30:57.640: INFO: rc: 1
Jul 17 12:30:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 --namespace=crd-publish-openapi-109 apply -f -'
Jul 17 12:30:58.063: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 17 12:30:58.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 explain e2e-test-crd-publish-openapi-5280-crds'
Jul 17 12:30:58.494: INFO: stderr: ""
Jul 17 12:30:58.494: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5280-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 17 12:30:58.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 explain e2e-test-crd-publish-openapi-5280-crds.metadata'
Jul 17 12:30:58.907: INFO: stderr: ""
Jul 17 12:30:58.907: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5280-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 17 12:30:58.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 explain e2e-test-crd-publish-openapi-5280-crds.spec'
Jul 17 12:30:59.478: INFO: stderr: ""
Jul 17 12:30:59.478: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5280-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 17 12:30:59.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 explain e2e-test-crd-publish-openapi-5280-crds.spec.bars'
Jul 17 12:30:59.893: INFO: stderr: ""
Jul 17 12:30:59.893: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5280-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 17 12:30:59.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-109 explain e2e-test-crd-publish-openapi-5280-crds.spec.bars2'
Jul 17 12:31:00.277: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:31:05.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-109" for this suite.

• [SLOW TEST:16.607 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":146,"skipped":2435,"failed":0}
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:31:05.627: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-6746
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6746 to expose endpoints map[]
Jul 17 12:31:05.706: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 17 12:31:06.719: INFO: successfully validated that service multi-endpoint-test in namespace services-6746 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6746
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6746 to expose endpoints map[pod1:[100]]
Jul 17 12:31:08.968: INFO: successfully validated that service multi-endpoint-test in namespace services-6746 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-6746
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6746 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 17 12:31:11.011: INFO: successfully validated that service multi-endpoint-test in namespace services-6746 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-6746
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6746 to expose endpoints map[pod2:[101]]
Jul 17 12:31:11.075: INFO: successfully validated that service multi-endpoint-test in namespace services-6746 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-6746
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6746 to expose endpoints map[]
Jul 17 12:31:11.123: INFO: successfully validated that service multi-endpoint-test in namespace services-6746 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:31:11.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6746" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.551 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":147,"skipped":2437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:31:11.185: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:31:11.945: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 17 12:31:13.966: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121871, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121871, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121872, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762121871, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:31:17.001: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:31:17.006: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:31:18.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8292" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.146 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":148,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:31:18.352: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:31:25.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2953" for this suite.

• [SLOW TEST:7.132 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":149,"skipped":2516,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:31:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 17 12:31:27.552: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7689 PodName:var-expansion-5bb9aee4-5f92-42c6-8338-91138b0d1b25 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:31:27.553: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: test for file in mounted path
Jul 17 12:31:27.673: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7689 PodName:var-expansion-5bb9aee4-5f92-42c6-8338-91138b0d1b25 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:31:27.673: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: updating the annotation value
Jul 17 12:31:28.315: INFO: Successfully updated pod "var-expansion-5bb9aee4-5f92-42c6-8338-91138b0d1b25"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 17 12:31:28.337: INFO: Deleting pod "var-expansion-5bb9aee4-5f92-42c6-8338-91138b0d1b25" in namespace "var-expansion-7689"
Jul 17 12:31:28.356: INFO: Wait up to 5m0s for pod "var-expansion-5bb9aee4-5f92-42c6-8338-91138b0d1b25" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:14.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7689" for this suite.

• [SLOW TEST:48.906 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":150,"skipped":2524,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:14.397: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-8h84
STEP: Creating a pod to test atomic-volume-subpath
Jul 17 12:32:14.467: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8h84" in namespace "subpath-633" to be "Succeeded or Failed"
Jul 17 12:32:14.472: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529724ms
Jul 17 12:32:16.479: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011155823s
Jul 17 12:32:18.485: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 4.017673414s
Jul 17 12:32:20.493: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 6.025502178s
Jul 17 12:32:22.511: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 8.043660289s
Jul 17 12:32:24.519: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 10.051627218s
Jul 17 12:32:26.526: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 12.058318853s
Jul 17 12:32:28.532: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 14.064869138s
Jul 17 12:32:30.537: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 16.069320117s
Jul 17 12:32:32.541: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 18.073226898s
Jul 17 12:32:34.548: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 20.08070413s
Jul 17 12:32:36.554: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Running", Reason="", readiness=true. Elapsed: 22.086740676s
Jul 17 12:32:38.566: INFO: Pod "pod-subpath-test-downwardapi-8h84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.099091503s
STEP: Saw pod success
Jul 17 12:32:38.567: INFO: Pod "pod-subpath-test-downwardapi-8h84" satisfied condition "Succeeded or Failed"
Jul 17 12:32:38.573: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-subpath-test-downwardapi-8h84 container test-container-subpath-downwardapi-8h84: <nil>
STEP: delete the pod
Jul 17 12:32:38.619: INFO: Waiting for pod pod-subpath-test-downwardapi-8h84 to disappear
Jul 17 12:32:38.623: INFO: Pod pod-subpath-test-downwardapi-8h84 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8h84
Jul 17 12:32:38.624: INFO: Deleting pod "pod-subpath-test-downwardapi-8h84" in namespace "subpath-633"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:38.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-633" for this suite.

• [SLOW TEST:24.242 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":151,"skipped":2536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:38.655: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a943cbb8-018d-4d9a-948f-00ffcf91a2f0
STEP: Creating a pod to test consume configMaps
Jul 17 12:32:38.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af" in namespace "projected-683" to be "Succeeded or Failed"
Jul 17 12:32:38.728: INFO: Pod "pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797908ms
Jul 17 12:32:40.733: INFO: Pod "pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008669611s
STEP: Saw pod success
Jul 17 12:32:40.733: INFO: Pod "pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af" satisfied condition "Succeeded or Failed"
Jul 17 12:32:40.736: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 12:32:40.765: INFO: Waiting for pod pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af to disappear
Jul 17 12:32:40.775: INFO: Pod pod-projected-configmaps-5cf26b00-5559-45ac-b25f-fe57b26680af no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:40.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-683" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":152,"skipped":2579,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:40.794: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-b830d592-21d2-4ade-a1bc-00db772721f2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b830d592-21d2-4ade-a1bc-00db772721f2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:44.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2020" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2591,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:44.928: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-add67a3a-811a-4111-8eba-3bf461260648
STEP: Creating a pod to test consume secrets
Jul 17 12:32:45.009: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241" in namespace "projected-1844" to be "Succeeded or Failed"
Jul 17 12:32:45.018: INFO: Pod "pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241": Phase="Pending", Reason="", readiness=false. Elapsed: 9.288205ms
Jul 17 12:32:47.022: INFO: Pod "pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013515925s
STEP: Saw pod success
Jul 17 12:32:47.023: INFO: Pod "pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241" satisfied condition "Succeeded or Failed"
Jul 17 12:32:47.026: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:32:47.047: INFO: Waiting for pod pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241 to disappear
Jul 17 12:32:47.050: INFO: Pod pod-projected-secrets-c9d0d632-5a88-41de-a8f4-0d1a45db8241 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:47.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1844" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":154,"skipped":2592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:47.075: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:55.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-835" for this suite.

• [SLOW TEST:8.098 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":155,"skipped":2616,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-1730/secret-test-ec482342-9797-47ba-9012-ea31809edb05
STEP: Creating a pod to test consume secrets
Jul 17 12:32:55.266: INFO: Waiting up to 5m0s for pod "pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f" in namespace "secrets-1730" to be "Succeeded or Failed"
Jul 17 12:32:55.272: INFO: Pod "pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.798035ms
Jul 17 12:32:57.279: INFO: Pod "pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012380385s
STEP: Saw pod success
Jul 17 12:32:57.279: INFO: Pod "pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f" satisfied condition "Succeeded or Failed"
Jul 17 12:32:57.284: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f container env-test: <nil>
STEP: delete the pod
Jul 17 12:32:57.311: INFO: Waiting for pod pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f to disappear
Jul 17 12:32:57.314: INFO: Pod pod-configmaps-0f38607e-d1d1-4c29-93fa-7f1d6a85494f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:57.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1730" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":156,"skipped":2621,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:57.326: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-84ec8b83-ed7f-411f-aafe-66880451d6c3
STEP: Creating a pod to test consume secrets
Jul 17 12:32:57.393: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473" in namespace "projected-7532" to be "Succeeded or Failed"
Jul 17 12:32:57.399: INFO: Pod "pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473": Phase="Pending", Reason="", readiness=false. Elapsed: 5.799364ms
Jul 17 12:32:59.407: INFO: Pod "pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014290331s
STEP: Saw pod success
Jul 17 12:32:59.408: INFO: Pod "pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473" satisfied condition "Succeeded or Failed"
Jul 17 12:32:59.412: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:32:59.436: INFO: Waiting for pod pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473 to disappear
Jul 17 12:32:59.441: INFO: Pod pod-projected-secrets-fe80a474-5854-4d47-afa0-e7b21e2b3473 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:59.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7532" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":157,"skipped":2637,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:59.462: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-fcb5b87e-b485-4aad-a3a5-eab768ac74bb
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:59.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3452" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":158,"skipped":2641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:59.535: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 17 12:32:59.589: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9742 /api/v1/namespaces/watch-9742/configmaps/e2e-watch-test-watch-closed b01c2ae7-ddb9-455e-84df-c055d7f23a47 83391 0 2021-07-17 12:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-17 12:32:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:32:59.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9742 /api/v1/namespaces/watch-9742/configmaps/e2e-watch-test-watch-closed b01c2ae7-ddb9-455e-84df-c055d7f23a47 83392 0 2021-07-17 12:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-17 12:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 17 12:32:59.608: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9742 /api/v1/namespaces/watch-9742/configmaps/e2e-watch-test-watch-closed b01c2ae7-ddb9-455e-84df-c055d7f23a47 83393 0 2021-07-17 12:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-17 12:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:32:59.608: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9742 /api/v1/namespaces/watch-9742/configmaps/e2e-watch-test-watch-closed b01c2ae7-ddb9-455e-84df-c055d7f23a47 83394 0 2021-07-17 12:32:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-17 12:32:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:32:59.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9742" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":159,"skipped":2664,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:32:59.630: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 17 12:32:59.677: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:13.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6885" for this suite.

• [SLOW TEST:13.522 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":160,"skipped":2687,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:13.153: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:13.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6401" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":161,"skipped":2690,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:13.304: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-d9rf
STEP: Creating a pod to test atomic-volume-subpath
Jul 17 12:33:13.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-d9rf" in namespace "subpath-4292" to be "Succeeded or Failed"
Jul 17 12:33:13.371: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146897ms
Jul 17 12:33:15.379: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 2.012225745s
Jul 17 12:33:17.386: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 4.019209107s
Jul 17 12:33:19.394: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 6.027053546s
Jul 17 12:33:21.400: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 8.033044642s
Jul 17 12:33:23.407: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 10.039561544s
Jul 17 12:33:25.413: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 12.046375773s
Jul 17 12:33:27.418: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 14.051373099s
Jul 17 12:33:29.424: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 16.057157968s
Jul 17 12:33:31.433: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 18.066361237s
Jul 17 12:33:33.440: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 20.073387264s
Jul 17 12:33:35.449: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Running", Reason="", readiness=true. Elapsed: 22.082004604s
Jul 17 12:33:37.455: INFO: Pod "pod-subpath-test-configmap-d9rf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.087878186s
STEP: Saw pod success
Jul 17 12:33:37.455: INFO: Pod "pod-subpath-test-configmap-d9rf" satisfied condition "Succeeded or Failed"
Jul 17 12:33:37.459: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-subpath-test-configmap-d9rf container test-container-subpath-configmap-d9rf: <nil>
STEP: delete the pod
Jul 17 12:33:37.481: INFO: Waiting for pod pod-subpath-test-configmap-d9rf to disappear
Jul 17 12:33:37.485: INFO: Pod pod-subpath-test-configmap-d9rf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-d9rf
Jul 17 12:33:37.485: INFO: Deleting pod "pod-subpath-test-configmap-d9rf" in namespace "subpath-4292"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:37.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4292" for this suite.

• [SLOW TEST:24.205 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":162,"skipped":2690,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:37.510: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 17 12:33:37.599: INFO: Waiting up to 5m0s for pod "pod-2b46d88d-65aa-46aa-b115-75c91c93a525" in namespace "emptydir-4764" to be "Succeeded or Failed"
Jul 17 12:33:37.604: INFO: Pod "pod-2b46d88d-65aa-46aa-b115-75c91c93a525": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439263ms
Jul 17 12:33:39.612: INFO: Pod "pod-2b46d88d-65aa-46aa-b115-75c91c93a525": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012823132s
Jul 17 12:33:41.617: INFO: Pod "pod-2b46d88d-65aa-46aa-b115-75c91c93a525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017787147s
STEP: Saw pod success
Jul 17 12:33:41.617: INFO: Pod "pod-2b46d88d-65aa-46aa-b115-75c91c93a525" satisfied condition "Succeeded or Failed"
Jul 17 12:33:41.621: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-2b46d88d-65aa-46aa-b115-75c91c93a525 container test-container: <nil>
STEP: delete the pod
Jul 17 12:33:41.652: INFO: Waiting for pod pod-2b46d88d-65aa-46aa-b115-75c91c93a525 to disappear
Jul 17 12:33:41.656: INFO: Pod pod-2b46d88d-65aa-46aa-b115-75c91c93a525 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:41.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4764" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":163,"skipped":2695,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:41.677: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Jul 17 12:33:41.731: INFO: Waiting up to 5m0s for pod "client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee" in namespace "containers-9967" to be "Succeeded or Failed"
Jul 17 12:33:41.734: INFO: Pod "client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357001ms
Jul 17 12:33:43.744: INFO: Pod "client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012926294s
STEP: Saw pod success
Jul 17 12:33:43.745: INFO: Pod "client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee" satisfied condition "Succeeded or Failed"
Jul 17 12:33:43.750: INFO: Trying to get logs from node taikun-1-1188-w-1 pod client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee container test-container: <nil>
STEP: delete the pod
Jul 17 12:33:43.770: INFO: Waiting for pod client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee to disappear
Jul 17 12:33:43.780: INFO: Pod client-containers-54639764-2fb3-4298-8f26-6c2098ae23ee no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:43.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9967" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2701,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 17 12:33:43.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-8884 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jul 17 12:33:44.005: INFO: stderr: ""
Jul 17 12:33:44.005: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 17 12:33:44.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-8884 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jul 17 12:33:44.741: INFO: stderr: ""
Jul 17 12:33:44.741: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jul 17 12:33:44.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-8884 delete pods e2e-test-httpd-pod'
Jul 17 12:33:45.478: INFO: stderr: ""
Jul 17 12:33:45.478: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:45.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8884" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":165,"skipped":2710,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-427db707-1015-431f-8d20-12545ae32d89
STEP: Creating a pod to test consume secrets
Jul 17 12:33:45.566: INFO: Waiting up to 5m0s for pod "pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda" in namespace "secrets-8370" to be "Succeeded or Failed"
Jul 17 12:33:45.574: INFO: Pod "pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17133ms
Jul 17 12:33:47.583: INFO: Pod "pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016080638s
STEP: Saw pod success
Jul 17 12:33:47.583: INFO: Pod "pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda" satisfied condition "Succeeded or Failed"
Jul 17 12:33:47.587: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:33:47.630: INFO: Waiting for pod pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda to disappear
Jul 17 12:33:47.633: INFO: Pod pod-secrets-d50f5e4b-ab5f-4260-bcac-742083b3fbda no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:33:47.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8370" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2713,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:33:47.646: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Jul 17 12:33:47.708: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:34:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9911" for this suite.

• [SLOW TEST:30.002 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":167,"skipped":2714,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:34:17.654: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 17 12:34:17.695: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:34:21.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4747" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":168,"skipped":2719,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:34:21.264: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Jul 17 12:34:21.313: INFO: Waiting up to 5m0s for pod "client-containers-d652d251-6f9f-4d08-a6c2-98298028f405" in namespace "containers-3975" to be "Succeeded or Failed"
Jul 17 12:34:21.323: INFO: Pod "client-containers-d652d251-6f9f-4d08-a6c2-98298028f405": Phase="Pending", Reason="", readiness=false. Elapsed: 9.020687ms
Jul 17 12:34:23.329: INFO: Pod "client-containers-d652d251-6f9f-4d08-a6c2-98298028f405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01501633s
STEP: Saw pod success
Jul 17 12:34:23.329: INFO: Pod "client-containers-d652d251-6f9f-4d08-a6c2-98298028f405" satisfied condition "Succeeded or Failed"
Jul 17 12:34:23.334: INFO: Trying to get logs from node taikun-1-1188-w-1 pod client-containers-d652d251-6f9f-4d08-a6c2-98298028f405 container test-container: <nil>
STEP: delete the pod
Jul 17 12:34:23.566: INFO: Waiting for pod client-containers-d652d251-6f9f-4d08-a6c2-98298028f405 to disappear
Jul 17 12:34:23.572: INFO: Pod client-containers-d652d251-6f9f-4d08-a6c2-98298028f405 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:34:23.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3975" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":169,"skipped":2729,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:34:23.598: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 17 12:34:24.612: INFO: starting watch
STEP: patching
STEP: updating
Jul 17 12:34:24.626: INFO: waiting for watch events with expected annotations
Jul 17 12:34:24.626: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:34:24.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-468" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":170,"skipped":2782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:34:24.722: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:34:40.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8306" for this suite.

• [SLOW TEST:16.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":171,"skipped":2818,"failed":0}
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:34:40.952: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7321, will wait for the garbage collector to delete the pods
Jul 17 12:34:45.082: INFO: Deleting Job.batch foo took: 12.732628ms
Jul 17 12:34:45.783: INFO: Terminating Job.batch foo pods took: 700.67203ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:35:23.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7321" for this suite.

• [SLOW TEST:42.259 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":172,"skipped":2818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:35:23.212: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3292
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3292
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3292
Jul 17 12:35:23.299: INFO: Found 0 stateful pods, waiting for 1
Jul 17 12:35:33.307: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 17 12:35:33.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:35:33.599: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:35:33.599: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:35:33.599: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:35:33.607: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 17 12:35:43.612: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:35:43.612: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:35:43.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999939s
Jul 17 12:35:44.643: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995140428s
Jul 17 12:35:45.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990669641s
Jul 17 12:35:46.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981882268s
Jul 17 12:35:47.664: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975802927s
Jul 17 12:35:48.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970159548s
Jul 17 12:35:49.680: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.961877027s
Jul 17 12:35:50.717: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.953206814s
Jul 17 12:35:51.935: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.916531215s
Jul 17 12:35:52.940: INFO: Verifying statefulset ss doesn't scale past 1 for another 698.155541ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3292
Jul 17 12:35:53.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:35:54.216: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:35:54.216: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:35:54.216: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:35:54.222: INFO: Found 1 stateful pods, waiting for 3
Jul 17 12:36:04.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:36:04.231: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:36:04.231: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 17 12:36:04.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:36:04.518: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:36:04.518: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:36:04.518: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:36:04.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:36:04.815: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:36:04.815: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:36:04.815: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:36:04.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:36:05.102: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:36:05.102: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:36:05.102: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:36:05.102: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:36:05.107: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 17 12:36:15.118: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:36:15.118: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:36:15.118: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:36:15.134: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999629s
Jul 17 12:36:16.140: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992823866s
Jul 17 12:36:17.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986531661s
Jul 17 12:36:18.163: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975322029s
Jul 17 12:36:19.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964579967s
Jul 17 12:36:20.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955100445s
Jul 17 12:36:21.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948196861s
Jul 17 12:36:22.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940701846s
Jul 17 12:36:23.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.93464619s
Jul 17 12:36:24.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.633547ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3292
Jul 17 12:36:25.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:36:25.494: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:36:25.494: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:36:25.494: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:36:25.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:36:25.784: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:36:25.784: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:36:25.784: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:36:25.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-3292 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:36:26.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:36:26.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:36:26.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:36:26.078: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 12:36:46.100: INFO: Deleting all statefulset in ns statefulset-3292
Jul 17 12:36:46.105: INFO: Scaling statefulset ss to 0
Jul 17 12:36:46.119: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:36:46.123: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:36:46.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3292" for this suite.

• [SLOW TEST:82.953 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":173,"skipped":2853,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:36:46.167: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-16f4d8b6-b00e-4818-8734-1ba1a4bf5487
STEP: Creating secret with name s-test-opt-upd-ada5b9eb-3675-48a9-9aae-297fa394fd0f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-16f4d8b6-b00e-4818-8734-1ba1a4bf5487
STEP: Updating secret s-test-opt-upd-ada5b9eb-3675-48a9-9aae-297fa394fd0f
STEP: Creating secret with name s-test-opt-create-ea3cb2dd-5e36-48e7-b19e-3e5f5fe7c943
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:08.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8424" for this suite.

• [SLOW TEST:82.708 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":174,"skipped":2860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:08.880: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:38:08.926: INFO: Creating deployment "test-recreate-deployment"
Jul 17 12:38:08.934: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 17 12:38:08.951: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 17 12:38:10.959: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 17 12:38:10.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122288, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122288, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122289, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122288, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 12:38:12.967: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 17 12:38:12.985: INFO: Updating deployment test-recreate-deployment
Jul 17 12:38:12.986: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 17 12:38:13.144: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9124 /apis/apps/v1/namespaces/deployment-9124/deployments/test-recreate-deployment 5f044cc9-0f84-4762-9f58-6a15f1924fd4 85392 2 2021-07-17 12:38:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-17 12:38:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 12:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405f968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-17 12:38:13 +0000 UTC,LastTransitionTime:2021-07-17 12:38:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-07-17 12:38:13 +0000 UTC,LastTransitionTime:2021-07-17 12:38:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 17 12:38:13.163: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9124 /apis/apps/v1/namespaces/deployment-9124/replicasets/test-recreate-deployment-f79dd4667 71c28991-5894-439d-8f2f-b606d17cba8a 85391 1 2021-07-17 12:38:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5f044cc9-0f84-4762-9f58-6a15f1924fd4 0xc00405fea0 0xc00405fea1}] []  [{kube-controller-manager Update apps/v1 2021-07-17 12:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f044cc9-0f84-4762-9f58-6a15f1924fd4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405ff18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 12:38:13.163: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 17 12:38:13.164: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-9124 /apis/apps/v1/namespaces/deployment-9124/replicasets/test-recreate-deployment-c96cf48f cb2f56ed-a03a-47db-b41b-1fdab27bec49 85380 2 2021-07-17 12:38:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5f044cc9-0f84-4762-9f58-6a15f1924fd4 0xc00405fdaf 0xc00405fdc0}] []  [{kube-controller-manager Update apps/v1 2021-07-17 12:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f044cc9-0f84-4762-9f58-6a15f1924fd4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405fe38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 12:38:13.168: INFO: Pod "test-recreate-deployment-f79dd4667-2drdw" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-2drdw test-recreate-deployment-f79dd4667- deployment-9124 /api/v1/namespaces/deployment-9124/pods/test-recreate-deployment-f79dd4667-2drdw 43931927-2497-4647-9352-de6972ab6846 85390 0 2021-07-17 12:38:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 71c28991-5894-439d-8f2f-b606d17cba8a 0xc00396a400 0xc00396a401}] []  [{kube-controller-manager Update v1 2021-07-17 12:38:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"71c28991-5894-439d-8f2f-b606d17cba8a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 12:38:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r5z9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r5z9z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r5z9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:38:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:38:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:38:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:38:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:,StartTime:2021-07-17 12:38:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9124" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":175,"skipped":2902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-9278
STEP: creating replication controller nodeport-test in namespace services-9278
I0717 12:38:13.274235      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9278, replica count: 2
I0717 12:38:16.328785      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:38:16.329: INFO: Creating new exec pod
Jul 17 12:38:19.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-9278 exec execpodmp2qz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 17 12:38:19.668: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 17 12:38:19.668: INFO: stdout: ""
Jul 17 12:38:19.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-9278 exec execpodmp2qz -- /bin/sh -x -c nc -zv -t -w 2 10.233.35.59 80'
Jul 17 12:38:19.953: INFO: stderr: "+ nc -zv -t -w 2 10.233.35.59 80\nConnection to 10.233.35.59 80 port [tcp/http] succeeded!\n"
Jul 17 12:38:19.953: INFO: stdout: ""
Jul 17 12:38:19.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-9278 exec execpodmp2qz -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.100 30148'
Jul 17 12:38:20.225: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.100 30148\nConnection to 192.168.101.100 30148 port [tcp/30148] succeeded!\n"
Jul 17 12:38:20.225: INFO: stdout: ""
Jul 17 12:38:20.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-9278 exec execpodmp2qz -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.67 30148'
Jul 17 12:38:20.477: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.67 30148\nConnection to 192.168.101.67 30148 port [tcp/30148] succeeded!\n"
Jul 17 12:38:20.477: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:20.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9278" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.309 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":176,"skipped":2924,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:20.499: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:38:20.544: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 17 12:38:22.590: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:23.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-19" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":177,"skipped":2930,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:23.650: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-9398
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9398
STEP: Deleting pre-stop pod
Jul 17 12:38:34.783: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:34.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9398" for this suite.

• [SLOW TEST:11.208 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":178,"skipped":2932,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:34.862: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:38:35.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4" in namespace "downward-api-1055" to be "Succeeded or Failed"
Jul 17 12:38:35.020: INFO: Pod "downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.811438ms
Jul 17 12:38:37.028: INFO: Pod "downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01636494s
Jul 17 12:38:39.034: INFO: Pod "downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022230343s
STEP: Saw pod success
Jul 17 12:38:39.034: INFO: Pod "downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4" satisfied condition "Succeeded or Failed"
Jul 17 12:38:39.039: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4 container client-container: <nil>
STEP: delete the pod
Jul 17 12:38:39.064: INFO: Waiting for pod downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4 to disappear
Jul 17 12:38:39.067: INFO: Pod downwardapi-volume-616ff890-ed2a-4bb1-a1eb-57605a71d6c4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:39.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1055" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2951,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:39.087: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:38:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 17 12:38:45.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-5041 --namespace=crd-publish-openapi-5041 create -f -'
Jul 17 12:38:46.240: INFO: stderr: ""
Jul 17 12:38:46.240: INFO: stdout: "e2e-test-crd-publish-openapi-7177-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 17 12:38:46.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-5041 --namespace=crd-publish-openapi-5041 delete e2e-test-crd-publish-openapi-7177-crds test-cr'
Jul 17 12:38:46.396: INFO: stderr: ""
Jul 17 12:38:46.396: INFO: stdout: "e2e-test-crd-publish-openapi-7177-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 17 12:38:46.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-5041 --namespace=crd-publish-openapi-5041 apply -f -'
Jul 17 12:38:46.847: INFO: stderr: ""
Jul 17 12:38:46.847: INFO: stdout: "e2e-test-crd-publish-openapi-7177-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 17 12:38:46.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-5041 --namespace=crd-publish-openapi-5041 delete e2e-test-crd-publish-openapi-7177-crds test-cr'
Jul 17 12:38:46.997: INFO: stderr: ""
Jul 17 12:38:46.997: INFO: stdout: "e2e-test-crd-publish-openapi-7177-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 17 12:38:46.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-5041 explain e2e-test-crd-publish-openapi-7177-crds'
Jul 17 12:38:47.463: INFO: stderr: ""
Jul 17 12:38:47.463: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7177-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:38:52.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5041" for this suite.

• [SLOW TEST:13.711 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":180,"skipped":2973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:38:52.807: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2305
STEP: creating service affinity-nodeport-transition in namespace services-2305
STEP: creating replication controller affinity-nodeport-transition in namespace services-2305
I0717 12:38:52.872577      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-2305, replica count: 3
I0717 12:38:55.923198      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:38:55.943: INFO: Creating new exec pod
Jul 17 12:38:58.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jul 17 12:38:59.224: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 17 12:38:59.224: INFO: stdout: ""
Jul 17 12:38:59.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c nc -zv -t -w 2 10.233.37.119 80'
Jul 17 12:38:59.493: INFO: stderr: "+ nc -zv -t -w 2 10.233.37.119 80\nConnection to 10.233.37.119 80 port [tcp/http] succeeded!\n"
Jul 17 12:38:59.493: INFO: stdout: ""
Jul 17 12:38:59.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.100 30569'
Jul 17 12:38:59.728: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.100 30569\nConnection to 192.168.101.100 30569 port [tcp/30569] succeeded!\n"
Jul 17 12:38:59.728: INFO: stdout: ""
Jul 17 12:38:59.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.67 30569'
Jul 17 12:38:59.988: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.67 30569\nConnection to 192.168.101.67 30569 port [tcp/30569] succeeded!\n"
Jul 17 12:38:59.988: INFO: stdout: ""
Jul 17 12:39:00.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.100:30569/ ; done'
Jul 17 12:39:00.526: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n"
Jul 17 12:39:00.526: INFO: stdout: "\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb"
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:00.526: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:00.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.100:30569/ ; done'
Jul 17 12:39:01.040: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n"
Jul 17 12:39:01.040: INFO: stdout: "\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-zm6lb\naffinity-nodeport-transition-mwdtr\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm"
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-zm6lb
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-mwdtr
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:01.040: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2305 exec execpod-affinity5xkvc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.100:30569/ ; done'
Jul 17 12:39:31.421: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30569/\n"
Jul 17 12:39:31.421: INFO: stdout: "\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm\naffinity-nodeport-transition-f52rm"
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Received response from host: affinity-nodeport-transition-f52rm
Jul 17 12:39:31.421: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2305, will wait for the garbage collector to delete the pods
Jul 17 12:39:31.553: INFO: Deleting ReplicationController affinity-nodeport-transition took: 31.00509ms
Jul 17 12:39:32.261: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 708.142808ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:43.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2305" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:50.533 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":181,"skipped":3003,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1307
STEP: creating the pod
Jul 17 12:39:43.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 create -f -'
Jul 17 12:39:44.067: INFO: stderr: ""
Jul 17 12:39:44.067: INFO: stdout: "pod/pause created\n"
Jul 17 12:39:44.067: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 17 12:39:44.068: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6994" to be "running and ready"
Jul 17 12:39:44.080: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.525038ms
Jul 17 12:39:46.084: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016861081s
Jul 17 12:39:46.085: INFO: Pod "pause" satisfied condition "running and ready"
Jul 17 12:39:46.085: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 17 12:39:46.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 label pods pause testing-label=testing-label-value'
Jul 17 12:39:46.233: INFO: stderr: ""
Jul 17 12:39:46.233: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 17 12:39:46.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 get pod pause -L testing-label'
Jul 17 12:39:46.371: INFO: stderr: ""
Jul 17 12:39:46.371: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 17 12:39:46.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 label pods pause testing-label-'
Jul 17 12:39:46.522: INFO: stderr: ""
Jul 17 12:39:46.522: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 17 12:39:46.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 get pod pause -L testing-label'
Jul 17 12:39:46.667: INFO: stderr: ""
Jul 17 12:39:46.667: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1313
STEP: using delete to clean up resources
Jul 17 12:39:46.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 delete --grace-period=0 --force -f -'
Jul 17 12:39:46.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 17 12:39:46.830: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 17 12:39:46.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 get rc,svc -l name=pause --no-headers'
Jul 17 12:39:47.040: INFO: stderr: "No resources found in kubectl-6994 namespace.\n"
Jul 17 12:39:47.040: INFO: stdout: ""
Jul 17 12:39:47.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-6994 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 17 12:39:47.272: INFO: stderr: ""
Jul 17 12:39:47.272: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:47.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6994" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":182,"skipped":3004,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:47.300: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 17 12:39:50.409: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:50.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2461" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":183,"skipped":3017,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:50.464: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:52.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1766" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":184,"skipped":3038,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:52.588: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8febfd4d-1bd6-4a8c-9444-65f77eae45a5
STEP: Creating a pod to test consume secrets
Jul 17 12:39:52.672: INFO: Waiting up to 5m0s for pod "pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757" in namespace "secrets-8820" to be "Succeeded or Failed"
Jul 17 12:39:52.681: INFO: Pod "pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757": Phase="Pending", Reason="", readiness=false. Elapsed: 9.164325ms
Jul 17 12:39:54.691: INFO: Pod "pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01923832s
STEP: Saw pod success
Jul 17 12:39:54.691: INFO: Pod "pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757" satisfied condition "Succeeded or Failed"
Jul 17 12:39:54.697: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:39:54.759: INFO: Waiting for pod pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757 to disappear
Jul 17 12:39:54.764: INFO: Pod pod-secrets-1bf7494a-d3f8-41a6-96cc-d68ba2d1b757 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8820" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-543a4489-b162-4f38-96cb-5daaf97e1b16
STEP: Creating a pod to test consume configMaps
Jul 17 12:39:54.871: INFO: Waiting up to 5m0s for pod "pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27" in namespace "configmap-5814" to be "Succeeded or Failed"
Jul 17 12:39:54.888: INFO: Pod "pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27": Phase="Pending", Reason="", readiness=false. Elapsed: 15.76819ms
Jul 17 12:39:56.894: INFO: Pod "pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021504779s
STEP: Saw pod success
Jul 17 12:39:56.894: INFO: Pod "pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27" satisfied condition "Succeeded or Failed"
Jul 17 12:39:56.899: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 12:39:56.931: INFO: Waiting for pod pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27 to disappear
Jul 17 12:39:56.953: INFO: Pod pod-configmaps-36acc31c-f4bd-4923-b83b-dfdd55e64d27 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:56.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5814" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":186,"skipped":3078,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:57.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2744" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":187,"skipped":3079,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:57.114: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 17 12:39:57.159: INFO: Created pod &Pod{ObjectMeta:{dns-5252  dns-5252 /api/v1/namespaces/dns-5252/pods/dns-5252 d6bde52c-92b0-4469-9473-673046667d9b 86435 0 2021-07-17 12:39:57 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-07-17 12:39:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nwrvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nwrvw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nwrvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 17 12:39:57.168: INFO: The status of Pod dns-5252 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 12:39:59.181: INFO: The status of Pod dns-5252 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 17 12:39:59.181: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5252 PodName:dns-5252 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:39:59.182: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Verifying customized DNS server is configured on pod...
Jul 17 12:39:59.312: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5252 PodName:dns-5252 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:39:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:39:59.431: INFO: Deleting pod dns-5252...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:39:59.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5252" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":188,"skipped":3084,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:39:59.477: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Jul 17 12:39:59.549: INFO: Waiting up to 5m0s for pod "var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7" in namespace "var-expansion-5867" to be "Succeeded or Failed"
Jul 17 12:39:59.576: INFO: Pod "var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.957858ms
Jul 17 12:40:01.589: INFO: Pod "var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.040071605s
STEP: Saw pod success
Jul 17 12:40:01.590: INFO: Pod "var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7" satisfied condition "Succeeded or Failed"
Jul 17 12:40:01.600: INFO: Trying to get logs from node taikun-1-1188-w-1 pod var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7 container dapi-container: <nil>
STEP: delete the pod
Jul 17 12:40:01.627: INFO: Waiting for pod var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7 to disappear
Jul 17 12:40:01.635: INFO: Pod var-expansion-f360cbf0-87cf-48a7-9d60-3d387b0484e7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:40:01.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5867" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":189,"skipped":3091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:40:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 17 12:40:01.712: INFO: Waiting up to 5m0s for pod "pod-20d2391a-fefc-4ed2-863d-dd644e538c9f" in namespace "emptydir-5196" to be "Succeeded or Failed"
Jul 17 12:40:01.716: INFO: Pod "pod-20d2391a-fefc-4ed2-863d-dd644e538c9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015829ms
Jul 17 12:40:03.729: INFO: Pod "pod-20d2391a-fefc-4ed2-863d-dd644e538c9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017278969s
STEP: Saw pod success
Jul 17 12:40:03.729: INFO: Pod "pod-20d2391a-fefc-4ed2-863d-dd644e538c9f" satisfied condition "Succeeded or Failed"
Jul 17 12:40:03.737: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-20d2391a-fefc-4ed2-863d-dd644e538c9f container test-container: <nil>
STEP: delete the pod
Jul 17 12:40:03.767: INFO: Waiting for pod pod-20d2391a-fefc-4ed2-863d-dd644e538c9f to disappear
Jul 17 12:40:03.777: INFO: Pod pod-20d2391a-fefc-4ed2-863d-dd644e538c9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:40:03.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5196" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":190,"skipped":3149,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:40:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 17 12:40:03.836: INFO: Waiting up to 5m0s for pod "pod-31c479f3-679e-4154-b5f7-800325e7e2dc" in namespace "emptydir-4188" to be "Succeeded or Failed"
Jul 17 12:40:03.842: INFO: Pod "pod-31c479f3-679e-4154-b5f7-800325e7e2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.823383ms
Jul 17 12:40:05.847: INFO: Pod "pod-31c479f3-679e-4154-b5f7-800325e7e2dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010874444s
STEP: Saw pod success
Jul 17 12:40:05.847: INFO: Pod "pod-31c479f3-679e-4154-b5f7-800325e7e2dc" satisfied condition "Succeeded or Failed"
Jul 17 12:40:05.850: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-31c479f3-679e-4154-b5f7-800325e7e2dc container test-container: <nil>
STEP: delete the pod
Jul 17 12:40:05.919: INFO: Waiting for pod pod-31c479f3-679e-4154-b5f7-800325e7e2dc to disappear
Jul 17 12:40:05.935: INFO: Pod pod-31c479f3-679e-4154-b5f7-800325e7e2dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:40:05.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4188" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3157,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:40:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454
Jul 17 12:40:06.030: INFO: Pod name my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454: Found 0 pods out of 1
Jul 17 12:40:11.042: INFO: Pod name my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454: Found 1 pods out of 1
Jul 17 12:40:11.043: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454" are running
Jul 17 12:40:11.052: INFO: Pod "my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454-kqr6f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:40:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:40:07 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:40:07 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:40:06 +0000 UTC Reason: Message:}])
Jul 17 12:40:11.053: INFO: Trying to dial the pod
Jul 17 12:40:16.067: INFO: Controller my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454: Got expected result from replica 1 [my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454-kqr6f]: "my-hostname-basic-d4612372-29e1-460e-9242-af60a5eab454-kqr6f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:40:16.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-822" for this suite.

• [SLOW TEST:10.119 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":192,"skipped":3171,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:40:16.082: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2276
Jul 17 12:40:18.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 17 12:40:18.456: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 17 12:40:18.456: INFO: stdout: "ipvs"
Jul 17 12:40:18.456: INFO: proxyMode: ipvs
Jul 17 12:40:18.469: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 12:40:18.474: INFO: Pod kube-proxy-mode-detector still exists
Jul 17 12:40:20.474: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 12:40:20.480: INFO: Pod kube-proxy-mode-detector still exists
Jul 17 12:40:22.474: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 12:40:22.478: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2276
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2276
I0717 12:40:22.518941      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2276, replica count: 3
I0717 12:40:25.571688      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:40:25.581: INFO: Creating new exec pod
Jul 17 12:40:28.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec execpod-affinitycv8ls -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jul 17 12:40:28.903: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 17 12:40:28.903: INFO: stdout: ""
Jul 17 12:40:28.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec execpod-affinitycv8ls -- /bin/sh -x -c nc -zv -t -w 2 10.233.44.205 80'
Jul 17 12:40:29.187: INFO: stderr: "+ nc -zv -t -w 2 10.233.44.205 80\nConnection to 10.233.44.205 80 port [tcp/http] succeeded!\n"
Jul 17 12:40:29.187: INFO: stdout: ""
Jul 17 12:40:29.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec execpod-affinitycv8ls -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.44.205:80/ ; done'
Jul 17 12:40:29.490: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n"
Jul 17 12:40:29.490: INFO: stdout: "\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd\naffinity-clusterip-timeout-fwmcd"
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Received response from host: affinity-clusterip-timeout-fwmcd
Jul 17 12:40:29.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec execpod-affinitycv8ls -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.44.205:80/'
Jul 17 12:40:29.773: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n"
Jul 17 12:40:29.773: INFO: stdout: "affinity-clusterip-timeout-fwmcd"
Jul 17 12:42:34.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-2276 exec execpod-affinitycv8ls -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.233.44.205:80/'
Jul 17 12:42:35.089: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.233.44.205:80/\n"
Jul 17 12:42:35.089: INFO: stdout: "affinity-clusterip-timeout-jv4mt"
Jul 17 12:42:35.089: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2276, will wait for the garbage collector to delete the pods
Jul 17 12:42:35.226: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 19.919593ms
Jul 17 12:42:35.927: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 700.968308ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:42:43.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2276" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:147.212 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":193,"skipped":3178,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:42:43.299: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0717 12:42:44.638965      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 12:43:46.668: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:43:46.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9846" for this suite.

• [SLOW TEST:63.388 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":194,"skipped":3181,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:43:46.690: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:43:46.742: INFO: Creating ReplicaSet my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c
Jul 17 12:43:46.756: INFO: Pod name my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c: Found 0 pods out of 1
Jul 17 12:43:51.776: INFO: Pod name my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c: Found 1 pods out of 1
Jul 17 12:43:51.776: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c" is running
Jul 17 12:43:51.789: INFO: Pod "my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c-zrlzn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:43:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:43:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:43:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-17 12:43:46 +0000 UTC Reason: Message:}])
Jul 17 12:43:51.791: INFO: Trying to dial the pod
Jul 17 12:43:56.807: INFO: Controller my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c: Got expected result from replica 1 [my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c-zrlzn]: "my-hostname-basic-77ea3061-7609-4554-bcba-dde4c4e3012c-zrlzn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:43:56.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1524" for this suite.

• [SLOW TEST:10.129 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":195,"skipped":3199,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:43:56.825: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:44:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6059" for this suite.

• [SLOW TEST:11.127 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":196,"skipped":3200,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:44:07.955: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:44:10.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5570" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":197,"skipped":3209,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:44:10.079: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 17 12:44:12.680: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148"
Jul 17 12:44:12.680: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148" in namespace "pods-2336" to be "terminated due to deadline exceeded"
Jul 17 12:44:12.695: INFO: Pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148": Phase="Running", Reason="", readiness=true. Elapsed: 15.294566ms
Jul 17 12:44:14.700: INFO: Pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148": Phase="Running", Reason="", readiness=true. Elapsed: 2.020102784s
Jul 17 12:44:16.705: INFO: Pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.024788488s
Jul 17 12:44:16.705: INFO: Pod "pod-update-activedeadlineseconds-b0bb3a16-1e25-4668-a7a2-2c2cf022a148" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:44:16.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2336" for this suite.

• [SLOW TEST:6.638 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3215,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:44:16.717: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:44:16.765: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783" in namespace "downward-api-369" to be "Succeeded or Failed"
Jul 17 12:44:16.768: INFO: Pod "downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783": Phase="Pending", Reason="", readiness=false. Elapsed: 3.692867ms
Jul 17 12:44:18.774: INFO: Pod "downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009239025s
STEP: Saw pod success
Jul 17 12:44:18.774: INFO: Pod "downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783" satisfied condition "Succeeded or Failed"
Jul 17 12:44:18.779: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783 container client-container: <nil>
STEP: delete the pod
Jul 17 12:44:18.807: INFO: Waiting for pod downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783 to disappear
Jul 17 12:44:18.811: INFO: Pod downwardapi-volume-ccc41d4b-aac1-4865-b216-10f499e3f783 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:44:18.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-369" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3227,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:44:18.824: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5528
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-5528
Jul 17 12:44:18.951: INFO: Found 0 stateful pods, waiting for 1
Jul 17 12:44:28.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 12:44:28.977: INFO: Deleting all statefulset in ns statefulset-5528
Jul 17 12:44:28.983: INFO: Scaling statefulset ss to 0
Jul 17 12:44:39.039: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:44:39.045: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:44:39.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5528" for this suite.

• [SLOW TEST:20.272 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":200,"skipped":3241,"failed":0}
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:44:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-61eb0dd9-3715-4a7a-a19f-1057b3b7c9bc in namespace container-probe-6571
Jul 17 12:44:41.175: INFO: Started pod liveness-61eb0dd9-3715-4a7a-a19f-1057b3b7c9bc in namespace container-probe-6571
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 12:44:41.181: INFO: Initial restart count of pod liveness-61eb0dd9-3715-4a7a-a19f-1057b3b7c9bc is 0
Jul 17 12:45:01.240: INFO: Restart count of pod container-probe-6571/liveness-61eb0dd9-3715-4a7a-a19f-1057b3b7c9bc is now 1 (20.059262359s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:45:01.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6571" for this suite.

• [SLOW TEST:22.221 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":201,"skipped":3241,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:45:01.325: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 17 12:45:01.709: INFO: Pod name wrapped-volume-race-5a2e4c63-9d5a-49a7-acd1-a00490e4e7ea: Found 0 pods out of 5
Jul 17 12:45:06.727: INFO: Pod name wrapped-volume-race-5a2e4c63-9d5a-49a7-acd1-a00490e4e7ea: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5a2e4c63-9d5a-49a7-acd1-a00490e4e7ea in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
Jul 17 12:45:16.840: INFO: Deleting ReplicationController wrapped-volume-race-5a2e4c63-9d5a-49a7-acd1-a00490e4e7ea took: 14.473978ms
Jul 17 12:45:17.540: INFO: Terminating ReplicationController wrapped-volume-race-5a2e4c63-9d5a-49a7-acd1-a00490e4e7ea pods took: 700.244387ms
STEP: Creating RC which spawns configmap-volume pods
Jul 17 12:45:33.269: INFO: Pod name wrapped-volume-race-aa112bd2-87fc-4e2d-8c5b-ad4456f3bd3e: Found 0 pods out of 5
Jul 17 12:45:38.285: INFO: Pod name wrapped-volume-race-aa112bd2-87fc-4e2d-8c5b-ad4456f3bd3e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-aa112bd2-87fc-4e2d-8c5b-ad4456f3bd3e in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
Jul 17 12:45:48.394: INFO: Deleting ReplicationController wrapped-volume-race-aa112bd2-87fc-4e2d-8c5b-ad4456f3bd3e took: 19.476826ms
Jul 17 12:45:49.097: INFO: Terminating ReplicationController wrapped-volume-race-aa112bd2-87fc-4e2d-8c5b-ad4456f3bd3e pods took: 703.558591ms
STEP: Creating RC which spawns configmap-volume pods
Jul 17 12:45:53.319: INFO: Pod name wrapped-volume-race-8de2cd38-1408-47eb-87af-5a9a95348230: Found 0 pods out of 5
Jul 17 12:45:58.335: INFO: Pod name wrapped-volume-race-8de2cd38-1408-47eb-87af-5a9a95348230: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8de2cd38-1408-47eb-87af-5a9a95348230 in namespace emptydir-wrapper-9309, will wait for the garbage collector to delete the pods
Jul 17 12:46:08.448: INFO: Deleting ReplicationController wrapped-volume-race-8de2cd38-1408-47eb-87af-5a9a95348230 took: 8.775975ms
Jul 17 12:46:09.148: INFO: Terminating ReplicationController wrapped-volume-race-8de2cd38-1408-47eb-87af-5a9a95348230 pods took: 700.485616ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:46:23.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9309" for this suite.

• [SLOW TEST:82.418 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":202,"skipped":3250,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:46:23.749: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0717 12:46:30.095829      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 12:47:32.125: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:47:32.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1199" for this suite.

• [SLOW TEST:68.390 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":203,"skipped":3258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:47:32.143: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:47:32.219: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9d4996c4-d2e3-48c1-87fd-beeff850f688" in namespace "security-context-test-3541" to be "Succeeded or Failed"
Jul 17 12:47:32.226: INFO: Pod "busybox-privileged-false-9d4996c4-d2e3-48c1-87fd-beeff850f688": Phase="Pending", Reason="", readiness=false. Elapsed: 7.430899ms
Jul 17 12:47:34.235: INFO: Pod "busybox-privileged-false-9d4996c4-d2e3-48c1-87fd-beeff850f688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016160849s
Jul 17 12:47:34.235: INFO: Pod "busybox-privileged-false-9d4996c4-d2e3-48c1-87fd-beeff850f688" satisfied condition "Succeeded or Failed"
Jul 17 12:47:34.256: INFO: Got logs for pod "busybox-privileged-false-9d4996c4-d2e3-48c1-87fd-beeff850f688": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:47:34.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3541" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3351,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:47:34.271: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 17 12:47:34.563: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 17 12:48:34.624: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 17 12:48:34.660: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 17 12:48:34.717: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:48:46.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4331" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:72.638 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":205,"skipped":3352,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:48:46.909: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 17 12:48:46.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7397 /api/v1/namespaces/watch-7397/configmaps/e2e-watch-test-resource-version c5ab6584-946f-40c5-877d-564b99eeb922 90472 0 2021-07-17 12:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-17 12:48:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 17 12:48:46.995: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7397 /api/v1/namespaces/watch-7397/configmaps/e2e-watch-test-resource-version c5ab6584-946f-40c5-877d-564b99eeb922 90473 0 2021-07-17 12:48:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-17 12:48:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:48:46.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7397" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":206,"skipped":3363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:48:47.010: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:03.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1872" for this suite.

• [SLOW TEST:16.130 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":207,"skipped":3387,"failed":0}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:03.141: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:49:03.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60" in namespace "downward-api-6945" to be "Succeeded or Failed"
Jul 17 12:49:03.245: INFO: Pod "downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290356ms
Jul 17 12:49:05.252: INFO: Pod "downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013060716s
STEP: Saw pod success
Jul 17 12:49:05.252: INFO: Pod "downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60" satisfied condition "Succeeded or Failed"
Jul 17 12:49:05.256: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60 container client-container: <nil>
STEP: delete the pod
Jul 17 12:49:05.317: INFO: Waiting for pod downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60 to disappear
Jul 17 12:49:05.324: INFO: Pod downwardapi-volume-717e869b-daca-4ae0-8916-b36672085d60 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:05.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6945" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3387,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:05.339: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:49:07.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 12:49:09.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122947, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122947, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122947, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122946, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:49:12.194: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:12.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4215" for this suite.
STEP: Destroying namespace "webhook-4215-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.025 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":209,"skipped":3401,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:12.366: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:49:12.403: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 17 12:49:12.411: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 17 12:49:17.419: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 17 12:49:17.419: INFO: Creating deployment "test-rolling-update-deployment"
Jul 17 12:49:17.426: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 17 12:49:17.441: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 17 12:49:19.456: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 17 12:49:19.458: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 17 12:49:19.468: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6020 /apis/apps/v1/namespaces/deployment-6020/deployments/test-rolling-update-deployment f42a5cc8-3ee5-427b-a75b-576aebcdf26b 90806 1 2021-07-17 12:49:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-17 12:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a19758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-17 12:49:17 +0000 UTC,LastTransitionTime:2021-07-17 12:49:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-07-17 12:49:18 +0000 UTC,LastTransitionTime:2021-07-17 12:49:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 17 12:49:19.473: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-6020 /apis/apps/v1/namespaces/deployment-6020/replicasets/test-rolling-update-deployment-c4cb8d6d9 8b8917b2-d120-44a8-8957-8f2664d36d3c 90793 1 2021-07-17 12:49:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f42a5cc8-3ee5-427b-a75b-576aebcdf26b 0xc003a19d00 0xc003a19d01}] []  [{kube-controller-manager Update apps/v1 2021-07-17 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f42a5cc8-3ee5-427b-a75b-576aebcdf26b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a19dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 17 12:49:19.473: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 17 12:49:19.473: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6020 /apis/apps/v1/namespaces/deployment-6020/replicasets/test-rolling-update-controller 249f0dc0-9fde-4800-b3d5-7d8fdded78b7 90805 2 2021-07-17 12:49:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f42a5cc8-3ee5-427b-a75b-576aebcdf26b 0xc003a19ba7 0xc003a19ba8}] []  [{e2e.test Update apps/v1 2021-07-17 12:49:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f42a5cc8-3ee5-427b-a75b-576aebcdf26b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a19c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 12:49:19.478: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-bc85b" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-bc85b test-rolling-update-deployment-c4cb8d6d9- deployment-6020 /api/v1/namespaces/deployment-6020/pods/test-rolling-update-deployment-c4cb8d6d9-bc85b fe18f4f9-570e-44ec-b622-3603aa9d42ef 90792 0 2021-07-17 12:49:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:10.233.64.129/32 cni.projectcalico.org/podIPs:10.233.64.129/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 8b8917b2-d120-44a8-8957-8f2664d36d3c 0xc002c57b50 0xc002c57b51}] []  [{kube-controller-manager Update v1 2021-07-17 12:49:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b8917b2-d120-44a8-8957-8f2664d36d3c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 12:49:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 12:49:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gdj62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gdj62,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gdj62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:49:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:49:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.129,StartTime:2021-07-17 12:49:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 12:49:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://e4d635429e9f346c4c16fe0995b9b7454b016c1e3c72854c680eab060d11a3e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:19.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6020" for this suite.

• [SLOW TEST:7.133 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":210,"skipped":3402,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:19.504: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:49:19.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715" in namespace "projected-7209" to be "Succeeded or Failed"
Jul 17 12:49:19.564: INFO: Pod "downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715": Phase="Pending", Reason="", readiness=false. Elapsed: 4.542582ms
Jul 17 12:49:21.571: INFO: Pod "downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010790455s
STEP: Saw pod success
Jul 17 12:49:21.571: INFO: Pod "downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715" satisfied condition "Succeeded or Failed"
Jul 17 12:49:21.576: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715 container client-container: <nil>
STEP: delete the pod
Jul 17 12:49:21.606: INFO: Waiting for pod downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715 to disappear
Jul 17 12:49:21.611: INFO: Pod downwardapi-volume-0cf6c55b-e9c9-4955-a4a5-8341456cf715 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:21.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7209" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":211,"skipped":3409,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:21.630: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 17 12:49:21.676: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 17 12:49:21.688: INFO: Waiting for terminating namespaces to be deleted...
Jul 17 12:49:21.693: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-1 before test
Jul 17 12:49:21.717: INFO: test-rolling-update-deployment-c4cb8d6d9-bc85b from deployment-6020 started at 2021-07-17 12:49:17 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container agnhost ready: true, restart count 0
Jul 17 12:49:21.717: INFO: calico-node-ggcth from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:49:21.717: INFO: csi-cinder-nodeplugin-hww5z from kube-system started at 2021-07-17 10:41:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:49:21.717: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:49:21.717: INFO: kube-proxy-rwc5f from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:49:21.717: INFO: nginx-proxy-taikun-1-1188-w-1 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:49:21.717: INFO: nodelocaldns-dsbzl from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:49:21.717: INFO: sonobuoy from sonobuoy started at 2021-07-17 11:46:52 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 17 12:49:21.717: INFO: sonobuoy-e2e-job-bd3d388fed13450e from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container e2e ready: true, restart count 0
Jul 17 12:49:21.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:49:21.717: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:49:21.717: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:49:21.717: INFO: 	Container systemd-logs ready: false, restart count 16
Jul 17 12:49:21.717: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-2 before test
Jul 17 12:49:21.728: INFO: calico-node-zsmxq from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.728: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:49:21.729: INFO: coredns-7677f9bb54-c5jf9 from kube-system started at 2021-07-17 08:47:31 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container coredns ready: true, restart count 0
Jul 17 12:49:21.729: INFO: csi-cinder-controllerplugin-694ccb959f-kxjgc from kube-system started at 2021-07-17 09:09:41 +0000 UTC (5 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 17 12:49:21.729: INFO: csi-cinder-nodeplugin-4v7m8 from kube-system started at 2021-07-17 08:48:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:49:21.729: INFO: kube-proxy-jvfsd from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:49:21.729: INFO: nginx-proxy-taikun-1-1188-w-2 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:49:21.729: INFO: nodelocaldns-pr4zp from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:49:21.729: INFO: snapshot-controller-0 from kube-system started at 2021-07-17 08:48:27 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul 17 12:49:21.729: INFO: event-exporter-6664b95b84-szltf from monitoring started at 2021-07-17 08:49:00 +0000 UTC (1 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container event-exporter ready: true, restart count 0
Jul 17 12:49:21.729: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:49:21.729: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:49:21.729: INFO: 	Container systemd-logs ready: false, restart count 17
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.169294ed2bc9bce3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.169294ed2cea9b88], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:22.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3637" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":212,"skipped":3411,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:22.833: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:49:23.668: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 12:49:25.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122963, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122963, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122963, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762122963, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:49:28.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:28.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5756" for this suite.
STEP: Destroying namespace "webhook-5756-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.961 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":213,"skipped":3423,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:28.797: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-ba4fbb93-b36e-416f-abea-eee1905805a0
STEP: Creating a pod to test consume secrets
Jul 17 12:49:28.889: INFO: Waiting up to 5m0s for pod "pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854" in namespace "secrets-2452" to be "Succeeded or Failed"
Jul 17 12:49:28.895: INFO: Pod "pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665291ms
Jul 17 12:49:30.902: INFO: Pod "pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01219763s
STEP: Saw pod success
Jul 17 12:49:30.902: INFO: Pod "pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854" satisfied condition "Succeeded or Failed"
Jul 17 12:49:30.906: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 12:49:30.945: INFO: Waiting for pod pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854 to disappear
Jul 17 12:49:30.948: INFO: Pod pod-secrets-c58102ba-862d-4b5b-b7f0-a0da822c0854 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:30.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2452" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":214,"skipped":3425,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:30.990: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:49:31.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:49:34.509: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:46.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7664" for this suite.
STEP: Destroying namespace "webhook-7664-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.844 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":215,"skipped":3429,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6836.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6836.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6836.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6836.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 18.17.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.17.18_udp@PTR;check="$$(dig +tcp +noall +answer +search 18.17.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.17.18_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6836.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6836.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6836.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6836.svc.taikun-1-1188 SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6836.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6836.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 18.17.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.17.18_udp@PTR;check="$$(dig +tcp +noall +answer +search 18.17.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.17.18_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:49:48.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:48.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:48.967: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:48.971: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:49.019: INFO: Unable to read jessie_udp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:49.028: INFO: Unable to read jessie_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:49.033: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:49.039: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:49.071: INFO: Lookups using dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27 failed for: [wheezy_udp@dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 jessie_udp@dns-test-service.dns-6836.svc.taikun-1-1188 jessie_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 jessie_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 jessie_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188]

Jul 17 12:49:54.078: INFO: Unable to read wheezy_udp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:54.084: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:54.088: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:54.091: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 from pod dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27: the server could not find the requested resource (get pods dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27)
Jul 17 12:49:54.143: INFO: Lookups using dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27 failed for: [wheezy_udp@dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_tcp@dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_udp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188 wheezy_tcp@_http._tcp.dns-test-service.dns-6836.svc.taikun-1-1188]

Jul 17 12:49:59.173: INFO: DNS probes using dns-6836/dns-test-39c06d84-e70d-4397-b08b-d562aa9f4e27 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:59.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6836" for this suite.

• [SLOW TEST:12.460 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":216,"skipped":3441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:59.301: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:49:59.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4353" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":217,"skipped":3472,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:49:59.430: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 17 12:50:01.517: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:01.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3123" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3473,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 12:50:01.662: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6" in namespace "downward-api-844" to be "Succeeded or Failed"
Jul 17 12:50:01.670: INFO: Pod "downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.857728ms
Jul 17 12:50:03.675: INFO: Pod "downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012778622s
STEP: Saw pod success
Jul 17 12:50:03.675: INFO: Pod "downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6" satisfied condition "Succeeded or Failed"
Jul 17 12:50:03.678: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6 container client-container: <nil>
STEP: delete the pod
Jul 17 12:50:03.722: INFO: Waiting for pod downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6 to disappear
Jul 17 12:50:03.725: INFO: Pod downwardapi-volume-ecfb76c6-d1dc-4df7-bbcb-499ca84a0ce6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:03.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-844" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:03.748: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Jul 17 12:50:03.790: INFO: created test-pod-1
Jul 17 12:50:03.800: INFO: created test-pod-2
Jul 17 12:50:03.827: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:03.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8103" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":220,"skipped":3562,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:03.963: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-d67b56cb-ed7d-4e92-8aff-9422649fc619
STEP: Creating a pod to test consume configMaps
Jul 17 12:50:04.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1" in namespace "projected-9802" to be "Succeeded or Failed"
Jul 17 12:50:04.059: INFO: Pod "pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516697ms
Jul 17 12:50:06.065: INFO: Pod "pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013812647s
STEP: Saw pod success
Jul 17 12:50:06.065: INFO: Pod "pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1" satisfied condition "Succeeded or Failed"
Jul 17 12:50:06.070: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 12:50:06.096: INFO: Waiting for pod pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1 to disappear
Jul 17 12:50:06.100: INFO: Pod pod-projected-configmaps-fed11694-c495-4245-860e-cc8471e4fdd1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:06.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9802" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":221,"skipped":3566,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:06.135: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Jul 17 12:50:08.750: INFO: Successfully updated pod "annotationupdate4e2f7208-edff-4db3-872c-c82652377e18"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:10.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4041" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3574,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:10.807: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 17 12:50:10.848: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 17 12:50:10.861: INFO: Waiting for terminating namespaces to be deleted...
Jul 17 12:50:10.865: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-1 before test
Jul 17 12:50:10.876: INFO: annotationupdate4e2f7208-edff-4db3-872c-c82652377e18 from downward-api-4041 started at 2021-07-17 12:50:06 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container client-container ready: true, restart count 0
Jul 17 12:50:10.876: INFO: calico-node-ggcth from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:50:10.876: INFO: csi-cinder-nodeplugin-hww5z from kube-system started at 2021-07-17 10:41:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:50:10.876: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:50:10.876: INFO: kube-proxy-rwc5f from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:50:10.876: INFO: nginx-proxy-taikun-1-1188-w-1 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:50:10.876: INFO: nodelocaldns-dsbzl from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.876: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:50:10.877: INFO: sonobuoy from sonobuoy started at 2021-07-17 11:46:52 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.877: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 17 12:50:10.877: INFO: sonobuoy-e2e-job-bd3d388fed13450e from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:50:10.877: INFO: 	Container e2e ready: true, restart count 0
Jul 17 12:50:10.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:50:10.877: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:50:10.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:50:10.877: INFO: 	Container systemd-logs ready: false, restart count 17
Jul 17 12:50:10.877: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-2 before test
Jul 17 12:50:10.886: INFO: calico-node-zsmxq from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:50:10.886: INFO: coredns-7677f9bb54-c5jf9 from kube-system started at 2021-07-17 08:47:31 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container coredns ready: true, restart count 0
Jul 17 12:50:10.886: INFO: csi-cinder-controllerplugin-694ccb959f-kxjgc from kube-system started at 2021-07-17 09:09:41 +0000 UTC (5 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 17 12:50:10.886: INFO: csi-cinder-nodeplugin-4v7m8 from kube-system started at 2021-07-17 08:48:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:50:10.886: INFO: kube-proxy-jvfsd from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:50:10.886: INFO: nginx-proxy-taikun-1-1188-w-2 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:50:10.886: INFO: nodelocaldns-pr4zp from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:50:10.886: INFO: snapshot-controller-0 from kube-system started at 2021-07-17 08:48:27 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul 17 12:50:10.886: INFO: event-exporter-6664b95b84-szltf from monitoring started at 2021-07-17 08:49:00 +0000 UTC (1 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container event-exporter ready: true, restart count 0
Jul 17 12:50:10.886: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:50:10.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:50:10.886: INFO: 	Container systemd-logs ready: false, restart count 17
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node taikun-1-1188-w-1
STEP: verifying the node has the label node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod annotationupdate4e2f7208-edff-4db3-872c-c82652377e18 requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod calico-node-ggcth requesting resource cpu=150m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod calico-node-zsmxq requesting resource cpu=150m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod coredns-7677f9bb54-c5jf9 requesting resource cpu=100m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod csi-cinder-controllerplugin-694ccb959f-kxjgc requesting resource cpu=0m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod csi-cinder-nodeplugin-4v7m8 requesting resource cpu=0m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod csi-cinder-nodeplugin-hww5z requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod kube-proxy-jvfsd requesting resource cpu=0m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod kube-proxy-rwc5f requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod nginx-proxy-taikun-1-1188-w-1 requesting resource cpu=25m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod nginx-proxy-taikun-1-1188-w-2 requesting resource cpu=25m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod nodelocaldns-dsbzl requesting resource cpu=100m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod nodelocaldns-pr4zp requesting resource cpu=100m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod event-exporter-6664b95b84-szltf requesting resource cpu=0m on Node taikun-1-1188-w-2
Jul 17 12:50:10.980: INFO: Pod sonobuoy requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod sonobuoy-e2e-job-bd3d388fed13450e requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 requesting resource cpu=0m on Node taikun-1-1188-w-1
Jul 17 12:50:10.980: INFO: Pod sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz requesting resource cpu=0m on Node taikun-1-1188-w-2
STEP: Starting Pods to consume most of the cluster CPU.
Jul 17 12:50:10.980: INFO: Creating a pod which consumes cpu=1067m on Node taikun-1-1188-w-2
Jul 17 12:50:11.000: INFO: Creating a pod which consumes cpu=1137m on Node taikun-1-1188-w-1
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3035058-8405-49b8-ada0-a29997fc6569.169294f8a087f8bd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3725/filler-pod-c3035058-8405-49b8-ada0-a29997fc6569 to taikun-1-1188-w-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3035058-8405-49b8-ada0-a29997fc6569.169294f8d225f602], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3035058-8405-49b8-ada0-a29997fc6569.169294f8d8c2299d], Reason = [Created], Message = [Created container filler-pod-c3035058-8405-49b8-ada0-a29997fc6569]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c3035058-8405-49b8-ada0-a29997fc6569.169294f8df2d8a6a], Reason = [Started], Message = [Started container filler-pod-c3035058-8405-49b8-ada0-a29997fc6569]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c.169294f8a2f90dab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3725/filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c to taikun-1-1188-w-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c.169294f8d200c40a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c.169294f8d7f3aa35], Reason = [Created], Message = [Created container filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c.169294f8df2318f7], Reason = [Started], Message = [Started container filler-pod-e247e82e-4866-48be-a032-49ba7b2d6f3c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169294f91c1ec994], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.169294f91d0c4237], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node taikun-1-1188-w-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node taikun-1-1188-w-1
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:14.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3725" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":223,"skipped":3575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:14.158: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-969
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 17 12:50:14.198: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 17 12:50:14.237: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 12:50:16.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:18.245: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:20.242: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:22.242: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:24.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:26.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:28.245: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:30.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:32.243: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 12:50:34.243: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 17 12:50:34.251: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 17 12:50:36.287: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.149:8080/dial?request=hostname&protocol=udp&host=10.233.64.148&port=8081&tries=1'] Namespace:pod-network-test-969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:50:36.287: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:50:36.409: INFO: Waiting for responses: map[]
Jul 17 12:50:36.413: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.149:8080/dial?request=hostname&protocol=udp&host=10.233.69.120&port=8081&tries=1'] Namespace:pod-network-test-969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:50:36.413: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:50:36.541: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:36.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-969" for this suite.

• [SLOW TEST:22.401 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":224,"skipped":3608,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:50:36.639: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 17 12:50:41.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-216 --namespace=crd-publish-openapi-216 create -f -'
Jul 17 12:50:42.792: INFO: stderr: ""
Jul 17 12:50:42.792: INFO: stdout: "e2e-test-crd-publish-openapi-1330-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 17 12:50:42.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-216 --namespace=crd-publish-openapi-216 delete e2e-test-crd-publish-openapi-1330-crds test-cr'
Jul 17 12:50:42.941: INFO: stderr: ""
Jul 17 12:50:42.941: INFO: stdout: "e2e-test-crd-publish-openapi-1330-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 17 12:50:42.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-216 --namespace=crd-publish-openapi-216 apply -f -'
Jul 17 12:50:43.433: INFO: stderr: ""
Jul 17 12:50:43.433: INFO: stdout: "e2e-test-crd-publish-openapi-1330-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 17 12:50:43.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-216 --namespace=crd-publish-openapi-216 delete e2e-test-crd-publish-openapi-1330-crds test-cr'
Jul 17 12:50:43.557: INFO: stderr: ""
Jul 17 12:50:43.557: INFO: stdout: "e2e-test-crd-publish-openapi-1330-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 17 12:50:43.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-216 explain e2e-test-crd-publish-openapi-1330-crds'
Jul 17 12:50:44.025: INFO: stderr: ""
Jul 17 12:50:44.025: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1330-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:49.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-216" for this suite.

• [SLOW TEST:13.243 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":225,"skipped":3637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:50:50.604: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 17 12:50:52.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123050, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123050, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123050, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123050, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:50:55.650: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 17 12:50:55.685: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:50:55.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3465" for this suite.
STEP: Destroying namespace "webhook-3465-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":226,"skipped":3671,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:50:56.066: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:50:56.126: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 17 12:51:01.135: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 17 12:51:01.135: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 17 12:51:05.202: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6219 /apis/apps/v1/namespaces/deployment-6219/deployments/test-cleanup-deployment 081094f1-7a33-4ff2-b4f0-4a9487016695 91985 1 2021-07-17 12:51:01 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-17 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050e3bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-17 12:51:01 +0000 UTC,LastTransitionTime:2021-07-17 12:51:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5d446bdd47" has successfully progressed.,LastUpdateTime:2021-07-17 12:51:03 +0000 UTC,LastTransitionTime:2021-07-17 12:51:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 17 12:51:05.206: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-6219 /apis/apps/v1/namespaces/deployment-6219/replicasets/test-cleanup-deployment-5d446bdd47 103ebb95-e7de-45c3-8fe7-8e7ce8df2879 91974 1 2021-07-17 12:51:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 081094f1-7a33-4ff2-b4f0-4a9487016695 0xc00510e287 0xc00510e288}] []  [{kube-controller-manager Update apps/v1 2021-07-17 12:51:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"081094f1-7a33-4ff2-b4f0-4a9487016695\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510e418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 17 12:51:05.211: INFO: Pod "test-cleanup-deployment-5d446bdd47-rh556" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-rh556 test-cleanup-deployment-5d446bdd47- deployment-6219 /api/v1/namespaces/deployment-6219/pods/test-cleanup-deployment-5d446bdd47-rh556 095f6d92-113c-40ba-b455-3d5a5fe635a3 91973 0 2021-07-17 12:51:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[cni.projectcalico.org/podIP:10.233.64.150/32 cni.projectcalico.org/podIPs:10.233.64.150/32] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 103ebb95-e7de-45c3-8fe7-8e7ce8df2879 0xc0050b9547 0xc0050b9548}] []  [{calico Update v1 2021-07-17 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2021-07-17 12:51:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"103ebb95-e7de-45c3-8fe7-8e7ce8df2879\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-17 12:51:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7j86c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7j86c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7j86c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:51:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:51:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 12:51:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.150,StartTime:2021-07-17 12:51:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 12:51:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://14c0fa99539b3dc7f5c946ba94392d5789ef1e9b06232659e82343234a42ddd5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:51:05.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6219" for this suite.

• [SLOW TEST:9.184 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":227,"skipped":3677,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:51:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:51:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:51:06.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6203" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":228,"skipped":3679,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:51:06.585: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:51:06.626: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:51:07.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3592" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":229,"skipped":3723,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:51:07.193: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 17 12:51:09.795: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6025 pod-service-account-863a5e9a-41d8-480c-8de5-ddf7e3b2893b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 17 12:51:10.065: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6025 pod-service-account-863a5e9a-41d8-480c-8de5-ddf7e3b2893b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 17 12:51:10.346: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6025 pod-service-account-863a5e9a-41d8-480c-8de5-ddf7e3b2893b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:51:10.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6025" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":230,"skipped":3742,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:51:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-5aadc616-ce81-4984-94eb-617f33498f85 in namespace container-probe-6574
Jul 17 12:51:12.779: INFO: Started pod liveness-5aadc616-ce81-4984-94eb-617f33498f85 in namespace container-probe-6574
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 12:51:12.783: INFO: Initial restart count of pod liveness-5aadc616-ce81-4984-94eb-617f33498f85 is 0
Jul 17 12:51:30.849: INFO: Restart count of pod container-probe-6574/liveness-5aadc616-ce81-4984-94eb-617f33498f85 is now 1 (18.065416012s elapsed)
Jul 17 12:51:50.908: INFO: Restart count of pod container-probe-6574/liveness-5aadc616-ce81-4984-94eb-617f33498f85 is now 2 (38.124873415s elapsed)
Jul 17 12:52:10.999: INFO: Restart count of pod container-probe-6574/liveness-5aadc616-ce81-4984-94eb-617f33498f85 is now 3 (58.215749807s elapsed)
Jul 17 12:52:31.057: INFO: Restart count of pod container-probe-6574/liveness-5aadc616-ce81-4984-94eb-617f33498f85 is now 4 (1m18.273434534s elapsed)
Jul 17 12:53:43.274: INFO: Restart count of pod container-probe-6574/liveness-5aadc616-ce81-4984-94eb-617f33498f85 is now 5 (2m30.490177536s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:53:43.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6574" for this suite.

• [SLOW TEST:152.627 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":231,"skipped":3747,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:53:43.350: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c81f1721-323c-4526-8822-5bec1ef93530 in namespace container-probe-687
Jul 17 12:53:45.426: INFO: Started pod liveness-c81f1721-323c-4526-8822-5bec1ef93530 in namespace container-probe-687
STEP: checking the pod's current state and verifying that restartCount is present
Jul 17 12:53:45.429: INFO: Initial restart count of pod liveness-c81f1721-323c-4526-8822-5bec1ef93530 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:57:46.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-687" for this suite.

• [SLOW TEST:242.913 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":232,"skipped":3754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:57:46.280: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 17 12:57:50.362: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7734 PodName:pod-sharedvolume-19a4208a-e191-41ef-b718-601c5403e6a7 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 12:57:50.362: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 12:57:50.482: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:57:50.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7734" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":233,"skipped":3785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:57:50.503: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 17 12:57:50.560: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 17 12:57:50.575: INFO: Waiting for terminating namespaces to be deleted...
Jul 17 12:57:50.579: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-1 before test
Jul 17 12:57:50.591: INFO: pod-sharedvolume-19a4208a-e191-41ef-b718-601c5403e6a7 from emptydir-7734 started at 2021-07-17 12:57:46 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.592: INFO: 	Container busybox-main-container ready: true, restart count 0
Jul 17 12:57:50.592: INFO: 	Container busybox-sub-container ready: false, restart count 0
Jul 17 12:57:50.592: INFO: calico-node-ggcth from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.592: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:57:50.592: INFO: csi-cinder-nodeplugin-hww5z from kube-system started at 2021-07-17 10:41:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.592: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:57:50.592: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:57:50.592: INFO: kube-proxy-rwc5f from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.593: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:57:50.593: INFO: nginx-proxy-taikun-1-1188-w-1 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.593: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:57:50.593: INFO: nodelocaldns-dsbzl from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.593: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:57:50.593: INFO: sonobuoy from sonobuoy started at 2021-07-17 11:46:52 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.593: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 17 12:57:50.593: INFO: sonobuoy-e2e-job-bd3d388fed13450e from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.593: INFO: 	Container e2e ready: true, restart count 0
Jul 17 12:57:50.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:57:50.594: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:57:50.594: INFO: 	Container systemd-logs ready: false, restart count 18
Jul 17 12:57:50.594: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-2 before test
Jul 17 12:57:50.610: INFO: calico-node-zsmxq from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.610: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 12:57:50.610: INFO: coredns-7677f9bb54-c5jf9 from kube-system started at 2021-07-17 08:47:31 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.610: INFO: 	Container coredns ready: true, restart count 0
Jul 17 12:57:50.611: INFO: csi-cinder-controllerplugin-694ccb959f-kxjgc from kube-system started at 2021-07-17 09:09:41 +0000 UTC (5 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:57:50.611: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 17 12:57:50.611: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 17 12:57:50.611: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 17 12:57:50.611: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 17 12:57:50.611: INFO: csi-cinder-nodeplugin-4v7m8 from kube-system started at 2021-07-17 08:48:19 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 12:57:50.611: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 12:57:50.611: INFO: kube-proxy-jvfsd from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 12:57:50.611: INFO: nginx-proxy-taikun-1-1188-w-2 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 12:57:50.611: INFO: nodelocaldns-pr4zp from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 12:57:50.611: INFO: snapshot-controller-0 from kube-system started at 2021-07-17 08:48:27 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul 17 12:57:50.611: INFO: event-exporter-6664b95b84-szltf from monitoring started at 2021-07-17 08:49:00 +0000 UTC (1 container statuses recorded)
Jul 17 12:57:50.611: INFO: 	Container event-exporter ready: true, restart count 0
Jul 17 12:57:50.611: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 12:57:50.612: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 12:57:50.612: INFO: 	Container systemd-logs ready: false, restart count 18
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-94696898-d2ab-4b2e-9319-e7704a94af5b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-94696898-d2ab-4b2e-9319-e7704a94af5b off the node taikun-1-1188-w-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-94696898-d2ab-4b2e-9319-e7704a94af5b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:57:54.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6527" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":234,"skipped":3817,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:57:54.784: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1845
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1845
I0717 12:57:54.880040      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-1845, replica count: 2
I0717 12:57:57.931425      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 12:57:57.931: INFO: Creating new exec pod
Jul 17 12:58:00.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-1845 exec execpodqfn77 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 17 12:58:01.302: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 17 12:58:01.303: INFO: stdout: ""
Jul 17 12:58:01.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-1845 exec execpodqfn77 -- /bin/sh -x -c nc -zv -t -w 2 10.233.4.50 80'
Jul 17 12:58:01.575: INFO: stderr: "+ nc -zv -t -w 2 10.233.4.50 80\nConnection to 10.233.4.50 80 port [tcp/http] succeeded!\n"
Jul 17 12:58:01.575: INFO: stdout: ""
Jul 17 12:58:01.575: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:58:01.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1845" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:6.840 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":235,"skipped":3820,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:58:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 17 12:58:02.261: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 12:58:05.515: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 12:58:05.520: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:58:07.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6599" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.824 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":236,"skipped":3833,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:58:07.449: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8664
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-8664
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8664
Jul 17 12:58:07.608: INFO: Found 0 stateful pods, waiting for 1
Jul 17 12:58:17.614: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 17 12:58:17.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:58:17.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:58:17.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:58:17.911: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:58:17.925: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 17 12:58:27.930: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:58:27.930: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:58:27.983: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jul 17 12:58:27.983: INFO: ss-0  taikun-1-1188-w-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  }]
Jul 17 12:58:27.983: INFO: ss-1                     Pending         []
Jul 17 12:58:27.983: INFO: 
Jul 17 12:58:27.983: INFO: StatefulSet ss has not reached scale 3, at 2
Jul 17 12:58:28.993: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987353899s
Jul 17 12:58:29.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.976822496s
Jul 17 12:58:31.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.971161183s
Jul 17 12:58:32.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965625956s
Jul 17 12:58:33.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958086202s
Jul 17 12:58:34.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951115965s
Jul 17 12:58:35.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943635361s
Jul 17 12:58:36.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932583901s
Jul 17 12:58:37.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.60137ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8664
Jul 17 12:58:38.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:58:38.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 17 12:58:38.306: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:58:38.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:58:38.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:58:38.570: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 17 12:58:38.570: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:58:38.570: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:58:38.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 17 12:58:38.817: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 17 12:58:38.817: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 17 12:58:38.817: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 17 12:58:38.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 17 12:58:48.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:58:48.827: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:58:48.827: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 17 12:58:48.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:58:49.059: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:58:49.059: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:58:49.059: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:58:49.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:58:49.352: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:58:49.352: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:58:49.352: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:58:49.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=statefulset-8664 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 17 12:58:49.601: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 17 12:58:49.601: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 17 12:58:49.601: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 17 12:58:49.601: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:58:49.606: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 17 12:58:59.616: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:58:59.616: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:58:59.617: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 17 12:58:59.635: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jul 17 12:58:59.635: INFO: ss-0  taikun-1-1188-w-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  }]
Jul 17 12:58:59.643: INFO: ss-1  taikun-1-1188-w-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:27 +0000 UTC  }]
Jul 17 12:58:59.643: INFO: ss-2  taikun-1-1188-w-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  }]
Jul 17 12:58:59.644: INFO: 
Jul 17 12:58:59.644: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 17 12:59:00.649: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jul 17 12:59:00.650: INFO: ss-0  taikun-1-1188-w-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  }]
Jul 17 12:59:00.650: INFO: ss-1  taikun-1-1188-w-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:27 +0000 UTC  }]
Jul 17 12:59:00.650: INFO: ss-2  taikun-1-1188-w-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  }]
Jul 17 12:59:00.650: INFO: 
Jul 17 12:59:00.650: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 17 12:59:01.656: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jul 17 12:59:01.657: INFO: ss-0  taikun-1-1188-w-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:07 +0000 UTC  }]
Jul 17 12:59:01.657: INFO: ss-1  taikun-1-1188-w-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:27 +0000 UTC  }]
Jul 17 12:59:01.657: INFO: ss-2  taikun-1-1188-w-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  }]
Jul 17 12:59:01.658: INFO: 
Jul 17 12:59:01.658: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 17 12:59:02.668: INFO: POD   NODE               PHASE    GRACE  CONDITIONS
Jul 17 12:59:02.668: INFO: ss-1  taikun-1-1188-w-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-17 12:58:27 +0000 UTC  }]
Jul 17 12:59:02.669: INFO: 
Jul 17 12:59:02.669: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 17 12:59:03.676: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.962082851s
Jul 17 12:59:04.680: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.955757514s
Jul 17 12:59:05.687: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.951133652s
Jul 17 12:59:06.699: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.94425601s
Jul 17 12:59:07.704: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.932464298s
Jul 17 12:59:08.711: INFO: Verifying statefulset ss doesn't scale past 0 for another 927.257883ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8664
Jul 17 12:59:09.720: INFO: Scaling statefulset ss to 0
Jul 17 12:59:09.735: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 12:59:09.739: INFO: Deleting all statefulset in ns statefulset-8664
Jul 17 12:59:09.743: INFO: Scaling statefulset ss to 0
Jul 17 12:59:09.756: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 12:59:09.759: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:59:09.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8664" for this suite.

• [SLOW TEST:62.353 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":237,"skipped":3840,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:59:09.803: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5448 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5448;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5448 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5448;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5448.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5448.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5448.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5448.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5448.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 168.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.168_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5448 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5448;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5448 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5448;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5448.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5448.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5448.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5448.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5448.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5448.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5448.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5448.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 168.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.168_udp@PTR;check="$$(dig +tcp +noall +answer +search 168.41.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.41.168_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 12:59:13.935: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.939: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.949: INFO: Unable to read wheezy_udp@dns-test-service.dns-5448 from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.955: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5448 from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.959: INFO: Unable to read wheezy_udp@dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.966: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:13.973: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.041: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.045: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.048: INFO: Unable to read jessie_udp@dns-test-service.dns-5448 from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.051: INFO: Unable to read jessie_tcp@dns-test-service.dns-5448 from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.055: INFO: Unable to read jessie_udp@dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.063: INFO: Unable to read jessie_tcp@dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.067: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.072: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5448.svc from pod dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051: the server could not find the requested resource (get pods dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051)
Jul 17 12:59:14.113: INFO: Lookups using dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5448 wheezy_tcp@dns-test-service.dns-5448 wheezy_udp@dns-test-service.dns-5448.svc wheezy_tcp@dns-test-service.dns-5448.svc wheezy_udp@_http._tcp.dns-test-service.dns-5448.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5448.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5448 jessie_tcp@dns-test-service.dns-5448 jessie_udp@dns-test-service.dns-5448.svc jessie_tcp@dns-test-service.dns-5448.svc jessie_udp@_http._tcp.dns-test-service.dns-5448.svc jessie_tcp@_http._tcp.dns-test-service.dns-5448.svc]

Jul 17 12:59:19.235: INFO: DNS probes using dns-5448/dns-test-0bdf05a3-aa1c-42d8-bf86-1be126ed6051 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 12:59:19.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5448" for this suite.

• [SLOW TEST:9.563 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":238,"skipped":3847,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 12:59:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6758
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Jul 17 12:59:19.475: INFO: Found 0 stateful pods, waiting for 3
Jul 17 12:59:29.484: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:59:29.484: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 12:59:29.484: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 17 12:59:29.526: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 17 12:59:39.574: INFO: Updating stateful set ss2
Jul 17 12:59:39.612: INFO: Waiting for Pod statefulset-6758/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 12:59:49.628: INFO: Waiting for Pod statefulset-6758/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 17 12:59:59.712: INFO: Found 2 stateful pods, waiting for 3
Jul 17 13:00:09.720: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 13:00:09.721: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 17 13:00:09.721: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 17 13:00:09.753: INFO: Updating stateful set ss2
Jul 17 13:00:10.032: INFO: Waiting for Pod statefulset-6758/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 13:00:20.041: INFO: Waiting for Pod statefulset-6758/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 13:00:30.067: INFO: Updating stateful set ss2
Jul 17 13:00:30.113: INFO: Waiting for StatefulSet statefulset-6758/ss2 to complete update
Jul 17 13:00:30.114: INFO: Waiting for Pod statefulset-6758/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 17 13:00:40.123: INFO: Waiting for StatefulSet statefulset-6758/ss2 to complete update
Jul 17 13:00:40.123: INFO: Waiting for Pod statefulset-6758/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jul 17 13:00:50.125: INFO: Deleting all statefulset in ns statefulset-6758
Jul 17 13:00:50.129: INFO: Scaling statefulset ss2 to 0
Jul 17 13:01:10.162: INFO: Waiting for statefulset status.replicas updated to 0
Jul 17 13:01:10.167: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:01:10.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6758" for this suite.

• [SLOW TEST:110.822 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":239,"skipped":3857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:01:10.211: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-4209/configmap-test-cda0954b-8845-41dc-9545-329d1e58c4f4
STEP: Creating a pod to test consume configMaps
Jul 17 13:01:10.269: INFO: Waiting up to 5m0s for pod "pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c" in namespace "configmap-4209" to be "Succeeded or Failed"
Jul 17 13:01:10.273: INFO: Pod "pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.686355ms
Jul 17 13:01:12.278: INFO: Pod "pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008203072s
STEP: Saw pod success
Jul 17 13:01:12.278: INFO: Pod "pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c" satisfied condition "Succeeded or Failed"
Jul 17 13:01:12.282: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c container env-test: <nil>
STEP: delete the pod
Jul 17 13:01:12.329: INFO: Waiting for pod pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c to disappear
Jul 17 13:01:12.333: INFO: Pod pod-configmaps-c49197d9-b8cb-4d1b-a4d3-7507cb61565c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:01:12.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4209" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:01:12.367: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 17 13:01:12.453: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 17 13:02:12.523: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:02:12.529: INFO: Starting informer...
STEP: Starting pod...
Jul 17 13:02:12.750: INFO: Pod is running on taikun-1-1188-w-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 17 13:02:12.789: INFO: Pod wasn't evicted. Proceeding
Jul 17 13:02:12.789: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 17 13:03:27.842: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:03:27.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9902" for this suite.

• [SLOW TEST:135.498 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":241,"skipped":3940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:03:27.872: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 17 13:03:27.926: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 17 13:04:27.973: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:04:27.979: INFO: Starting informer...
STEP: Starting pods...
Jul 17 13:04:28.209: INFO: Pod1 is running on taikun-1-1188-w-1. Tainting Node
Jul 17 13:04:30.441: INFO: Pod2 is running on taikun-1-1188-w-1. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 17 13:04:43.117: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 17 13:05:03.123: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:03.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4633" for this suite.

• [SLOW TEST:95.322 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":242,"skipped":3973,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:03.196: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:05:04.394: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 13:05:06.414: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123904, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123904, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123904, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123904, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:05:09.601: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:09.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6893" for this suite.
STEP: Destroying namespace "webhook-6893-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.762 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":243,"skipped":3975,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:09.961: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:05:10.689: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 13:05:12.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123910, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123910, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123910, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762123910, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:05:15.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:16.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1758" for this suite.
STEP: Destroying namespace "webhook-1758-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":244,"skipped":3984,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:16.681: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:16.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3958" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":245,"skipped":3992,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:16.791: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-471cf96a-a826-4583-83a0-a269d3caa5c4
STEP: Creating a pod to test consume secrets
Jul 17 13:05:16.880: INFO: Waiting up to 5m0s for pod "pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed" in namespace "secrets-530" to be "Succeeded or Failed"
Jul 17 13:05:16.894: INFO: Pod "pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed": Phase="Pending", Reason="", readiness=false. Elapsed: 14.307887ms
Jul 17 13:05:18.914: INFO: Pod "pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034253101s
Jul 17 13:05:20.919: INFO: Pod "pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039803883s
STEP: Saw pod success
Jul 17 13:05:20.919: INFO: Pod "pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed" satisfied condition "Succeeded or Failed"
Jul 17 13:05:20.925: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed container secret-env-test: <nil>
STEP: delete the pod
Jul 17 13:05:21.236: INFO: Waiting for pod pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed to disappear
Jul 17 13:05:21.242: INFO: Pod pod-secrets-4bebf597-f409-467e-ab95-58e9b08686ed no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:21.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-530" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":246,"skipped":3998,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:21.265: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 17 13:05:21.321: INFO: Waiting up to 5m0s for pod "downward-api-2667c159-3305-41b1-8413-b6cf7e64214e" in namespace "downward-api-6668" to be "Succeeded or Failed"
Jul 17 13:05:21.325: INFO: Pod "downward-api-2667c159-3305-41b1-8413-b6cf7e64214e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.222604ms
Jul 17 13:05:23.330: INFO: Pod "downward-api-2667c159-3305-41b1-8413-b6cf7e64214e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008088304s
STEP: Saw pod success
Jul 17 13:05:23.330: INFO: Pod "downward-api-2667c159-3305-41b1-8413-b6cf7e64214e" satisfied condition "Succeeded or Failed"
Jul 17 13:05:23.334: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downward-api-2667c159-3305-41b1-8413-b6cf7e64214e container dapi-container: <nil>
STEP: delete the pod
Jul 17 13:05:23.361: INFO: Waiting for pod downward-api-2667c159-3305-41b1-8413-b6cf7e64214e to disappear
Jul 17 13:05:23.364: INFO: Pod downward-api-2667c159-3305-41b1-8413-b6cf7e64214e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:23.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6668" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":247,"skipped":4002,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:23.379: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-2204/configmap-test-88e7804e-c056-4db1-8f23-31bedd8db61a
STEP: Creating a pod to test consume configMaps
Jul 17 13:05:23.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7" in namespace "configmap-2204" to be "Succeeded or Failed"
Jul 17 13:05:23.451: INFO: Pod "pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.369436ms
Jul 17 13:05:25.459: INFO: Pod "pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011966038s
STEP: Saw pod success
Jul 17 13:05:25.460: INFO: Pod "pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7" satisfied condition "Succeeded or Failed"
Jul 17 13:05:25.465: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7 container env-test: <nil>
STEP: delete the pod
Jul 17 13:05:25.502: INFO: Waiting for pod pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7 to disappear
Jul 17 13:05:25.506: INFO: Pod pod-configmaps-073cf979-1d6f-45f9-8b3d-e535b9288ce7 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:05:25.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2204" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":4010,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:05:25.528: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 17 13:05:25.622: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 17 13:05:25.636: INFO: Waiting for terminating namespaces to be deleted...
Jul 17 13:05:25.644: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-1 before test
Jul 17 13:05:25.659: INFO: calico-node-ggcth from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 13:05:25.659: INFO: csi-cinder-nodeplugin-t5kwd from kube-system started at 2021-07-17 13:05:03 +0000 UTC (2 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 13:05:25.659: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 13:05:25.659: INFO: kube-proxy-rwc5f from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 13:05:25.659: INFO: nginx-proxy-taikun-1-1188-w-1 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 13:05:25.659: INFO: nodelocaldns-dsbzl from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 13:05:25.659: INFO: sonobuoy from sonobuoy started at 2021-07-17 11:46:52 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 17 13:05:25.659: INFO: sonobuoy-e2e-job-bd3d388fed13450e from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container e2e ready: true, restart count 0
Jul 17 13:05:25.659: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 13:05:25.659: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-k2nx7 from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 13:05:25.659: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 13:05:25.659: INFO: 	Container systemd-logs ready: false, restart count 20
Jul 17 13:05:25.659: INFO: 
Logging pods the apiserver thinks is on node taikun-1-1188-w-2 before test
Jul 17 13:05:25.683: INFO: calico-node-zsmxq from kube-system started at 2021-07-17 08:46:19 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container calico-node ready: true, restart count 0
Jul 17 13:05:25.683: INFO: coredns-7677f9bb54-c5jf9 from kube-system started at 2021-07-17 08:47:31 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container coredns ready: true, restart count 0
Jul 17 13:05:25.683: INFO: csi-cinder-controllerplugin-694ccb959f-kxjgc from kube-system started at 2021-07-17 09:09:41 +0000 UTC (5 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 17 13:05:25.683: INFO: csi-cinder-nodeplugin-4v7m8 from kube-system started at 2021-07-17 08:48:19 +0000 UTC (2 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jul 17 13:05:25.683: INFO: kube-proxy-jvfsd from kube-system started at 2021-07-17 08:45:58 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 17 13:05:25.683: INFO: nginx-proxy-taikun-1-1188-w-2 from kube-system started at 2021-07-17 08:45:55 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container nginx-proxy ready: true, restart count 0
Jul 17 13:05:25.683: INFO: nodelocaldns-pr4zp from kube-system started at 2021-07-17 08:47:32 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container node-cache ready: true, restart count 0
Jul 17 13:05:25.683: INFO: snapshot-controller-0 from kube-system started at 2021-07-17 08:48:27 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container snapshot-controller ready: true, restart count 0
Jul 17 13:05:25.683: INFO: event-exporter-6664b95b84-szltf from monitoring started at 2021-07-17 08:49:00 +0000 UTC (1 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container event-exporter ready: true, restart count 0
Jul 17 13:05:25.683: INFO: sonobuoy-systemd-logs-daemon-set-398d6e5c972b4207-pdfqz from sonobuoy started at 2021-07-17 11:46:54 +0000 UTC (2 container statuses recorded)
Jul 17 13:05:25.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 17 13:05:25.683: INFO: 	Container systemd-logs ready: false, restart count 20
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4f561108-6409-4ed5-8eca-130dab7fcb09 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4f561108-6409-4ed5-8eca-130dab7fcb09 off the node taikun-1-1188-w-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4f561108-6409-4ed5-8eca-130dab7fcb09
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:10:29.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6028" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.351 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":249,"skipped":4027,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:10:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:10:47.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1670" for this suite.

• [SLOW TEST:17.178 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":250,"skipped":4043,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:10:47.062: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Jul 17 13:10:47.109: INFO: Major version: 1
STEP: Confirm minor version
Jul 17 13:10:47.109: INFO: cleanMinorVersion: 19
Jul 17 13:10:47.109: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:10:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2921" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":251,"skipped":4062,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:10:47.123: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Jul 17 13:10:47.702: INFO: created pod pod-service-account-defaultsa
Jul 17 13:10:47.702: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 17 13:10:47.716: INFO: created pod pod-service-account-mountsa
Jul 17 13:10:47.716: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 17 13:10:47.732: INFO: created pod pod-service-account-nomountsa
Jul 17 13:10:47.733: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 17 13:10:47.750: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 17 13:10:47.750: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 17 13:10:47.764: INFO: created pod pod-service-account-mountsa-mountspec
Jul 17 13:10:47.764: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 17 13:10:47.778: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 17 13:10:47.778: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 17 13:10:47.804: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 17 13:10:47.804: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 17 13:10:47.811: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 17 13:10:47.811: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 17 13:10:47.817: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 17 13:10:47.817: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:10:47.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1290" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":252,"skipped":4099,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:10:47.864: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-1c917002-cc6c-4d68-a5f6-f2b3a1c0cf59
STEP: Creating a pod to test consume secrets
Jul 17 13:10:48.020: INFO: Waiting up to 5m0s for pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8" in namespace "secrets-2982" to be "Succeeded or Failed"
Jul 17 13:10:48.027: INFO: Pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.226028ms
Jul 17 13:10:50.036: INFO: Pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016056716s
Jul 17 13:10:52.046: INFO: Pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02523663s
Jul 17 13:10:54.055: INFO: Pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034388207s
STEP: Saw pod success
Jul 17 13:10:54.055: INFO: Pod "pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8" satisfied condition "Succeeded or Failed"
Jul 17 13:10:54.074: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8 container secret-volume-test: <nil>
STEP: delete the pod
Jul 17 13:10:54.343: INFO: Waiting for pod pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8 to disappear
Jul 17 13:10:54.349: INFO: Pod pod-secrets-9df98da8-3382-4c51-9d39-f69f013fb2b8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:10:54.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2982" for this suite.

• [SLOW TEST:6.498 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":253,"skipped":4103,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:10:54.361: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Jul 17 13:10:54.434: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 17 13:11:54.502: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Jul 17 13:11:54.534: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 17 13:11:54.557: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3776" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:72.317 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":254,"skipped":4109,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7093f351-1910-4b4a-92b1-120adc931b94
STEP: Creating a pod to test consume configMaps
Jul 17 13:12:06.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7" in namespace "configmap-8949" to be "Succeeded or Failed"
Jul 17 13:12:06.756: INFO: Pod "pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.958751ms
Jul 17 13:12:08.764: INFO: Pod "pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011203145s
STEP: Saw pod success
Jul 17 13:12:08.764: INFO: Pod "pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7" satisfied condition "Succeeded or Failed"
Jul 17 13:12:08.769: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:12:08.802: INFO: Waiting for pod pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7 to disappear
Jul 17 13:12:08.811: INFO: Pod pod-configmaps-d1acde43-5228-40e4-9d56-41ca9db881c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:08.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8949" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:08.843: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 13:12:08.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5" in namespace "projected-2520" to be "Succeeded or Failed"
Jul 17 13:12:08.902: INFO: Pod "downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.366622ms
Jul 17 13:12:10.908: INFO: Pod "downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00988754s
STEP: Saw pod success
Jul 17 13:12:10.908: INFO: Pod "downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5" satisfied condition "Succeeded or Failed"
Jul 17 13:12:10.912: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5 container client-container: <nil>
STEP: delete the pod
Jul 17 13:12:10.961: INFO: Waiting for pod downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5 to disappear
Jul 17 13:12:10.966: INFO: Pod downwardapi-volume-a258aba2-bbea-4be7-b686-f944ec96cee5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:10.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2520" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":256,"skipped":4177,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:10.986: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8b96e19e-3d78-486d-bf8e-98c5a909cfd2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8b96e19e-3d78-486d-bf8e-98c5a909cfd2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:15.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3672" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":257,"skipped":4183,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:15.185: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-5d5091df-19ad-440f-aecb-6a74e5e2e509
STEP: Creating a pod to test consume configMaps
Jul 17 13:12:15.245: INFO: Waiting up to 5m0s for pod "pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd" in namespace "configmap-9764" to be "Succeeded or Failed"
Jul 17 13:12:15.258: INFO: Pod "pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.902219ms
Jul 17 13:12:17.262: INFO: Pod "pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017287957s
STEP: Saw pod success
Jul 17 13:12:17.262: INFO: Pod "pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd" satisfied condition "Succeeded or Failed"
Jul 17 13:12:17.266: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:12:17.292: INFO: Waiting for pod pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd to disappear
Jul 17 13:12:17.295: INFO: Pod pod-configmaps-d20b06ec-8f01-4c68-89ca-50fead5939dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:17.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9764" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":258,"skipped":4187,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:17.307: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-56e96e3d-d5dd-45d7-9065-c700f1c9ff27
STEP: Creating a pod to test consume configMaps
Jul 17 13:12:17.382: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2" in namespace "configmap-8612" to be "Succeeded or Failed"
Jul 17 13:12:17.390: INFO: Pod "pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.649138ms
Jul 17 13:12:19.397: INFO: Pod "pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014308877s
STEP: Saw pod success
Jul 17 13:12:19.397: INFO: Pod "pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2" satisfied condition "Succeeded or Failed"
Jul 17 13:12:19.402: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:12:19.427: INFO: Waiting for pod pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2 to disappear
Jul 17 13:12:19.436: INFO: Pod pod-configmaps-d1c10024-7a57-4375-8ddb-8c25b9998fc2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:19.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8612" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4197,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:19.454: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-6af7baca-915e-4151-ba49-9b195ce24442
STEP: Creating a pod to test consume configMaps
Jul 17 13:12:19.519: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3" in namespace "projected-699" to be "Succeeded or Failed"
Jul 17 13:12:19.525: INFO: Pod "pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742805ms
Jul 17 13:12:21.533: INFO: Pod "pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013548598s
STEP: Saw pod success
Jul 17 13:12:21.533: INFO: Pod "pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3" satisfied condition "Succeeded or Failed"
Jul 17 13:12:21.539: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:12:21.558: INFO: Waiting for pod pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3 to disappear
Jul 17 13:12:21.564: INFO: Pod pod-projected-configmaps-4fc58b3e-f6b7-435e-acd2-a59b5a58d1c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:21.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-699" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":260,"skipped":4207,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:21.582: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-05704c34-d816-4498-ae4c-20a8fec4f0a6
STEP: Creating a pod to test consume configMaps
Jul 17 13:12:21.634: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455" in namespace "configmap-707" to be "Succeeded or Failed"
Jul 17 13:12:21.642: INFO: Pod "pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698371ms
Jul 17 13:12:23.649: INFO: Pod "pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014089788s
STEP: Saw pod success
Jul 17 13:12:23.649: INFO: Pod "pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455" satisfied condition "Succeeded or Failed"
Jul 17 13:12:23.654: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:12:23.677: INFO: Waiting for pod pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455 to disappear
Jul 17 13:12:23.682: INFO: Pod pod-configmaps-bb514aa2-d512-48f7-8eb9-b9a63e3e5455 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-707" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4228,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:23.705: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:12:27.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6999" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":262,"skipped":4247,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:12:27.864: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0717 13:12:38.077580      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 17 13:13:40.120: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Jul 17 13:13:40.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r4xp" in namespace "gc-6774"
Jul 17 13:13:40.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-97z6s" in namespace "gc-6774"
Jul 17 13:13:40.190: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkrww" in namespace "gc-6774"
Jul 17 13:13:40.224: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv2mc" in namespace "gc-6774"
Jul 17 13:13:40.261: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjcdx" in namespace "gc-6774"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:13:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6774" for this suite.

• [SLOW TEST:72.455 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":263,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:13:40.320: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 17 13:13:40.388: INFO: Waiting up to 5m0s for pod "pod-9cba1a14-3405-4121-a279-4c79f65b7b26" in namespace "emptydir-6068" to be "Succeeded or Failed"
Jul 17 13:13:40.393: INFO: Pod "pod-9cba1a14-3405-4121-a279-4c79f65b7b26": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32614ms
Jul 17 13:13:42.398: INFO: Pod "pod-9cba1a14-3405-4121-a279-4c79f65b7b26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009120394s
STEP: Saw pod success
Jul 17 13:13:42.399: INFO: Pod "pod-9cba1a14-3405-4121-a279-4c79f65b7b26" satisfied condition "Succeeded or Failed"
Jul 17 13:13:42.402: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-9cba1a14-3405-4121-a279-4c79f65b7b26 container test-container: <nil>
STEP: delete the pod
Jul 17 13:13:42.425: INFO: Waiting for pod pod-9cba1a14-3405-4121-a279-4c79f65b7b26 to disappear
Jul 17 13:13:42.429: INFO: Pod pod-9cba1a14-3405-4121-a279-4c79f65b7b26 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:13:42.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6068" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":264,"skipped":4285,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:13:42.440: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 17 13:13:42.510: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:13:42.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8322" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":265,"skipped":4285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:13:42.545: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 13:13:42.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1" in namespace "downward-api-8233" to be "Succeeded or Failed"
Jul 17 13:13:42.648: INFO: Pod "downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.167858ms
Jul 17 13:13:44.654: INFO: Pod "downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015801431s
STEP: Saw pod success
Jul 17 13:13:44.654: INFO: Pod "downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1" satisfied condition "Succeeded or Failed"
Jul 17 13:13:44.658: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1 container client-container: <nil>
STEP: delete the pod
Jul 17 13:13:44.703: INFO: Waiting for pod downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1 to disappear
Jul 17 13:13:44.707: INFO: Pod downwardapi-volume-6bedc076-f5aa-4123-bbac-53d0d6ffd2f1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:13:44.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8233" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":266,"skipped":4336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:13:44.729: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8277
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 17 13:13:44.777: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 17 13:13:44.852: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 17 13:13:46.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:48.856: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:50.859: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:52.859: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:54.861: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:56.865: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:13:58.857: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:14:00.858: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:14:02.860: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 17 13:14:04.858: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 17 13:14:04.868: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 17 13:14:06.908: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.211:8080/dial?request=hostname&protocol=http&host=10.233.64.209&port=8080&tries=1'] Namespace:pod-network-test-8277 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:14:06.908: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:14:07.026: INFO: Waiting for responses: map[]
Jul 17 13:14:07.034: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.211:8080/dial?request=hostname&protocol=http&host=10.233.69.131&port=8080&tries=1'] Namespace:pod-network-test-8277 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:14:07.034: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:14:07.158: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:14:07.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8277" for this suite.

• [SLOW TEST:22.447 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4383,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:14:07.178: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 17 13:14:07.267: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 17 13:14:28.228: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:14:33.055: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:14:53.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6872" for this suite.

• [SLOW TEST:46.645 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":268,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:14:53.826: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 17 13:14:53.860: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Jul 17 13:14:54.664: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 17 13:14:58.467: INFO: Waited 1.726893806s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:14:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7518" for this suite.

• [SLOW TEST:5.753 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":269,"skipped":4427,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:14:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 13:14:59.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c" in namespace "downward-api-9198" to be "Succeeded or Failed"
Jul 17 13:14:59.653: INFO: Pod "downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.943039ms
Jul 17 13:15:01.660: INFO: Pod "downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020563019s
STEP: Saw pod success
Jul 17 13:15:01.660: INFO: Pod "downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c" satisfied condition "Succeeded or Failed"
Jul 17 13:15:01.663: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c container client-container: <nil>
STEP: delete the pod
Jul 17 13:15:01.685: INFO: Waiting for pod downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c to disappear
Jul 17 13:15:01.689: INFO: Pod downwardapi-volume-23eb9205-f275-4158-91b2-332e5446fd6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:01.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9198" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":270,"skipped":4434,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 17 13:15:01.773: INFO: Waiting up to 5m0s for pod "downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a" in namespace "downward-api-1939" to be "Succeeded or Failed"
Jul 17 13:15:01.788: INFO: Pod "downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.501007ms
Jul 17 13:15:03.792: INFO: Pod "downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018212606s
STEP: Saw pod success
Jul 17 13:15:03.792: INFO: Pod "downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a" satisfied condition "Succeeded or Failed"
Jul 17 13:15:03.795: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a container dapi-container: <nil>
STEP: delete the pod
Jul 17 13:15:03.827: INFO: Waiting for pod downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a to disappear
Jul 17 13:15:03.835: INFO: Pod downward-api-d8b9189a-30d8-4c58-a4f5-078aa2c8973a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:03.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1939" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":271,"skipped":4455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:15:03.901: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-baea5099-b7c3-4094-b138-b52c2ebaf4f7" in namespace "security-context-test-6852" to be "Succeeded or Failed"
Jul 17 13:15:03.911: INFO: Pod "busybox-readonly-false-baea5099-b7c3-4094-b138-b52c2ebaf4f7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.385942ms
Jul 17 13:15:05.915: INFO: Pod "busybox-readonly-false-baea5099-b7c3-4094-b138-b52c2ebaf4f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013320095s
Jul 17 13:15:05.915: INFO: Pod "busybox-readonly-false-baea5099-b7c3-4094-b138-b52c2ebaf4f7" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:05.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6852" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":272,"skipped":4525,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:05.930: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 17 13:15:12.047: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.047: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.149: INFO: Exec stderr: ""
Jul 17 13:15:12.149: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.149: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.262: INFO: Exec stderr: ""
Jul 17 13:15:12.262: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.262: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.360: INFO: Exec stderr: ""
Jul 17 13:15:12.360: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.360: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.468: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 17 13:15:12.469: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.469: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.570: INFO: Exec stderr: ""
Jul 17 13:15:12.570: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.571: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.675: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 17 13:15:12.675: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.675: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.796: INFO: Exec stderr: ""
Jul 17 13:15:12.796: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.796: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:12.900: INFO: Exec stderr: ""
Jul 17 13:15:12.900: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:13.009: INFO: Exec stderr: ""
Jul 17 13:15:13.009: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-589 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 17 13:15:13.009: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
Jul 17 13:15:13.119: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:13.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-589" for this suite.

• [SLOW TEST:7.204 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4571,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:13.134: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-629ef217-0789-4df8-ad2f-747f22496278
STEP: Creating a pod to test consume configMaps
Jul 17 13:15:13.191: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb" in namespace "projected-4772" to be "Succeeded or Failed"
Jul 17 13:15:13.193: INFO: Pod "pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.803812ms
Jul 17 13:15:15.201: INFO: Pod "pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010740957s
STEP: Saw pod success
Jul 17 13:15:15.201: INFO: Pod "pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb" satisfied condition "Succeeded or Failed"
Jul 17 13:15:15.206: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:15:15.239: INFO: Waiting for pod pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb to disappear
Jul 17 13:15:15.243: INFO: Pod pod-projected-configmaps-c733c756-ec9e-4d92-99b8-4dff1a2d76bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:15.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4772" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4583,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 17 13:15:19.400: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 17 13:15:19.404: INFO: Pod pod-with-poststart-http-hook still exists
Jul 17 13:15:21.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 17 13:15:21.426: INFO: Pod pod-with-poststart-http-hook still exists
Jul 17 13:15:23.405: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 17 13:15:23.410: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:23.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3930" for this suite.

• [SLOW TEST:8.134 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":275,"skipped":4611,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:23.428: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:15:24.040: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 13:15:26.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124524, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124524, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124524, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124524, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:15:29.075: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:29.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5360" for this suite.
STEP: Destroying namespace "webhook-5360-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.038 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":276,"skipped":4613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:29.471: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 17 13:15:29.531: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 17 13:15:34.541: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:34.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4681" for this suite.

• [SLOW TEST:5.137 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":277,"skipped":4647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:34.610: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:15:34.707: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 17 13:15:40.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-7228 --namespace=crd-publish-openapi-7228 create -f -'
Jul 17 13:15:41.858: INFO: stderr: ""
Jul 17 13:15:41.858: INFO: stdout: "e2e-test-crd-publish-openapi-1267-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 17 13:15:41.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-7228 --namespace=crd-publish-openapi-7228 delete e2e-test-crd-publish-openapi-1267-crds test-cr'
Jul 17 13:15:41.996: INFO: stderr: ""
Jul 17 13:15:41.996: INFO: stdout: "e2e-test-crd-publish-openapi-1267-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 17 13:15:41.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-7228 --namespace=crd-publish-openapi-7228 apply -f -'
Jul 17 13:15:42.413: INFO: stderr: ""
Jul 17 13:15:42.413: INFO: stdout: "e2e-test-crd-publish-openapi-1267-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 17 13:15:42.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-7228 --namespace=crd-publish-openapi-7228 delete e2e-test-crd-publish-openapi-1267-crds test-cr'
Jul 17 13:15:42.577: INFO: stderr: ""
Jul 17 13:15:42.577: INFO: stdout: "e2e-test-crd-publish-openapi-1267-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 17 13:15:42.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=crd-publish-openapi-7228 explain e2e-test-crd-publish-openapi-1267-crds'
Jul 17 13:15:42.993: INFO: stderr: ""
Jul 17 13:15:42.993: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1267-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:49.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7228" for this suite.

• [SLOW TEST:14.409 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":278,"skipped":4673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:49.023: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Jul 17 13:15:49.083: INFO: Waiting up to 5m0s for pod "var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b" in namespace "var-expansion-3653" to be "Succeeded or Failed"
Jul 17 13:15:49.087: INFO: Pod "var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.300498ms
Jul 17 13:15:51.091: INFO: Pod "var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007992213s
STEP: Saw pod success
Jul 17 13:15:51.091: INFO: Pod "var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b" satisfied condition "Succeeded or Failed"
Jul 17 13:15:51.093: INFO: Trying to get logs from node taikun-1-1188-w-1 pod var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b container dapi-container: <nil>
STEP: delete the pod
Jul 17 13:15:51.123: INFO: Waiting for pod var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b to disappear
Jul 17 13:15:51.129: INFO: Pod var-expansion-2bdf4899-01f6-4faa-a8fc-83d055696b1b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:51.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3653" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4733,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:51.150: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:15:51.766: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:15:54.806: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:54.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2039" for this suite.
STEP: Destroying namespace "webhook-2039-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":280,"skipped":4751,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:55.259: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:15:55.367: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:15:56.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4303" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":281,"skipped":4754,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:15:56.427: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:15:56.646: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 17 13:15:56.686: INFO: Number of nodes with available pods: 0
Jul 17 13:15:56.686: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 17 13:15:56.843: INFO: Number of nodes with available pods: 0
Jul 17 13:15:56.843: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:15:57.848: INFO: Number of nodes with available pods: 0
Jul 17 13:15:57.848: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:15:58.848: INFO: Number of nodes with available pods: 1
Jul 17 13:15:58.848: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 17 13:15:58.886: INFO: Number of nodes with available pods: 1
Jul 17 13:15:58.886: INFO: Number of running nodes: 0, number of available pods: 1
Jul 17 13:15:59.892: INFO: Number of nodes with available pods: 0
Jul 17 13:15:59.892: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 17 13:15:59.907: INFO: Number of nodes with available pods: 0
Jul 17 13:15:59.907: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:00.923: INFO: Number of nodes with available pods: 0
Jul 17 13:16:00.923: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:01.911: INFO: Number of nodes with available pods: 0
Jul 17 13:16:01.912: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:02.914: INFO: Number of nodes with available pods: 0
Jul 17 13:16:02.914: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:03.913: INFO: Number of nodes with available pods: 0
Jul 17 13:16:03.913: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:04.912: INFO: Number of nodes with available pods: 0
Jul 17 13:16:04.912: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:05.911: INFO: Number of nodes with available pods: 0
Jul 17 13:16:05.912: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:06.911: INFO: Number of nodes with available pods: 0
Jul 17 13:16:06.912: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:07.911: INFO: Number of nodes with available pods: 0
Jul 17 13:16:07.911: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:08.911: INFO: Number of nodes with available pods: 0
Jul 17 13:16:08.911: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:09.915: INFO: Number of nodes with available pods: 0
Jul 17 13:16:09.916: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:10.919: INFO: Number of nodes with available pods: 0
Jul 17 13:16:10.919: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:11.913: INFO: Number of nodes with available pods: 0
Jul 17 13:16:11.915: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:12.912: INFO: Number of nodes with available pods: 0
Jul 17 13:16:12.912: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:13.915: INFO: Number of nodes with available pods: 0
Jul 17 13:16:13.916: INFO: Node taikun-1-1188-w-2 is running more than one daemon pod
Jul 17 13:16:14.916: INFO: Number of nodes with available pods: 1
Jul 17 13:16:14.916: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-512, will wait for the garbage collector to delete the pods
Jul 17 13:16:15.002: INFO: Deleting DaemonSet.extensions daemon-set took: 25.902412ms
Jul 17 13:16:15.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.352535ms
Jul 17 13:16:23.209: INFO: Number of nodes with available pods: 0
Jul 17 13:16:23.209: INFO: Number of running nodes: 0, number of available pods: 0
Jul 17 13:16:23.213: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-512/daemonsets","resourceVersion":"101072"},"items":null}

Jul 17 13:16:23.217: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-512/pods","resourceVersion":"101072"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:23.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-512" for this suite.

• [SLOW TEST:26.844 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":282,"skipped":4762,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:23.280: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Jul 17 13:16:23.349: INFO: created test-podtemplate-1
Jul 17 13:16:23.355: INFO: created test-podtemplate-2
Jul 17 13:16:23.360: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 17 13:16:23.364: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 17 13:16:23.381: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:23.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1885" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":283,"skipped":4780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:23.404: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 17 13:16:23.513: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 17 13:16:23.518: INFO: starting watch
STEP: patching
STEP: updating
Jul 17 13:16:23.542: INFO: waiting for watch events with expected annotations
Jul 17 13:16:23.542: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:23.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8872" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":284,"skipped":4804,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:23.692: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 13:16:23.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be" in namespace "downward-api-3602" to be "Succeeded or Failed"
Jul 17 13:16:23.777: INFO: Pod "downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.001003ms
Jul 17 13:16:25.782: INFO: Pod "downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be": Phase="Running", Reason="", readiness=true. Elapsed: 2.009573601s
Jul 17 13:16:27.786: INFO: Pod "downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013351442s
STEP: Saw pod success
Jul 17 13:16:27.786: INFO: Pod "downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be" satisfied condition "Succeeded or Failed"
Jul 17 13:16:27.789: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be container client-container: <nil>
STEP: delete the pod
Jul 17 13:16:27.815: INFO: Waiting for pod downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be to disappear
Jul 17 13:16:27.819: INFO: Pod downwardapi-volume-10f58ab8-a9fe-4d35-92ef-deaec828e4be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:27.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3602" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4810,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:27.841: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Jul 17 13:16:27.880: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=kubectl-8792 proxy --unix-socket=/tmp/kubectl-proxy-unix446972910/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:27.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8792" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":286,"skipped":4837,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:28.004: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:16:28.872: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 13:16:30.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124588, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124588, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124588, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124588, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:16:33.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:33.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4528" for this suite.
STEP: Destroying namespace "webhook-4528-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.069 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":287,"skipped":4845,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:34.073: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 17 13:16:36.163: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:36.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7444" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":288,"skipped":4856,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:36.212: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:39.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4640" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":289,"skipped":4863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:39.334: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 17 13:16:39.413: INFO: Waiting up to 5m0s for pod "pod-334a78d5-7456-4deb-b377-07612ed3579e" in namespace "emptydir-8734" to be "Succeeded or Failed"
Jul 17 13:16:39.424: INFO: Pod "pod-334a78d5-7456-4deb-b377-07612ed3579e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.742195ms
Jul 17 13:16:41.429: INFO: Pod "pod-334a78d5-7456-4deb-b377-07612ed3579e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015546752s
STEP: Saw pod success
Jul 17 13:16:41.429: INFO: Pod "pod-334a78d5-7456-4deb-b377-07612ed3579e" satisfied condition "Succeeded or Failed"
Jul 17 13:16:41.432: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-334a78d5-7456-4deb-b377-07612ed3579e container test-container: <nil>
STEP: delete the pod
Jul 17 13:16:41.454: INFO: Waiting for pod pod-334a78d5-7456-4deb-b377-07612ed3579e to disappear
Jul 17 13:16:41.460: INFO: Pod pod-334a78d5-7456-4deb-b377-07612ed3579e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:41.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8734" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:41.480: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 17 13:16:44.096: INFO: Successfully updated pod "adopt-release-p75xp"
STEP: Checking that the Job readopts the Pod
Jul 17 13:16:44.096: INFO: Waiting up to 15m0s for pod "adopt-release-p75xp" in namespace "job-5512" to be "adopted"
Jul 17 13:16:44.106: INFO: Pod "adopt-release-p75xp": Phase="Running", Reason="", readiness=true. Elapsed: 8.832363ms
Jul 17 13:16:46.114: INFO: Pod "adopt-release-p75xp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01623133s
Jul 17 13:16:46.114: INFO: Pod "adopt-release-p75xp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 17 13:16:46.639: INFO: Successfully updated pod "adopt-release-p75xp"
STEP: Checking that the Job releases the Pod
Jul 17 13:16:46.639: INFO: Waiting up to 15m0s for pod "adopt-release-p75xp" in namespace "job-5512" to be "released"
Jul 17 13:16:46.649: INFO: Pod "adopt-release-p75xp": Phase="Running", Reason="", readiness=true. Elapsed: 10.180455ms
Jul 17 13:16:46.650: INFO: Pod "adopt-release-p75xp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5512" for this suite.

• [SLOW TEST:5.210 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":291,"skipped":4933,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:16:48.890: INFO: Waiting up to 5m0s for pod "client-envvars-9219e537-14df-437e-836d-e7131785226d" in namespace "pods-6161" to be "Succeeded or Failed"
Jul 17 13:16:48.904: INFO: Pod "client-envvars-9219e537-14df-437e-836d-e7131785226d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.519887ms
Jul 17 13:16:50.909: INFO: Pod "client-envvars-9219e537-14df-437e-836d-e7131785226d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018860952s
STEP: Saw pod success
Jul 17 13:16:50.909: INFO: Pod "client-envvars-9219e537-14df-437e-836d-e7131785226d" satisfied condition "Succeeded or Failed"
Jul 17 13:16:50.913: INFO: Trying to get logs from node taikun-1-1188-w-1 pod client-envvars-9219e537-14df-437e-836d-e7131785226d container env3cont: <nil>
STEP: delete the pod
Jul 17 13:16:50.948: INFO: Waiting for pod client-envvars-9219e537-14df-437e-836d-e7131785226d to disappear
Jul 17 13:16:50.966: INFO: Pod client-envvars-9219e537-14df-437e-836d-e7131785226d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:50.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6161" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":292,"skipped":4948,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:50.987: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Jul 17 13:16:55.064: INFO: Pod pod-hostip-b31e0b1c-15ea-4188-bf65-06742b5fcedb has hostIP: 192.168.101.100
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:16:55.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1658" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":293,"skipped":4956,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:16:55.085: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-4507
STEP: creating service affinity-clusterip in namespace services-4507
STEP: creating replication controller affinity-clusterip in namespace services-4507
I0717 13:16:55.190138      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-4507, replica count: 3
I0717 13:16:58.240902      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 13:16:58.248: INFO: Creating new exec pod
Jul 17 13:17:01.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-4507 exec execpod-affinityxxrwq -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jul 17 13:17:01.554: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 17 13:17:01.554: INFO: stdout: ""
Jul 17 13:17:01.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-4507 exec execpod-affinityxxrwq -- /bin/sh -x -c nc -zv -t -w 2 10.233.24.207 80'
Jul 17 13:17:01.830: INFO: stderr: "+ nc -zv -t -w 2 10.233.24.207 80\nConnection to 10.233.24.207 80 port [tcp/http] succeeded!\n"
Jul 17 13:17:01.830: INFO: stdout: ""
Jul 17 13:17:01.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-4507 exec execpod-affinityxxrwq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.24.207:80/ ; done'
Jul 17 13:17:02.189: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.24.207:80/\n"
Jul 17 13:17:02.190: INFO: stdout: "\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98\naffinity-clusterip-4tx98"
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Received response from host: affinity-clusterip-4tx98
Jul 17 13:17:02.190: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4507, will wait for the garbage collector to delete the pods
Jul 17 13:17:02.285: INFO: Deleting ReplicationController affinity-clusterip took: 16.014911ms
Jul 17 13:17:02.985: INFO: Terminating ReplicationController affinity-clusterip pods took: 700.183374ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:17:13.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4507" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:18.144 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":294,"skipped":4977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:17:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:17:13.296: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 17 13:17:15.318: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 17 13:17:17.324: INFO: Creating deployment "test-rollover-deployment"
Jul 17 13:17:17.336: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 17 13:17:19.349: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 17 13:17:19.358: INFO: Ensure that both replica sets have 1 created replica
Jul 17 13:17:19.366: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 17 13:17:19.376: INFO: Updating deployment test-rollover-deployment
Jul 17 13:17:19.376: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 17 13:17:21.388: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 17 13:17:21.398: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 17 13:17:21.406: INFO: all replica sets need to contain the pod-template-hash label
Jul 17 13:17:21.406: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124641, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 13:17:23.415: INFO: all replica sets need to contain the pod-template-hash label
Jul 17 13:17:23.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124641, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 13:17:25.419: INFO: all replica sets need to contain the pod-template-hash label
Jul 17 13:17:25.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124641, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 13:17:27.418: INFO: all replica sets need to contain the pod-template-hash label
Jul 17 13:17:27.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124641, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 13:17:29.419: INFO: all replica sets need to contain the pod-template-hash label
Jul 17 13:17:29.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124641, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762124637, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 17 13:17:31.418: INFO: 
Jul 17 13:17:31.418: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Jul 17 13:17:31.429: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8388 /apis/apps/v1/namespaces/deployment-8388/deployments/test-rollover-deployment 9febbb9a-d82f-4124-85bf-224b1810bf63 101995 2 2021-07-17 13:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-17 13:17:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 13:17:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ca5988 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-17 13:17:17 +0000 UTC,LastTransitionTime:2021-07-17 13:17:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-07-17 13:17:31 +0000 UTC,LastTransitionTime:2021-07-17 13:17:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 17 13:17:31.433: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-8388 /apis/apps/v1/namespaces/deployment-8388/replicasets/test-rollover-deployment-5797c7764 eff500dd-9184-4960-ba43-ece38369aaf8 101984 2 2021-07-17 13:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9febbb9a-d82f-4124-85bf-224b1810bf63 0xc003cf4110 0xc003cf4111}] []  [{kube-controller-manager Update apps/v1 2021-07-17 13:17:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9febbb9a-d82f-4124-85bf-224b1810bf63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cf4198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 17 13:17:31.433: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 17 13:17:31.434: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8388 /apis/apps/v1/namespaces/deployment-8388/replicasets/test-rollover-controller 3179ebea-24d0-4378-b88e-8e3bd35dad16 101993 2 2021-07-17 13:17:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9febbb9a-d82f-4124-85bf-224b1810bf63 0xc003ca5fc7 0xc003ca5fc8}] []  [{e2e.test Update apps/v1 2021-07-17 13:17:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-17 13:17:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9febbb9a-d82f-4124-85bf-224b1810bf63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003cf4098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 13:17:31.434: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-8388 /apis/apps/v1/namespaces/deployment-8388/replicasets/test-rollover-deployment-78bc8b888c fc0d6bfc-a875-4bb7-8931-6942e5de2eac 101905 2 2021-07-17 13:17:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9febbb9a-d82f-4124-85bf-224b1810bf63 0xc003cf4207 0xc003cf4208}] []  [{kube-controller-manager Update apps/v1 2021-07-17 13:17:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9febbb9a-d82f-4124-85bf-224b1810bf63\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cf4298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 17 13:17:31.439: INFO: Pod "test-rollover-deployment-5797c7764-vb948" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-vb948 test-rollover-deployment-5797c7764- deployment-8388 /api/v1/namespaces/deployment-8388/pods/test-rollover-deployment-5797c7764-vb948 0bac3163-e440-4b81-a927-87d7b344298f 101931 0 2021-07-17 13:17:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:10.233.64.237/32 cni.projectcalico.org/podIPs:10.233.64.237/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 eff500dd-9184-4960-ba43-ece38369aaf8 0xc003cf4a80 0xc003cf4a81}] []  [{kube-controller-manager Update v1 2021-07-17 13:17:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eff500dd-9184-4960-ba43-ece38369aaf8\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-17 13:17:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-17 13:17:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4sw5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4sw5q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4sw5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:taikun-1-1188-w-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 13:17:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 13:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 13:17:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-17 13:17:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.101.100,PodIP:10.233.64.237,StartTime:2021-07-17 13:17:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-17 13:17:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:containerd://90df1fcee601e65c24c94bf63b48f4935b99a898bbbb8732615268a8023b3e0a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.237,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:17:31.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8388" for this suite.

• [SLOW TEST:18.218 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":295,"skipped":5006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:17:31.461: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:17:31.521: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:17:33.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-835" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":5035,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:17:33.593: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-3848
Jul 17 13:17:35.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 17 13:17:35.933: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 17 13:17:35.933: INFO: stdout: "ipvs"
Jul 17 13:17:35.933: INFO: proxyMode: ipvs
Jul 17 13:17:35.941: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 13:17:35.945: INFO: Pod kube-proxy-mode-detector still exists
Jul 17 13:17:37.945: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 13:17:37.952: INFO: Pod kube-proxy-mode-detector still exists
Jul 17 13:17:39.945: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 17 13:17:39.952: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3848
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3848
I0717 13:17:40.004772      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3848, replica count: 3
I0717 13:17:43.055983      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 17 13:17:43.081: INFO: Creating new exec pod
Jul 17 13:17:46.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jul 17 13:17:46.400: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 17 13:17:46.400: INFO: stdout: ""
Jul 17 13:17:46.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c nc -zv -t -w 2 10.233.39.77 80'
Jul 17 13:17:46.649: INFO: stderr: "+ nc -zv -t -w 2 10.233.39.77 80\nConnection to 10.233.39.77 80 port [tcp/http] succeeded!\n"
Jul 17 13:17:46.649: INFO: stdout: ""
Jul 17 13:17:46.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.100 30376'
Jul 17 13:17:46.899: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.100 30376\nConnection to 192.168.101.100 30376 port [tcp/30376] succeeded!\n"
Jul 17 13:17:46.899: INFO: stdout: ""
Jul 17 13:17:46.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c nc -zv -t -w 2 192.168.101.67 30376'
Jul 17 13:17:47.174: INFO: stderr: "+ nc -zv -t -w 2 192.168.101.67 30376\nConnection to 192.168.101.67 30376 port [tcp/30376] succeeded!\n"
Jul 17 13:17:47.175: INFO: stdout: ""
Jul 17 13:17:47.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.101.100:30376/ ; done'
Jul 17 13:17:47.534: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n"
Jul 17 13:17:47.535: INFO: stdout: "\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc\naffinity-nodeport-timeout-rn7tc"
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Received response from host: affinity-nodeport-timeout-rn7tc
Jul 17 13:17:47.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.100:30376/'
Jul 17 13:17:47.816: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n"
Jul 17 13:17:47.816: INFO: stdout: "affinity-nodeport-timeout-rn7tc"
Jul 17 13:19:52.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.100:30376/'
Jul 17 13:19:53.104: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n"
Jul 17 13:19:53.104: INFO: stdout: "affinity-nodeport-timeout-rn7tc"
Jul 17 13:21:58.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-578146847 --namespace=services-3848 exec execpod-affinityqf67q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://192.168.101.100:30376/'
Jul 17 13:21:58.361: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://192.168.101.100:30376/\n"
Jul 17 13:21:58.361: INFO: stdout: "affinity-nodeport-timeout-g6gwj"
Jul 17 13:21:58.361: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3848, will wait for the garbage collector to delete the pods
Jul 17 13:21:58.473: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 19.439018ms
Jul 17 13:21:59.174: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 701.10263ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:22:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3848" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:279.676 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":297,"skipped":5045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:22:13.270: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Jul 17 13:22:13.354: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264" in namespace "projected-9816" to be "Succeeded or Failed"
Jul 17 13:22:13.358: INFO: Pod "downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264": Phase="Pending", Reason="", readiness=false. Elapsed: 3.866091ms
Jul 17 13:22:15.363: INFO: Pod "downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009142285s
Jul 17 13:22:17.369: INFO: Pod "downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01538042s
STEP: Saw pod success
Jul 17 13:22:17.369: INFO: Pod "downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264" satisfied condition "Succeeded or Failed"
Jul 17 13:22:17.374: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264 container client-container: <nil>
STEP: delete the pod
Jul 17 13:22:17.413: INFO: Waiting for pod downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264 to disappear
Jul 17 13:22:17.416: INFO: Pod downwardapi-volume-e3fc320b-1ebe-4d69-b3d7-2b4aa33b7264 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:22:17.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9816" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":298,"skipped":5076,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:22:17.433: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:24:17.508: INFO: Deleting pod "var-expansion-89dc21fd-2468-42ab-b5fc-6e8c2eeb527d" in namespace "var-expansion-5951"
Jul 17 13:24:17.521: INFO: Wait up to 5m0s for pod "var-expansion-89dc21fd-2468-42ab-b5fc-6e8c2eeb527d" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:24:19.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5951" for this suite.

• [SLOW TEST:122.275 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":299,"skipped":5089,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:24:19.710: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Jul 17 13:24:19.783: INFO: Waiting up to 5m0s for pod "downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587" in namespace "downward-api-1717" to be "Succeeded or Failed"
Jul 17 13:24:19.797: INFO: Pod "downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587": Phase="Pending", Reason="", readiness=false. Elapsed: 13.751745ms
Jul 17 13:24:21.823: INFO: Pod "downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039961428s
STEP: Saw pod success
Jul 17 13:24:21.823: INFO: Pod "downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587" satisfied condition "Succeeded or Failed"
Jul 17 13:24:21.830: INFO: Trying to get logs from node taikun-1-1188-w-1 pod downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587 container dapi-container: <nil>
STEP: delete the pod
Jul 17 13:24:21.868: INFO: Waiting for pod downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587 to disappear
Jul 17 13:24:21.872: INFO: Pod downward-api-e10e7cfd-cb0c-4981-8eb6-7b4aac285587 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:24:21.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1717" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":5098,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:24:21.893: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Jul 17 13:24:21.935: INFO: PodSpec: initContainers in spec.initContainers
Jul 17 13:25:03.966: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4d0786d4-d336-44e9-a55a-e74cb2c409c1", GenerateName:"", Namespace:"init-container-1881", SelfLink:"/api/v1/namespaces/init-container-1881/pods/pod-init-4d0786d4-d336-44e9-a55a-e74cb2c409c1", UID:"13a7ea3b-ccba-4486-854a-0c9a2dae7894", ResourceVersion:"104066", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63762125061, loc:(*time.Location)(0x77148e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"935443721"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.233.64.246/32", "cni.projectcalico.org/podIPs":"10.233.64.246/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0054fcde0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0054fce00)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0054fce20), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0054fce40)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0054fce60), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0054fce80)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-x6vs5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000bfee00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x6vs5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x6vs5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-x6vs5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0048dd718), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"taikun-1-1188-w-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034bc000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0048dd790)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0048dd7b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0048dd7b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0048dd7bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003cb9180), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125061, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125061, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125061, loc:(*time.Location)(0x77148e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125061, loc:(*time.Location)(0x77148e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.101.100", PodIP:"10.233.64.246", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.64.246"}}, StartTime:(*v1.Time)(0xc0054fcea0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034bc0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034bc150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://089b4e545de1a57abc8972df0d8ee3f20dd35c28872f2964fefc8fae1a32fc1b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0054fcee0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0054fcec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0048dd834)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:25:03.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1881" for this suite.

• [SLOW TEST:42.102 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":301,"skipped":5121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:25:04.000: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Jul 17 13:25:04.052: INFO: Waiting up to 5m0s for pod "busybox-user-65534-082fa92c-9cb6-4455-836f-56b4fdb8c5d6" in namespace "security-context-test-4201" to be "Succeeded or Failed"
Jul 17 13:25:04.062: INFO: Pod "busybox-user-65534-082fa92c-9cb6-4455-836f-56b4fdb8c5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.22253ms
Jul 17 13:25:06.067: INFO: Pod "busybox-user-65534-082fa92c-9cb6-4455-836f-56b4fdb8c5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013317351s
Jul 17 13:25:06.067: INFO: Pod "busybox-user-65534-082fa92c-9cb6-4455-836f-56b4fdb8c5d6" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:25:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4201" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":302,"skipped":5143,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:25:06.084: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2855.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.taikun-1-1188;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.taikun-1-1188 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.taikun-1-1188;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2855.pod.taikun-1-1188"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 17 13:25:08.213: INFO: DNS probes using dns-2855/dns-test-4b32da57-251d-45d7-a9db-2611e664675f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:25:08.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2855" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":303,"skipped":5168,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:25:08.272: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 17 13:25:09.086: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 17 13:25:11.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125109, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125109, loc:(*time.Location)(0x77148e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125109, loc:(*time.Location)(0x77148e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762125109, loc:(*time.Location)(0x77148e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 17 13:25:14.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:25:24.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4849" for this suite.
STEP: Destroying namespace "webhook-4849-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.196 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":304,"skipped":5176,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jul 17 13:25:24.468: INFO: >>> kubeConfig: /tmp/kubeconfig-578146847
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-5ce4461e-31f0-46c7-8a92-b5a317b219c0
STEP: Creating a pod to test consume configMaps
Jul 17 13:25:24.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09" in namespace "projected-4518" to be "Succeeded or Failed"
Jul 17 13:25:24.586: INFO: Pod "pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976495ms
Jul 17 13:25:26.594: INFO: Pod "pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010913462s
STEP: Saw pod success
Jul 17 13:25:26.594: INFO: Pod "pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09" satisfied condition "Succeeded or Failed"
Jul 17 13:25:26.600: INFO: Trying to get logs from node taikun-1-1188-w-1 pod pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 17 13:25:26.629: INFO: Waiting for pod pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09 to disappear
Jul 17 13:25:26.633: INFO: Pod pod-projected-configmaps-e028c65e-8510-4006-9567-d9538281fd09 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jul 17 13:25:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4518" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":5178,"failed":0}
SJul 17 13:25:26.644: INFO: Running AfterSuite actions on all nodes
Jul 17 13:25:26.644: INFO: Running AfterSuite actions on node 1
Jul 17 13:25:26.644: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":5179,"failed":0}

Ran 305 of 5484 Specs in 5901.070 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 5179 Skipped
PASS

Ginkgo ran 1 suite in 1h38m23.461141218s
Test Suite Passed
