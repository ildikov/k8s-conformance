I0517 17:00:27.231753      19 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-261591685
I0517 17:00:27.231784      19 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0517 17:00:27.232101      19 e2e.go:129] Starting e2e run "7637ae22-a266-40c2-9be5-42855f8312df" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621270825 - Will randomize all specs
Will run 311 of 5667 specs

May 17 17:00:27.268: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
E0517 17:00:27.290675      19 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May 17 17:00:27.295: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 17 17:00:27.312: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 17 17:00:27.364: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 17 17:00:27.364: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 17 17:00:27.364: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 17 17:00:27.372: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 17 17:00:27.372: INFO: e2e test version: v1.20.2
May 17 17:00:27.373: INFO: kube-apiserver version: v1.20.2
May 17 17:00:27.374: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:00:27.379: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:00:27.380: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubelet-test
May 17 17:00:27.446: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 17 17:00:27.465: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:00:31.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6106" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":21,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:00:31.536: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
STEP: Verifying LimitRange creation was observed
May 17 17:00:31.591: INFO: observed the limitRanges list
STEP: Fetching the LimitRange to ensure it has proper values
May 17 17:00:31.607: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 17:00:31.607: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 17 17:00:31.631: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 17 17:00:31.631: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 17 17:00:31.649: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 17 17:00:31.652: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 17 17:00:38.700: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:00:38.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5716" for this suite.

• [SLOW TEST:7.189 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":2,"skipped":56,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:00:38.729: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 17 17:00:38.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8658 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 17 17:00:39.176: INFO: stderr: ""
May 17 17:00:39.176: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 17 17:00:39.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8658 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 17 17:00:39.610: INFO: stderr: ""
May 17 17:00:39.610: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 17 17:00:39.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8658 delete pods e2e-test-httpd-pod'
May 17 17:01:48.761: INFO: stderr: ""
May 17 17:01:48.761: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:01:48.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8658" for this suite.

• [SLOW TEST:70.044 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":3,"skipped":59,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:01:48.773: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:01:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2275" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":59,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:01:48.844: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 17 17:01:48.885: INFO: Waiting up to 5m0s for pod "downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a" in namespace "downward-api-8296" to be "Succeeded or Failed"
May 17 17:01:48.889: INFO: Pod "downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.250999ms
May 17 17:01:50.895: INFO: Pod "downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009746785s
May 17 17:01:52.899: INFO: Pod "downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013805123s
STEP: Saw pod success
May 17 17:01:52.899: INFO: Pod "downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a" satisfied condition "Succeeded or Failed"
May 17 17:01:52.913: INFO: Trying to get logs from node 20test-worker-1 pod downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a container dapi-container: <nil>
STEP: delete the pod
May 17 17:01:52.950: INFO: Waiting for pod downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a to disappear
May 17 17:01:52.954: INFO: Pod downward-api-fabcb58d-43e8-42bd-8c2f-3338c93f7d6a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:01:52.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8296" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":79,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:01:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 17 17:01:57.048: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9443 PodName:var-expansion-669eb054-0d6f-4262-b4db-d0e19cb6ca25 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:01:57.048: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: test for file in mounted path
May 17 17:01:57.140: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9443 PodName:var-expansion-669eb054-0d6f-4262-b4db-d0e19cb6ca25 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:01:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: updating the annotation value
May 17 17:01:57.718: INFO: Successfully updated pod "var-expansion-669eb054-0d6f-4262-b4db-d0e19cb6ca25"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 17 17:01:57.722: INFO: Deleting pod "var-expansion-669eb054-0d6f-4262-b4db-d0e19cb6ca25" in namespace "var-expansion-9443"
May 17 17:01:57.727: INFO: Wait up to 5m0s for pod "var-expansion-669eb054-0d6f-4262-b4db-d0e19cb6ca25" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:02:39.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9443" for this suite.

• [SLOW TEST:46.774 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":6,"skipped":91,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:02:39.746: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 17 17:02:39.811: INFO: Waiting up to 5m0s for pod "downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1" in namespace "downward-api-1335" to be "Succeeded or Failed"
May 17 17:02:39.818: INFO: Pod "downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.354502ms
May 17 17:02:41.822: INFO: Pod "downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009594077s
STEP: Saw pod success
May 17 17:02:41.822: INFO: Pod "downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1" satisfied condition "Succeeded or Failed"
May 17 17:02:41.825: INFO: Trying to get logs from node 20test-worker-3 pod downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1 container dapi-container: <nil>
STEP: delete the pod
May 17 17:02:41.857: INFO: Waiting for pod downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1 to disappear
May 17 17:02:41.861: INFO: Pod downward-api-b0f6291f-6f61-42ab-a6b2-0a51eef372d1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:02:41.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1335" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:02:41.874: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:02:41.911: INFO: Creating ReplicaSet my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458
May 17 17:02:41.926: INFO: Pod name my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458: Found 0 pods out of 1
May 17 17:02:46.931: INFO: Pod name my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458: Found 1 pods out of 1
May 17 17:02:46.931: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458" is running
May 17 17:02:48.939: INFO: Pod "my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458-qp82v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-17 17:02:41 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-17 17:02:41 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-17 17:02:41 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-17 17:02:41 +0000 UTC Reason: Message:}])
May 17 17:02:48.940: INFO: Trying to dial the pod
May 17 17:02:53.973: INFO: Controller my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458: Got expected result from replica 1 [my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458-qp82v]: "my-hostname-basic-2bfdd358-0747-4053-8f12-09ba5422d458-qp82v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:02:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-769" for this suite.

• [SLOW TEST:12.111 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":8,"skipped":169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:02:53.986: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5266
May 17 17:03:00.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 17 17:03:00.235: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 17 17:03:00.235: INFO: stdout: "iptables"
May 17 17:03:00.235: INFO: proxyMode: iptables
May 17 17:03:00.249: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 17 17:03:00.252: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5266
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5266
I0517 17:03:00.334411      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5266, replica count: 3
I0517 17:03:03.384728      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 17:03:06.384997      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:03:06.389: INFO: Creating new exec pod
May 17 17:03:09.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec execpod-affinityrf7bg -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 17 17:03:09.577: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 17 17:03:09.578: INFO: stdout: ""
May 17 17:03:09.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec execpod-affinityrf7bg -- /bin/sh -x -c nc -zv -t -w 2 172.30.109.83 80'
May 17 17:03:09.751: INFO: stderr: "+ nc -zv -t -w 2 172.30.109.83 80\nConnection to 172.30.109.83 80 port [tcp/http] succeeded!\n"
May 17 17:03:09.751: INFO: stdout: ""
May 17 17:03:09.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec execpod-affinityrf7bg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.109.83:80/ ; done'
May 17 17:03:09.993: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n"
May 17 17:03:09.993: INFO: stdout: "\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq\naffinity-clusterip-timeout-pk8sq"
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Received response from host: affinity-clusterip-timeout-pk8sq
May 17 17:03:09.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec execpod-affinityrf7bg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.109.83:80/'
May 17 17:03:10.160: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n"
May 17 17:03:10.160: INFO: stdout: "affinity-clusterip-timeout-pk8sq"
May 17 17:03:30.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5266 exec execpod-affinityrf7bg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.109.83:80/'
May 17 17:03:30.332: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.109.83:80/\n"
May 17 17:03:30.332: INFO: stdout: "affinity-clusterip-timeout-mxxnf"
May 17 17:03:30.332: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5266, will wait for the garbage collector to delete the pods
May 17 17:03:30.407: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.520402ms
May 17 17:03:31.308: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 900.19704ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:04:35.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5266" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:101.274 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":9,"skipped":206,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:04:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 17 17:04:35.331: INFO: Waiting up to 5m0s for pod "pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9" in namespace "emptydir-6009" to be "Succeeded or Failed"
May 17 17:04:35.341: INFO: Pod "pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.557451ms
May 17 17:04:37.346: INFO: Pod "pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015045349s
May 17 17:04:39.349: INFO: Pod "pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018363382s
STEP: Saw pod success
May 17 17:04:39.349: INFO: Pod "pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9" satisfied condition "Succeeded or Failed"
May 17 17:04:39.351: INFO: Trying to get logs from node 20test-worker-1 pod pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9 container test-container: <nil>
STEP: delete the pod
May 17 17:04:39.389: INFO: Waiting for pod pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9 to disappear
May 17 17:04:39.391: INFO: Pod pod-ba1b8252-d044-49b6-b82b-1fb67e58e8f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:04:39.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6009" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":223,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:04:39.407: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 17 17:04:40.595: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May 17 17:04:42.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756867880, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756867880, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756867880, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756867880, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:04:45.638: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:04:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:04:46.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9335" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.692 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":11,"skipped":223,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:04:47.099: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 17 17:04:53.255: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 17:04:53.258: INFO: Pod pod-with-poststart-http-hook still exists
May 17 17:04:55.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 17:04:55.263: INFO: Pod pod-with-poststart-http-hook still exists
May 17 17:04:57.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 17:04:57.264: INFO: Pod pod-with-poststart-http-hook still exists
May 17 17:04:59.259: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 17 17:04:59.262: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:04:59.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2152" for this suite.

• [SLOW TEST:12.173 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":241,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:04:59.275: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-5a3e8fb1-7e81-4620-9588-e562dca26976 in namespace container-probe-871
May 17 17:05:01.355: INFO: Started pod liveness-5a3e8fb1-7e81-4620-9588-e562dca26976 in namespace container-probe-871
STEP: checking the pod's current state and verifying that restartCount is present
May 17 17:05:01.358: INFO: Initial restart count of pod liveness-5a3e8fb1-7e81-4620-9588-e562dca26976 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:09:01.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-871" for this suite.

• [SLOW TEST:242.688 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":243,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:09:01.964: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:09:02.022: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 17 17:09:02.042: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:02.060: INFO: Number of nodes with available pods: 0
May 17 17:09:02.064: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:09:03.073: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:03.079: INFO: Number of nodes with available pods: 0
May 17 17:09:03.080: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:09:04.071: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:04.073: INFO: Number of nodes with available pods: 0
May 17 17:09:04.073: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:09:05.086: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:05.089: INFO: Number of nodes with available pods: 1
May 17 17:09:05.089: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:06.072: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:06.075: INFO: Number of nodes with available pods: 1
May 17 17:09:06.075: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:07.072: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:07.089: INFO: Number of nodes with available pods: 1
May 17 17:09:07.090: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:08.075: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:08.078: INFO: Number of nodes with available pods: 1
May 17 17:09:08.079: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:09.078: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:09.087: INFO: Number of nodes with available pods: 1
May 17 17:09:09.087: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:10.072: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:10.075: INFO: Number of nodes with available pods: 2
May 17 17:09:10.075: INFO: Node 20test-worker-3 is running more than one daemon pod
May 17 17:09:11.074: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:11.087: INFO: Number of nodes with available pods: 2
May 17 17:09:11.087: INFO: Node 20test-worker-3 is running more than one daemon pod
May 17 17:09:12.072: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:12.077: INFO: Number of nodes with available pods: 3
May 17 17:09:12.077: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 17 17:09:12.120: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:12.121: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:12.121: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:12.129: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:13.133: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:13.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:13.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:13.145: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:14.132: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:14.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:14.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:14.142: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:15.146: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:15.146: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:15.146: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:15.146: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:15.181: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:16.135: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:16.135: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:16.135: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:16.135: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:16.140: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:17.134: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:17.134: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:17.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:17.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:17.138: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:18.134: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:18.134: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:18.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:18.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:18.138: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:19.134: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:19.134: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:19.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:19.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:19.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:20.136: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:20.136: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:20.136: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:20.136: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:20.140: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:21.134: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:21.134: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:21.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:21.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:21.138: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:22.135: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:22.135: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:22.136: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:22.136: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:22.140: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:23.132: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:23.132: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:23.132: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:23.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:23.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:24.133: INFO: Wrong image for pod: daemon-set-8hwvk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:24.133: INFO: Pod daemon-set-8hwvk is not available
May 17 17:09:24.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:24.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:24.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:25.155: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:25.156: INFO: Pod daemon-set-pz5hf is not available
May 17 17:09:25.156: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:25.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:26.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:26.134: INFO: Pod daemon-set-pz5hf is not available
May 17 17:09:26.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:26.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:27.136: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:27.136: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:27.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:28.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:28.134: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:28.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:28.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:29.132: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:29.132: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:29.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:29.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:30.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:30.134: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:30.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:30.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:31.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:31.134: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:31.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:31.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:32.131: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:32.131: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:32.131: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:32.135: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:33.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:33.133: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:33.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:33.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:34.133: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:34.133: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:34.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:34.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:35.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:35.134: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:35.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:35.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:36.135: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:36.135: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:36.136: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:36.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:37.135: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:37.135: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:37.135: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:37.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:38.134: INFO: Wrong image for pod: daemon-set-jsb4v. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:38.134: INFO: Pod daemon-set-jsb4v is not available
May 17 17:09:38.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:38.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:39.132: INFO: Pod daemon-set-nslnt is not available
May 17 17:09:39.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:39.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:40.132: INFO: Pod daemon-set-nslnt is not available
May 17 17:09:40.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:40.135: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:41.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:41.132: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:41.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:42.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:42.135: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:42.140: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:43.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:43.133: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:43.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:44.151: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:44.151: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:44.154: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:45.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:45.133: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:45.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:46.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:46.132: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:46.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:47.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:47.134: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:47.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:48.134: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:48.134: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:48.137: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:49.135: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:49.135: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:49.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:50.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:50.133: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:50.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:51.132: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:51.133: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:51.135: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:52.133: INFO: Wrong image for pod: daemon-set-rq6kq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 17 17:09:52.133: INFO: Pod daemon-set-rq6kq is not available
May 17 17:09:52.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:53.133: INFO: Pod daemon-set-ndgmg is not available
May 17 17:09:53.136: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 17 17:09:53.139: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:53.142: INFO: Number of nodes with available pods: 2
May 17 17:09:53.142: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:54.147: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:54.153: INFO: Number of nodes with available pods: 2
May 17 17:09:54.156: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:55.147: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:55.150: INFO: Number of nodes with available pods: 2
May 17 17:09:55.150: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:56.148: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:56.153: INFO: Number of nodes with available pods: 2
May 17 17:09:56.153: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:57.146: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:57.148: INFO: Number of nodes with available pods: 2
May 17 17:09:57.149: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:58.148: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:58.151: INFO: Number of nodes with available pods: 2
May 17 17:09:58.151: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:09:59.147: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:09:59.152: INFO: Number of nodes with available pods: 2
May 17 17:09:59.152: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:10:00.149: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:10:00.151: INFO: Number of nodes with available pods: 3
May 17 17:10:00.151: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9096, will wait for the garbage collector to delete the pods
May 17 17:10:00.231: INFO: Deleting DaemonSet.extensions daemon-set took: 5.118501ms
May 17 17:10:01.131: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.263174ms
May 17 17:10:48.835: INFO: Number of nodes with available pods: 0
May 17 17:10:48.836: INFO: Number of running nodes: 0, number of available pods: 0
May 17 17:10:48.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9406"},"items":null}

May 17 17:10:48.841: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9406"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:10:48.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9096" for this suite.

• [SLOW TEST:106.900 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":14,"skipped":244,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:10:48.866: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-6t4f
STEP: Creating a pod to test atomic-volume-subpath
May 17 17:10:48.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6t4f" in namespace "subpath-6563" to be "Succeeded or Failed"
May 17 17:10:48.939: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.38623ms
May 17 17:10:50.945: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019190678s
May 17 17:10:52.952: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.026022029s
May 17 17:10:54.960: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 6.034152419s
May 17 17:10:56.967: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 8.041093873s
May 17 17:10:58.970: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 10.044490429s
May 17 17:11:00.976: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 12.049826582s
May 17 17:11:02.983: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 14.056884807s
May 17 17:11:04.988: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 16.06242253s
May 17 17:11:06.994: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 18.068502606s
May 17 17:11:08.999: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 20.073023592s
May 17 17:11:11.006: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Running", Reason="", readiness=true. Elapsed: 22.079728863s
May 17 17:11:13.010: INFO: Pod "pod-subpath-test-projected-6t4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084691992s
STEP: Saw pod success
May 17 17:11:13.011: INFO: Pod "pod-subpath-test-projected-6t4f" satisfied condition "Succeeded or Failed"
May 17 17:11:13.013: INFO: Trying to get logs from node 20test-worker-3 pod pod-subpath-test-projected-6t4f container test-container-subpath-projected-6t4f: <nil>
STEP: delete the pod
May 17 17:11:13.051: INFO: Waiting for pod pod-subpath-test-projected-6t4f to disappear
May 17 17:11:13.054: INFO: Pod pod-subpath-test-projected-6t4f no longer exists
STEP: Deleting pod pod-subpath-test-projected-6t4f
May 17 17:11:13.054: INFO: Deleting pod "pod-subpath-test-projected-6t4f" in namespace "subpath-6563"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6563" for this suite.

• [SLOW TEST:24.198 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":15,"skipped":250,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:13.069: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 17 17:11:15.673: INFO: Successfully updated pod "annotationupdate7da5d1be-d60e-4fcc-a884-4d74a2ebbb92"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:17.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-363" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":263,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:17.701: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:33.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2641" for this suite.

• [SLOW TEST:16.181 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":17,"skipped":264,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:33.881: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 17 17:11:33.940: INFO: Waiting up to 5m0s for pod "pod-4351535c-20fb-45f7-92c7-83bbb0d09d40" in namespace "emptydir-2060" to be "Succeeded or Failed"
May 17 17:11:33.966: INFO: Pod "pod-4351535c-20fb-45f7-92c7-83bbb0d09d40": Phase="Pending", Reason="", readiness=false. Elapsed: 26.117766ms
May 17 17:11:35.969: INFO: Pod "pod-4351535c-20fb-45f7-92c7-83bbb0d09d40": Phase="Running", Reason="", readiness=true. Elapsed: 2.029687461s
May 17 17:11:37.973: INFO: Pod "pod-4351535c-20fb-45f7-92c7-83bbb0d09d40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032926136s
STEP: Saw pod success
May 17 17:11:37.973: INFO: Pod "pod-4351535c-20fb-45f7-92c7-83bbb0d09d40" satisfied condition "Succeeded or Failed"
May 17 17:11:37.977: INFO: Trying to get logs from node 20test-worker-3 pod pod-4351535c-20fb-45f7-92c7-83bbb0d09d40 container test-container: <nil>
STEP: delete the pod
May 17 17:11:38.028: INFO: Waiting for pod pod-4351535c-20fb-45f7-92c7-83bbb0d09d40 to disappear
May 17 17:11:38.032: INFO: Pod pod-4351535c-20fb-45f7-92c7-83bbb0d09d40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:38.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2060" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:38.041: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 17 17:11:38.085: INFO: Waiting up to 5m0s for pod "var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d" in namespace "var-expansion-1225" to be "Succeeded or Failed"
May 17 17:11:38.090: INFO: Pod "var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.931658ms
May 17 17:11:40.094: INFO: Pod "var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008770825s
STEP: Saw pod success
May 17 17:11:40.094: INFO: Pod "var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d" satisfied condition "Succeeded or Failed"
May 17 17:11:40.096: INFO: Trying to get logs from node 20test-worker-1 pod var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d container dapi-container: <nil>
STEP: delete the pod
May 17 17:11:40.121: INFO: Waiting for pod var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d to disappear
May 17 17:11:40.123: INFO: Pod var-expansion-29d8472b-0a38-4e70-9392-1bbcc17aba8d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:40.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1225" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":305,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:40.137: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-bc1baed9-26e1-4665-98d0-6157075968c6
STEP: Creating a pod to test consume secrets
May 17 17:11:40.178: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b" in namespace "projected-9280" to be "Succeeded or Failed"
May 17 17:11:40.191: INFO: Pod "pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.675671ms
May 17 17:11:42.194: INFO: Pod "pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011870424s
STEP: Saw pod success
May 17 17:11:42.194: INFO: Pod "pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b" satisfied condition "Succeeded or Failed"
May 17 17:11:42.195: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b container secret-volume-test: <nil>
STEP: delete the pod
May 17 17:11:42.215: INFO: Waiting for pod pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b to disappear
May 17 17:11:42.217: INFO: Pod pod-projected-secrets-cf68aa66-4da1-4533-9703-0ea942f3604b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:11:42.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9280" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:11:42.224: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4415
STEP: creating service affinity-nodeport in namespace services-4415
STEP: creating replication controller affinity-nodeport in namespace services-4415
I0517 17:11:42.289127      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4415, replica count: 3
I0517 17:11:45.346681      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:11:45.359: INFO: Creating new exec pod
May 17 17:11:48.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-4415 exec execpod-affinity4xvdj -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 17 17:11:48.972: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 17 17:11:48.972: INFO: stdout: ""
May 17 17:11:48.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-4415 exec execpod-affinity4xvdj -- /bin/sh -x -c nc -zv -t -w 2 172.30.123.255 80'
May 17 17:11:49.162: INFO: stderr: "+ nc -zv -t -w 2 172.30.123.255 80\nConnection to 172.30.123.255 80 port [tcp/http] succeeded!\n"
May 17 17:11:49.162: INFO: stdout: ""
May 17 17:11:49.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-4415 exec execpod-affinity4xvdj -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.161 32578'
May 17 17:11:49.341: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.161 32578\nConnection to 10.30.20.161 32578 port [tcp/32578] succeeded!\n"
May 17 17:11:49.341: INFO: stdout: ""
May 17 17:11:49.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-4415 exec execpod-affinity4xvdj -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.187 32578'
May 17 17:11:49.540: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.187 32578\nConnection to 10.30.20.187 32578 port [tcp/32578] succeeded!\n"
May 17 17:11:49.540: INFO: stdout: ""
May 17 17:11:49.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-4415 exec execpod-affinity4xvdj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.30.20.193:32578/ ; done'
May 17 17:11:49.813: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32578/\n"
May 17 17:11:49.813: INFO: stdout: "\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf\naffinity-nodeport-g7zrf"
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Received response from host: affinity-nodeport-g7zrf
May 17 17:11:49.813: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4415, will wait for the garbage collector to delete the pods
May 17 17:11:49.905: INFO: Deleting ReplicationController affinity-nodeport took: 24.21745ms
May 17 17:11:50.805: INFO: Terminating ReplicationController affinity-nodeport pods took: 900.177049ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:12:35.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4415" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:53.029 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":21,"skipped":347,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:12:35.253: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3844.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3844.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3844.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3844.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:12:47.355: INFO: DNS probes using dns-3844/dns-test-71aa0d88-e885-47ea-bf25-56d807175a58 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:12:47.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3844" for this suite.

• [SLOW TEST:12.130 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":22,"skipped":356,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:12:47.392: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 17 17:12:47.439: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 17:13:47.511: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:13:47.515: INFO: Starting informer...
STEP: Starting pod...
May 17 17:13:47.561: INFO: Pod is running on 20test-worker-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 17 17:13:48.263: INFO: Pod wasn't evicted. Proceeding
May 17 17:13:48.264: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 17 17:15:03.331: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:15:03.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8322" for this suite.

• [SLOW TEST:135.950 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":23,"skipped":374,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:15:03.343: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-g2vm
STEP: Creating a pod to test atomic-volume-subpath
May 17 17:15:03.397: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g2vm" in namespace "subpath-6127" to be "Succeeded or Failed"
May 17 17:15:03.403: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758173ms
May 17 17:15:05.406: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008849004s
May 17 17:15:07.409: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 4.011595554s
May 17 17:15:09.412: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 6.015127879s
May 17 17:15:11.417: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 8.019547481s
May 17 17:15:13.422: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 10.024510016s
May 17 17:15:15.426: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 12.028885742s
May 17 17:15:17.430: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 14.033061066s
May 17 17:15:19.434: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 16.036979882s
May 17 17:15:21.436: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 18.039439267s
May 17 17:15:23.440: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Running", Reason="", readiness=true. Elapsed: 20.043360492s
May 17 17:15:25.444: INFO: Pod "pod-subpath-test-configmap-g2vm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.046919919s
STEP: Saw pod success
May 17 17:15:25.444: INFO: Pod "pod-subpath-test-configmap-g2vm" satisfied condition "Succeeded or Failed"
May 17 17:15:25.447: INFO: Trying to get logs from node 20test-worker-3 pod pod-subpath-test-configmap-g2vm container test-container-subpath-configmap-g2vm: <nil>
STEP: delete the pod
May 17 17:15:25.473: INFO: Waiting for pod pod-subpath-test-configmap-g2vm to disappear
May 17 17:15:25.477: INFO: Pod pod-subpath-test-configmap-g2vm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g2vm
May 17 17:15:25.477: INFO: Deleting pod "pod-subpath-test-configmap-g2vm" in namespace "subpath-6127"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:15:25.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6127" for this suite.

• [SLOW TEST:22.145 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":24,"skipped":382,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:15:25.493: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:15:25.527: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 17 17:15:31.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-8584 --namespace=crd-publish-openapi-8584 create -f -'
May 17 17:15:32.304: INFO: stderr: ""
May 17 17:15:32.304: INFO: stdout: "e2e-test-crd-publish-openapi-1136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 17:15:32.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-8584 --namespace=crd-publish-openapi-8584 delete e2e-test-crd-publish-openapi-1136-crds test-cr'
May 17 17:15:32.424: INFO: stderr: ""
May 17 17:15:32.424: INFO: stdout: "e2e-test-crd-publish-openapi-1136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 17 17:15:32.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-8584 --namespace=crd-publish-openapi-8584 apply -f -'
May 17 17:15:32.761: INFO: stderr: ""
May 17 17:15:32.761: INFO: stdout: "e2e-test-crd-publish-openapi-1136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 17 17:15:32.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-8584 --namespace=crd-publish-openapi-8584 delete e2e-test-crd-publish-openapi-1136-crds test-cr'
May 17 17:15:32.857: INFO: stderr: ""
May 17 17:15:32.857: INFO: stdout: "e2e-test-crd-publish-openapi-1136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 17 17:15:32.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-8584 explain e2e-test-crd-publish-openapi-1136-crds'
May 17 17:15:33.162: INFO: stderr: ""
May 17 17:15:33.162: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1136-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:15:38.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8584" for this suite.

• [SLOW TEST:13.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":25,"skipped":384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:15:38.625: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-e9d6ba4e-c994-426b-94d5-9d806618906d
STEP: Creating a pod to test consume secrets
May 17 17:15:38.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28" in namespace "projected-5482" to be "Succeeded or Failed"
May 17 17:15:38.664: INFO: Pod "pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.690775ms
May 17 17:15:40.669: INFO: Pod "pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008781093s
STEP: Saw pod success
May 17 17:15:40.669: INFO: Pod "pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28" satisfied condition "Succeeded or Failed"
May 17 17:15:40.671: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 17 17:15:40.689: INFO: Waiting for pod pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28 to disappear
May 17 17:15:40.692: INFO: Pod pod-projected-secrets-00a1762e-5473-42f2-9d94-3c39befe1d28 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:15:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5482" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:15:40.702: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-9546/secret-test-54c34667-6f87-4f8a-b32a-025494784645
STEP: Creating a pod to test consume secrets
May 17 17:15:40.749: INFO: Waiting up to 5m0s for pod "pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4" in namespace "secrets-9546" to be "Succeeded or Failed"
May 17 17:15:40.762: INFO: Pod "pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.519018ms
May 17 17:15:42.766: INFO: Pod "pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016228769s
May 17 17:15:44.769: INFO: Pod "pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019233735s
STEP: Saw pod success
May 17 17:15:44.769: INFO: Pod "pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4" satisfied condition "Succeeded or Failed"
May 17 17:15:44.771: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4 container env-test: <nil>
STEP: delete the pod
May 17 17:15:44.789: INFO: Waiting for pod pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4 to disappear
May 17 17:15:44.798: INFO: Pod pod-configmaps-6736eb9d-9f00-442c-9f65-55e3708bfce4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:15:44.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9546" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":452,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:15:44.812: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-725, will wait for the garbage collector to delete the pods
May 17 17:15:48.928: INFO: Deleting Job.batch foo took: 7.078232ms
May 17 17:15:49.828: INFO: Terminating Job.batch foo pods took: 900.326304ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:16:35.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-725" for this suite.

• [SLOW TEST:50.328 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":28,"skipped":467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:16:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 17 17:16:35.179: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 17:16:35.184: INFO: Waiting for terminating namespaces to be deleted...
May 17 17:16:35.185: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-1 before test
May 17 17:16:35.200: INFO: calico-node-fjsxq from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.200: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:16:35.200: INFO: calico-typha-954b59468-864hr from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.200: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:16:35.200: INFO: kube-proxy-d7hzd from kube-system started at 2021-05-17 16:45:38 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.200: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:16:35.200: INFO: fluent-bit-vm2vm from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.201: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:16:35.201: INFO: alertmanager-main-0 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.201: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:16:35.201: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:16:35.201: INFO: grafana-f8cd57fcf-p45c2 from monitoring started at 2021-05-17 16:46:48 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.202: INFO: 	Container grafana ready: true, restart count 0
May 17 17:16:35.202: INFO: kube-state-metrics-587bfd4f97-6qs5t from monitoring started at 2021-05-17 16:46:48 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.202: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 17 17:16:35.202: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 17 17:16:35.202: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 17 17:16:35.202: INFO: node-exporter-5dkbc from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.202: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:16:35.203: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:16:35.203: INFO: prometheus-operator-7649c7454f-5xts8 from monitoring started at 2021-05-17 16:46:39 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.203: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:16:35.203: INFO: 	Container prometheus-operator ready: true, restart count 0
May 17 17:16:35.203: INFO: csi-cephfsplugin-cgwrr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.203: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:16:35.203: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.203: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.204: INFO: csi-cephfsplugin-provisioner-8658f67749-mj486 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:16:35.204: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:16:35.204: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:16:35.204: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:16:35.204: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:16:35.204: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:16:35.205: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.205: INFO: csi-rbdplugin-6jrsr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.205: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:16:35.205: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.205: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.205: INFO: csi-rbdplugin-provisioner-94f699d86-r49r5 from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (6 container statuses recorded)
May 17 17:16:35.206: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:16:35.206: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:16:35.206: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:16:35.206: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:16:35.206: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:16:35.206: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.206: INFO: rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.206: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:16:35.207: INFO: rook-ceph-mgr-a-5db4bfbc6-fnj5z from rook-ceph started at 2021-05-17 16:49:56 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.207: INFO: 	Container mgr ready: true, restart count 0
May 17 17:16:35.207: INFO: rook-ceph-mon-c-545cb7776f-tsblp from rook-ceph started at 2021-05-17 16:49:42 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.207: INFO: 	Container mon ready: true, restart count 0
May 17 17:16:35.207: INFO: rook-ceph-osd-2-96fd479f5-88zbz from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.207: INFO: 	Container osd ready: true, restart count 0
May 17 17:16:35.207: INFO: rook-ceph-osd-prepare-20test-worker-1-qb5xq from rook-ceph started at 2021-05-17 17:14:22 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.208: INFO: 	Container provision ready: false, restart count 0
May 17 17:16:35.208: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.208: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:16:35.208: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:16:35.208: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-2 before test
May 17 17:16:35.224: INFO: calico-kube-controllers-5fb9f7f6d8-gpg2w from calico-system started at 2021-05-17 16:46:37 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.224: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 17:16:35.224: INFO: calico-node-sxlpp from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.224: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:16:35.224: INFO: calico-typha-954b59468-jks4v from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.225: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:16:35.225: INFO: coredns-74ff55c5b-hjgsp from kube-system started at 2021-05-17 16:46:42 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.225: INFO: 	Container coredns ready: true, restart count 0
May 17 17:16:35.225: INFO: kube-proxy-wmdvj from kube-system started at 2021-05-17 16:45:52 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.225: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:16:35.225: INFO: fluent-bit-mjvsf from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.225: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:16:35.225: INFO: alertmanager-main-2 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.226: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:16:35.226: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:16:35.226: INFO: node-exporter-gcj5f from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.226: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:16:35.226: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:16:35.226: INFO: prometheus-adapter-69b8496df6-tns5m from monitoring started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.226: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 17 17:16:35.227: INFO: prometheus-k8s-0 from monitoring started at 2021-05-17 16:47:03 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.227: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:16:35.227: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:16:35.227: INFO: csi-cephfsplugin-8lpms from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.227: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:16:35.227: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.228: INFO: csi-cephfsplugin-provisioner-8658f67749-vcx69 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:16:35.228: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:16:35.228: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.228: INFO: csi-rbdplugin-7rpck from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.229: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:16:35.229: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.229: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.229: INFO: csi-rbdplugin-provisioner-94f699d86-xznpd from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:16:35.229: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:16:35.229: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:16:35.229: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:16:35.230: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:16:35.230: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:16:35.230: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.230: INFO: rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.230: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:16:35.230: INFO: rook-ceph-mon-b-589bbc6556-w592m from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.230: INFO: 	Container mon ready: true, restart count 0
May 17 17:16:35.230: INFO: rook-ceph-operator-5f6ffc46c7-hj2ml from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.231: INFO: 	Container rook-ceph-operator ready: true, restart count 0
May 17 17:16:35.231: INFO: rook-ceph-osd-0-7896cc545c-psfck from rook-ceph started at 2021-05-17 16:50:08 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.231: INFO: 	Container osd ready: true, restart count 0
May 17 17:16:35.231: INFO: rook-ceph-osd-prepare-20test-worker-2-lhtcq from rook-ceph started at 2021-05-17 17:14:24 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.231: INFO: 	Container provision ready: false, restart count 0
May 17 17:16:35.231: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:16:35.231: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:16:35.232: INFO: tigera-operator-657cc89589-qhpft from tigera-operator started at 2021-05-17 16:46:10 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.232: INFO: 	Container tigera-operator ready: true, restart count 0
May 17 17:16:35.232: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-3 before test
May 17 17:16:35.245: INFO: calico-node-rb678 from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:16:35.245: INFO: calico-typha-954b59468-v2z88 from calico-system started at 2021-05-17 16:46:16 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:16:35.245: INFO: kube-proxy-n6xv8 from kube-system started at 2021-05-17 16:45:25 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:16:35.245: INFO: fluent-bit-hg8tt from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:16:35.245: INFO: alertmanager-main-1 from monitoring started at 2021-05-17 17:14:35 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:16:35.245: INFO: node-exporter-ccd9w from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:16:35.245: INFO: prometheus-k8s-1 from monitoring started at 2021-05-17 17:14:37 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:16:35.245: INFO: csi-cephfsplugin-9hmwc from rook-ceph started at 2021-05-17 17:14:36 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.245: INFO: csi-rbdplugin-9hr4g from rook-ceph started at 2021-05-17 17:14:38 +0000 UTC (3 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:16:35.245: INFO: rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:16:35.245: INFO: rook-ceph-mon-a-6947dc47f-8wtqd from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container mon ready: true, restart count 0
May 17 17:16:35.245: INFO: rook-ceph-osd-1-6c8669bc9-89l9s from rook-ceph started at 2021-05-17 17:13:51 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container osd ready: true, restart count 0
May 17 17:16:35.245: INFO: rook-ceph-osd-prepare-20test-worker-3-cqpxz from rook-ceph started at 2021-05-17 17:14:26 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container provision ready: false, restart count 0
May 17 17:16:35.245: INFO: sonobuoy from sonobuoy started at 2021-05-17 16:59:52 +0000 UTC (1 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 17:16:35.245: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:16:35.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:16:35.245: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-976c06be-e0ff-4fe5-a946-340d82146592 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.30.20.187 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.30.20.187 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 17 17:16:47.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.30.20.187 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:47.393: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321
May 17 17:16:47.487: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.30.20.187:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:47.487: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321 UDP
May 17 17:16:47.572: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.30.20.187 54321] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:47.572: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 17 17:16:52.660: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.30.20.187 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:52.660: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321
May 17 17:16:52.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.30.20.187:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:52.754: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321 UDP
May 17 17:16:52.833: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.30.20.187 54321] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 17 17:16:57.915: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.30.20.187 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:57.916: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321
May 17 17:16:58.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.30.20.187:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:58.000: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321 UDP
May 17 17:16:58.103: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.30.20.187 54321] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:16:58.103: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 17 17:17:03.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.30.20.187 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:03.183: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321
May 17 17:17:03.286: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.30.20.187:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:03.286: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321 UDP
May 17 17:17:03.381: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.30.20.187 54321] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:03.381: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 17 17:17:08.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.30.20.187 http://127.0.0.1:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:08.457: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321
May 17 17:17:08.529: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.30.20.187:54321/hostname] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:08.529: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.30.20.187, port: 54321 UDP
May 17 17:17:08.606: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.30.20.187 54321] Namespace:sched-pred-7128 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:08.606: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: removing the label kubernetes.io/e2e-976c06be-e0ff-4fe5-a946-340d82146592 off the node 20test-worker-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-976c06be-e0ff-4fe5-a946-340d82146592
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:13.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7128" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:38.553 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":29,"skipped":490,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:13.699: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:17:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:14.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6392" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":30,"skipped":493,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:14.921: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:17:14.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb" in namespace "projected-2591" to be "Succeeded or Failed"
May 17 17:17:15.007: INFO: Pod "downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.550239ms
May 17 17:17:17.012: INFO: Pod "downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029830855s
STEP: Saw pod success
May 17 17:17:17.012: INFO: Pod "downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb" satisfied condition "Succeeded or Failed"
May 17 17:17:17.016: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb container client-container: <nil>
STEP: delete the pod
May 17 17:17:17.051: INFO: Waiting for pod downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb to disappear
May 17 17:17:17.053: INFO: Pod downwardapi-volume-1b16c3a4-2a65-4d76-a7ec-83a3e859d1cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:17.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2591" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":506,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:17.070: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:17:17.110: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:17.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5419" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":32,"skipped":513,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:17.846: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3961
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 17 17:17:17.889: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 17:17:17.983: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 17:17:20.151: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:21.997: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:23.987: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:25.996: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:27.988: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:29.986: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:31.989: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:33.987: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:35.989: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:17:37.987: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 17 17:17:37.991: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 17 17:17:37.995: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 17 17:17:40.023: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 17:17:40.023: INFO: Breadth first check of 172.20.227.224 on host 10.30.20.193...
May 17 17:17:40.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.49:9080/dial?request=hostname&protocol=http&host=172.20.227.224&port=8080&tries=1'] Namespace:pod-network-test-3961 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:40.025: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:17:40.105: INFO: Waiting for responses: map[]
May 17 17:17:40.105: INFO: reached 172.20.227.224 after 0/1 tries
May 17 17:17:40.105: INFO: Breadth first check of 172.20.206.211 on host 10.30.20.161...
May 17 17:17:40.108: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.49:9080/dial?request=hostname&protocol=http&host=172.20.206.211&port=8080&tries=1'] Namespace:pod-network-test-3961 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:40.108: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:17:40.177: INFO: Waiting for responses: map[]
May 17 17:17:40.177: INFO: reached 172.20.206.211 after 0/1 tries
May 17 17:17:40.177: INFO: Breadth first check of 172.20.150.48 on host 10.30.20.187...
May 17 17:17:40.180: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.49:9080/dial?request=hostname&protocol=http&host=172.20.150.48&port=8080&tries=1'] Namespace:pod-network-test-3961 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:17:40.180: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:17:40.256: INFO: Waiting for responses: map[]
May 17 17:17:40.256: INFO: reached 172.20.150.48 after 0/1 tries
May 17 17:17:40.256: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:40.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3961" for this suite.

• [SLOW TEST:22.423 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":33,"skipped":515,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:40.273: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-7920aa51-de8c-4b40-bd5d-7c7ec17905f5
STEP: Creating a pod to test consume configMaps
May 17 17:17:40.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8" in namespace "projected-7692" to be "Succeeded or Failed"
May 17 17:17:40.349: INFO: Pod "pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.622579ms
May 17 17:17:42.353: INFO: Pod "pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012863039s
May 17 17:17:44.356: INFO: Pod "pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016504952s
STEP: Saw pod success
May 17 17:17:44.356: INFO: Pod "pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8" satisfied condition "Succeeded or Failed"
May 17 17:17:44.359: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:17:44.387: INFO: Waiting for pod pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8 to disappear
May 17 17:17:44.389: INFO: Pod pod-projected-configmaps-cea45c9d-8a61-4bd1-88b9-60a253ae7ad8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:44.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7692" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":518,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:44.402: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:17:44.456: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 17 17:17:49.473: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 17 17:17:49.473: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:17:53.538: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1853  5f8f515e-6bbd-4e53-8204-a3e2df2da079 12716 1 2021-05-17 17:17:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-17 17:17:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bc5fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-17 17:17:49 +0000 UTC,LastTransitionTime:2021-05-17 17:17:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-05-17 17:17:51 +0000 UTC,LastTransitionTime:2021-05-17 17:17:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 17:17:53.540: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-1853  1b08c347-47e7-4787-8bd7-5ffe59943c88 12705 1 2021-05-17 17:17:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 5f8f515e-6bbd-4e53-8204-a3e2df2da079 0xc0033bc547 0xc0033bc548}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:17:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f8f515e-6bbd-4e53-8204-a3e2df2da079\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033bc5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 17:17:53.542: INFO: Pod "test-cleanup-deployment-685c4f8568-cd28g" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-cd28g test-cleanup-deployment-685c4f8568- deployment-1853  5ccef3b8-4386-4afe-b117-ebb81b169672 12704 0 2021-05-17 17:17:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:172.20.150.52/32 cni.projectcalico.org/podIPs:172.20.150.52/32] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 1b08c347-47e7-4787-8bd7-5ffe59943c88 0xc0033bc947 0xc0033bc948}] []  [{kube-controller-manager Update v1 2021-05-17 17:17:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b08c347-47e7-4787-8bd7-5ffe59943c88\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:17:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:17:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ws44q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ws44q,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ws44q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:17:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:17:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:17:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:17:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.52,StartTime:2021-05-17 17:17:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:17:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://88b89c313031e7a7e26d9992d9e1566eaef95a694023ccad4a9ce4aad1b2e9e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:53.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1853" for this suite.

• [SLOW TEST:9.164 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":35,"skipped":521,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:53.578: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-afeb2db7-5aa3-419c-bb70-f8eefde15a09
STEP: Creating a pod to test consume configMaps
May 17 17:17:53.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df" in namespace "configmap-5257" to be "Succeeded or Failed"
May 17 17:17:53.647: INFO: Pod "pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506487ms
May 17 17:17:55.651: INFO: Pod "pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010414275s
STEP: Saw pod success
May 17 17:17:55.651: INFO: Pod "pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df" satisfied condition "Succeeded or Failed"
May 17 17:17:55.654: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df container agnhost-container: <nil>
STEP: delete the pod
May 17 17:17:55.676: INFO: Waiting for pod pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df to disappear
May 17 17:17:55.679: INFO: Pod pod-configmaps-08b70821-c70e-4659-be95-37830a5d68df no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:55.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5257" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:55.692: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 17 17:17:55.775: INFO: Waiting up to 5m0s for pod "client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19" in namespace "containers-6979" to be "Succeeded or Failed"
May 17 17:17:55.784: INFO: Pod "client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831976ms
May 17 17:17:57.788: INFO: Pod "client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013696052s
STEP: Saw pod success
May 17 17:17:57.789: INFO: Pod "client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19" satisfied condition "Succeeded or Failed"
May 17 17:17:57.791: INFO: Trying to get logs from node 20test-worker-3 pod client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:17:57.813: INFO: Waiting for pod client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19 to disappear
May 17 17:17:57.816: INFO: Pod client-containers-fb2622b4-9af5-4bd1-b4fe-21a4f8f9cf19 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:17:57.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6979" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":554,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:17:57.837: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3013.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3013.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3013.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3013.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.244.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.244.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.244.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.244.6_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3013.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3013.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3013.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3013.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3013.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3013.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 6.244.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.244.6_udp@PTR;check="$$(dig +tcp +noall +answer +search 6.244.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.244.6_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:18:02.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:02.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:02.055: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:02.058: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:02.083: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:07.109: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:07.117: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:07.154: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:07.160: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:07.179: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:12.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:12.106: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:12.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:12.148: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:12.165: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:17.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:17.098: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:17.122: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:17.125: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:17.139: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:22.097: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:22.101: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:22.147: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:22.152: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:22.171: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:27.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:27.094: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:27.112: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:27.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local from pod dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e: the server could not find the requested resource (get pods dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e)
May 17 17:18:27.127: INFO: Lookups using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3013.svc.cluster.local]

May 17 17:18:32.194: INFO: DNS probes using dns-3013/dns-test-7813d90a-66b9-48ce-8996-d63aa8e7cd4e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:18:32.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3013" for this suite.

• [SLOW TEST:34.469 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":38,"skipped":573,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:18:32.311: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:18:33.367: INFO: Checking APIGroup: apiregistration.k8s.io
May 17 17:18:33.368: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 17 17:18:33.368: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.368: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 17 17:18:33.368: INFO: Checking APIGroup: apps
May 17 17:18:33.369: INFO: PreferredVersion.GroupVersion: apps/v1
May 17 17:18:33.369: INFO: Versions found [{apps/v1 v1}]
May 17 17:18:33.369: INFO: apps/v1 matches apps/v1
May 17 17:18:33.369: INFO: Checking APIGroup: events.k8s.io
May 17 17:18:33.370: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 17 17:18:33.370: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.370: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 17 17:18:33.370: INFO: Checking APIGroup: authentication.k8s.io
May 17 17:18:33.371: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 17 17:18:33.371: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.371: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 17 17:18:33.371: INFO: Checking APIGroup: authorization.k8s.io
May 17 17:18:33.372: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 17 17:18:33.372: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.372: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 17 17:18:33.372: INFO: Checking APIGroup: autoscaling
May 17 17:18:33.372: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 17 17:18:33.372: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 17 17:18:33.372: INFO: autoscaling/v1 matches autoscaling/v1
May 17 17:18:33.372: INFO: Checking APIGroup: batch
May 17 17:18:33.373: INFO: PreferredVersion.GroupVersion: batch/v1
May 17 17:18:33.373: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 17 17:18:33.373: INFO: batch/v1 matches batch/v1
May 17 17:18:33.373: INFO: Checking APIGroup: certificates.k8s.io
May 17 17:18:33.374: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 17 17:18:33.374: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.374: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 17 17:18:33.374: INFO: Checking APIGroup: networking.k8s.io
May 17 17:18:33.376: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 17 17:18:33.376: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.376: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 17 17:18:33.376: INFO: Checking APIGroup: extensions
May 17 17:18:33.377: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 17 17:18:33.377: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 17 17:18:33.377: INFO: extensions/v1beta1 matches extensions/v1beta1
May 17 17:18:33.377: INFO: Checking APIGroup: policy
May 17 17:18:33.378: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 17 17:18:33.378: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 17 17:18:33.378: INFO: policy/v1beta1 matches policy/v1beta1
May 17 17:18:33.378: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 17 17:18:33.378: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 17 17:18:33.378: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.378: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 17 17:18:33.378: INFO: Checking APIGroup: storage.k8s.io
May 17 17:18:33.380: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 17 17:18:33.380: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.380: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 17 17:18:33.380: INFO: Checking APIGroup: admissionregistration.k8s.io
May 17 17:18:33.380: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 17 17:18:33.380: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.380: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 17 17:18:33.380: INFO: Checking APIGroup: apiextensions.k8s.io
May 17 17:18:33.381: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 17 17:18:33.381: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.381: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 17 17:18:33.382: INFO: Checking APIGroup: scheduling.k8s.io
May 17 17:18:33.383: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 17 17:18:33.383: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.383: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 17 17:18:33.383: INFO: Checking APIGroup: coordination.k8s.io
May 17 17:18:33.385: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 17 17:18:33.385: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.385: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 17 17:18:33.385: INFO: Checking APIGroup: node.k8s.io
May 17 17:18:33.386: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 17 17:18:33.387: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.387: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 17 17:18:33.387: INFO: Checking APIGroup: discovery.k8s.io
May 17 17:18:33.389: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 17 17:18:33.389: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.389: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 17 17:18:33.389: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 17 17:18:33.390: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 17 17:18:33.390: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.390: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 17 17:18:33.390: INFO: Checking APIGroup: ceph.rook.io
May 17 17:18:33.392: INFO: PreferredVersion.GroupVersion: ceph.rook.io/v1
May 17 17:18:33.392: INFO: Versions found [{ceph.rook.io/v1 v1}]
May 17 17:18:33.392: INFO: ceph.rook.io/v1 matches ceph.rook.io/v1
May 17 17:18:33.392: INFO: Checking APIGroup: crd.projectcalico.org
May 17 17:18:33.393: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 17 17:18:33.393: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 17 17:18:33.393: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May 17 17:18:33.393: INFO: Checking APIGroup: monitoring.coreos.com
May 17 17:18:33.394: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
May 17 17:18:33.394: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
May 17 17:18:33.394: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
May 17 17:18:33.394: INFO: Checking APIGroup: operator.tigera.io
May 17 17:18:33.397: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
May 17 17:18:33.397: INFO: Versions found [{operator.tigera.io/v1 v1}]
May 17 17:18:33.397: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
May 17 17:18:33.397: INFO: Checking APIGroup: objectbucket.io
May 17 17:18:33.398: INFO: PreferredVersion.GroupVersion: objectbucket.io/v1alpha1
May 17 17:18:33.398: INFO: Versions found [{objectbucket.io/v1alpha1 v1alpha1}]
May 17 17:18:33.398: INFO: objectbucket.io/v1alpha1 matches objectbucket.io/v1alpha1
May 17 17:18:33.399: INFO: Checking APIGroup: rook.io
May 17 17:18:33.403: INFO: PreferredVersion.GroupVersion: rook.io/v1alpha2
May 17 17:18:33.403: INFO: Versions found [{rook.io/v1alpha2 v1alpha2}]
May 17 17:18:33.403: INFO: rook.io/v1alpha2 matches rook.io/v1alpha2
May 17 17:18:33.403: INFO: Checking APIGroup: metrics.k8s.io
May 17 17:18:33.404: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 17 17:18:33.404: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 17 17:18:33.404: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:18:33.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9947" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":39,"skipped":610,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:18:33.416: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:18:33.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2" in namespace "downward-api-9844" to be "Succeeded or Failed"
May 17 17:18:33.460: INFO: Pod "downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.640272ms
May 17 17:18:35.465: INFO: Pod "downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0094633s
May 17 17:18:37.472: INFO: Pod "downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016244683s
STEP: Saw pod success
May 17 17:18:37.472: INFO: Pod "downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2" satisfied condition "Succeeded or Failed"
May 17 17:18:37.475: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2 container client-container: <nil>
STEP: delete the pod
May 17 17:18:37.521: INFO: Waiting for pod downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2 to disappear
May 17 17:18:37.526: INFO: Pod downwardapi-volume-790ac1be-6b97-4899-adee-668defc409e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:18:37.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9844" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:18:37.550: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:18:37.980: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:18:40.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:18:51.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4496" for this suite.
STEP: Destroying namespace "webhook-4496-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.667 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":41,"skipped":644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:18:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:18:51.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8925" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":42,"skipped":672,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:18:51.335: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 17 17:18:59.438: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 17:18:59.441: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 17:19:01.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 17:19:01.445: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 17:19:03.441: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 17:19:03.445: INFO: Pod pod-with-prestop-exec-hook still exists
May 17 17:19:05.443: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 17 17:19:05.451: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2002" for this suite.

• [SLOW TEST:14.132 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:05.470: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 17 17:19:11.573: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:11.580: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:13.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:13.586: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:15.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:15.585: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:17.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:17.586: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:19.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:19.586: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:21.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:21.587: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:23.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:23.587: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:25.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:25.585: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:27.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:27.588: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:29.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:29.593: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:31.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:31.593: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:33.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:33.584: INFO: Pod pod-with-poststart-exec-hook still exists
May 17 17:19:35.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 17 17:19:35.584: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:35.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4009" for this suite.

• [SLOW TEST:30.121 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":713,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:35.591: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-2c4f0f1e-9f28-4b08-836e-80ad7c9e5d66
STEP: Creating a pod to test consume configMaps
May 17 17:19:35.639: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017" in namespace "projected-8280" to be "Succeeded or Failed"
May 17 17:19:35.649: INFO: Pod "pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094672ms
May 17 17:19:37.654: INFO: Pod "pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017": Phase="Running", Reason="", readiness=true. Elapsed: 2.015694884s
May 17 17:19:39.659: INFO: Pod "pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020256254s
STEP: Saw pod success
May 17 17:19:39.659: INFO: Pod "pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017" satisfied condition "Succeeded or Failed"
May 17 17:19:39.661: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:19:39.678: INFO: Waiting for pod pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017 to disappear
May 17 17:19:39.682: INFO: Pod pod-projected-configmaps-a991214b-560d-48d4-a1c4-45fe49049017 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:39.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8280" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:39.691: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 17 17:19:39.735: INFO: Waiting up to 5m0s for pod "pod-1bb78843-7ee9-4516-ac89-d76c2cdea929" in namespace "emptydir-3949" to be "Succeeded or Failed"
May 17 17:19:39.757: INFO: Pod "pod-1bb78843-7ee9-4516-ac89-d76c2cdea929": Phase="Pending", Reason="", readiness=false. Elapsed: 21.45421ms
May 17 17:19:41.761: INFO: Pod "pod-1bb78843-7ee9-4516-ac89-d76c2cdea929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025471732s
May 17 17:19:43.765: INFO: Pod "pod-1bb78843-7ee9-4516-ac89-d76c2cdea929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02955708s
STEP: Saw pod success
May 17 17:19:43.765: INFO: Pod "pod-1bb78843-7ee9-4516-ac89-d76c2cdea929" satisfied condition "Succeeded or Failed"
May 17 17:19:43.767: INFO: Trying to get logs from node 20test-worker-3 pod pod-1bb78843-7ee9-4516-ac89-d76c2cdea929 container test-container: <nil>
STEP: delete the pod
May 17 17:19:43.787: INFO: Waiting for pod pod-1bb78843-7ee9-4516-ac89-d76c2cdea929 to disappear
May 17 17:19:43.790: INFO: Pod pod-1bb78843-7ee9-4516-ac89-d76c2cdea929 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3949" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":774,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:43.799: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 17 17:19:43.846: INFO: Waiting up to 5m0s for pod "pod-ef240736-b7ad-41a8-b824-7ec8c7646869" in namespace "emptydir-1842" to be "Succeeded or Failed"
May 17 17:19:43.851: INFO: Pod "pod-ef240736-b7ad-41a8-b824-7ec8c7646869": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74251ms
May 17 17:19:45.855: INFO: Pod "pod-ef240736-b7ad-41a8-b824-7ec8c7646869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008811775s
May 17 17:19:47.859: INFO: Pod "pod-ef240736-b7ad-41a8-b824-7ec8c7646869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013078908s
STEP: Saw pod success
May 17 17:19:47.859: INFO: Pod "pod-ef240736-b7ad-41a8-b824-7ec8c7646869" satisfied condition "Succeeded or Failed"
May 17 17:19:47.861: INFO: Trying to get logs from node 20test-worker-3 pod pod-ef240736-b7ad-41a8-b824-7ec8c7646869 container test-container: <nil>
STEP: delete the pod
May 17 17:19:47.877: INFO: Waiting for pod pod-ef240736-b7ad-41a8-b824-7ec8c7646869 to disappear
May 17 17:19:47.879: INFO: Pod pod-ef240736-b7ad-41a8-b824-7ec8c7646869 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:47.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1842" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":780,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:47.888: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:19:47.940: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-bf6d7737-d6c3-482e-a6df-3661813e0d3d" in namespace "security-context-test-509" to be "Succeeded or Failed"
May 17 17:19:47.954: INFO: Pod "alpine-nnp-false-bf6d7737-d6c3-482e-a6df-3661813e0d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.869472ms
May 17 17:19:49.958: INFO: Pod "alpine-nnp-false-bf6d7737-d6c3-482e-a6df-3661813e0d3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018139558s
May 17 17:19:51.966: INFO: Pod "alpine-nnp-false-bf6d7737-d6c3-482e-a6df-3661813e0d3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026272728s
May 17 17:19:51.966: INFO: Pod "alpine-nnp-false-bf6d7737-d6c3-482e-a6df-3661813e0d3d" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:19:51.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-509" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:19:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:19:52.033: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Pending, waiting for it to be Running (with Ready = true)
May 17 17:19:54.037: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:19:56.036: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:19:58.038: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:00.036: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:02.036: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:04.038: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:06.037: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:08.036: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:10.037: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:12.040: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = false)
May 17 17:20:14.038: INFO: The status of Pod test-webserver-b10f17ea-4467-484b-9a17-91e9c29df362 is Running (Ready = true)
May 17 17:20:14.041: INFO: Container started at 2021-05-17 17:19:53 +0000 UTC, pod became ready at 2021-05-17 17:20:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:20:14.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4649" for this suite.

• [SLOW TEST:22.071 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":809,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:20:14.053: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-4854328f-74da-4ec5-8c9e-d5f8c3e636f3
STEP: Creating a pod to test consume secrets
May 17 17:20:14.114: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29" in namespace "projected-9091" to be "Succeeded or Failed"
May 17 17:20:14.123: INFO: Pod "pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.571338ms
May 17 17:20:16.128: INFO: Pod "pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013404988s
STEP: Saw pod success
May 17 17:20:16.128: INFO: Pod "pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29" satisfied condition "Succeeded or Failed"
May 17 17:20:16.131: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 17 17:20:16.154: INFO: Waiting for pod pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29 to disappear
May 17 17:20:16.160: INFO: Pod pod-projected-secrets-57cfba47-621d-4e98-9ba1-a78a34ae6d29 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:20:16.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9091" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":50,"skipped":816,"failed":0}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:20:16.171: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:20:16.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5661" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":51,"skipped":826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:20:16.291: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:20:23.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1516" for this suite.

• [SLOW TEST:7.673 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":52,"skipped":848,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:20:23.966: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:20:26.089: INFO: DNS probes using dns-test-77633b63-2198-4a03-a258-e3b29fb5a492 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:20:28.157: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:28.160: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:28.160: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:33.164: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:33.167: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:33.167: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:38.164: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:38.167: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:38.167: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:43.163: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:43.166: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:43.166: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:48.165: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:48.167: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:48.167: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:53.163: INFO: File wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:53.166: INFO: File jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local from pod  dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e contains 'foo.example.com.
' instead of 'bar.example.com.'
May 17 17:20:53.166: INFO: Lookups using dns-2072/dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e failed for: [wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local]

May 17 17:20:58.166: INFO: DNS probes using dns-test-f84975a3-bd53-48e8-81e0-8391b086f34e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2072.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2072.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:21:00.244: INFO: DNS probes using dns-test-57e68851-fccf-4390-a29f-a16411e664da succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:21:00.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2072" for this suite.

• [SLOW TEST:36.322 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":53,"skipped":849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:21:00.300: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 17 17:21:00.356: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 17:21:00.367: INFO: Waiting for terminating namespaces to be deleted...
May 17 17:21:00.370: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-1 before test
May 17 17:21:00.411: INFO: calico-node-fjsxq from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:21:00.411: INFO: calico-typha-954b59468-864hr from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:21:00.411: INFO: kube-proxy-d7hzd from kube-system started at 2021-05-17 16:45:38 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:21:00.411: INFO: fluent-bit-vm2vm from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:21:00.411: INFO: alertmanager-main-0 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:21:00.411: INFO: grafana-f8cd57fcf-p45c2 from monitoring started at 2021-05-17 16:46:48 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container grafana ready: true, restart count 0
May 17 17:21:00.411: INFO: kube-state-metrics-587bfd4f97-6qs5t from monitoring started at 2021-05-17 16:46:48 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 17 17:21:00.411: INFO: node-exporter-5dkbc from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:21:00.411: INFO: prometheus-operator-7649c7454f-5xts8 from monitoring started at 2021-05-17 16:46:39 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container prometheus-operator ready: true, restart count 0
May 17 17:21:00.411: INFO: csi-cephfsplugin-cgwrr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.411: INFO: csi-cephfsplugin-provisioner-8658f67749-mj486 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.411: INFO: csi-rbdplugin-6jrsr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.411: INFO: csi-rbdplugin-provisioner-94f699d86-r49r5 from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (6 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:21:00.411: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.411: INFO: rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:21:00.411: INFO: rook-ceph-mgr-a-5db4bfbc6-fnj5z from rook-ceph started at 2021-05-17 16:49:56 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container mgr ready: true, restart count 0
May 17 17:21:00.411: INFO: rook-ceph-mon-c-545cb7776f-tsblp from rook-ceph started at 2021-05-17 16:49:42 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.411: INFO: 	Container mon ready: true, restart count 0
May 17 17:21:00.411: INFO: rook-ceph-osd-2-96fd479f5-88zbz from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.412: INFO: 	Container osd ready: true, restart count 0
May 17 17:21:00.412: INFO: rook-ceph-osd-prepare-20test-worker-1-qb5xq from rook-ceph started at 2021-05-17 17:14:22 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.412: INFO: 	Container provision ready: false, restart count 0
May 17 17:21:00.412: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:21:00.412: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:21:00.412: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-2 before test
May 17 17:21:00.430: INFO: calico-kube-controllers-5fb9f7f6d8-gpg2w from calico-system started at 2021-05-17 16:46:37 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 17:21:00.430: INFO: calico-node-sxlpp from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:21:00.430: INFO: calico-typha-954b59468-jks4v from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:21:00.430: INFO: coredns-74ff55c5b-hjgsp from kube-system started at 2021-05-17 16:46:42 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container coredns ready: true, restart count 0
May 17 17:21:00.430: INFO: kube-proxy-wmdvj from kube-system started at 2021-05-17 16:45:52 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:21:00.430: INFO: fluent-bit-mjvsf from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:21:00.430: INFO: alertmanager-main-2 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:21:00.430: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:21:00.430: INFO: node-exporter-gcj5f from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:21:00.430: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:21:00.430: INFO: prometheus-adapter-69b8496df6-tns5m from monitoring started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 17 17:21:00.430: INFO: prometheus-k8s-0 from monitoring started at 2021-05-17 16:47:03 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:21:00.430: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:21:00.430: INFO: csi-cephfsplugin-8lpms from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.430: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:21:00.430: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.430: INFO: csi-cephfsplugin-provisioner-8658f67749-vcx69 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.431: INFO: csi-rbdplugin-7rpck from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.431: INFO: csi-rbdplugin-provisioner-94f699d86-xznpd from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.431: INFO: rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:21:00.431: INFO: rook-ceph-mon-b-589bbc6556-w592m from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container mon ready: true, restart count 0
May 17 17:21:00.431: INFO: rook-ceph-operator-5f6ffc46c7-hj2ml from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container rook-ceph-operator ready: true, restart count 0
May 17 17:21:00.431: INFO: rook-ceph-osd-0-7896cc545c-psfck from rook-ceph started at 2021-05-17 16:50:08 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container osd ready: true, restart count 0
May 17 17:21:00.431: INFO: rook-ceph-osd-prepare-20test-worker-2-lhtcq from rook-ceph started at 2021-05-17 17:14:24 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container provision ready: false, restart count 0
May 17 17:21:00.431: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:21:00.431: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:21:00.431: INFO: tigera-operator-657cc89589-qhpft from tigera-operator started at 2021-05-17 16:46:10 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.431: INFO: 	Container tigera-operator ready: true, restart count 0
May 17 17:21:00.431: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-3 before test
May 17 17:21:00.452: INFO: calico-node-rb678 from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:21:00.452: INFO: calico-typha-954b59468-v2z88 from calico-system started at 2021-05-17 16:46:16 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:21:00.452: INFO: kube-proxy-n6xv8 from kube-system started at 2021-05-17 16:45:25 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:21:00.452: INFO: fluent-bit-hg8tt from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:21:00.452: INFO: alertmanager-main-1 from monitoring started at 2021-05-17 17:14:35 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:21:00.452: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:21:00.452: INFO: node-exporter-ccd9w from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.452: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:21:00.452: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:21:00.452: INFO: prometheus-k8s-1 from monitoring started at 2021-05-17 17:14:37 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:21:00.453: INFO: csi-cephfsplugin-9hmwc from rook-ceph started at 2021-05-17 17:14:36 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.453: INFO: csi-rbdplugin-9hr4g from rook-ceph started at 2021-05-17 17:14:38 +0000 UTC (3 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:21:00.453: INFO: rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:21:00.453: INFO: rook-ceph-mon-a-6947dc47f-8wtqd from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container mon ready: true, restart count 0
May 17 17:21:00.453: INFO: rook-ceph-osd-1-6c8669bc9-89l9s from rook-ceph started at 2021-05-17 17:13:51 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container osd ready: true, restart count 0
May 17 17:21:00.453: INFO: rook-ceph-osd-prepare-20test-worker-3-cqpxz from rook-ceph started at 2021-05-17 17:14:26 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container provision ready: false, restart count 0
May 17 17:21:00.453: INFO: sonobuoy from sonobuoy started at 2021-05-17 16:59:52 +0000 UTC (1 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 17:21:00.453: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:21:00.453: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:21:00.453: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167fea5961f357d7], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:21:01.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8710" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":54,"skipped":880,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:21:01.517: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:21:01.559: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 17 17:21:06.564: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 17 17:21:06.570: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 17 17:21:08.581: INFO: Creating deployment "test-rollover-deployment"
May 17 17:21:08.601: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 17 17:21:10.610: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 17 17:21:10.620: INFO: Ensure that both replica sets have 1 created replica
May 17 17:21:10.627: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 17 17:21:10.636: INFO: Updating deployment test-rollover-deployment
May 17 17:21:10.636: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 17 17:21:12.652: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 17 17:21:12.656: INFO: Make sure deployment "test-rollover-deployment" is complete
May 17 17:21:12.660: INFO: all replica sets need to contain the pod-template-hash label
May 17 17:21:12.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868872, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:21:14.670: INFO: all replica sets need to contain the pod-template-hash label
May 17 17:21:14.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868872, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:21:16.670: INFO: all replica sets need to contain the pod-template-hash label
May 17 17:21:16.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868872, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:21:18.668: INFO: all replica sets need to contain the pod-template-hash label
May 17 17:21:18.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868872, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:21:20.666: INFO: all replica sets need to contain the pod-template-hash label
May 17 17:21:20.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868872, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756868868, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:21:22.665: INFO: 
May 17 17:21:22.665: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:21:22.671: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7960  f4331b7c-8964-4f0e-a111-a8ad0288e55b 14510 2 2021-05-17 17:21:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-17 17:21:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d36c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-17 17:21:08 +0000 UTC,LastTransitionTime:2021-05-17 17:21:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-05-17 17:21:22 +0000 UTC,LastTransitionTime:2021-05-17 17:21:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 17:21:22.674: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-7960  08a09e04-9dbc-4f65-9fea-e85b24da1677 14499 2 2021-05-17 17:21:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment f4331b7c-8964-4f0e-a111-a8ad0288e55b 0xc002d370c7 0xc002d370c8}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4331b7c-8964-4f0e-a111-a8ad0288e55b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d37158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 17:21:22.674: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 17 17:21:22.674: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7960  88e17b11-d20f-4fbb-801b-f3c37c809ff8 14509 2 2021-05-17 17:21:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment f4331b7c-8964-4f0e-a111-a8ad0288e55b 0xc002d36fa7 0xc002d36fa8}] []  [{e2e.test Update apps/v1 2021-05-17 17:21:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4331b7c-8964-4f0e-a111-a8ad0288e55b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d37048 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:21:22.675: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7960  8ded75ec-d61c-46ec-b031-32d19aac89d4 14445 2 2021-05-17 17:21:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment f4331b7c-8964-4f0e-a111-a8ad0288e55b 0xc002d371c7 0xc002d371c8}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:21:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f4331b7c-8964-4f0e-a111-a8ad0288e55b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d37258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:21:22.678: INFO: Pod "test-rollover-deployment-668db69979-g2bq2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-g2bq2 test-rollover-deployment-668db69979- deployment-7960  f6ee5b60-466f-402d-b77c-3761c03bcb30 14460 0 2021-05-17 17:21:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:172.20.150.23/32 cni.projectcalico.org/podIPs:172.20.150.23/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 08a09e04-9dbc-4f65-9fea-e85b24da1677 0xc002d379d7 0xc002d379d8}] []  [{kube-controller-manager Update v1 2021-05-17 17:21:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"08a09e04-9dbc-4f65-9fea-e85b24da1677\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:21:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:21:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l7t2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l7t2x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l7t2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:21:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:21:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:21:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:21:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.23,StartTime:2021-05-17 17:21:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:21:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://d87c669905f05c08d450cf9f5d018240a67eb07708e1593203cbc9019b449c5f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:21:22.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7960" for this suite.

• [SLOW TEST:21.169 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":55,"skipped":894,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:21:22.687: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:21:22.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9913" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":56,"skipped":904,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:21:22.790: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 17 17:21:22.847: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:22.851: INFO: Number of nodes with available pods: 0
May 17 17:21:22.851: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:21:23.857: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:23.861: INFO: Number of nodes with available pods: 0
May 17 17:21:23.861: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:21:24.856: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:24.860: INFO: Number of nodes with available pods: 2
May 17 17:21:24.860: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 17:21:25.857: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:25.862: INFO: Number of nodes with available pods: 3
May 17 17:21:25.862: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 17 17:21:25.925: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:25.936: INFO: Number of nodes with available pods: 2
May 17 17:21:25.936: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:21:26.941: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:26.944: INFO: Number of nodes with available pods: 2
May 17 17:21:26.944: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 17:21:27.963: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 17:21:27.971: INFO: Number of nodes with available pods: 3
May 17 17:21:27.971: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8275, will wait for the garbage collector to delete the pods
May 17 17:21:28.073: INFO: Deleting DaemonSet.extensions daemon-set took: 17.435811ms
May 17 17:21:28.774: INFO: Terminating DaemonSet.extensions daemon-set pods took: 701.620516ms
May 17 17:22:02.878: INFO: Number of nodes with available pods: 0
May 17 17:22:02.879: INFO: Number of running nodes: 0, number of available pods: 0
May 17 17:22:02.880: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14819"},"items":null}

May 17 17:22:02.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14819"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:22:02.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8275" for this suite.

• [SLOW TEST:40.106 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":57,"skipped":908,"failed":0}
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:22:02.898: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7442
STEP: creating service affinity-clusterip in namespace services-7442
STEP: creating replication controller affinity-clusterip in namespace services-7442
I0517 17:22:02.953307      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-7442, replica count: 3
I0517 17:22:06.025092      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 17:22:09.025357      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:22:09.031: INFO: Creating new exec pod
May 17 17:22:12.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-7442 exec execpod-affinitygq2c4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 17 17:22:12.257: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 17 17:22:12.257: INFO: stdout: ""
May 17 17:22:12.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-7442 exec execpod-affinitygq2c4 -- /bin/sh -x -c nc -zv -t -w 2 172.30.166.56 80'
May 17 17:22:12.465: INFO: stderr: "+ nc -zv -t -w 2 172.30.166.56 80\nConnection to 172.30.166.56 80 port [tcp/http] succeeded!\n"
May 17 17:22:12.465: INFO: stdout: ""
May 17 17:22:12.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-7442 exec execpod-affinitygq2c4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.166.56:80/ ; done'
May 17 17:22:12.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.166.56:80/\n"
May 17 17:22:12.699: INFO: stdout: "\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc\naffinity-clusterip-c9jxc"
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Received response from host: affinity-clusterip-c9jxc
May 17 17:22:12.699: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7442, will wait for the garbage collector to delete the pods
May 17 17:22:12.802: INFO: Deleting ReplicationController affinity-clusterip took: 25.952455ms
May 17 17:22:13.804: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.002169679s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:22:45.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7442" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.459 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":58,"skipped":908,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:22:45.357: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-940
May 17 17:22:47.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 17 17:22:47.638: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 17 17:22:47.638: INFO: stdout: "iptables"
May 17 17:22:47.638: INFO: proxyMode: iptables
May 17 17:22:47.651: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 17 17:22:47.654: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-940
STEP: creating replication controller affinity-nodeport-timeout in namespace services-940
I0517 17:22:47.701129      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-940, replica count: 3
I0517 17:22:50.774148      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:22:50.781: INFO: Creating new exec pod
May 17 17:22:53.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 17 17:22:53.990: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 17 17:22:53.990: INFO: stdout: ""
May 17 17:22:53.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c nc -zv -t -w 2 172.30.50.59 80'
May 17 17:22:54.160: INFO: stderr: "+ nc -zv -t -w 2 172.30.50.59 80\nConnection to 172.30.50.59 80 port [tcp/http] succeeded!\n"
May 17 17:22:54.160: INFO: stdout: ""
May 17 17:22:54.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.193 32507'
May 17 17:22:54.339: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.193 32507\nConnection to 10.30.20.193 32507 port [tcp/32507] succeeded!\n"
May 17 17:22:54.339: INFO: stdout: ""
May 17 17:22:54.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.161 32507'
May 17 17:22:54.554: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.161 32507\nConnection to 10.30.20.161 32507 port [tcp/32507] succeeded!\n"
May 17 17:22:54.555: INFO: stdout: ""
May 17 17:22:54.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.30.20.193:32507/ ; done'
May 17 17:22:54.829: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n"
May 17 17:22:54.829: INFO: stdout: "\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb\naffinity-nodeport-timeout-m6hfb"
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Received response from host: affinity-nodeport-timeout-m6hfb
May 17 17:22:54.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.30.20.193:32507/'
May 17 17:22:55.007: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n"
May 17 17:22:55.007: INFO: stdout: "affinity-nodeport-timeout-m6hfb"
May 17 17:23:15.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-940 exec execpod-affinitykvg42 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.30.20.193:32507/'
May 17 17:23:15.209: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.30.20.193:32507/\n"
May 17 17:23:15.209: INFO: stdout: "affinity-nodeport-timeout-nkbf8"
May 17 17:23:15.209: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-940, will wait for the garbage collector to delete the pods
May 17 17:23:15.322: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.741286ms
May 17 17:23:15.423: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.159056ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:23:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-940" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:59.828 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":59,"skipped":908,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:23:45.194: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-9515de9d-867e-4664-8756-bddd25e703d3
STEP: Creating a pod to test consume secrets
May 17 17:23:45.292: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61" in namespace "projected-6532" to be "Succeeded or Failed"
May 17 17:23:45.295: INFO: Pod "pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987637ms
May 17 17:23:47.298: INFO: Pod "pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006847928s
STEP: Saw pod success
May 17 17:23:47.298: INFO: Pod "pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61" satisfied condition "Succeeded or Failed"
May 17 17:23:47.301: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 17 17:23:47.338: INFO: Waiting for pod pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61 to disappear
May 17 17:23:47.340: INFO: Pod pod-projected-secrets-ca4ec987-8231-46db-8428-040fdbef6f61 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:23:47.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6532" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":928,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:23:47.353: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 17 17:23:47.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15548 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 17:23:47.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15549 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 17:23:47.410: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15550 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 17 17:23:57.437: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15635 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 17:23:57.437: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15636 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 17:23:57.438: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-386  d859bb02-1632-4496-813d-1f49827d2d6d 15637 0 2021-05-17 17:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-17 17:23:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:23:57.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-386" for this suite.

• [SLOW TEST:10.098 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":61,"skipped":944,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:23:57.460: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:23:58.239: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:24:00.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869038, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869038, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869038, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869038, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:24:03.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:24:03.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1945" for this suite.
STEP: Destroying namespace "webhook-1945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":62,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:24:03.397: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 17 17:24:04.379: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:24:07.399: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:24:07.405: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:24:08.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-737" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.553 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":63,"skipped":998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:24:08.951: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-1112402b-bb54-404a-a90d-5aa4a53c1af7 in namespace container-probe-5094
May 17 17:24:11.024: INFO: Started pod busybox-1112402b-bb54-404a-a90d-5aa4a53c1af7 in namespace container-probe-5094
STEP: checking the pod's current state and verifying that restartCount is present
May 17 17:24:11.026: INFO: Initial restart count of pod busybox-1112402b-bb54-404a-a90d-5aa4a53c1af7 is 0
May 17 17:25:01.176: INFO: Restart count of pod container-probe-5094/busybox-1112402b-bb54-404a-a90d-5aa4a53c1af7 is now 1 (50.149886147s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:25:01.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5094" for this suite.

• [SLOW TEST:52.253 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":1025,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:25:01.208: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7563
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-7563
May 17 17:25:01.262: INFO: Found 0 stateful pods, waiting for 1
May 17 17:25:11.267: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 17:25:11.301: INFO: Deleting all statefulset in ns statefulset-7563
May 17 17:25:11.306: INFO: Scaling statefulset ss to 0
May 17 17:25:41.355: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:25:41.357: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:25:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7563" for this suite.

• [SLOW TEST:40.185 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":65,"skipped":1030,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:25:41.393: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:25:41.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7587" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":66,"skipped":1034,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:25:41.621: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 17 17:25:44.226: INFO: Successfully updated pod "labelsupdate9c4ff337-7d49-451c-b81b-3b67d1318cfb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:25:48.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1069" for this suite.

• [SLOW TEST:6.630 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":1036,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:25:48.257: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:25:49.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:25:51.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869149, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869149, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869149, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869149, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:25:54.318: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:06.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7670" for this suite.
STEP: Destroying namespace "webhook-7670-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:18.232 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":68,"skipped":1043,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:06.492: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 17 17:26:06.557: INFO: Waiting up to 5m0s for pod "var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d" in namespace "var-expansion-9830" to be "Succeeded or Failed"
May 17 17:26:06.569: INFO: Pod "var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.960762ms
May 17 17:26:08.573: INFO: Pod "var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015494298s
STEP: Saw pod success
May 17 17:26:08.573: INFO: Pod "var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d" satisfied condition "Succeeded or Failed"
May 17 17:26:08.577: INFO: Trying to get logs from node 20test-worker-3 pod var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d container dapi-container: <nil>
STEP: delete the pod
May 17 17:26:08.601: INFO: Waiting for pod var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d to disappear
May 17 17:26:08.604: INFO: Pod var-expansion-0b10d2ba-3669-4df5-a047-1351d14f572d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:08.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9830" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":69,"skipped":1048,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:08.615: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9752
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9752
I0517 17:26:08.719488      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9752, replica count: 2
I0517 17:26:11.773702      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:26:11.774: INFO: Creating new exec pod
May 17 17:26:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-9752 exec execpod68qdx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 17 17:26:15.441: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 17:26:15.441: INFO: stdout: ""
May 17 17:26:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-9752 exec execpod68qdx -- /bin/sh -x -c nc -zv -t -w 2 172.30.195.21 80'
May 17 17:26:15.640: INFO: stderr: "+ nc -zv -t -w 2 172.30.195.21 80\nConnection to 172.30.195.21 80 port [tcp/http] succeeded!\n"
May 17 17:26:15.640: INFO: stdout: ""
May 17 17:26:15.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-9752 exec execpod68qdx -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.161 31881'
May 17 17:26:15.817: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.161 31881\nConnection to 10.30.20.161 31881 port [tcp/31881] succeeded!\n"
May 17 17:26:15.817: INFO: stdout: ""
May 17 17:26:15.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-9752 exec execpod68qdx -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.187 31881'
May 17 17:26:15.998: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.187 31881\nConnection to 10.30.20.187 31881 port [tcp/31881] succeeded!\n"
May 17 17:26:15.998: INFO: stdout: ""
May 17 17:26:15.998: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:16.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9752" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.428 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":70,"skipped":1048,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:16.043: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 17 17:26:16.663: INFO: created pod pod-service-account-defaultsa
May 17 17:26:16.663: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 17 17:26:16.674: INFO: created pod pod-service-account-mountsa
May 17 17:26:16.675: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 17 17:26:16.706: INFO: created pod pod-service-account-nomountsa
May 17 17:26:16.706: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 17 17:26:16.717: INFO: created pod pod-service-account-defaultsa-mountspec
May 17 17:26:16.717: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 17 17:26:16.725: INFO: created pod pod-service-account-mountsa-mountspec
May 17 17:26:16.726: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 17 17:26:16.739: INFO: created pod pod-service-account-nomountsa-mountspec
May 17 17:26:16.739: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 17 17:26:16.747: INFO: created pod pod-service-account-defaultsa-nomountspec
May 17 17:26:16.750: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 17 17:26:16.767: INFO: created pod pod-service-account-mountsa-nomountspec
May 17 17:26:16.768: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 17 17:26:16.775: INFO: created pod pod-service-account-nomountsa-nomountspec
May 17 17:26:16.775: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:16.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2925" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":71,"skipped":1053,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:16.859: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 17 17:26:22.006: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:22.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1444" for this suite.

• [SLOW TEST:5.222 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1082,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:22.127: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:26:23.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:26:25.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869182, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:26:27.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869182, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 17:26:29.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869183, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869182, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:26:32.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:32.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4006" for this suite.
STEP: Destroying namespace "webhook-4006-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:10.201 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":73,"skipped":1087,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:32.335: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:26:32.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8" in namespace "downward-api-9932" to be "Succeeded or Failed"
May 17 17:26:32.428: INFO: Pod "downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.045374ms
May 17 17:26:34.433: INFO: Pod "downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02827339s
STEP: Saw pod success
May 17 17:26:34.433: INFO: Pod "downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8" satisfied condition "Succeeded or Failed"
May 17 17:26:34.435: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8 container client-container: <nil>
STEP: delete the pod
May 17 17:26:34.455: INFO: Waiting for pod downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8 to disappear
May 17 17:26:34.457: INFO: Pod downwardapi-volume-500282ce-965b-4a89-8724-54524f11e6f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:34.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9932" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:34.466: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 17 17:26:37.054: INFO: Successfully updated pod "labelsupdate0c71b318-ae2f-4324-bcce-aa412b9d939f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:26:39.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4222" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1157,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:26:39.083: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0517 17:26:49.166494      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 17:27:51.180: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:27:51.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-141" for this suite.

• [SLOW TEST:72.106 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":76,"skipped":1157,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:27:51.191: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:27:59.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1131" for this suite.

• [SLOW TEST:8.045 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":77,"skipped":1164,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:27:59.237: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:27:59.931: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:28:01.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869279, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869279, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869279, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869279, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:28:04.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:28:05.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6325" for this suite.
STEP: Destroying namespace "webhook-6325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":78,"skipped":1179,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:28:05.076: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:28:05.127: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:28:07.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5770" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:28:07.233: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 17 17:28:07.282: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-6175 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:28:07.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6175" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":80,"skipped":1227,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:28:07.373: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 17 17:28:07.421: INFO: Waiting up to 5m0s for pod "test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f" in namespace "svcaccounts-8117" to be "Succeeded or Failed"
May 17 17:28:07.430: INFO: Pod "test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.857132ms
May 17 17:28:09.433: INFO: Pod "test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012312921s
STEP: Saw pod success
May 17 17:28:09.433: INFO: Pod "test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f" satisfied condition "Succeeded or Failed"
May 17 17:28:09.435: INFO: Trying to get logs from node 20test-worker-3 pod test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f container agnhost-container: <nil>
STEP: delete the pod
May 17 17:28:09.468: INFO: Waiting for pod test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f to disappear
May 17 17:28:09.472: INFO: Pod test-pod-9d525b4d-32b7-4f67-a363-8f092c85c31f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:28:09.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8117" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":81,"skipped":1230,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:28:09.482: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1234 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1234;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1234 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1234;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1234.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1234.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1234.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1234.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1234.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1234.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1234.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 92.118.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.118.92_udp@PTR;check="$$(dig +tcp +noall +answer +search 92.118.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.118.92_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1234 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1234;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1234 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1234;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1234.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1234.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1234.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1234.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1234.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1234.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1234.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1234.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1234.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 92.118.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.118.92_udp@PTR;check="$$(dig +tcp +noall +answer +search 92.118.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.118.92_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:28:13.588: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.590: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.594: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.597: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.601: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.603: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.618: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.620: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.623: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.625: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.627: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.631: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.633: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:13.645: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:18.650: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.653: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.655: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.658: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.663: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.666: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.668: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.686: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.689: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.692: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.695: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.697: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.701: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.703: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.705: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:18.716: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:23.649: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.652: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.654: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.659: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.661: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.665: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.668: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.685: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.687: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.690: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.692: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.695: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.703: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.708: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.714: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:23.737: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:28.649: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.651: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.656: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.660: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.662: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.665: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.684: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.688: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.691: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.694: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.696: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.699: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.703: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.706: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:28.724: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:33.649: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.652: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.654: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.656: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.661: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.663: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.665: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.679: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.681: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.683: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.685: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.688: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:33.708: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:38.650: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.654: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.660: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.663: INFO: Unable to read wheezy_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.669: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.671: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.689: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.692: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.699: INFO: Unable to read jessie_udp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.708: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234 from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.711: INFO: Unable to read jessie_udp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.718: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.721: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc from pod dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b: the server could not find the requested resource (get pods dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b)
May 17 17:28:38.747: INFO: Lookups using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1234 wheezy_tcp@dns-test-service.dns-1234 wheezy_udp@dns-test-service.dns-1234.svc wheezy_tcp@dns-test-service.dns-1234.svc wheezy_udp@_http._tcp.dns-test-service.dns-1234.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1234.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1234 jessie_tcp@dns-test-service.dns-1234 jessie_udp@dns-test-service.dns-1234.svc jessie_tcp@dns-test-service.dns-1234.svc jessie_udp@_http._tcp.dns-test-service.dns-1234.svc jessie_tcp@_http._tcp.dns-test-service.dns-1234.svc]

May 17 17:28:43.718: INFO: DNS probes using dns-1234/dns-test-440e4a27-7b06-4fd7-95e1-cd233fbb212b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:28:43.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1234" for this suite.

• [SLOW TEST:34.345 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":82,"skipped":1231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:28:43.831: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-sml6
STEP: Creating a pod to test atomic-volume-subpath
May 17 17:28:43.889: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sml6" in namespace "subpath-3505" to be "Succeeded or Failed"
May 17 17:28:43.896: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.801221ms
May 17 17:28:45.902: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011254788s
May 17 17:28:47.905: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 4.014766529s
May 17 17:28:49.911: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 6.020335559s
May 17 17:28:51.923: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 8.032412947s
May 17 17:28:53.928: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 10.0368829s
May 17 17:28:55.933: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 12.042012429s
May 17 17:28:57.938: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 14.047770501s
May 17 17:28:59.943: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 16.05196274s
May 17 17:29:01.949: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 18.058740688s
May 17 17:29:03.959: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Running", Reason="", readiness=true. Elapsed: 20.067922048s
May 17 17:29:05.968: INFO: Pod "pod-subpath-test-downwardapi-sml6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.077069508s
STEP: Saw pod success
May 17 17:29:05.968: INFO: Pod "pod-subpath-test-downwardapi-sml6" satisfied condition "Succeeded or Failed"
May 17 17:29:05.970: INFO: Trying to get logs from node 20test-worker-3 pod pod-subpath-test-downwardapi-sml6 container test-container-subpath-downwardapi-sml6: <nil>
STEP: delete the pod
May 17 17:29:05.990: INFO: Waiting for pod pod-subpath-test-downwardapi-sml6 to disappear
May 17 17:29:05.996: INFO: Pod pod-subpath-test-downwardapi-sml6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-sml6
May 17 17:29:05.998: INFO: Deleting pod "pod-subpath-test-downwardapi-sml6" in namespace "subpath-3505"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:06.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3505" for this suite.

• [SLOW TEST:22.197 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":83,"skipped":1264,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:29:06.094: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f8e1b115-2990-43a3-bc70-d5a76e8403b9" in namespace "security-context-test-8035" to be "Succeeded or Failed"
May 17 17:29:06.101: INFO: Pod "busybox-privileged-false-f8e1b115-2990-43a3-bc70-d5a76e8403b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.727166ms
May 17 17:29:08.109: INFO: Pod "busybox-privileged-false-f8e1b115-2990-43a3-bc70-d5a76e8403b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014943885s
May 17 17:29:08.109: INFO: Pod "busybox-privileged-false-f8e1b115-2990-43a3-bc70-d5a76e8403b9" satisfied condition "Succeeded or Failed"
May 17 17:29:08.115: INFO: Got logs for pod "busybox-privileged-false-f8e1b115-2990-43a3-bc70-d5a76e8403b9": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:08.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8035" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1275,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:08.135: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 17 17:29:11.218: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:12.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2091" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":85,"skipped":1283,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:12.291: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7194.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7194.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7194.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 17:29:16.414: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.421: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.425: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.430: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.437: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.439: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.442: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.444: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:16.452: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:21.459: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.463: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.466: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.469: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.490: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.492: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.494: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:21.498: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:26.457: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.461: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.466: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.470: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.478: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.487: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.495: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.499: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:26.503: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:31.455: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.467: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.471: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.474: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.480: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.483: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.486: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.488: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:31.493: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:36.457: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.459: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.462: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.464: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.471: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.473: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.475: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.478: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:36.483: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:41.457: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.462: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.465: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.468: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.474: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.477: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.479: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.482: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local from pod dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0: the server could not find the requested resource (get pods dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0)
May 17 17:29:41.487: INFO: Lookups using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7194.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7194.svc.cluster.local jessie_udp@dns-test-service-2.dns-7194.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7194.svc.cluster.local]

May 17 17:29:46.492: INFO: DNS probes using dns-7194/dns-test-002114c7-7a95-4977-b9c5-46117c54a4e0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:46.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7194" for this suite.

• [SLOW TEST:34.307 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":86,"skipped":1299,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:46.599: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:48.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1750" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":87,"skipped":1300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:48.723: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:29:49.349: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:29:51.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869389, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869389, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869389, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869389, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:29:54.402: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:54.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9626" for this suite.
STEP: Destroying namespace "webhook-9626-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":88,"skipped":1329,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:54.507: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:29:54.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-9629 version'
May 17 17:29:54.784: INFO: stderr: ""
May 17 17:29:54.784: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:28:09Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.2\", GitCommit:\"faecb196815e248d3ecfb03c680a4507229c2a56\", GitTreeState:\"clean\", BuildDate:\"2021-01-13T13:20:00Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:29:54.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9629" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":89,"skipped":1344,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:29:54.805: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0517 17:29:55.413794      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 17:30:57.427: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:30:57.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-946" for this suite.

• [SLOW TEST:62.636 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":90,"skipped":1375,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:30:57.444: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:30:57.504: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 17 17:31:03.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-2416 --namespace=crd-publish-openapi-2416 create -f -'
May 17 17:31:04.022: INFO: stderr: ""
May 17 17:31:04.022: INFO: stdout: "e2e-test-crd-publish-openapi-6320-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 17:31:04.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-2416 --namespace=crd-publish-openapi-2416 delete e2e-test-crd-publish-openapi-6320-crds test-cr'
May 17 17:31:04.132: INFO: stderr: ""
May 17 17:31:04.132: INFO: stdout: "e2e-test-crd-publish-openapi-6320-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 17 17:31:04.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-2416 --namespace=crd-publish-openapi-2416 apply -f -'
May 17 17:31:04.444: INFO: stderr: ""
May 17 17:31:04.444: INFO: stdout: "e2e-test-crd-publish-openapi-6320-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 17 17:31:04.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-2416 --namespace=crd-publish-openapi-2416 delete e2e-test-crd-publish-openapi-6320-crds test-cr'
May 17 17:31:04.534: INFO: stderr: ""
May 17 17:31:04.534: INFO: stdout: "e2e-test-crd-publish-openapi-6320-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 17 17:31:04.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-2416 explain e2e-test-crd-publish-openapi-6320-crds'
May 17 17:31:04.826: INFO: stderr: ""
May 17 17:31:04.826: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6320-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:31:10.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2416" for this suite.

• [SLOW TEST:12.788 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":91,"skipped":1384,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:31:10.234: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3506
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3506
STEP: creating replication controller externalsvc in namespace services-3506
I0517 17:31:10.554753      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3506, replica count: 2
I0517 17:31:13.612041      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 17 17:31:13.641: INFO: Creating new exec pod
May 17 17:31:15.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-3506 exec execpodwchfj -- /bin/sh -x -c nslookup clusterip-service.services-3506.svc.cluster.local'
May 17 17:31:15.996: INFO: stderr: "+ nslookup clusterip-service.services-3506.svc.cluster.local\n"
May 17 17:31:15.996: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-3506.svc.cluster.local\tcanonical name = externalsvc.services-3506.svc.cluster.local.\nName:\texternalsvc.services-3506.svc.cluster.local\nAddress: 172.30.90.180\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3506, will wait for the garbage collector to delete the pods
May 17 17:31:16.058: INFO: Deleting ReplicationController externalsvc took: 4.304164ms
May 17 17:31:19.358: INFO: Terminating ReplicationController externalsvc pods took: 3.300683338s
May 17 17:31:25.183: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:31:25.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3506" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:14.987 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":92,"skipped":1385,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:31:25.226: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-f71dab40-1ddc-4bf5-9f4c-ab502fedaf67
STEP: Creating secret with name s-test-opt-upd-d050ae86-eb1c-4900-995d-af02ee8b33c1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f71dab40-1ddc-4bf5-9f4c-ab502fedaf67
STEP: Updating secret s-test-opt-upd-d050ae86-eb1c-4900-995d-af02ee8b33c1
STEP: Creating secret with name s-test-opt-create-61331c0d-e250-4470-bdcd-5c45efd4367a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:31:31.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2214" for this suite.

• [SLOW TEST:6.180 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":93,"skipped":1389,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:31:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:31:31.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5832" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":94,"skipped":1393,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:31:31.477: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-9b9fd9a2-6a14-4327-b63d-39c02ea38671
STEP: Creating a pod to test consume configMaps
May 17 17:31:31.524: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9" in namespace "projected-6793" to be "Succeeded or Failed"
May 17 17:31:31.528: INFO: Pod "pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.647495ms
May 17 17:31:33.533: INFO: Pod "pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009414123s
STEP: Saw pod success
May 17 17:31:33.534: INFO: Pod "pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9" satisfied condition "Succeeded or Failed"
May 17 17:31:33.537: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:31:33.563: INFO: Waiting for pod pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9 to disappear
May 17 17:31:33.566: INFO: Pod pod-projected-configmaps-af317384-f356-49e3-8111-e9e4e17ab5f9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:31:33.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6793" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:31:33.581: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4445
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4445
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4445
May 17 17:31:33.665: INFO: Found 0 stateful pods, waiting for 1
May 17 17:31:43.670: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 17 17:31:43.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:31:43.869: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:31:43.869: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:31:43.869: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:31:43.873: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 17:31:53.878: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:31:53.878: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:31:53.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999719s
May 17 17:31:54.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993677456s
May 17 17:31:55.902: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990368118s
May 17 17:31:56.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981718504s
May 17 17:31:57.912: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976462208s
May 17 17:31:58.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973806799s
May 17 17:31:59.927: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967268419s
May 17 17:32:00.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.9589455s
May 17 17:32:01.938: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953559869s
May 17 17:32:02.942: INFO: Verifying statefulset ss doesn't scale past 1 for another 948.406846ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4445
May 17 17:32:03.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:32:04.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 17:32:04.132: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:32:04.132: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:32:04.137: INFO: Found 1 stateful pods, waiting for 3
May 17 17:32:14.142: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 17:32:14.142: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 17:32:14.142: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 17 17:32:14.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:32:14.329: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:32:14.329: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:32:14.329: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:32:14.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:32:14.514: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:32:14.514: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:32:14.514: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:32:14.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:32:14.704: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:32:14.704: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:32:14.704: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:32:14.704: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:32:14.707: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 17 17:32:24.715: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:32:24.715: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:32:24.715: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:32:24.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999379s
May 17 17:32:25.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990449897s
May 17 17:32:26.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982949276s
May 17 17:32:27.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978290172s
May 17 17:32:28.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972255195s
May 17 17:32:29.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.966659541s
May 17 17:32:30.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961573078s
May 17 17:32:31.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955940525s
May 17 17:32:32.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9496933s
May 17 17:32:33.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 944.963104ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4445
May 17 17:32:34.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:32:34.973: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 17:32:34.973: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:32:34.973: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:32:34.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:32:35.171: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 17:32:35.171: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:32:35.171: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:32:35.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-4445 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:32:35.334: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 17:32:35.334: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:32:35.334: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:32:35.334: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 17:34:05.354: INFO: Deleting all statefulset in ns statefulset-4445
May 17 17:34:05.358: INFO: Scaling statefulset ss to 0
May 17 17:34:05.367: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:34:05.370: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:05.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4445" for this suite.

• [SLOW TEST:151.829 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":96,"skipped":1428,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:05.417: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:34:05.524: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"9ac5b248-4175-4f05-a393-6dcb94d2c9ba", Controller:(*bool)(0xc00839d77a), BlockOwnerDeletion:(*bool)(0xc00839d77b)}}
May 17 17:34:05.530: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"272c2979-aa39-4e1d-ac13-e0953fb2bca2", Controller:(*bool)(0xc00839d9c6), BlockOwnerDeletion:(*bool)(0xc00839d9c7)}}
May 17 17:34:05.540: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"58cd5214-3278-492e-bf48-9bbdcbae251f", Controller:(*bool)(0xc002af3716), BlockOwnerDeletion:(*bool)(0xc002af3717)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:10.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-867" for this suite.

• [SLOW TEST:5.150 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":97,"skipped":1439,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:10.602: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:34:10.650: INFO: Creating deployment "webserver-deployment"
May 17 17:34:10.656: INFO: Waiting for observed generation 1
May 17 17:34:12.662: INFO: Waiting for all required pods to come up
May 17 17:34:12.674: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 17 17:34:18.711: INFO: Waiting for deployment "webserver-deployment" to complete
May 17 17:34:18.716: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 17 17:34:18.725: INFO: Updating deployment webserver-deployment
May 17 17:34:18.725: INFO: Waiting for observed generation 2
May 17 17:34:20.786: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 17 17:34:20.789: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 17 17:34:20.792: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 17:34:20.797: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 17 17:34:20.797: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 17 17:34:20.799: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 17 17:34:20.802: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 17 17:34:20.803: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 17 17:34:20.811: INFO: Updating deployment webserver-deployment
May 17 17:34:20.811: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 17 17:34:20.852: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 17 17:34:20.949: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:34:23.016: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-400  818f8a30-648a-4811-927a-69d395b72980 20591 3 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009025bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-17 17:34:20 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-17 17:34:21 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 17 17:34:23.029: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-400  b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 20585 3 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 818f8a30-648a-4811-927a-69d395b72980 0xc009025fd7 0xc009025fd8}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"818f8a30-648a-4811-927a-69d395b72980\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009044058 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:34:23.029: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 17 17:34:23.030: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-400  37087da5-8817-416c-a6c1-820143918c00 20584 3 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 818f8a30-648a-4811-927a-69d395b72980 0xc0090440c7 0xc0090440c8}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"818f8a30-648a-4811-927a-69d395b72980\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009044138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 17 17:34:23.055: INFO: Pod "webserver-deployment-795d758f88-46ljp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-46ljp webserver-deployment-795d758f88- deployment-400  7c5e53dc-a52a-4244-af15-172d541c99c0 20506 0 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.49/32 cni.projectcalico.org/podIPs:172.20.150.49/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090445e7 0xc0090445e8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.063: INFO: Pod "webserver-deployment-795d758f88-5sdhh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5sdhh webserver-deployment-795d758f88- deployment-400  e696355f-6ef6-48d5-bb5b-cb5bb2682025 20636 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.41/32 cni.projectcalico.org/podIPs:172.20.150.41/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090447c7 0xc0090447c8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.064: INFO: Pod "webserver-deployment-795d758f88-6nvjn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6nvjn webserver-deployment-795d758f88- deployment-400  486d3ca1-02f4-40dd-9c63-ec1d8b3f605b 20706 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.46/32 cni.projectcalico.org/podIPs:172.20.150.46/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090449a7 0xc0090449a8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.064: INFO: Pod "webserver-deployment-795d758f88-7bm2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7bm2g webserver-deployment-795d758f88- deployment-400  deb26229-25c1-4b8b-8170-37a252628514 20664 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.50/32 cni.projectcalico.org/podIPs:172.20.150.50/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009044b87 0xc009044b88}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.065: INFO: Pod "webserver-deployment-795d758f88-8bdft" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8bdft webserver-deployment-795d758f88- deployment-400  873a7e5c-a249-4385-a9f4-8da101c70899 20657 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.227.228/32 cni.projectcalico.org/podIPs:172.20.227.228/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009044d47 0xc009044d48}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.193,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.065: INFO: Pod "webserver-deployment-795d758f88-kdbrr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kdbrr webserver-deployment-795d758f88- deployment-400  b0a69a6a-aebe-440b-99ea-138d3483116d 20496 0 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.34/32 cni.projectcalico.org/podIPs:172.20.150.34/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009044f27 0xc009044f28}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.066: INFO: Pod "webserver-deployment-795d758f88-kg5fm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kg5fm webserver-deployment-795d758f88- deployment-400  e748860e-fb52-4e5f-88bf-f3e826aac13a 20722 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.39/32 cni.projectcalico.org/podIPs:172.20.150.39/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009045107 0xc009045108}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.067: INFO: Pod "webserver-deployment-795d758f88-l4rnr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l4rnr webserver-deployment-795d758f88- deployment-400  d4d08a54-c2b6-42ad-b1e6-892881cc570b 20511 0 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.40/32 cni.projectcalico.org/podIPs:172.20.150.40/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090452f7 0xc0090452f8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.067: INFO: Pod "webserver-deployment-795d758f88-mhpcm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mhpcm webserver-deployment-795d758f88- deployment-400  e1df266f-f3f2-41f4-b1e9-99fad8bd1a27 20690 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.227.230/32 cni.projectcalico.org/podIPs:172.20.227.230/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090454c7 0xc0090454c8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.193,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.068: INFO: Pod "webserver-deployment-795d758f88-nvzgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nvzgj webserver-deployment-795d758f88- deployment-400  c5df21af-f1e6-48b2-9dfd-883b3b7ae60a 20499 0 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.35/32 cni.projectcalico.org/podIPs:172.20.150.35/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc0090456b7 0xc0090456b8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.070: INFO: Pod "webserver-deployment-795d758f88-qzsp4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qzsp4 webserver-deployment-795d758f88- deployment-400  c2a49d8d-05b5-4391-87a2-037d5dcc2519 20700 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.227.231/32 cni.projectcalico.org/podIPs:172.20.227.231/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009045897 0xc009045898}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.193,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.070: INFO: Pod "webserver-deployment-795d758f88-rnbc8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rnbc8 webserver-deployment-795d758f88- deployment-400  3b1a1168-7ebd-4bbe-8d50-f7560d0a3ded 20670 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009045a77 0xc009045a78}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.072: INFO: Pod "webserver-deployment-795d758f88-zjsrc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zjsrc webserver-deployment-795d758f88- deployment-400  ed8291c1-128b-4d7a-8db3-1eb6ea55e5bb 20502 0 2021-05-17 17:34:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.20.150.29/32 cni.projectcalico.org/podIPs:172.20.150.29/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 b5de1caf-5234-40ad-9dbf-1ab0eb7797f1 0xc009045c57 0xc009045c58}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5de1caf-5234-40ad-9dbf-1ab0eb7797f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.072: INFO: Pod "webserver-deployment-dd94f59b7-67l9l" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-67l9l webserver-deployment-dd94f59b7- deployment-400  572c6b0b-361a-4493-9c5d-9d5da85d54d6 20667 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.227.229/32 cni.projectcalico.org/podIPs:172.20.227.229/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc009045e37 0xc009045e38}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.193,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.072: INFO: Pod "webserver-deployment-dd94f59b7-6swkn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6swkn webserver-deployment-dd94f59b7- deployment-400  6d650eee-d358-4c35-ba44-2ff8db0fc8ed 20666 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.206.215/32 cni.projectcalico.org/podIPs:172.20.206.215/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc009045fd7 0xc009045fd8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.073: INFO: Pod "webserver-deployment-dd94f59b7-7q8gn" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7q8gn webserver-deployment-dd94f59b7- deployment-400  5da0f4fb-32f6-4681-b8b9-e1f231110c6f 20422 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.2/32 cni.projectcalico.org/podIPs:172.20.150.2/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908a197 0xc00908a198}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.2,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://19688d49598cd8797fc95d098f2610372637441812d9fdeb34c0f5b702cf485e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.074: INFO: Pod "webserver-deployment-dd94f59b7-8n45n" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8n45n webserver-deployment-dd94f59b7- deployment-400  eb795290-3651-44f3-b4d8-b2d177b9821d 20408 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.4/32 cni.projectcalico.org/podIPs:172.20.150.4/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908a407 0xc00908a408}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.4,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ad4c5d06d8435402a649afab393d03a0485ee487e5bd67b7405c11dd8e0b4097,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.076: INFO: Pod "webserver-deployment-dd94f59b7-9v87q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9v87q webserver-deployment-dd94f59b7- deployment-400  a760e94d-9fed-44dc-9de5-eced53f51cdb 20626 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908a5c7 0xc00908a5c8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.076: INFO: Pod "webserver-deployment-dd94f59b7-bt2xl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bt2xl webserver-deployment-dd94f59b7- deployment-400  a3141bfb-4527-4563-8a60-52dd3c7e7eeb 20414 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.9/32 cni.projectcalico.org/podIPs:172.20.150.9/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908a787 0xc00908a788}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.9,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b429e89846b043e2bc52d94c442793c1101145327fb472ad128cd33fc894a988,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.077: INFO: Pod "webserver-deployment-dd94f59b7-bvpjt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bvpjt webserver-deployment-dd94f59b7- deployment-400  d94006a9-2841-4eb5-9ca2-afda6494ab28 20419 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.3/32 cni.projectcalico.org/podIPs:172.20.150.3/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908a977 0xc00908a978}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.3,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://64d901c1b46131e3c731c9b4d5b806f1269ab0a04d57f95297fa2da721737043,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.077: INFO: Pod "webserver-deployment-dd94f59b7-cwvkf" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-cwvkf webserver-deployment-dd94f59b7- deployment-400  d4552c23-dcda-496f-a74e-7e3b04ece8cb 20386 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.11/32 cni.projectcalico.org/podIPs:172.20.150.11/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908ab67 0xc00908ab68}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.11,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8a36b6b7b062359b64220e7b12a2f0b2489457b81d994ad997949d56cd7a8aa4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.078: INFO: Pod "webserver-deployment-dd94f59b7-d4gs5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d4gs5 webserver-deployment-dd94f59b7- deployment-400  11f7ef13-8232-49f1-a7fd-ead67f9fea97 20691 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.206.216/32 cni.projectcalico.org/podIPs:172.20.206.216/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908ad27 0xc00908ad28}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.078: INFO: Pod "webserver-deployment-dd94f59b7-fzgj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fzgj7 webserver-deployment-dd94f59b7- deployment-400  39119d94-eb7e-4dc1-97ef-0ecf860f335b 20648 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908aec7 0xc00908aec8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.079: INFO: Pod "webserver-deployment-dd94f59b7-gc22c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gc22c webserver-deployment-dd94f59b7- deployment-400  709027ef-3781-4a33-ab6a-b3c9b8f81621 20638 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.206.213/32 cni.projectcalico.org/podIPs:172.20.206.213/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b057 0xc00908b058}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.080: INFO: Pod "webserver-deployment-dd94f59b7-j845c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j845c webserver-deployment-dd94f59b7- deployment-400  5ce8f3c9-16e2-44a7-be18-1852c2e12cf8 20698 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.44/32 cni.projectcalico.org/podIPs:172.20.150.44/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b217 0xc00908b218}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.081: INFO: Pod "webserver-deployment-dd94f59b7-lmn2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lmn2g webserver-deployment-dd94f59b7- deployment-400  bbc623f4-69b7-453c-b4a3-4c6f18e704fb 20652 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.42/32 cni.projectcalico.org/podIPs:172.20.150.42/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b3d7 0xc00908b3d8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.082: INFO: Pod "webserver-deployment-dd94f59b7-lrb9g" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lrb9g webserver-deployment-dd94f59b7- deployment-400  1d8ab940-369c-4b5b-baf5-b88ced75fc3c 20359 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.12/32 cni.projectcalico.org/podIPs:172.20.150.12/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b597 0xc00908b598}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.12,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://2bbdc9aabc50517a325521849fe5761e515550fc592e0ebe1a15ffe2bf69a5e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.082: INFO: Pod "webserver-deployment-dd94f59b7-nzst7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-nzst7 webserver-deployment-dd94f59b7- deployment-400  7f067a51-07ab-4673-bc23-3fbd79aa8acd 20634 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.227.227/32 cni.projectcalico.org/podIPs:172.20.227.227/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b757 0xc00908b758}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.193,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.083: INFO: Pod "webserver-deployment-dd94f59b7-q7djs" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-q7djs webserver-deployment-dd94f59b7- deployment-400  520c686e-ddef-4eda-a83e-3a27156e0e67 20379 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.8/32 cni.projectcalico.org/podIPs:172.20.150.8/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908b917 0xc00908b918}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.8,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://52c3e734aad4e2004ba24144181921ffed23995da9c208c57cf91fc01e760490,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.084: INFO: Pod "webserver-deployment-dd94f59b7-qtbxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qtbxc webserver-deployment-dd94f59b7- deployment-400  e4d9d84c-12d1-4ece-bba8-38a4268d3aeb 20542 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908bad7 0xc00908bad8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.084: INFO: Pod "webserver-deployment-dd94f59b7-scjd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-scjd6 webserver-deployment-dd94f59b7- deployment-400  a567544d-9875-4078-b323-1460ce5bf48d 20660 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.206.214/32 cni.projectcalico.org/podIPs:172.20.206.214/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908bc67 0xc00908bc68}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.085: INFO: Pod "webserver-deployment-dd94f59b7-vx7h9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vx7h9 webserver-deployment-dd94f59b7- deployment-400  495924e7-eff0-435d-a51b-7461797c3053 20353 0 2021-05-17 17:34:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.150.5/32 cni.projectcalico.org/podIPs:172.20.150.5/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908be27 0xc00908be28}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.5,StartTime:2021-05-17 17:34:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ca1b7861fc950b855a21c184adc648ce8dfc5d5a2b69c5bd699991a743a83241,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 17:34:23.085: INFO: Pod "webserver-deployment-dd94f59b7-z955r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z955r webserver-deployment-dd94f59b7- deployment-400  7d54381f-fca5-4330-9f5e-05f9e0f02da5 20723 0 2021-05-17 17:34:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.20.206.217/32 cni.projectcalico.org/podIPs:172.20.206.217/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 37087da5-8817-416c-a6c1-820143918c00 0xc00908bfe7 0xc00908bfe8}] []  [{kube-controller-manager Update v1 2021-05-17 17:34:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37087da5-8817-416c-a6c1-820143918c00\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:34:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-05-17 17:34:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qx85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qx85c,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qx85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.161,PodIP:,StartTime:2021-05-17 17:34:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:23.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-400" for this suite.

• [SLOW TEST:12.538 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":98,"skipped":1464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:23.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9262" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":99,"skipped":1490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:23.286: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 17 17:34:27.382: INFO: &Pod{ObjectMeta:{send-events-9038142f-3b1e-428f-968d-8eddf8e12007  events-9532  6aa0bda4-19d9-4d57-858c-e1687fc39a14 20827 0 2021-05-17 17:34:23 +0000 UTC <nil> <nil> map[name:foo time:340117167] map[cni.projectcalico.org/podIP:172.20.150.45/32 cni.projectcalico.org/podIPs:172.20.150.45/32] [] []  [{e2e.test Update v1 2021-05-17 17:34:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:34:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:34:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nsn65,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nsn65,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nsn65,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:34:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.45,StartTime:2021-05-17 17:34:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:34:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://cd4bf013dcc6676166106fff61cbd64a2e88c5774e4829b99b1f27c4f84895cb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 17 17:34:29.389: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 17 17:34:31.411: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9532" for this suite.

• [SLOW TEST:8.187 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":100,"skipped":1540,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:31.473: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 17 17:34:31.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 17 17:34:31.719: INFO: stderr: ""
May 17 17:34:31.719: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 17 17:34:31.720: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 17 17:34:31.720: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-961" to be "running and ready, or succeeded"
May 17 17:34:31.726: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474784ms
May 17 17:34:33.733: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013102646s
May 17 17:34:35.736: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.016415379s
May 17 17:34:35.736: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 17 17:34:35.736: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 17 17:34:35.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator'
May 17 17:34:35.869: INFO: stderr: ""
May 17 17:34:35.869: INFO: stdout: "I0517 17:34:33.334053       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/69ll 300\nI0517 17:34:33.534227       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/szm 289\nI0517 17:34:33.736722       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/8n8z 274\nI0517 17:34:33.934236       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/22m8 435\nI0517 17:34:34.134213       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dmf 551\nI0517 17:34:34.334142       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/cl9 396\nI0517 17:34:34.534144       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/w5l 594\nI0517 17:34:34.734195       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/49hw 387\nI0517 17:34:34.934164       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/lfsw 599\nI0517 17:34:35.134192       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/kl7 438\nI0517 17:34:35.334172       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/k2cx 574\nI0517 17:34:35.575091       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/9p6t 333\nI0517 17:34:35.734140       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/k4k 287\n"
STEP: limiting log lines
May 17 17:34:35.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator --tail=1'
May 17 17:34:35.979: INFO: stderr: ""
May 17 17:34:35.979: INFO: stdout: "I0517 17:34:35.934124       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/qvt 414\n"
May 17 17:34:35.979: INFO: got output "I0517 17:34:35.934124       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/qvt 414\n"
STEP: limiting log bytes
May 17 17:34:35.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator --limit-bytes=1'
May 17 17:34:36.100: INFO: stderr: ""
May 17 17:34:36.100: INFO: stdout: "I"
May 17 17:34:36.100: INFO: got output "I"
STEP: exposing timestamps
May 17 17:34:36.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator --tail=1 --timestamps'
May 17 17:34:36.198: INFO: stderr: ""
May 17 17:34:36.198: INFO: stdout: "2021-05-17T17:34:36.134458620Z I0517 17:34:36.134339       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tmlf 237\n"
May 17 17:34:36.198: INFO: got output "2021-05-17T17:34:36.134458620Z I0517 17:34:36.134339       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tmlf 237\n"
STEP: restricting to a time range
May 17 17:34:38.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator --since=1s'
May 17 17:34:38.798: INFO: stderr: ""
May 17 17:34:38.798: INFO: stdout: "I0517 17:34:37.934224       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/kdnz 435\nI0517 17:34:38.134209       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/2jj 493\nI0517 17:34:38.334138       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/wq28 453\nI0517 17:34:38.534181       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/rz6v 254\nI0517 17:34:38.734173       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/w8v2 269\n"
May 17 17:34:38.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 logs logs-generator logs-generator --since=24h'
May 17 17:34:38.902: INFO: stderr: ""
May 17 17:34:38.902: INFO: stdout: "I0517 17:34:33.334053       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/69ll 300\nI0517 17:34:33.534227       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/szm 289\nI0517 17:34:33.736722       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/8n8z 274\nI0517 17:34:33.934236       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/22m8 435\nI0517 17:34:34.134213       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/dmf 551\nI0517 17:34:34.334142       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/cl9 396\nI0517 17:34:34.534144       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/w5l 594\nI0517 17:34:34.734195       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/49hw 387\nI0517 17:34:34.934164       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/lfsw 599\nI0517 17:34:35.134192       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/kl7 438\nI0517 17:34:35.334172       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/k2cx 574\nI0517 17:34:35.575091       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/9p6t 333\nI0517 17:34:35.734140       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/k4k 287\nI0517 17:34:35.934124       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/qvt 414\nI0517 17:34:36.134339       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/tmlf 237\nI0517 17:34:36.334260       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/b8br 518\nI0517 17:34:36.534079       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/tp9j 364\nI0517 17:34:36.734177       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/x4c8 411\nI0517 17:34:36.934139       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/kq4g 283\nI0517 17:34:37.134159       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/8wj 339\nI0517 17:34:37.334161       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/sddf 386\nI0517 17:34:37.534146       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/kfqg 576\nI0517 17:34:37.734225       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/c7pn 450\nI0517 17:34:37.934224       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/kdnz 435\nI0517 17:34:38.134209       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/2jj 493\nI0517 17:34:38.334138       1 logs_generator.go:76] 25 GET /api/v1/namespaces/ns/pods/wq28 453\nI0517 17:34:38.534181       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/kube-system/pods/rz6v 254\nI0517 17:34:38.734173       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/w8v2 269\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 17 17:34:38.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-961 delete pod logs-generator'
May 17 17:34:45.106: INFO: stderr: ""
May 17 17:34:45.106: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:45.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-961" for this suite.

• [SLOW TEST:13.641 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":101,"skipped":1554,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:45.115: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:34:45.182: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b" in namespace "downward-api-3958" to be "Succeeded or Failed"
May 17 17:34:45.188: INFO: Pod "downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.365734ms
May 17 17:34:47.190: INFO: Pod "downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007538608s
STEP: Saw pod success
May 17 17:34:47.190: INFO: Pod "downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b" satisfied condition "Succeeded or Failed"
May 17 17:34:47.192: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b container client-container: <nil>
STEP: delete the pod
May 17 17:34:47.210: INFO: Waiting for pod downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b to disappear
May 17 17:34:47.213: INFO: Pod downwardapi-volume-6f4382cb-77e0-4a88-b3a2-70f64e0a279b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:47.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3958" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":102,"skipped":1565,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:47.235: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 17 17:34:47.320: INFO: Waiting up to 5m0s for pod "client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237" in namespace "containers-1900" to be "Succeeded or Failed"
May 17 17:34:47.340: INFO: Pod "client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237": Phase="Pending", Reason="", readiness=false. Elapsed: 19.554439ms
May 17 17:34:49.346: INFO: Pod "client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025654579s
STEP: Saw pod success
May 17 17:34:49.346: INFO: Pod "client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237" satisfied condition "Succeeded or Failed"
May 17 17:34:49.348: INFO: Trying to get logs from node 20test-worker-3 pod client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:34:49.366: INFO: Waiting for pod client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237 to disappear
May 17 17:34:49.372: INFO: Pod client-containers-c9ff3dee-2538-4f93-85ef-4c0def96f237 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:49.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1900" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1579,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 17 17:34:51.981: INFO: Successfully updated pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4"
May 17 17:34:51.981: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4" in namespace "pods-2117" to be "terminated due to deadline exceeded"
May 17 17:34:51.984: INFO: Pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.95187ms
May 17 17:34:53.988: INFO: Pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007248074s
May 17 17:34:55.995: INFO: Pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013618679s
May 17 17:34:55.995: INFO: Pod "pod-update-activedeadlineseconds-abcfba0f-ae71-415c-be5c-89b25e9eadd4" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:34:55.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2117" for this suite.

• [SLOW TEST:6.624 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":104,"skipped":1586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:34:56.014: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:34:56.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:34:58.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869696, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869696, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869696, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756869696, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:35:01.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:02.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5544" for this suite.
STEP: Destroying namespace "webhook-5544-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.340 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":105,"skipped":1614,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:02.358: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:35:02.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 create -f -'
May 17 17:35:02.902: INFO: stderr: ""
May 17 17:35:02.902: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 17 17:35:02.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 create -f -'
May 17 17:35:03.313: INFO: stderr: ""
May 17 17:35:03.313: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 17 17:35:04.316: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:35:04.316: INFO: Found 1 / 1
May 17 17:35:04.316: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 17:35:04.323: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:35:04.323: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 17:35:04.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 describe pod agnhost-primary-zxpxq'
May 17 17:35:04.470: INFO: stderr: ""
May 17 17:35:04.470: INFO: stdout: "Name:         agnhost-primary-zxpxq\nNamespace:    kubectl-1906\nPriority:     0\nNode:         20test-worker-3/10.30.20.187\nStart Time:   Mon, 17 May 2021 17:35:02 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.20.150.52/32\n              cni.projectcalico.org/podIPs: 172.20.150.52/32\nStatus:       Running\nIP:           172.20.150.52\nIPs:\n  IP:           172.20.150.52\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://92bd7ecd807a541d5efc6714bda0a619a89f4763ed43e003e974063d8ca1cd11\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 May 2021 17:35:04 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cj7kw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-cj7kw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-cj7kw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1906/agnhost-primary-zxpxq to 20test-worker-3\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
May 17 17:35:04.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 describe rc agnhost-primary'
May 17 17:35:04.603: INFO: stderr: ""
May 17 17:35:04.603: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1906\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-zxpxq\n"
May 17 17:35:04.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 describe service agnhost-primary'
May 17 17:35:04.724: INFO: stderr: ""
May 17 17:35:04.724: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1906\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.30.210.31\nIPs:               172.30.210.31\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.20.150.52:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 17 17:35:04.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 describe node 20test-master'
May 17 17:35:04.889: INFO: stderr: ""
May 17 17:35:04.889: INFO: stdout: "Name:               20test-master\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=20test-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.30.20.145/22\n                    projectcalico.org/IPv4VXLANTunnelAddr: 172.20.126.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 May 2021 16:41:41 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  20test-master\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 17 May 2021 17:35:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 May 2021 16:46:43 +0000   Mon, 17 May 2021 16:46:43 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 17 May 2021 17:31:01 +0000   Mon, 17 May 2021 16:41:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 17 May 2021 17:31:01 +0000   Mon, 17 May 2021 16:41:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 17 May 2021 17:31:01 +0000   Mon, 17 May 2021 16:41:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 17 May 2021 17:31:01 +0000   Mon, 17 May 2021 16:46:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.30.20.145\n  Hostname:    20test-master\nCapacity:\n  cpu:                2\n  ephemeral-storage:  41219296Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8122300Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  37987703131\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8019900Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3680042af0824a18919c7a8e99bcd1cf\n  System UUID:                055138BD-071B-44C4-BEFF-C380165EF278\n  Boot ID:                    b1a93266-8628-4d26-a6a1-7edafb15017e\n  Kernel Version:             4.15.0-45-generic\n  OS Image:                   Ubuntu 18.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.3\n  Kubelet Version:            v1.20.2\n  Kube-Proxy Version:         v1.20.2\nPodCIDR:                      172.20.0.0/24\nPodCIDRs:                     172.20.0.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-72m2m                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  calico-system               calico-typha-954b59468-5r8qt                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  kube-system                 coredns-74ff55c5b-jwxwr                                    100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     21m\n  kube-system                 etcd-20test-master                                         100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         53m\n  kube-system                 kube-apiserver-20test-master                               250m (12%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-controller-manager-20test-master                      200m (10%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-proxy-n6bjj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-scheduler-20test-master                               100m (5%)     0 (0%)      0 (0%)           0 (0%)         53m\n  logging                     fluent-bit-6czgs                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  monitoring                  node-exporter-hkc6p                                        112m (5%)     270m (13%)  200Mi (2%)       220Mi (2%)     48m\n  sonobuoy                    sonobuoy-e2e-job-482333440fc844c1                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         35m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-sxlz2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         35m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                862m (43%)  270m (13%)\n  memory             370Mi (4%)  390Mi (4%)\n  ephemeral-storage  100Mi (0%)  0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From        Message\n  ----     ------                   ----               ----        -------\n  Normal   NodeHasSufficientMemory  53m (x5 over 53m)  kubelet     Node 20test-master status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    53m (x5 over 53m)  kubelet     Node 20test-master status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     53m (x5 over 53m)  kubelet     Node 20test-master status is now: NodeHasSufficientPID\n  Normal   Starting                 53m                kubelet     Starting kubelet.\n  Warning  InvalidDiskCapacity      53m                kubelet     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  53m                kubelet     Node 20test-master status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    53m                kubelet     Node 20test-master status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     53m                kubelet     Node 20test-master status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  53m                kubelet     Updated Node Allocatable limit across pods\n  Normal   Starting                 53m                kube-proxy  Starting kube-proxy.\n  Normal   NodeReady                48m                kubelet     Node 20test-master status is now: NodeReady\n"
May 17 17:35:04.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-1906 describe namespace kubectl-1906'
May 17 17:35:04.979: INFO: stderr: ""
May 17 17:35:04.979: INFO: stdout: "Name:         kubectl-1906\nLabels:       e2e-framework=kubectl\n              e2e-run=7637ae22-a266-40c2-9be5-42855f8312df\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:04.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1906" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":106,"skipped":1617,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:04.988: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 17 17:35:09.574: INFO: Successfully updated pod "adopt-release-4gpdr"
STEP: Checking that the Job readopts the Pod
May 17 17:35:09.574: INFO: Waiting up to 15m0s for pod "adopt-release-4gpdr" in namespace "job-5806" to be "adopted"
May 17 17:35:09.578: INFO: Pod "adopt-release-4gpdr": Phase="Running", Reason="", readiness=true. Elapsed: 3.714595ms
May 17 17:35:09.578: INFO: Pod "adopt-release-4gpdr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 17 17:35:10.126: INFO: Successfully updated pod "adopt-release-4gpdr"
STEP: Checking that the Job releases the Pod
May 17 17:35:10.126: INFO: Waiting up to 15m0s for pod "adopt-release-4gpdr" in namespace "job-5806" to be "released"
May 17 17:35:10.148: INFO: Pod "adopt-release-4gpdr": Phase="Running", Reason="", readiness=true. Elapsed: 20.71693ms
May 17 17:35:10.149: INFO: Pod "adopt-release-4gpdr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:10.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5806" for this suite.

• [SLOW TEST:5.202 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":107,"skipped":1635,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:10.191: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:10.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 17 17:35:12.249: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 17:35:12.250: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 17 17:35:12.255: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 17 17:35:12.267: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 17 17:35:12.269: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.269: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.269: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.270: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.270: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.270: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.270: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.271: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 0
May 17 17:35:12.292: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:12.292: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:12.292: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.292: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.293: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.294: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.295: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.295: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.339: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.342: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 2
May 17 17:35:12.350: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
STEP: listing Deployments
May 17 17:35:12.363: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 17 17:35:12.384: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 17 17:35:12.408: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.409: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.467: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.491: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.508: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.521: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.558: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 17 17:35:12.569: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.416: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.420: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
May 17 17:35:15.420: INFO: observed Deployment test-deployment in namespace deployment-4371 with ReadyReplicas 1
STEP: deleting the Deployment
May 17 17:35:15.508: INFO: observed event type MODIFIED
May 17 17:35:15.508: INFO: observed event type MODIFIED
May 17 17:35:15.508: INFO: observed event type MODIFIED
May 17 17:35:15.508: INFO: observed event type MODIFIED
May 17 17:35:15.508: INFO: observed event type MODIFIED
May 17 17:35:15.509: INFO: observed event type MODIFIED
May 17 17:35:15.509: INFO: observed event type MODIFIED
May 17 17:35:15.509: INFO: observed event type MODIFIED
May 17 17:35:15.509: INFO: observed event type MODIFIED
May 17 17:35:15.540: INFO: observed event type MODIFIED
May 17 17:35:15.540: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:35:15.558: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:15.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4371" for this suite.

• [SLOW TEST:5.427 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":108,"skipped":1642,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:15.619: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 17 17:35:15.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3642 create -f -'
May 17 17:35:16.408: INFO: stderr: ""
May 17 17:35:16.408: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 17 17:35:16.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3642 diff -f -'
May 17 17:35:17.015: INFO: rc: 1
May 17 17:35:17.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3642 delete -f -'
May 17 17:35:17.136: INFO: stderr: ""
May 17 17:35:17.136: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:17.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3642" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":109,"skipped":1656,"failed":0}
SSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:17.160: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 17 17:35:25.265: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.266: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.338: INFO: Exec stderr: ""
May 17 17:35:25.338: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.338: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.401: INFO: Exec stderr: ""
May 17 17:35:25.402: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.402: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.480: INFO: Exec stderr: ""
May 17 17:35:25.480: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.557: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 17 17:35:25.558: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.558: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.635: INFO: Exec stderr: ""
May 17 17:35:25.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.635: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.699: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 17 17:35:25.699: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.699: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.791: INFO: Exec stderr: ""
May 17 17:35:25.792: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.793: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:25.923: INFO: Exec stderr: ""
May 17 17:35:25.923: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:25.923: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:26.028: INFO: Exec stderr: ""
May 17 17:35:26.028: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4776 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:35:26.028: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:35:26.211: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:35:26.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4776" for this suite.

• [SLOW TEST:9.068 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:35:26.231: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 17 17:35:26.271: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 17:35:26.276: INFO: Waiting for terminating namespaces to be deleted...
May 17 17:35:26.279: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-1 before test
May 17 17:35:26.302: INFO: calico-node-fjsxq from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.302: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:35:26.303: INFO: calico-typha-954b59468-864hr from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.303: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:35:26.303: INFO: kube-proxy-d7hzd from kube-system started at 2021-05-17 16:45:38 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.304: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:35:26.304: INFO: fluent-bit-vm2vm from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.304: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:35:26.304: INFO: alertmanager-main-0 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.305: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:35:26.305: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:35:26.305: INFO: grafana-f8cd57fcf-p45c2 from monitoring started at 2021-05-17 16:46:48 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.305: INFO: 	Container grafana ready: true, restart count 0
May 17 17:35:26.306: INFO: kube-state-metrics-587bfd4f97-6qs5t from monitoring started at 2021-05-17 16:46:48 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.306: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 17 17:35:26.306: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 17 17:35:26.306: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 17 17:35:26.307: INFO: node-exporter-5dkbc from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:35:26.307: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:35:26.307: INFO: prometheus-operator-7649c7454f-5xts8 from monitoring started at 2021-05-17 16:46:39 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.308: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:35:26.308: INFO: 	Container prometheus-operator ready: true, restart count 0
May 17 17:35:26.308: INFO: csi-cephfsplugin-cgwrr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.308: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:35:26.308: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.309: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.309: INFO: csi-cephfsplugin-provisioner-8658f67749-mj486 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:35:26.309: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:35:26.309: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:35:26.309: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:35:26.309: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:35:26.310: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:35:26.310: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.310: INFO: csi-rbdplugin-6jrsr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.310: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:35:26.310: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.311: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.311: INFO: csi-rbdplugin-provisioner-94f699d86-r49r5 from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (6 container statuses recorded)
May 17 17:35:26.311: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:35:26.311: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:35:26.311: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:35:26.311: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:35:26.312: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:35:26.312: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.312: INFO: rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.312: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:35:26.313: INFO: rook-ceph-mgr-a-5db4bfbc6-fnj5z from rook-ceph started at 2021-05-17 16:49:56 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.313: INFO: 	Container mgr ready: true, restart count 0
May 17 17:35:26.313: INFO: rook-ceph-mon-c-545cb7776f-tsblp from rook-ceph started at 2021-05-17 16:49:42 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.313: INFO: 	Container mon ready: true, restart count 0
May 17 17:35:26.313: INFO: rook-ceph-osd-2-96fd479f5-88zbz from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.313: INFO: 	Container osd ready: true, restart count 0
May 17 17:35:26.314: INFO: rook-ceph-osd-prepare-20test-worker-1-qb5xq from rook-ceph started at 2021-05-17 17:14:22 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.314: INFO: 	Container provision ready: false, restart count 0
May 17 17:35:26.314: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:35:26.314: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:35:26.315: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-2 before test
May 17 17:35:26.342: INFO: calico-kube-controllers-5fb9f7f6d8-gpg2w from calico-system started at 2021-05-17 16:46:37 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 17:35:26.343: INFO: calico-node-sxlpp from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:35:26.343: INFO: calico-typha-954b59468-jks4v from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:35:26.343: INFO: coredns-74ff55c5b-hjgsp from kube-system started at 2021-05-17 16:46:42 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container coredns ready: true, restart count 0
May 17 17:35:26.343: INFO: kube-proxy-wmdvj from kube-system started at 2021-05-17 16:45:52 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:35:26.343: INFO: fluent-bit-mjvsf from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:35:26.343: INFO: alertmanager-main-2 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:35:26.343: INFO: node-exporter-gcj5f from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:35:26.343: INFO: prometheus-adapter-69b8496df6-tns5m from monitoring started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 17 17:35:26.343: INFO: prometheus-k8s-0 from monitoring started at 2021-05-17 16:47:03 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:35:26.343: INFO: csi-cephfsplugin-8lpms from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.343: INFO: csi-cephfsplugin-provisioner-8658f67749-vcx69 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.343: INFO: csi-rbdplugin-7rpck from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.343: INFO: csi-rbdplugin-provisioner-94f699d86-xznpd from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.343: INFO: rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:35:26.343: INFO: rook-ceph-mon-b-589bbc6556-w592m from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container mon ready: true, restart count 0
May 17 17:35:26.343: INFO: rook-ceph-operator-5f6ffc46c7-hj2ml from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container rook-ceph-operator ready: true, restart count 0
May 17 17:35:26.343: INFO: rook-ceph-osd-0-7896cc545c-psfck from rook-ceph started at 2021-05-17 16:50:08 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container osd ready: true, restart count 0
May 17 17:35:26.343: INFO: rook-ceph-osd-prepare-20test-worker-2-lhtcq from rook-ceph started at 2021-05-17 17:14:24 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container provision ready: false, restart count 0
May 17 17:35:26.343: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:35:26.343: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:35:26.343: INFO: tigera-operator-657cc89589-qhpft from tigera-operator started at 2021-05-17 16:46:10 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.343: INFO: 	Container tigera-operator ready: true, restart count 0
May 17 17:35:26.343: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-3 before test
May 17 17:35:26.360: INFO: calico-node-rb678 from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:35:26.360: INFO: calico-typha-954b59468-v2z88 from calico-system started at 2021-05-17 16:46:16 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:35:26.360: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-4776 started at 2021-05-17 17:35:21 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container busybox-1 ready: true, restart count 0
May 17 17:35:26.360: INFO: 	Container busybox-2 ready: true, restart count 0
May 17 17:35:26.360: INFO: test-pod from e2e-kubelet-etc-hosts-4776 started at 2021-05-17 17:35:17 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container busybox-1 ready: true, restart count 0
May 17 17:35:26.360: INFO: 	Container busybox-2 ready: true, restart count 0
May 17 17:35:26.360: INFO: 	Container busybox-3 ready: true, restart count 0
May 17 17:35:26.360: INFO: adopt-release-4gpdr from job-5806 started at 2021-05-17 17:35:05 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container c ready: true, restart count 0
May 17 17:35:26.360: INFO: adopt-release-7zbqd from job-5806 started at 2021-05-17 17:35:10 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container c ready: true, restart count 0
May 17 17:35:26.360: INFO: adopt-release-cpwkm from job-5806 started at 2021-05-17 17:35:05 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container c ready: true, restart count 0
May 17 17:35:26.360: INFO: kube-proxy-n6xv8 from kube-system started at 2021-05-17 16:45:25 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:35:26.360: INFO: agnhost-primary-zxpxq from kubectl-1906 started at 2021-05-17 17:35:02 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container agnhost-primary ready: false, restart count 0
May 17 17:35:26.360: INFO: httpd-deployment-86bff9b6d7-wg9vw from kubectl-3642 started at 2021-05-17 17:35:16 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container httpd ready: true, restart count 0
May 17 17:35:26.360: INFO: fluent-bit-hg8tt from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:35:26.360: INFO: alertmanager-main-1 from monitoring started at 2021-05-17 17:14:35 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:35:26.360: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:35:26.360: INFO: node-exporter-ccd9w from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.360: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:35:26.360: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:35:26.361: INFO: prometheus-k8s-1 from monitoring started at 2021-05-17 17:14:37 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:35:26.361: INFO: csi-cephfsplugin-9hmwc from rook-ceph started at 2021-05-17 17:14:36 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.361: INFO: csi-rbdplugin-9hr4g from rook-ceph started at 2021-05-17 17:14:38 +0000 UTC (3 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:35:26.361: INFO: rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:35:26.361: INFO: rook-ceph-mon-a-6947dc47f-8wtqd from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container mon ready: true, restart count 0
May 17 17:35:26.361: INFO: rook-ceph-osd-1-6c8669bc9-89l9s from rook-ceph started at 2021-05-17 17:13:51 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container osd ready: true, restart count 0
May 17 17:35:26.361: INFO: rook-ceph-osd-prepare-20test-worker-3-cqpxz from rook-ceph started at 2021-05-17 17:14:26 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container provision ready: false, restart count 0
May 17 17:35:26.361: INFO: sonobuoy from sonobuoy started at 2021-05-17 16:59:52 +0000 UTC (1 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 17:35:26.361: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:35:26.361: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:35:26.361: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-73f2e791-235f-48e2-bd6c-89e083a755b5 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.30.20.187 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-73f2e791-235f-48e2-bd6c-89e083a755b5 off the node 20test-worker-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-73f2e791-235f-48e2-bd6c-89e083a755b5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:40:30.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8035" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.273 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":111,"skipped":1739,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:40:30.506: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 17 17:40:30.559: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:03.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4911" for this suite.

• [SLOW TEST:32.696 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":112,"skipped":1748,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:03.205: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 17 17:41:03.250: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:15.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4065" for this suite.

• [SLOW TEST:11.920 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":1749,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:15.130: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-b412f21a-8d2c-418e-a979-88c9b9aa102b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:17.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7729" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":1756,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:17.276: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:41:17.310: INFO: Creating deployment "test-recreate-deployment"
May 17 17:41:17.373: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 17 17:41:17.384: INFO: Waiting deployment "test-recreate-deployment" to complete
May 17 17:41:17.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756870077, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756870077, loc:(*time.Location)(0x7962e20)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-786dd7c454\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756870077, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756870077, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
May 17 17:41:19.391: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 17 17:41:19.398: INFO: Updating deployment test-recreate-deployment
May 17 17:41:19.398: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:41:19.486: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9984  8576a23a-1034-4031-8eeb-fe7d6ff37374 23573 2 2021-05-17 17:41:17 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ae36c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-17 17:41:19 +0000 UTC,LastTransitionTime:2021-05-17 17:41:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-17 17:41:19 +0000 UTC,LastTransitionTime:2021-05-17 17:41:17 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 17 17:41:19.489: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-9984  0a60baa5-8502-405b-afe5-2a0ddb05c7be 23572 1 2021-05-17 17:41:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8576a23a-1034-4031-8eeb-fe7d6ff37374 0xc003ae3bf0 0xc003ae3bf1}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8576a23a-1034-4031-8eeb-fe7d6ff37374\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ae3c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:41:19.489: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 17 17:41:19.489: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-9984  787bfd1d-8e47-4b3e-9b13-a2083fc3304f 23562 2 2021-05-17 17:41:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8576a23a-1034-4031-8eeb-fe7d6ff37374 0xc003ae3ad7 0xc003ae3ad8}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8576a23a-1034-4031-8eeb-fe7d6ff37374\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ae3b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:41:19.492: INFO: Pod "test-recreate-deployment-f79dd4667-q6fj2" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-q6fj2 test-recreate-deployment-f79dd4667- deployment-9984  330f7da1-3e29-400d-a8f6-dc0d8fb576fa 23574 0 2021-05-17 17:41:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 0a60baa5-8502-405b-afe5-2a0ddb05c7be 0xc003b05840 0xc003b05841}] []  [{kube-controller-manager Update v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a60baa5-8502-405b-afe5-2a0ddb05c7be\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-17 17:41:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t45sl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t45sl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t45sl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:41:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:41:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:41:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:41:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:,StartTime:2021-05-17 17:41:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:19.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9984" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":115,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:19.500: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 17 17:41:19.536: INFO: Waiting up to 5m0s for pod "pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224" in namespace "emptydir-7562" to be "Succeeded or Failed"
May 17 17:41:19.542: INFO: Pod "pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224": Phase="Pending", Reason="", readiness=false. Elapsed: 5.910996ms
May 17 17:41:21.545: INFO: Pod "pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009594559s
STEP: Saw pod success
May 17 17:41:21.545: INFO: Pod "pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224" satisfied condition "Succeeded or Failed"
May 17 17:41:21.548: INFO: Trying to get logs from node 20test-worker-3 pod pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224 container test-container: <nil>
STEP: delete the pod
May 17 17:41:21.573: INFO: Waiting for pod pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224 to disappear
May 17 17:41:21.576: INFO: Pod pod-608ba0bd-d3bb-45e7-a7ac-f057186e3224 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:21.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7562" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":1807,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-5dd1f788-bab4-4678-98c0-d1d25749f793
STEP: Creating a pod to test consume secrets
May 17 17:41:21.638: INFO: Waiting up to 5m0s for pod "pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9" in namespace "secrets-8270" to be "Succeeded or Failed"
May 17 17:41:21.646: INFO: Pod "pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.67073ms
May 17 17:41:23.650: INFO: Pod "pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010340595s
STEP: Saw pod success
May 17 17:41:23.650: INFO: Pod "pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9" satisfied condition "Succeeded or Failed"
May 17 17:41:23.652: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9 container secret-volume-test: <nil>
STEP: delete the pod
May 17 17:41:23.673: INFO: Waiting for pod pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9 to disappear
May 17 17:41:23.677: INFO: Pod pod-secrets-e847d9e0-0894-4aa6-98de-8a71f59819a9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:23.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8270" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":1809,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:23.689: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:25.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7829" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:25.767: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f7bf603b-b98d-4d58-a9ff-094bb959abfc
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f7bf603b-b98d-4d58-a9ff-094bb959abfc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:29.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-660" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":1853,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:29.874: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 17 17:41:29.913: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-508 proxy --unix-socket=/tmp/kubectl-proxy-unix226827880/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:41:30.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-508" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":120,"skipped":1854,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:41:30.038: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-a9dbf40d-674c-40f5-af16-c41a18d02fd1 in namespace container-probe-9755
May 17 17:41:34.092: INFO: Started pod test-webserver-a9dbf40d-674c-40f5-af16-c41a18d02fd1 in namespace container-probe-9755
STEP: checking the pod's current state and verifying that restartCount is present
May 17 17:41:34.094: INFO: Initial restart count of pod test-webserver-a9dbf40d-674c-40f5-af16-c41a18d02fd1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:35.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9755" for this suite.

• [SLOW TEST:244.982 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":1860,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:35.023: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 17 17:45:35.056: INFO: Major version: 1
STEP: Confirm minor version
May 17 17:45:35.056: INFO: cleanMinorVersion: 20
May 17 17:45:35.056: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:35.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2333" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":122,"skipped":1869,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 17 17:45:35.142: INFO: created test-event-1
May 17 17:45:35.144: INFO: created test-event-2
May 17 17:45:35.146: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 17 17:45:35.161: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 17 17:45:35.174: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-329" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":123,"skipped":1889,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:35.185: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:45:35.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a" in namespace "projected-227" to be "Succeeded or Failed"
May 17 17:45:35.225: INFO: Pod "downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.998696ms
May 17 17:45:37.229: INFO: Pod "downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011640858s
STEP: Saw pod success
May 17 17:45:37.229: INFO: Pod "downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a" satisfied condition "Succeeded or Failed"
May 17 17:45:37.231: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a container client-container: <nil>
STEP: delete the pod
May 17 17:45:37.255: INFO: Waiting for pod downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a to disappear
May 17 17:45:37.258: INFO: Pod downwardapi-volume-237974b8-4d19-4068-9eac-4fc590c1b36a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:37.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-227" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":1894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:37.268: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 17 17:45:37.303: INFO: namespace kubectl-8814
May 17 17:45:37.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8814 create -f -'
May 17 17:45:38.020: INFO: stderr: ""
May 17 17:45:38.020: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 17 17:45:39.027: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:45:39.027: INFO: Found 0 / 1
May 17 17:45:40.043: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:45:40.043: INFO: Found 1 / 1
May 17 17:45:40.043: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 17 17:45:40.046: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:45:40.046: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 17:45:40.046: INFO: wait on agnhost-primary startup in kubectl-8814 
May 17 17:45:40.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8814 logs agnhost-primary-v4kj2 agnhost-primary'
May 17 17:45:40.597: INFO: stderr: ""
May 17 17:45:40.597: INFO: stdout: "Paused\n"
STEP: exposing RC
May 17 17:45:40.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8814 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 17 17:45:40.956: INFO: stderr: ""
May 17 17:45:40.956: INFO: stdout: "service/rm2 exposed\n"
May 17 17:45:40.959: INFO: Service rm2 in namespace kubectl-8814 found.
STEP: exposing service
May 17 17:45:42.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8814 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 17 17:45:43.099: INFO: stderr: ""
May 17 17:45:43.107: INFO: stdout: "service/rm3 exposed\n"
May 17 17:45:43.114: INFO: Service rm3 in namespace kubectl-8814 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:45.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8814" for this suite.

• [SLOW TEST:7.864 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":125,"skipped":2008,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:45.133: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:45:45.207: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 17 17:45:45.211: INFO: Pod name sample-pod: Found 0 pods out of 1
May 17 17:45:50.268: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 17 17:45:50.268: INFO: Creating deployment "test-rolling-update-deployment"
May 17 17:45:50.272: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 17 17:45:50.280: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 17 17:45:52.290: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 17 17:45:52.296: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 17 17:45:52.312: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1750  d3bd4bbb-cff4-4925-9e5c-f4989dc6c1d1 24977 1 2021-05-17 17:45:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-17 17:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:45:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009c47d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-17 17:45:50 +0000 UTC,LastTransitionTime:2021-05-17 17:45:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-05-17 17:45:51 +0000 UTC,LastTransitionTime:2021-05-17 17:45:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 17 17:45:52.326: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-1750  5b078576-1419-4c57-aadb-db1862403f5e 24966 1 2021-05-17 17:45:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d3bd4bbb-cff4-4925-9e5c-f4989dc6c1d1 0xc009024f67 0xc009024f68}] []  [{kube-controller-manager Update apps/v1 2021-05-17 17:45:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3bd4bbb-cff4-4925-9e5c-f4989dc6c1d1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009024ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 17 17:45:52.326: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 17 17:45:52.327: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1750  8b72ede0-723e-44ad-af32-00c891ffab30 24976 2 2021-05-17 17:45:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d3bd4bbb-cff4-4925-9e5c-f4989dc6c1d1 0xc009024e57 0xc009024e58}] []  [{e2e.test Update apps/v1 2021-05-17 17:45:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-17 17:45:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3bd4bbb-cff4-4925-9e5c-f4989dc6c1d1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009024ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 17 17:45:52.336: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-8rkrv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-8rkrv test-rolling-update-deployment-6b6bf9df46- deployment-1750  fc7ae480-a734-45ee-a014-a9627779c450 24965 0 2021-05-17 17:45:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:172.20.150.24/32 cni.projectcalico.org/podIPs:172.20.150.24/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 5b078576-1419-4c57-aadb-db1862403f5e 0xc0009c4d67 0xc0009c4d68}] []  [{kube-controller-manager Update v1 2021-05-17 17:45:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b078576-1419-4c57-aadb-db1862403f5e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-05-17 17:45:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-05-17 17:45:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.150.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sj4bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sj4bw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sj4bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:20test-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:45:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:45:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:45:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-17 17:45:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.30.20.187,PodIP:172.20.150.24,StartTime:2021-05-17 17:45:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-17 17:45:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:containerd://8729d1f1f03e4803508062393dab62e72e7e05b9907612792187fbe7d6f1de2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.150.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:45:52.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1750" for this suite.

• [SLOW TEST:7.215 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":126,"skipped":2011,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:45:52.355: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 17 17:45:52.389: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 17 17:45:52.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:52.687: INFO: stderr: ""
May 17 17:45:52.687: INFO: stdout: "service/agnhost-replica created\n"
May 17 17:45:52.687: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 17 17:45:52.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:53.060: INFO: stderr: ""
May 17 17:45:53.060: INFO: stdout: "service/agnhost-primary created\n"
May 17 17:45:53.060: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 17 17:45:53.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:53.443: INFO: stderr: ""
May 17 17:45:53.443: INFO: stdout: "service/frontend created\n"
May 17 17:45:53.443: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 17 17:45:53.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:53.751: INFO: stderr: ""
May 17 17:45:53.751: INFO: stdout: "deployment.apps/frontend created\n"
May 17 17:45:53.760: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 17:45:53.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:54.114: INFO: stderr: ""
May 17 17:45:54.114: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 17 17:45:54.114: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 17 17:45:54.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 create -f -'
May 17 17:45:54.588: INFO: stderr: ""
May 17 17:45:54.588: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 17 17:45:54.588: INFO: Waiting for all frontend pods to be Running.
May 17 17:45:59.638: INFO: Waiting for frontend to serve content.
May 17 17:45:59.647: INFO: Trying to add a new entry to the guestbook.
May 17 17:45:59.655: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 17 17:45:59.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:45:59.767: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:45:59.767: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 17 17:45:59.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:45:59.896: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:45:59.896: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 17 17:45:59.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:46:00.013: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:46:00.013: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 17 17:46:00.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:46:00.165: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:46:00.165: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 17 17:46:00.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:46:00.295: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:46:00.295: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 17 17:46:00.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-8796 delete --grace-period=0 --force -f -'
May 17 17:46:00.405: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:46:00.405: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:46:00.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8796" for this suite.

• [SLOW TEST:8.072 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":127,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:46:00.428: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:46:13.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4231" for this suite.

• [SLOW TEST:13.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":128,"skipped":2054,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:46:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6689
STEP: creating service affinity-clusterip-transition in namespace services-6689
STEP: creating replication controller affinity-clusterip-transition in namespace services-6689
I0517 17:46:13.600009      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-6689, replica count: 3
I0517 17:46:16.658303      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:46:16.663: INFO: Creating new exec pod
May 17 17:46:19.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-6689 exec execpod-affinity688nt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 17 17:46:19.898: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 17 17:46:19.898: INFO: stdout: ""
May 17 17:46:19.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-6689 exec execpod-affinity688nt -- /bin/sh -x -c nc -zv -t -w 2 172.30.149.236 80'
May 17 17:46:20.108: INFO: stderr: "+ nc -zv -t -w 2 172.30.149.236 80\nConnection to 172.30.149.236 80 port [tcp/http] succeeded!\n"
May 17 17:46:20.108: INFO: stdout: ""
May 17 17:46:20.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-6689 exec execpod-affinity688nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.149.236:80/ ; done'
May 17 17:46:20.445: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n"
May 17 17:46:20.445: INFO: stdout: "\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-fnfk7\naffinity-clusterip-transition-fnfk7\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-fnfk7\naffinity-clusterip-transition-k8fbb\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-fnfk7\naffinity-clusterip-transition-k8fbb\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-fnfk7\naffinity-clusterip-transition-k8fbb\naffinity-clusterip-transition-k8fbb\naffinity-clusterip-transition-69cpx"
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-fnfk7
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-fnfk7
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-fnfk7
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-k8fbb
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-fnfk7
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-k8fbb
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-fnfk7
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-k8fbb
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-k8fbb
May 17 17:46:20.445: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-6689 exec execpod-affinity688nt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.149.236:80/ ; done'
May 17 17:46:20.732: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.149.236:80/\n"
May 17 17:46:20.732: INFO: stdout: "\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx\naffinity-clusterip-transition-69cpx"
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Received response from host: affinity-clusterip-transition-69cpx
May 17 17:46:20.732: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6689, will wait for the garbage collector to delete the pods
May 17 17:46:20.811: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.185722ms
May 17 17:46:21.712: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 900.478659ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:47:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6689" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:52.046 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":129,"skipped":2064,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:47:05.592: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-62fbae06-be57-40ec-b162-32aed97f3024
STEP: Creating configMap with name cm-test-opt-upd-ff3c3267-ccf8-465e-a9b8-97ba10323c57
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-62fbae06-be57-40ec-b162-32aed97f3024
STEP: Updating configmap cm-test-opt-upd-ff3c3267-ccf8-465e-a9b8-97ba10323c57
STEP: Creating configMap with name cm-test-opt-create-0173ab23-14f0-4d2d-9387-2a6e29f965c0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:13.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9816" for this suite.

• [SLOW TEST:68.383 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2122,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:13.976: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:16.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5633" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:16.072: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:48:16.116: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-86e63d06-2d47-4874-b334-105cc3939919" in namespace "security-context-test-7033" to be "Succeeded or Failed"
May 17 17:48:16.129: INFO: Pod "busybox-readonly-false-86e63d06-2d47-4874-b334-105cc3939919": Phase="Pending", Reason="", readiness=false. Elapsed: 13.732889ms
May 17 17:48:18.133: INFO: Pod "busybox-readonly-false-86e63d06-2d47-4874-b334-105cc3939919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017942065s
May 17 17:48:18.134: INFO: Pod "busybox-readonly-false-86e63d06-2d47-4874-b334-105cc3939919" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:18.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7033" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2150,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:18.150: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:25.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5617" for this suite.

• [SLOW TEST:7.065 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":133,"skipped":2158,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:25.215: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-8d97f490-5ed9-48f7-b0ea-f60abed2f98d
STEP: Creating secret with name secret-projected-all-test-volume-615c8f52-57e8-477b-acfa-234fe97d56fb
STEP: Creating a pod to test Check all projections for projected volume plugin
May 17 17:48:25.295: INFO: Waiting up to 5m0s for pod "projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989" in namespace "projected-8433" to be "Succeeded or Failed"
May 17 17:48:25.309: INFO: Pod "projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989": Phase="Pending", Reason="", readiness=false. Elapsed: 14.634547ms
May 17 17:48:27.314: INFO: Pod "projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019632775s
May 17 17:48:29.318: INFO: Pod "projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023602671s
STEP: Saw pod success
May 17 17:48:29.319: INFO: Pod "projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989" satisfied condition "Succeeded or Failed"
May 17 17:48:29.321: INFO: Trying to get logs from node 20test-worker-3 pod projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989 container projected-all-volume-test: <nil>
STEP: delete the pod
May 17 17:48:29.343: INFO: Waiting for pod projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989 to disappear
May 17 17:48:29.346: INFO: Pod projected-volume-b6a95e5b-a7e4-4126-a157-74c6fcb6c989 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:29.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8433" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2162,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:29.357: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:48:29.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4" in namespace "downward-api-4006" to be "Succeeded or Failed"
May 17 17:48:29.418: INFO: Pod "downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.099069ms
May 17 17:48:31.427: INFO: Pod "downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018182212s
May 17 17:48:33.432: INFO: Pod "downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022933618s
STEP: Saw pod success
May 17 17:48:33.432: INFO: Pod "downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4" satisfied condition "Succeeded or Failed"
May 17 17:48:33.434: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4 container client-container: <nil>
STEP: delete the pod
May 17 17:48:33.454: INFO: Waiting for pod downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4 to disappear
May 17 17:48:33.457: INFO: Pod downwardapi-volume-f0a2ed2b-c9d0-4e53-925c-c314d73293e4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:33.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4006" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2167,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:33.469: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 17 17:48:33.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-6253 cluster-info'
May 17 17:48:33.617: INFO: stderr: ""
May 17 17:48:33.618: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:33.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6253" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":136,"skipped":2167,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:33.626: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2208
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2208
I0517 17:48:33.706691      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2208, replica count: 2
May 17 17:48:36.758: INFO: Creating new exec pod
I0517 17:48:36.757972      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:48:39.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-2208 exec execpoddhmrs -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 17 17:48:39.971: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 17 17:48:39.971: INFO: stdout: ""
May 17 17:48:39.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-2208 exec execpoddhmrs -- /bin/sh -x -c nc -zv -t -w 2 172.30.214.255 80'
May 17 17:48:40.139: INFO: stderr: "+ nc -zv -t -w 2 172.30.214.255 80\nConnection to 172.30.214.255 80 port [tcp/http] succeeded!\n"
May 17 17:48:40.139: INFO: stdout: ""
May 17 17:48:40.139: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:48:40.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2208" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.549 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":137,"skipped":2182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:48:40.178: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0517 17:48:46.312550      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 17:49:48.338: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:49:48.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-689" for this suite.

• [SLOW TEST:68.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":138,"skipped":2264,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:49:48.350: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:49:48.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4292" for this suite.
STEP: Destroying namespace "nspatchtest-6ca4b5c5-af57-41cc-96ad-127e097cf996-6162" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":139,"skipped":2270,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:49:48.434: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:49:48.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7664" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":140,"skipped":2271,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:49:48.476: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 17 17:49:48.516: INFO: Waiting up to 5m0s for pod "var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40" in namespace "var-expansion-6809" to be "Succeeded or Failed"
May 17 17:49:48.529: INFO: Pod "var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40": Phase="Pending", Reason="", readiness=false. Elapsed: 12.634002ms
May 17 17:49:50.533: INFO: Pod "var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016678251s
STEP: Saw pod success
May 17 17:49:50.533: INFO: Pod "var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40" satisfied condition "Succeeded or Failed"
May 17 17:49:50.535: INFO: Trying to get logs from node 20test-worker-3 pod var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40 container dapi-container: <nil>
STEP: delete the pod
May 17 17:49:50.552: INFO: Waiting for pod var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40 to disappear
May 17 17:49:50.555: INFO: Pod var-expansion-67c8dd72-7257-44e1-8c47-6b0158d2bd40 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:49:50.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6809" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":141,"skipped":2274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:49:50.569: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:49:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1116" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2311,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:49:54.665: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8655
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 17 17:49:54.717: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 17:49:54.806: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 17:49:56.817: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 17:49:58.811: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:00.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:02.810: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:04.810: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:06.811: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:08.809: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:50:10.810: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 17 17:50:10.814: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 17 17:50:12.818: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 17 17:50:14.817: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 17 17:50:16.819: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 17 17:50:16.824: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 17 17:50:20.884: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 17:50:20.884: INFO: Going to poll 172.20.227.232 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 17:50:20.886: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.227.232 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:50:20.886: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:50:21.968: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 17:50:21.969: INFO: Going to poll 172.20.206.220 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 17:50:21.977: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.206.220 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:50:21.977: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:50:23.060: INFO: Found all 1 expected endpoints: [netserver-1]
May 17 17:50:23.061: INFO: Going to poll 172.20.150.55 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 17 17:50:23.065: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.150.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8655 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:50:23.066: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:50:24.151: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:50:24.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8655" for this suite.

• [SLOW TEST:29.499 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2316,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:50:24.166: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:50:24.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592" in namespace "projected-5637" to be "Succeeded or Failed"
May 17 17:50:24.213: INFO: Pod "downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592": Phase="Pending", Reason="", readiness=false. Elapsed: 7.331438ms
May 17 17:50:26.217: INFO: Pod "downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011169851s
STEP: Saw pod success
May 17 17:50:26.217: INFO: Pod "downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592" satisfied condition "Succeeded or Failed"
May 17 17:50:26.219: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592 container client-container: <nil>
STEP: delete the pod
May 17 17:50:26.236: INFO: Waiting for pod downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592 to disappear
May 17 17:50:26.241: INFO: Pod downwardapi-volume-1a82402f-0a37-4fc6-99d8-586d19a07592 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:50:26.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5637" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2316,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:50:26.252: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:50:26.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2228" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":145,"skipped":2325,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:50:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3262/configmap-test-b63e8bc4-765a-4242-9df3-7c899020f86d
STEP: Creating a pod to test consume configMaps
May 17 17:50:26.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031" in namespace "configmap-3262" to be "Succeeded or Failed"
May 17 17:50:26.421: INFO: Pod "pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031": Phase="Pending", Reason="", readiness=false. Elapsed: 5.339945ms
May 17 17:50:28.425: INFO: Pod "pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009648466s
STEP: Saw pod success
May 17 17:50:28.426: INFO: Pod "pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031" satisfied condition "Succeeded or Failed"
May 17 17:50:28.428: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031 container env-test: <nil>
STEP: delete the pod
May 17 17:50:28.451: INFO: Waiting for pod pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031 to disappear
May 17 17:50:28.453: INFO: Pod pod-configmaps-12cbfdd1-f13f-450d-9168-33517ed37031 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:50:28.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3262" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2341,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:50:28.461: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 17 17:50:28.506: INFO: Waiting up to 5m0s for pod "client-containers-68671d8d-47d3-4473-b1db-b88da12750d8" in namespace "containers-5675" to be "Succeeded or Failed"
May 17 17:50:28.517: INFO: Pod "client-containers-68671d8d-47d3-4473-b1db-b88da12750d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.983164ms
May 17 17:50:30.521: INFO: Pod "client-containers-68671d8d-47d3-4473-b1db-b88da12750d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015019295s
May 17 17:50:32.525: INFO: Pod "client-containers-68671d8d-47d3-4473-b1db-b88da12750d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018622012s
STEP: Saw pod success
May 17 17:50:32.525: INFO: Pod "client-containers-68671d8d-47d3-4473-b1db-b88da12750d8" satisfied condition "Succeeded or Failed"
May 17 17:50:32.527: INFO: Trying to get logs from node 20test-worker-3 pod client-containers-68671d8d-47d3-4473-b1db-b88da12750d8 container agnhost-container: <nil>
STEP: delete the pod
May 17 17:50:32.545: INFO: Waiting for pod client-containers-68671d8d-47d3-4473-b1db-b88da12750d8 to disappear
May 17 17:50:32.548: INFO: Pod client-containers-68671d8d-47d3-4473-b1db-b88da12750d8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:50:32.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5675" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:50:32.560: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1097
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-1097
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1097
May 17 17:50:32.788: INFO: Found 0 stateful pods, waiting for 1
May 17 17:50:42.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 17 17:50:42.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:50:43.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:50:43.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:50:43.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:50:43.020: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 17 17:50:53.028: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:50:53.028: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:50:53.044: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:50:53.045: INFO: ss-0  20test-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:50:53.045: INFO: 
May 17 17:50:53.046: INFO: StatefulSet ss has not reached scale 3, at 1
May 17 17:50:54.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994830859s
May 17 17:50:55.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991365322s
May 17 17:50:56.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986792158s
May 17 17:50:57.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982911372s
May 17 17:50:58.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975492921s
May 17 17:50:59.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967769214s
May 17 17:51:00.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963631505s
May 17 17:51:01.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959710642s
May 17 17:51:02.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 955.38149ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1097
May 17 17:51:03.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:03.296: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 17:51:03.296: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:51:03.296: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:51:03.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:03.531: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 17:51:03.531: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:51:03.531: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:51:03.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:03.732: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 17 17:51:03.732: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 17:51:03.732: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 17:51:03.739: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 17:51:03.739: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 17:51:03.739: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 17 17:51:03.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:51:03.934: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:51:03.934: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:51:03.934: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:51:03.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:51:04.121: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:51:04.121: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:51:04.121: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:51:04.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 17:51:04.346: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 17:51:04.346: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 17:51:04.346: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 17:51:04.346: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:51:04.350: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 17 17:51:14.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:51:14.356: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:51:14.356: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 17 17:51:14.381: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:14.381: INFO: ss-0  20test-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:14.381: INFO: ss-1  20test-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:14.381: INFO: ss-2  20test-worker-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:14.381: INFO: 
May 17 17:51:14.381: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:15.386: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:15.386: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:15.386: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:15.387: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:15.387: INFO: 
May 17 17:51:15.387: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:16.390: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:16.391: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:16.391: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:16.391: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:16.391: INFO: 
May 17 17:51:16.391: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:17.395: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:17.395: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:17.395: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:17.395: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:17.396: INFO: 
May 17 17:51:17.396: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:18.414: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:18.414: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:18.414: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:18.414: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:18.414: INFO: 
May 17 17:51:18.414: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:19.418: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:19.418: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:19.419: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:19.419: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:19.419: INFO: 
May 17 17:51:19.419: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:20.423: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:20.423: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:20.424: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:20.424: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:20.424: INFO: 
May 17 17:51:20.424: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:21.428: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:21.429: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:21.429: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:21.429: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:21.429: INFO: 
May 17 17:51:21.429: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:22.435: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:22.435: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:22.436: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:22.436: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:22.436: INFO: 
May 17 17:51:22.436: INFO: StatefulSet ss has not reached scale 0, at 3
May 17 17:51:23.444: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
May 17 17:51:23.444: INFO: ss-0  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:32 +0000 UTC  }]
May 17 17:51:23.444: INFO: ss-1  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:23.444: INFO: ss-2  20test-worker-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:51:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 17:50:53 +0000 UTC  }]
May 17 17:51:23.444: INFO: 
May 17 17:51:23.444: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1097
May 17 17:51:24.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:24.663: INFO: rc: 1
May 17 17:51:24.663: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 17 17:51:34.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:34.782: INFO: rc: 1
May 17 17:51:34.783: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 17 17:51:44.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:44.905: INFO: rc: 1
May 17 17:51:44.905: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 17 17:51:54.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:51:55.019: INFO: rc: 1
May 17 17:51:55.019: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
May 17 17:52:05.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:05.145: INFO: rc: 1
May 17 17:52:05.145: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:52:15.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:15.240: INFO: rc: 1
May 17 17:52:15.240: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:52:25.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:25.352: INFO: rc: 1
May 17 17:52:25.352: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:52:35.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:35.472: INFO: rc: 1
May 17 17:52:35.472: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:52:45.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:45.567: INFO: rc: 1
May 17 17:52:45.567: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:52:55.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:52:55.715: INFO: rc: 1
May 17 17:52:55.715: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:05.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:05.819: INFO: rc: 1
May 17 17:53:05.819: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:15.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:15.912: INFO: rc: 1
May 17 17:53:15.912: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:25.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:26.012: INFO: rc: 1
May 17 17:53:26.013: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:36.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:36.103: INFO: rc: 1
May 17 17:53:36.103: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:46.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:46.191: INFO: rc: 1
May 17 17:53:46.191: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:53:56.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:53:56.275: INFO: rc: 1
May 17 17:53:56.275: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:06.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:06.384: INFO: rc: 1
May 17 17:54:06.384: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:16.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:16.516: INFO: rc: 1
May 17 17:54:16.516: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:26.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:26.600: INFO: rc: 1
May 17 17:54:26.600: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:36.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:36.698: INFO: rc: 1
May 17 17:54:36.698: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:46.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:46.829: INFO: rc: 1
May 17 17:54:46.829: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:54:56.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:54:56.913: INFO: rc: 1
May 17 17:54:56.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:06.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:07.037: INFO: rc: 1
May 17 17:55:07.038: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:17.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:17.132: INFO: rc: 1
May 17 17:55:17.132: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:27.234: INFO: rc: 1
May 17 17:55:27.234: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:37.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:37.328: INFO: rc: 1
May 17 17:55:37.328: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:47.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:47.686: INFO: rc: 1
May 17 17:55:47.686: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:55:57.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:55:57.796: INFO: rc: 1
May 17 17:55:57.796: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:56:07.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:56:07.883: INFO: rc: 1
May 17 17:56:07.883: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:56:17.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:56:17.974: INFO: rc: 1
May 17 17:56:17.974: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
May 17 17:56:27.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-1097 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 17:56:28.085: INFO: rc: 1
May 17 17:56:28.085: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
May 17 17:56:28.085: INFO: Scaling statefulset ss to 0
May 17 17:56:28.096: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 17:56:28.106: INFO: Deleting all statefulset in ns statefulset-1097
May 17 17:56:28.110: INFO: Scaling statefulset ss to 0
May 17 17:56:28.118: INFO: Waiting for statefulset status.replicas updated to 0
May 17 17:56:28.120: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:56:28.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1097" for this suite.

• [SLOW TEST:355.598 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":148,"skipped":2384,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:56:28.158: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 17 17:56:32.760: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2287 pod-service-account-fecb9b23-419c-4b35-9439-341fc2701700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 17 17:56:32.918: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2287 pod-service-account-fecb9b23-419c-4b35-9439-341fc2701700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 17 17:56:33.111: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2287 pod-service-account-fecb9b23-419c-4b35-9439-341fc2701700 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:56:33.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2287" for this suite.

• [SLOW TEST:5.283 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":149,"skipped":2387,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:56:33.441: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-f5088f76-cc17-4aa9-9eb6-8fdf01ca020c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:56:33.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5699" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":150,"skipped":2391,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:56:33.522: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 17 17:56:33.570: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 17:56:33.598: INFO: Waiting for terminating namespaces to be deleted...
May 17 17:56:33.599: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-1 before test
May 17 17:56:33.616: INFO: calico-node-fjsxq from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:56:33.616: INFO: calico-typha-954b59468-864hr from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:56:33.616: INFO: kube-proxy-d7hzd from kube-system started at 2021-05-17 16:45:38 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:56:33.616: INFO: fluent-bit-vm2vm from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:56:33.616: INFO: alertmanager-main-0 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:56:33.616: INFO: grafana-f8cd57fcf-p45c2 from monitoring started at 2021-05-17 16:46:48 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container grafana ready: true, restart count 0
May 17 17:56:33.616: INFO: kube-state-metrics-587bfd4f97-6qs5t from monitoring started at 2021-05-17 16:46:48 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 17 17:56:33.616: INFO: node-exporter-5dkbc from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:56:33.616: INFO: prometheus-operator-7649c7454f-5xts8 from monitoring started at 2021-05-17 16:46:39 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container prometheus-operator ready: true, restart count 0
May 17 17:56:33.616: INFO: csi-cephfsplugin-cgwrr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.616: INFO: csi-cephfsplugin-provisioner-8658f67749-mj486 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.616: INFO: csi-rbdplugin-6jrsr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.616: INFO: csi-rbdplugin-provisioner-94f699d86-r49r5 from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (6 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.616: INFO: rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:56:33.616: INFO: rook-ceph-mgr-a-5db4bfbc6-fnj5z from rook-ceph started at 2021-05-17 16:49:56 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container mgr ready: true, restart count 0
May 17 17:56:33.616: INFO: rook-ceph-mon-c-545cb7776f-tsblp from rook-ceph started at 2021-05-17 16:49:42 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container mon ready: true, restart count 0
May 17 17:56:33.616: INFO: rook-ceph-osd-2-96fd479f5-88zbz from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container osd ready: true, restart count 0
May 17 17:56:33.616: INFO: rook-ceph-osd-prepare-20test-worker-1-qb5xq from rook-ceph started at 2021-05-17 17:14:22 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container provision ready: false, restart count 0
May 17 17:56:33.616: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.616: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:56:33.616: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:56:33.616: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-2 before test
May 17 17:56:33.632: INFO: calico-kube-controllers-5fb9f7f6d8-gpg2w from calico-system started at 2021-05-17 16:46:37 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 17:56:33.632: INFO: calico-node-sxlpp from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:56:33.632: INFO: calico-typha-954b59468-jks4v from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:56:33.632: INFO: coredns-74ff55c5b-hjgsp from kube-system started at 2021-05-17 16:46:42 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container coredns ready: true, restart count 0
May 17 17:56:33.632: INFO: kube-proxy-wmdvj from kube-system started at 2021-05-17 16:45:52 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:56:33.632: INFO: fluent-bit-mjvsf from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:56:33.632: INFO: alertmanager-main-2 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:56:33.632: INFO: node-exporter-gcj5f from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:56:33.632: INFO: prometheus-adapter-69b8496df6-tns5m from monitoring started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 17 17:56:33.632: INFO: prometheus-k8s-0 from monitoring started at 2021-05-17 16:47:03 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:56:33.632: INFO: csi-cephfsplugin-8lpms from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.632: INFO: csi-cephfsplugin-provisioner-8658f67749-vcx69 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.632: INFO: csi-rbdplugin-7rpck from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.632: INFO: csi-rbdplugin-provisioner-94f699d86-xznpd from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.632: INFO: rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:56:33.632: INFO: rook-ceph-mon-b-589bbc6556-w592m from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container mon ready: true, restart count 0
May 17 17:56:33.632: INFO: rook-ceph-operator-5f6ffc46c7-hj2ml from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container rook-ceph-operator ready: true, restart count 0
May 17 17:56:33.632: INFO: rook-ceph-osd-0-7896cc545c-psfck from rook-ceph started at 2021-05-17 16:50:08 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container osd ready: true, restart count 0
May 17 17:56:33.632: INFO: rook-ceph-osd-prepare-20test-worker-2-lhtcq from rook-ceph started at 2021-05-17 17:14:24 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container provision ready: false, restart count 0
May 17 17:56:33.632: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:56:33.632: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:56:33.632: INFO: tigera-operator-657cc89589-qhpft from tigera-operator started at 2021-05-17 16:46:10 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.632: INFO: 	Container tigera-operator ready: true, restart count 0
May 17 17:56:33.632: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-3 before test
May 17 17:56:33.645: INFO: calico-node-rb678 from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.645: INFO: 	Container calico-node ready: true, restart count 0
May 17 17:56:33.646: INFO: calico-typha-954b59468-v2z88 from calico-system started at 2021-05-17 16:46:16 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.646: INFO: 	Container calico-typha ready: true, restart count 0
May 17 17:56:33.646: INFO: kube-proxy-n6xv8 from kube-system started at 2021-05-17 16:45:25 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.646: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 17:56:33.646: INFO: fluent-bit-hg8tt from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.647: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 17:56:33.647: INFO: alertmanager-main-1 from monitoring started at 2021-05-17 17:14:35 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.647: INFO: 	Container alertmanager ready: true, restart count 0
May 17 17:56:33.648: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:56:33.648: INFO: node-exporter-ccd9w from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.648: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 17:56:33.648: INFO: 	Container node-exporter ready: true, restart count 0
May 17 17:56:33.649: INFO: prometheus-k8s-1 from monitoring started at 2021-05-17 17:14:37 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.649: INFO: 	Container config-reloader ready: true, restart count 0
May 17 17:56:33.649: INFO: 	Container prometheus ready: true, restart count 1
May 17 17:56:33.650: INFO: csi-cephfsplugin-9hmwc from rook-ceph started at 2021-05-17 17:14:36 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.650: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 17:56:33.650: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.650: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.651: INFO: csi-rbdplugin-9hr4g from rook-ceph started at 2021-05-17 17:14:38 +0000 UTC (3 container statuses recorded)
May 17 17:56:33.651: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 17:56:33.651: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 17:56:33.651: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 17:56:33.651: INFO: rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.652: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 17:56:33.652: INFO: rook-ceph-mon-a-6947dc47f-8wtqd from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.652: INFO: 	Container mon ready: true, restart count 0
May 17 17:56:33.652: INFO: rook-ceph-osd-1-6c8669bc9-89l9s from rook-ceph started at 2021-05-17 17:13:51 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.652: INFO: 	Container osd ready: true, restart count 0
May 17 17:56:33.653: INFO: rook-ceph-osd-prepare-20test-worker-3-cqpxz from rook-ceph started at 2021-05-17 17:14:26 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.653: INFO: 	Container provision ready: false, restart count 0
May 17 17:56:33.653: INFO: sonobuoy from sonobuoy started at 2021-05-17 16:59:52 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.653: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 17:56:33.653: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 17:56:33.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 17 17:56:33.654: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 17:56:33.654: INFO: pod-service-account-fecb9b23-419c-4b35-9439-341fc2701700 from svcaccounts-2287 started at 2021-05-17 17:56:28 +0000 UTC (1 container statuses recorded)
May 17 17:56:33.654: INFO: 	Container test ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 20test-worker-1
STEP: verifying the node has the label node 20test-worker-2
STEP: verifying the node has the label node 20test-worker-3
May 17 17:56:33.948: INFO: Pod calico-kube-controllers-5fb9f7f6d8-gpg2w requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.949: INFO: Pod calico-node-fjsxq requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.949: INFO: Pod calico-node-rb678 requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.949: INFO: Pod calico-node-sxlpp requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.949: INFO: Pod calico-typha-954b59468-864hr requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.949: INFO: Pod calico-typha-954b59468-jks4v requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.949: INFO: Pod calico-typha-954b59468-v2z88 requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.949: INFO: Pod coredns-74ff55c5b-hjgsp requesting resource cpu=100m on Node 20test-worker-2
May 17 17:56:33.949: INFO: Pod kube-proxy-d7hzd requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.949: INFO: Pod kube-proxy-n6xv8 requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.949: INFO: Pod kube-proxy-wmdvj requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.949: INFO: Pod fluent-bit-hg8tt requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.950: INFO: Pod fluent-bit-mjvsf requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.950: INFO: Pod fluent-bit-vm2vm requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node 20test-worker-3
May 17 17:56:33.950: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node 20test-worker-2
May 17 17:56:33.950: INFO: Pod grafana-f8cd57fcf-p45c2 requesting resource cpu=100m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod kube-state-metrics-587bfd4f97-6qs5t requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod node-exporter-5dkbc requesting resource cpu=112m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod node-exporter-ccd9w requesting resource cpu=112m on Node 20test-worker-3
May 17 17:56:33.950: INFO: Pod node-exporter-gcj5f requesting resource cpu=112m on Node 20test-worker-2
May 17 17:56:33.950: INFO: Pod prometheus-adapter-69b8496df6-tns5m requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.950: INFO: Pod prometheus-k8s-0 requesting resource cpu=100m on Node 20test-worker-2
May 17 17:56:33.950: INFO: Pod prometheus-k8s-1 requesting resource cpu=100m on Node 20test-worker-3
May 17 17:56:33.950: INFO: Pod prometheus-operator-7649c7454f-5xts8 requesting resource cpu=100m on Node 20test-worker-1
May 17 17:56:33.950: INFO: Pod csi-cephfsplugin-8lpms requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.951: INFO: Pod csi-cephfsplugin-9hmwc requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.951: INFO: Pod csi-cephfsplugin-cgwrr requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.951: INFO: Pod csi-cephfsplugin-provisioner-8658f67749-mj486 requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.951: INFO: Pod csi-cephfsplugin-provisioner-8658f67749-vcx69 requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.951: INFO: Pod csi-rbdplugin-6jrsr requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.951: INFO: Pod csi-rbdplugin-7rpck requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.951: INFO: Pod csi-rbdplugin-9hr4g requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.951: INFO: Pod csi-rbdplugin-provisioner-94f699d86-r49r5 requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.951: INFO: Pod csi-rbdplugin-provisioner-94f699d86-xznpd requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.951: INFO: Pod rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.951: INFO: Pod rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.951: INFO: Pod rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.951: INFO: Pod rook-ceph-mgr-a-5db4bfbc6-fnj5z requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.952: INFO: Pod rook-ceph-mon-a-6947dc47f-8wtqd requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.952: INFO: Pod rook-ceph-mon-b-589bbc6556-w592m requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.952: INFO: Pod rook-ceph-mon-c-545cb7776f-tsblp requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.952: INFO: Pod rook-ceph-operator-5f6ffc46c7-hj2ml requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.952: INFO: Pod rook-ceph-osd-0-7896cc545c-psfck requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.952: INFO: Pod rook-ceph-osd-1-6c8669bc9-89l9s requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.952: INFO: Pod rook-ceph-osd-2-96fd479f5-88zbz requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.952: INFO: Pod sonobuoy requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.952: INFO: Pod sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.952: INFO: Pod sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn requesting resource cpu=0m on Node 20test-worker-2
May 17 17:56:33.952: INFO: Pod sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb requesting resource cpu=0m on Node 20test-worker-1
May 17 17:56:33.952: INFO: Pod pod-service-account-fecb9b23-419c-4b35-9439-341fc2701700 requesting resource cpu=0m on Node 20test-worker-3
May 17 17:56:33.952: INFO: Pod tigera-operator-657cc89589-qhpft requesting resource cpu=0m on Node 20test-worker-2
STEP: Starting Pods to consume most of the cluster CPU.
May 17 17:56:33.953: INFO: Creating a pod which consumes cpu=1111m on Node 20test-worker-1
May 17 17:56:33.967: INFO: Creating a pod which consumes cpu=1111m on Node 20test-worker-2
May 17 17:56:33.980: INFO: Creating a pod which consumes cpu=1181m on Node 20test-worker-3
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a.167fec4a200618ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-112/filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a to 20test-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a.167fec4a5d8a4e3b], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a.167fec4a8a348caa], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 749.335491ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a.167fec4a8f206a37], Reason = [Created], Message = [Created container filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a.167fec4a98f892bd], Reason = [Started], Message = [Started container filler-pod-2c4c5915-71fc-4bcb-bf5a-6f21dd9a756a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd.167fec4a1f16ab33], Reason = [Scheduled], Message = [Successfully assigned sched-pred-112/filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd to 20test-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd.167fec4a5b045759], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd.167fec4a60640046], Reason = [Created], Message = [Created container filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd.167fec4a6ed8de27], Reason = [Started], Message = [Started container filler-pod-73e763b9-bf62-490d-9579-1f2b523541fd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375.167fec4a20e9a3e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-112/filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375 to 20test-worker-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375.167fec4a59d7d383], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375.167fec4a5eb4d77f], Reason = [Created], Message = [Created container filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375.167fec4a67c4aab9], Reason = [Started], Message = [Started container filler-pod-ff8cf0d7-d67c-45fc-92dc-b50561e34375]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167fec4b11798bfb], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node 20test-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 20test-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 20test-worker-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:56:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-112" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.578 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":151,"skipped":2394,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:56:39.103: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 17 17:56:39.154: INFO: Waiting up to 5m0s for pod "pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b" in namespace "emptydir-554" to be "Succeeded or Failed"
May 17 17:56:39.165: INFO: Pod "pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.89949ms
May 17 17:56:41.183: INFO: Pod "pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028836491s
STEP: Saw pod success
May 17 17:56:41.185: INFO: Pod "pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b" satisfied condition "Succeeded or Failed"
May 17 17:56:41.192: INFO: Trying to get logs from node 20test-worker-3 pod pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b container test-container: <nil>
STEP: delete the pod
May 17 17:56:41.227: INFO: Waiting for pod pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b to disappear
May 17 17:56:41.229: INFO: Pod pod-206cbc29-1cf9-4987-a6f6-d0ac03889f7b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:56:41.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-554" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2395,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:56:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-11e759b2-0321-4a22-9d30-038c60b834c8 in namespace container-probe-8643
May 17 17:56:43.316: INFO: Started pod liveness-11e759b2-0321-4a22-9d30-038c60b834c8 in namespace container-probe-8643
STEP: checking the pod's current state and verifying that restartCount is present
May 17 17:56:43.318: INFO: Initial restart count of pod liveness-11e759b2-0321-4a22-9d30-038c60b834c8 is 0
May 17 17:57:07.389: INFO: Restart count of pod container-probe-8643/liveness-11e759b2-0321-4a22-9d30-038c60b834c8 is now 1 (24.071041335s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:07.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8643" for this suite.

• [SLOW TEST:26.181 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2406,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:07.422: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:57:07.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5" in namespace "downward-api-903" to be "Succeeded or Failed"
May 17 17:57:07.483: INFO: Pod "downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.333002ms
May 17 17:57:09.488: INFO: Pod "downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016449601s
STEP: Saw pod success
May 17 17:57:09.488: INFO: Pod "downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5" satisfied condition "Succeeded or Failed"
May 17 17:57:09.490: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5 container client-container: <nil>
STEP: delete the pod
May 17 17:57:09.512: INFO: Waiting for pod downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5 to disappear
May 17 17:57:09.515: INFO: Pod downwardapi-volume-6453c232-c5bb-4e51-a61f-d3bab0c8a5d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:09.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-903" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:09.527: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 17 17:57:09.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-5204 create -f -'
May 17 17:57:09.910: INFO: stderr: ""
May 17 17:57:09.910: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 17 17:57:10.914: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:57:10.914: INFO: Found 0 / 1
May 17 17:57:11.921: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:57:11.921: INFO: Found 0 / 1
May 17 17:57:12.915: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:57:12.915: INFO: Found 1 / 1
May 17 17:57:12.915: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 17 17:57:12.917: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:57:12.917: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 17 17:57:12.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-5204 patch pod agnhost-primary-jt6sj -p {"metadata":{"annotations":{"x":"y"}}}'
May 17 17:57:13.032: INFO: stderr: ""
May 17 17:57:13.032: INFO: stdout: "pod/agnhost-primary-jt6sj patched\n"
STEP: checking annotations
May 17 17:57:13.035: INFO: Selector matched 1 pods for map[app:agnhost]
May 17 17:57:13.035: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5204" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":155,"skipped":2437,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:13.061: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 17 17:57:13.117: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:57:19.124: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:41.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4425" for this suite.

• [SLOW TEST:28.172 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":156,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:41.234: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 17 17:57:41.465: INFO: starting watch
STEP: patching
STEP: updating
May 17 17:57:41.475: INFO: waiting for watch events with expected annotations
May 17 17:57:41.475: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:41.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-7482" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":157,"skipped":2497,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:41.501: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 17 17:57:41.540: INFO: Waiting up to 5m0s for pod "pod-48fe373e-ca11-4d21-be62-10ec56326147" in namespace "emptydir-3198" to be "Succeeded or Failed"
May 17 17:57:41.549: INFO: Pod "pod-48fe373e-ca11-4d21-be62-10ec56326147": Phase="Pending", Reason="", readiness=false. Elapsed: 9.003649ms
May 17 17:57:43.552: INFO: Pod "pod-48fe373e-ca11-4d21-be62-10ec56326147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012633176s
STEP: Saw pod success
May 17 17:57:43.553: INFO: Pod "pod-48fe373e-ca11-4d21-be62-10ec56326147" satisfied condition "Succeeded or Failed"
May 17 17:57:43.554: INFO: Trying to get logs from node 20test-worker-3 pod pod-48fe373e-ca11-4d21-be62-10ec56326147 container test-container: <nil>
STEP: delete the pod
May 17 17:57:43.571: INFO: Waiting for pod pod-48fe373e-ca11-4d21-be62-10ec56326147 to disappear
May 17 17:57:43.573: INFO: Pod pod-48fe373e-ca11-4d21-be62-10ec56326147 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:43.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3198" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2505,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:43.581: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:57:43.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f" in namespace "downward-api-1217" to be "Succeeded or Failed"
May 17 17:57:43.638: INFO: Pod "downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.724443ms
May 17 17:57:45.641: INFO: Pod "downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011451258s
STEP: Saw pod success
May 17 17:57:45.641: INFO: Pod "downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f" satisfied condition "Succeeded or Failed"
May 17 17:57:45.644: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f container client-container: <nil>
STEP: delete the pod
May 17 17:57:45.679: INFO: Waiting for pod downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f to disappear
May 17 17:57:45.681: INFO: Pod downwardapi-volume-edfdaf13-62ca-49d6-9e18-c6c4aeddfd9f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:45.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1217" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2511,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:45.692: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:57:46.293: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:57:48.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871066, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871066, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871066, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871066, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:57:51.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:51.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1563" for this suite.
STEP: Destroying namespace "webhook-1563-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.763 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":160,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:51.455: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 17 17:57:51.501: INFO: created test-pod-1
May 17 17:57:51.519: INFO: created test-pod-2
May 17 17:57:51.527: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:51.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-297" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":161,"skipped":2576,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:51.622: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1b161374-8074-47ce-b2af-e132ccf64b18
STEP: Creating a pod to test consume secrets
May 17 17:57:51.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e" in namespace "projected-9671" to be "Succeeded or Failed"
May 17 17:57:51.713: INFO: Pod "pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.074613ms
May 17 17:57:53.723: INFO: Pod "pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015026393s
STEP: Saw pod success
May 17 17:57:53.723: INFO: Pod "pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e" satisfied condition "Succeeded or Failed"
May 17 17:57:53.726: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e container projected-secret-volume-test: <nil>
STEP: delete the pod
May 17 17:57:53.746: INFO: Waiting for pod pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e to disappear
May 17 17:57:53.749: INFO: Pod pod-projected-secrets-8a93d41b-98df-48fc-a5e6-9fd7dbbf368e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:53.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9671" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:53.765: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 17 17:57:53.824: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:57:53.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5344" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":163,"skipped":2623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:57:53.857: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 17 17:57:53.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 create -f -'
May 17 17:57:54.379: INFO: stderr: ""
May 17 17:57:54.379: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 17 17:57:54.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:57:54.543: INFO: stderr: ""
May 17 17:57:54.543: INFO: stdout: "update-demo-nautilus-c689b update-demo-nautilus-rmfv8 "
May 17 17:57:54.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:57:54.683: INFO: stderr: ""
May 17 17:57:54.683: INFO: stdout: ""
May 17 17:57:54.683: INFO: update-demo-nautilus-c689b is created but not running
May 17 17:57:59.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:57:59.792: INFO: stderr: ""
May 17 17:57:59.792: INFO: stdout: "update-demo-nautilus-c689b update-demo-nautilus-rmfv8 "
May 17 17:57:59.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:57:59.886: INFO: stderr: ""
May 17 17:57:59.886: INFO: stdout: "true"
May 17 17:57:59.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:57:59.980: INFO: stderr: ""
May 17 17:57:59.980: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:57:59.980: INFO: validating pod update-demo-nautilus-c689b
May 17 17:57:59.983: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:57:59.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:57:59.983: INFO: update-demo-nautilus-c689b is verified up and running
May 17 17:57:59.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-rmfv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:00.079: INFO: stderr: ""
May 17 17:58:00.079: INFO: stdout: "true"
May 17 17:58:00.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-rmfv8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:58:00.165: INFO: stderr: ""
May 17 17:58:00.165: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:58:00.165: INFO: validating pod update-demo-nautilus-rmfv8
May 17 17:58:00.168: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:58:00.168: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:58:00.168: INFO: update-demo-nautilus-rmfv8 is verified up and running
STEP: scaling down the replication controller
May 17 17:58:00.171: INFO: scanned /root for discovery docs: <nil>
May 17 17:58:00.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 17 17:58:00.306: INFO: stderr: ""
May 17 17:58:00.306: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 17 17:58:00.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:58:00.418: INFO: stderr: ""
May 17 17:58:00.418: INFO: stdout: "update-demo-nautilus-c689b update-demo-nautilus-rmfv8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 17 17:58:05.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:58:05.535: INFO: stderr: ""
May 17 17:58:05.535: INFO: stdout: "update-demo-nautilus-c689b "
May 17 17:58:05.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:05.632: INFO: stderr: ""
May 17 17:58:05.632: INFO: stdout: "true"
May 17 17:58:05.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:58:05.733: INFO: stderr: ""
May 17 17:58:05.733: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:58:05.733: INFO: validating pod update-demo-nautilus-c689b
May 17 17:58:05.737: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:58:05.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:58:05.737: INFO: update-demo-nautilus-c689b is verified up and running
STEP: scaling up the replication controller
May 17 17:58:05.740: INFO: scanned /root for discovery docs: <nil>
May 17 17:58:05.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 17 17:58:05.882: INFO: stderr: ""
May 17 17:58:05.882: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 17 17:58:05.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:58:05.973: INFO: stderr: ""
May 17 17:58:05.973: INFO: stdout: "update-demo-nautilus-c689b update-demo-nautilus-rmpqd "
May 17 17:58:05.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:06.071: INFO: stderr: ""
May 17 17:58:06.071: INFO: stdout: "true"
May 17 17:58:06.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:58:06.195: INFO: stderr: ""
May 17 17:58:06.195: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:58:06.195: INFO: validating pod update-demo-nautilus-c689b
May 17 17:58:06.198: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:58:06.198: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:58:06.199: INFO: update-demo-nautilus-c689b is verified up and running
May 17 17:58:06.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-rmpqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:06.329: INFO: stderr: ""
May 17 17:58:06.329: INFO: stdout: ""
May 17 17:58:06.329: INFO: update-demo-nautilus-rmpqd is created but not running
May 17 17:58:11.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 17:58:11.424: INFO: stderr: ""
May 17 17:58:11.424: INFO: stdout: "update-demo-nautilus-c689b update-demo-nautilus-rmpqd "
May 17 17:58:11.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:11.541: INFO: stderr: ""
May 17 17:58:11.541: INFO: stdout: "true"
May 17 17:58:11.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-c689b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:58:11.641: INFO: stderr: ""
May 17 17:58:11.641: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:58:11.641: INFO: validating pod update-demo-nautilus-c689b
May 17 17:58:11.644: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:58:11.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:58:11.644: INFO: update-demo-nautilus-c689b is verified up and running
May 17 17:58:11.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-rmpqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 17:58:11.731: INFO: stderr: ""
May 17 17:58:11.731: INFO: stdout: "true"
May 17 17:58:11.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods update-demo-nautilus-rmpqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 17:58:11.832: INFO: stderr: ""
May 17 17:58:11.832: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 17:58:11.832: INFO: validating pod update-demo-nautilus-rmpqd
May 17 17:58:11.839: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 17:58:11.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 17:58:11.839: INFO: update-demo-nautilus-rmpqd is verified up and running
STEP: using delete to clean up resources
May 17 17:58:11.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 delete --grace-period=0 --force -f -'
May 17 17:58:11.977: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 17:58:11.977: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 17:58:11.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get rc,svc -l name=update-demo --no-headers'
May 17 17:58:12.218: INFO: stderr: "No resources found in kubectl-3972 namespace.\n"
May 17 17:58:12.218: INFO: stdout: ""
May 17 17:58:12.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-3972 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 17:58:12.426: INFO: stderr: ""
May 17 17:58:12.426: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:58:12.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3972" for this suite.

• [SLOW TEST:18.597 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":164,"skipped":2646,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:58:12.459: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 17:58:12.539: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9478
I0517 17:58:12.808135      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9478, replica count: 1
I0517 17:58:13.858524      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 17:58:14.859213      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 17:58:14.977: INFO: Created: latency-svc-4scr7
May 17 17:58:14.980: INFO: Got endpoints: latency-svc-4scr7 [20.992793ms]
May 17 17:58:15.004: INFO: Created: latency-svc-pq79c
May 17 17:58:15.022: INFO: Got endpoints: latency-svc-pq79c [41.06099ms]
May 17 17:58:15.029: INFO: Created: latency-svc-qsqwl
May 17 17:58:15.029: INFO: Created: latency-svc-f4gj7
May 17 17:58:15.030: INFO: Got endpoints: latency-svc-f4gj7 [48.775601ms]
May 17 17:58:15.034: INFO: Created: latency-svc-ksmxn
May 17 17:58:15.060: INFO: Got endpoints: latency-svc-qsqwl [77.743899ms]
May 17 17:58:15.080: INFO: Created: latency-svc-wskqx
May 17 17:58:15.080: INFO: Got endpoints: latency-svc-ksmxn [99.132827ms]
May 17 17:58:15.098: INFO: Created: latency-svc-l45b6
May 17 17:58:15.103: INFO: Created: latency-svc-7b6jz
May 17 17:58:15.109: INFO: Got endpoints: latency-svc-wskqx [127.465951ms]
May 17 17:58:15.122: INFO: Got endpoints: latency-svc-l45b6 [140.738265ms]
May 17 17:58:15.131: INFO: Created: latency-svc-dnhpb
May 17 17:58:15.131: INFO: Created: latency-svc-q99rf
May 17 17:58:15.131: INFO: Got endpoints: latency-svc-q99rf [149.098768ms]
May 17 17:58:15.137: INFO: Got endpoints: latency-svc-7b6jz [154.747405ms]
May 17 17:58:15.137: INFO: Got endpoints: latency-svc-dnhpb [155.068049ms]
May 17 17:58:15.149: INFO: Created: latency-svc-bfzrj
May 17 17:58:15.150: INFO: Created: latency-svc-pm7xl
May 17 17:58:15.150: INFO: Got endpoints: latency-svc-pm7xl [160.726466ms]
May 17 17:58:15.168: INFO: Created: latency-svc-qckbc
May 17 17:58:15.168: INFO: Got endpoints: latency-svc-qckbc [177.679326ms]
May 17 17:58:15.168: INFO: Got endpoints: latency-svc-bfzrj [178.111554ms]
May 17 17:58:15.196: INFO: Created: latency-svc-4x2r2
May 17 17:58:15.218: INFO: Got endpoints: latency-svc-4x2r2 [228.184241ms]
May 17 17:58:15.230: INFO: Created: latency-svc-c5x6x
May 17 17:58:15.241: INFO: Got endpoints: latency-svc-c5x6x [251.146204ms]
May 17 17:58:15.245: INFO: Created: latency-svc-pk6mg
May 17 17:58:15.294: INFO: Created: latency-svc-9qkgp
May 17 17:58:15.295: INFO: Created: latency-svc-wbrxf
May 17 17:58:15.295: INFO: Got endpoints: latency-svc-wbrxf [265.043629ms]
May 17 17:58:15.295: INFO: Created: latency-svc-jmgk5
May 17 17:58:15.295: INFO: Got endpoints: latency-svc-jmgk5 [234.864744ms]
May 17 17:58:15.295: INFO: Got endpoints: latency-svc-9qkgp [273.018133ms]
May 17 17:58:15.295: INFO: Got endpoints: latency-svc-pk6mg [314.222301ms]
May 17 17:58:15.307: INFO: Created: latency-svc-vb47q
May 17 17:58:15.341: INFO: Created: latency-svc-m97k7
May 17 17:58:15.351: INFO: Created: latency-svc-gt5jq
May 17 17:58:15.354: INFO: Got endpoints: latency-svc-gt5jq [232.416905ms]
May 17 17:58:15.359: INFO: Got endpoints: latency-svc-vb47q [278.313225ms]
May 17 17:58:15.363: INFO: Created: latency-svc-7wkhc
May 17 17:58:15.374: INFO: Got endpoints: latency-svc-7wkhc [236.144067ms]
May 17 17:58:15.364: INFO: Created: latency-svc-kvqpn
May 17 17:58:15.375: INFO: Got endpoints: latency-svc-kvqpn [245.479642ms]
May 17 17:58:15.373: INFO: Got endpoints: latency-svc-m97k7 [241.38108ms]
May 17 17:58:15.374: INFO: Created: latency-svc-kngmx
May 17 17:58:15.401: INFO: Got endpoints: latency-svc-kngmx [264.078983ms]
May 17 17:58:15.427: INFO: Created: latency-svc-8zk8t
May 17 17:58:15.427: INFO: Created: latency-svc-f594f
May 17 17:58:15.427: INFO: Created: latency-svc-ghmqv
May 17 17:58:15.427: INFO: Got endpoints: latency-svc-8zk8t [276.861622ms]
May 17 17:58:15.434: INFO: Created: latency-svc-mshg2
May 17 17:58:15.441: INFO: Got endpoints: latency-svc-ghmqv [273.527003ms]
May 17 17:58:15.445: INFO: Got endpoints: latency-svc-f594f [276.789243ms]
May 17 17:58:15.445: INFO: Got endpoints: latency-svc-mshg2 [226.875532ms]
May 17 17:58:15.446: INFO: Created: latency-svc-s7zml
May 17 17:58:15.460: INFO: Created: latency-svc-n56rz
May 17 17:58:15.462: INFO: Got endpoints: latency-svc-s7zml [220.801197ms]
May 17 17:58:15.471: INFO: Created: latency-svc-pz5h7
May 17 17:58:15.485: INFO: Created: latency-svc-bmznn
May 17 17:58:15.485: INFO: Got endpoints: latency-svc-pz5h7 [189.521019ms]
May 17 17:58:15.488: INFO: Got endpoints: latency-svc-n56rz [192.611177ms]
May 17 17:58:15.503: INFO: Got endpoints: latency-svc-bmznn [208.28467ms]
May 17 17:58:15.511: INFO: Created: latency-svc-dfhtq
May 17 17:58:15.511: INFO: Got endpoints: latency-svc-dfhtq [213.810679ms]
May 17 17:58:15.520: INFO: Created: latency-svc-z95wb
May 17 17:58:15.537: INFO: Got endpoints: latency-svc-z95wb [183.682378ms]
May 17 17:58:15.560: INFO: Created: latency-svc-bm9z5
May 17 17:58:15.564: INFO: Created: latency-svc-snzbs
May 17 17:58:15.565: INFO: Created: latency-svc-nsn5n
May 17 17:58:15.565: INFO: Got endpoints: latency-svc-nsn5n [190.72752ms]
May 17 17:58:15.566: INFO: Got endpoints: latency-svc-snzbs [207.17336ms]
May 17 17:58:15.576: INFO: Created: latency-svc-ffnjr
May 17 17:58:15.576: INFO: Got endpoints: latency-svc-bm9z5 [200.4597ms]
May 17 17:58:15.580: INFO: Got endpoints: latency-svc-ffnjr [203.387207ms]
May 17 17:58:15.589: INFO: Created: latency-svc-6bwnk
May 17 17:58:15.598: INFO: Got endpoints: latency-svc-6bwnk [183.176385ms]
May 17 17:58:15.604: INFO: Created: latency-svc-rr66r
May 17 17:58:15.607: INFO: Created: latency-svc-q7dhh
May 17 17:58:15.608: INFO: Got endpoints: latency-svc-rr66r [166.667268ms]
May 17 17:58:15.608: INFO: Got endpoints: latency-svc-q7dhh [181.431348ms]
May 17 17:58:15.615: INFO: Created: latency-svc-pc88q
May 17 17:58:15.634: INFO: Got endpoints: latency-svc-pc88q [189.19707ms]
May 17 17:58:15.642: INFO: Created: latency-svc-2sqrs
May 17 17:58:15.645: INFO: Created: latency-svc-k2t2n
May 17 17:58:15.645: INFO: Created: latency-svc-s24dk
May 17 17:58:15.645: INFO: Created: latency-svc-gwk4n
May 17 17:58:15.652: INFO: Created: latency-svc-8nf56
May 17 17:58:15.652: INFO: Created: latency-svc-668c5
May 17 17:58:15.661: INFO: Created: latency-svc-w95x9
May 17 17:58:15.661: INFO: Created: latency-svc-xfjfv
May 17 17:58:15.667: INFO: Created: latency-svc-g22pm
May 17 17:58:15.674: INFO: Created: latency-svc-qsn9m
May 17 17:58:15.678: INFO: Got endpoints: latency-svc-gwk4n [232.945898ms]
May 17 17:58:15.690: INFO: Created: latency-svc-k95pz
May 17 17:58:15.697: INFO: Created: latency-svc-d7bdh
May 17 17:58:15.721: INFO: Created: latency-svc-jkhc4
May 17 17:58:15.730: INFO: Created: latency-svc-kwts4
May 17 17:58:15.731: INFO: Got endpoints: latency-svc-k2t2n [268.592944ms]
May 17 17:58:15.738: INFO: Created: latency-svc-dbxdf
May 17 17:58:15.744: INFO: Created: latency-svc-nhtb6
May 17 17:58:15.749: INFO: Created: latency-svc-fsm66
May 17 17:58:15.780: INFO: Got endpoints: latency-svc-2sqrs [294.607747ms]
May 17 17:58:15.789: INFO: Created: latency-svc-hmd7w
May 17 17:58:15.836: INFO: Got endpoints: latency-svc-s24dk [347.652162ms]
May 17 17:58:15.848: INFO: Created: latency-svc-c6x9f
May 17 17:58:15.875: INFO: Got endpoints: latency-svc-668c5 [371.841221ms]
May 17 17:58:15.884: INFO: Created: latency-svc-kp4dz
May 17 17:58:15.929: INFO: Got endpoints: latency-svc-8nf56 [417.289337ms]
May 17 17:58:15.938: INFO: Created: latency-svc-mhw7v
May 17 17:58:15.977: INFO: Got endpoints: latency-svc-w95x9 [439.597718ms]
May 17 17:58:16.013: INFO: Created: latency-svc-7slfs
May 17 17:58:16.029: INFO: Got endpoints: latency-svc-xfjfv [463.306898ms]
May 17 17:58:16.039: INFO: Created: latency-svc-vxdjj
May 17 17:58:16.081: INFO: Got endpoints: latency-svc-g22pm [514.632918ms]
May 17 17:58:16.090: INFO: Created: latency-svc-6htqc
May 17 17:58:16.130: INFO: Got endpoints: latency-svc-qsn9m [554.165119ms]
May 17 17:58:16.141: INFO: Created: latency-svc-54npx
May 17 17:58:16.178: INFO: Got endpoints: latency-svc-k95pz [598.534027ms]
May 17 17:58:16.187: INFO: Created: latency-svc-tgw57
May 17 17:58:16.227: INFO: Got endpoints: latency-svc-d7bdh [627.999821ms]
May 17 17:58:16.235: INFO: Created: latency-svc-w9tss
May 17 17:58:16.288: INFO: Got endpoints: latency-svc-jkhc4 [679.113974ms]
May 17 17:58:16.300: INFO: Created: latency-svc-bj75x
May 17 17:58:16.332: INFO: Got endpoints: latency-svc-kwts4 [723.762876ms]
May 17 17:58:16.345: INFO: Created: latency-svc-8hbfq
May 17 17:58:16.377: INFO: Got endpoints: latency-svc-dbxdf [742.443466ms]
May 17 17:58:16.385: INFO: Created: latency-svc-4b897
May 17 17:58:16.428: INFO: Got endpoints: latency-svc-nhtb6 [749.717995ms]
May 17 17:58:16.436: INFO: Created: latency-svc-gdxdr
May 17 17:58:16.477: INFO: Got endpoints: latency-svc-fsm66 [745.324362ms]
May 17 17:58:16.485: INFO: Created: latency-svc-qjjs2
May 17 17:58:16.526: INFO: Got endpoints: latency-svc-hmd7w [745.947503ms]
May 17 17:58:16.536: INFO: Created: latency-svc-zdsfm
May 17 17:58:16.579: INFO: Got endpoints: latency-svc-c6x9f [742.684623ms]
May 17 17:58:16.590: INFO: Created: latency-svc-x585m
May 17 17:58:16.625: INFO: Got endpoints: latency-svc-kp4dz [749.632823ms]
May 17 17:58:16.639: INFO: Created: latency-svc-pg8f6
May 17 17:58:16.676: INFO: Got endpoints: latency-svc-mhw7v [747.051497ms]
May 17 17:58:16.695: INFO: Created: latency-svc-jtjtc
May 17 17:58:16.728: INFO: Got endpoints: latency-svc-7slfs [750.643382ms]
May 17 17:58:16.738: INFO: Created: latency-svc-c9th5
May 17 17:58:16.780: INFO: Got endpoints: latency-svc-vxdjj [751.074577ms]
May 17 17:58:16.791: INFO: Created: latency-svc-9p9t6
May 17 17:58:16.832: INFO: Got endpoints: latency-svc-6htqc [750.937164ms]
May 17 17:58:16.844: INFO: Created: latency-svc-svn9c
May 17 17:58:16.880: INFO: Got endpoints: latency-svc-54npx [749.129002ms]
May 17 17:58:16.892: INFO: Created: latency-svc-qdr5f
May 17 17:58:16.926: INFO: Got endpoints: latency-svc-tgw57 [747.577352ms]
May 17 17:58:16.935: INFO: Created: latency-svc-q2s94
May 17 17:58:16.978: INFO: Got endpoints: latency-svc-w9tss [751.136622ms]
May 17 17:58:16.990: INFO: Created: latency-svc-hg4dm
May 17 17:58:17.029: INFO: Got endpoints: latency-svc-bj75x [741.219844ms]
May 17 17:58:17.044: INFO: Created: latency-svc-ngbmw
May 17 17:58:17.080: INFO: Got endpoints: latency-svc-8hbfq [747.724212ms]
May 17 17:58:17.115: INFO: Created: latency-svc-96p45
May 17 17:58:17.125: INFO: Got endpoints: latency-svc-4b897 [748.448429ms]
May 17 17:58:17.149: INFO: Created: latency-svc-pmbdd
May 17 17:58:17.180: INFO: Got endpoints: latency-svc-gdxdr [750.992235ms]
May 17 17:58:17.191: INFO: Created: latency-svc-tbfvw
May 17 17:58:17.229: INFO: Got endpoints: latency-svc-qjjs2 [751.453815ms]
May 17 17:58:17.245: INFO: Created: latency-svc-gp6tz
May 17 17:58:17.277: INFO: Got endpoints: latency-svc-zdsfm [751.152339ms]
May 17 17:58:17.286: INFO: Created: latency-svc-p5zs2
May 17 17:58:17.332: INFO: Got endpoints: latency-svc-x585m [753.036406ms]
May 17 17:58:17.339: INFO: Created: latency-svc-vf4xl
May 17 17:58:17.375: INFO: Got endpoints: latency-svc-pg8f6 [748.528053ms]
May 17 17:58:17.387: INFO: Created: latency-svc-7sqb9
May 17 17:58:17.427: INFO: Got endpoints: latency-svc-jtjtc [751.046089ms]
May 17 17:58:17.436: INFO: Created: latency-svc-ctq4r
May 17 17:58:17.477: INFO: Got endpoints: latency-svc-c9th5 [749.553669ms]
May 17 17:58:17.489: INFO: Created: latency-svc-7x5v6
May 17 17:58:17.527: INFO: Got endpoints: latency-svc-9p9t6 [747.345109ms]
May 17 17:58:17.538: INFO: Created: latency-svc-c5crc
May 17 17:58:17.577: INFO: Got endpoints: latency-svc-svn9c [744.303699ms]
May 17 17:58:17.588: INFO: Created: latency-svc-f6ls2
May 17 17:58:17.629: INFO: Got endpoints: latency-svc-qdr5f [748.897343ms]
May 17 17:58:17.649: INFO: Created: latency-svc-q2q6g
May 17 17:58:17.681: INFO: Got endpoints: latency-svc-q2s94 [754.259889ms]
May 17 17:58:17.708: INFO: Created: latency-svc-6rl8c
May 17 17:58:17.731: INFO: Got endpoints: latency-svc-hg4dm [753.22107ms]
May 17 17:58:17.741: INFO: Created: latency-svc-h5jdm
May 17 17:58:17.779: INFO: Got endpoints: latency-svc-ngbmw [749.636386ms]
May 17 17:58:17.791: INFO: Created: latency-svc-5hc6g
May 17 17:58:17.827: INFO: Got endpoints: latency-svc-96p45 [746.404421ms]
May 17 17:58:17.837: INFO: Created: latency-svc-8qqwp
May 17 17:58:17.882: INFO: Got endpoints: latency-svc-pmbdd [753.321925ms]
May 17 17:58:17.892: INFO: Created: latency-svc-cmts6
May 17 17:58:17.930: INFO: Got endpoints: latency-svc-tbfvw [750.469498ms]
May 17 17:58:17.940: INFO: Created: latency-svc-v82bb
May 17 17:58:17.978: INFO: Got endpoints: latency-svc-gp6tz [749.163783ms]
May 17 17:58:17.991: INFO: Created: latency-svc-c45zb
May 17 17:58:18.029: INFO: Got endpoints: latency-svc-p5zs2 [752.187991ms]
May 17 17:58:18.043: INFO: Created: latency-svc-g6n8s
May 17 17:58:18.078: INFO: Got endpoints: latency-svc-vf4xl [746.619246ms]
May 17 17:58:18.099: INFO: Created: latency-svc-2zp8l
May 17 17:58:18.129: INFO: Got endpoints: latency-svc-7sqb9 [753.271796ms]
May 17 17:58:18.139: INFO: Created: latency-svc-2qtkj
May 17 17:58:18.178: INFO: Got endpoints: latency-svc-ctq4r [751.43435ms]
May 17 17:58:18.190: INFO: Created: latency-svc-8brbc
May 17 17:58:18.226: INFO: Got endpoints: latency-svc-7x5v6 [748.492064ms]
May 17 17:58:18.237: INFO: Created: latency-svc-cnhln
May 17 17:58:18.281: INFO: Got endpoints: latency-svc-c5crc [753.296199ms]
May 17 17:58:18.296: INFO: Created: latency-svc-dcbq8
May 17 17:58:18.335: INFO: Got endpoints: latency-svc-f6ls2 [757.456709ms]
May 17 17:58:18.347: INFO: Created: latency-svc-tw8df
May 17 17:58:18.378: INFO: Got endpoints: latency-svc-q2q6g [749.609166ms]
May 17 17:58:18.389: INFO: Created: latency-svc-8n24g
May 17 17:58:18.441: INFO: Got endpoints: latency-svc-6rl8c [760.584285ms]
May 17 17:58:18.453: INFO: Created: latency-svc-bspvf
May 17 17:58:18.475: INFO: Got endpoints: latency-svc-h5jdm [743.626156ms]
May 17 17:58:18.485: INFO: Created: latency-svc-bn4k6
May 17 17:58:18.529: INFO: Got endpoints: latency-svc-5hc6g [748.334465ms]
May 17 17:58:18.539: INFO: Created: latency-svc-l6fjz
May 17 17:58:18.580: INFO: Got endpoints: latency-svc-8qqwp [753.009215ms]
May 17 17:58:18.595: INFO: Created: latency-svc-27xhq
May 17 17:58:18.631: INFO: Got endpoints: latency-svc-cmts6 [748.399891ms]
May 17 17:58:18.645: INFO: Created: latency-svc-gqtdw
May 17 17:58:18.677: INFO: Got endpoints: latency-svc-v82bb [746.413509ms]
May 17 17:58:18.691: INFO: Created: latency-svc-btjht
May 17 17:58:18.728: INFO: Got endpoints: latency-svc-c45zb [749.878053ms]
May 17 17:58:18.738: INFO: Created: latency-svc-df9jv
May 17 17:58:18.777: INFO: Got endpoints: latency-svc-g6n8s [746.940604ms]
May 17 17:58:18.785: INFO: Created: latency-svc-h5tqk
May 17 17:58:18.827: INFO: Got endpoints: latency-svc-2zp8l [748.412684ms]
May 17 17:58:18.835: INFO: Created: latency-svc-lx65g
May 17 17:58:18.875: INFO: Got endpoints: latency-svc-2qtkj [746.47298ms]
May 17 17:58:18.885: INFO: Created: latency-svc-2pdsr
May 17 17:58:18.925: INFO: Got endpoints: latency-svc-8brbc [746.459492ms]
May 17 17:58:18.935: INFO: Created: latency-svc-trhlg
May 17 17:58:18.977: INFO: Got endpoints: latency-svc-cnhln [750.472595ms]
May 17 17:58:18.987: INFO: Created: latency-svc-cfxmh
May 17 17:58:19.032: INFO: Got endpoints: latency-svc-dcbq8 [750.869375ms]
May 17 17:58:19.045: INFO: Created: latency-svc-976qw
May 17 17:58:19.084: INFO: Got endpoints: latency-svc-tw8df [749.049278ms]
May 17 17:58:19.109: INFO: Created: latency-svc-br9nk
May 17 17:58:19.133: INFO: Got endpoints: latency-svc-8n24g [754.547196ms]
May 17 17:58:19.141: INFO: Created: latency-svc-dd2z9
May 17 17:58:19.176: INFO: Got endpoints: latency-svc-bspvf [734.376798ms]
May 17 17:58:19.183: INFO: Created: latency-svc-mq2xt
May 17 17:58:19.228: INFO: Got endpoints: latency-svc-bn4k6 [752.917366ms]
May 17 17:58:19.238: INFO: Created: latency-svc-bdjnk
May 17 17:58:19.280: INFO: Got endpoints: latency-svc-l6fjz [750.990327ms]
May 17 17:58:19.290: INFO: Created: latency-svc-jbnl6
May 17 17:58:19.330: INFO: Got endpoints: latency-svc-27xhq [749.917138ms]
May 17 17:58:19.341: INFO: Created: latency-svc-7zdbn
May 17 17:58:19.377: INFO: Got endpoints: latency-svc-gqtdw [745.629504ms]
May 17 17:58:19.386: INFO: Created: latency-svc-h8r7c
May 17 17:58:19.428: INFO: Got endpoints: latency-svc-btjht [748.547376ms]
May 17 17:58:19.448: INFO: Created: latency-svc-8bw2j
May 17 17:58:19.479: INFO: Got endpoints: latency-svc-df9jv [750.428764ms]
May 17 17:58:19.489: INFO: Created: latency-svc-r9crg
May 17 17:58:19.525: INFO: Got endpoints: latency-svc-h5tqk [747.932819ms]
May 17 17:58:19.535: INFO: Created: latency-svc-x4f27
May 17 17:58:19.579: INFO: Got endpoints: latency-svc-lx65g [751.594176ms]
May 17 17:58:19.590: INFO: Created: latency-svc-qrjt9
May 17 17:58:19.626: INFO: Got endpoints: latency-svc-2pdsr [750.946909ms]
May 17 17:58:19.637: INFO: Created: latency-svc-xwjhk
May 17 17:58:19.677: INFO: Got endpoints: latency-svc-trhlg [751.902162ms]
May 17 17:58:19.686: INFO: Created: latency-svc-wchxr
May 17 17:58:19.729: INFO: Got endpoints: latency-svc-cfxmh [752.106819ms]
May 17 17:58:19.738: INFO: Created: latency-svc-lwqcr
May 17 17:58:19.776: INFO: Got endpoints: latency-svc-976qw [744.233719ms]
May 17 17:58:19.792: INFO: Created: latency-svc-vzp4q
May 17 17:58:19.828: INFO: Got endpoints: latency-svc-br9nk [744.086159ms]
May 17 17:58:19.840: INFO: Created: latency-svc-xp9sx
May 17 17:58:19.879: INFO: Got endpoints: latency-svc-dd2z9 [745.988842ms]
May 17 17:58:19.889: INFO: Created: latency-svc-mr4rk
May 17 17:58:19.926: INFO: Got endpoints: latency-svc-mq2xt [750.403666ms]
May 17 17:58:19.934: INFO: Created: latency-svc-4dtz5
May 17 17:58:19.974: INFO: Got endpoints: latency-svc-bdjnk [745.261928ms]
May 17 17:58:19.987: INFO: Created: latency-svc-x6pwk
May 17 17:58:20.027: INFO: Got endpoints: latency-svc-jbnl6 [746.722864ms]
May 17 17:58:20.040: INFO: Created: latency-svc-zgqvv
May 17 17:58:20.078: INFO: Got endpoints: latency-svc-7zdbn [747.509152ms]
May 17 17:58:20.089: INFO: Created: latency-svc-cg5bk
May 17 17:58:20.128: INFO: Got endpoints: latency-svc-h8r7c [751.053038ms]
May 17 17:58:20.138: INFO: Created: latency-svc-ng8b5
May 17 17:58:20.177: INFO: Got endpoints: latency-svc-8bw2j [749.389385ms]
May 17 17:58:20.187: INFO: Created: latency-svc-7f9hq
May 17 17:58:20.226: INFO: Got endpoints: latency-svc-r9crg [746.258751ms]
May 17 17:58:20.237: INFO: Created: latency-svc-jktnt
May 17 17:58:20.285: INFO: Got endpoints: latency-svc-x4f27 [760.288505ms]
May 17 17:58:20.309: INFO: Created: latency-svc-d8c7d
May 17 17:58:20.337: INFO: Got endpoints: latency-svc-qrjt9 [758.453889ms]
May 17 17:58:20.347: INFO: Created: latency-svc-gzfd4
May 17 17:58:20.377: INFO: Got endpoints: latency-svc-xwjhk [750.485231ms]
May 17 17:58:20.388: INFO: Created: latency-svc-2crjx
May 17 17:58:20.427: INFO: Got endpoints: latency-svc-wchxr [750.401862ms]
May 17 17:58:20.441: INFO: Created: latency-svc-26l7j
May 17 17:58:20.476: INFO: Got endpoints: latency-svc-lwqcr [747.735065ms]
May 17 17:58:20.485: INFO: Created: latency-svc-xn8g6
May 17 17:58:20.527: INFO: Got endpoints: latency-svc-vzp4q [750.920417ms]
May 17 17:58:20.537: INFO: Created: latency-svc-tg4wb
May 17 17:58:20.577: INFO: Got endpoints: latency-svc-xp9sx [748.431308ms]
May 17 17:58:20.587: INFO: Created: latency-svc-hkpdn
May 17 17:58:20.628: INFO: Got endpoints: latency-svc-mr4rk [748.876559ms]
May 17 17:58:20.639: INFO: Created: latency-svc-fzz94
May 17 17:58:20.684: INFO: Got endpoints: latency-svc-4dtz5 [757.833268ms]
May 17 17:58:20.698: INFO: Created: latency-svc-82g8n
May 17 17:58:20.726: INFO: Got endpoints: latency-svc-x6pwk [751.820875ms]
May 17 17:58:20.737: INFO: Created: latency-svc-q9fvq
May 17 17:58:20.777: INFO: Got endpoints: latency-svc-zgqvv [750.674243ms]
May 17 17:58:20.788: INFO: Created: latency-svc-42knp
May 17 17:58:20.828: INFO: Got endpoints: latency-svc-cg5bk [750.578344ms]
May 17 17:58:20.840: INFO: Created: latency-svc-x8684
May 17 17:58:20.876: INFO: Got endpoints: latency-svc-ng8b5 [748.046887ms]
May 17 17:58:20.888: INFO: Created: latency-svc-x6mg4
May 17 17:58:20.929: INFO: Got endpoints: latency-svc-7f9hq [751.674967ms]
May 17 17:58:20.939: INFO: Created: latency-svc-lb22m
May 17 17:58:20.981: INFO: Got endpoints: latency-svc-jktnt [754.827814ms]
May 17 17:58:20.994: INFO: Created: latency-svc-tgr98
May 17 17:58:21.030: INFO: Got endpoints: latency-svc-d8c7d [744.02029ms]
May 17 17:58:21.047: INFO: Created: latency-svc-ztlsb
May 17 17:58:21.081: INFO: Got endpoints: latency-svc-gzfd4 [743.630892ms]
May 17 17:58:21.094: INFO: Created: latency-svc-zrl9f
May 17 17:58:21.152: INFO: Got endpoints: latency-svc-2crjx [774.529314ms]
May 17 17:58:21.163: INFO: Created: latency-svc-rqkcj
May 17 17:58:21.182: INFO: Got endpoints: latency-svc-26l7j [753.982107ms]
May 17 17:58:21.195: INFO: Created: latency-svc-tfszs
May 17 17:58:21.226: INFO: Got endpoints: latency-svc-xn8g6 [749.915444ms]
May 17 17:58:21.235: INFO: Created: latency-svc-nbjsg
May 17 17:58:21.277: INFO: Got endpoints: latency-svc-tg4wb [749.107837ms]
May 17 17:58:21.286: INFO: Created: latency-svc-g6m7w
May 17 17:58:21.331: INFO: Got endpoints: latency-svc-hkpdn [753.429473ms]
May 17 17:58:21.346: INFO: Created: latency-svc-qm8gf
May 17 17:58:21.382: INFO: Got endpoints: latency-svc-fzz94 [753.927001ms]
May 17 17:58:21.394: INFO: Created: latency-svc-fx5np
May 17 17:58:21.430: INFO: Got endpoints: latency-svc-82g8n [745.90916ms]
May 17 17:58:21.440: INFO: Created: latency-svc-j26df
May 17 17:58:21.479: INFO: Got endpoints: latency-svc-q9fvq [752.551348ms]
May 17 17:58:21.487: INFO: Created: latency-svc-cd8b6
May 17 17:58:21.527: INFO: Got endpoints: latency-svc-42knp [749.770308ms]
May 17 17:58:21.536: INFO: Created: latency-svc-kml6l
May 17 17:58:21.578: INFO: Got endpoints: latency-svc-x8684 [749.144891ms]
May 17 17:58:21.588: INFO: Created: latency-svc-6pcdm
May 17 17:58:21.629: INFO: Got endpoints: latency-svc-x6mg4 [752.858384ms]
May 17 17:58:21.641: INFO: Created: latency-svc-rzbj5
May 17 17:58:21.675: INFO: Got endpoints: latency-svc-lb22m [745.371914ms]
May 17 17:58:21.687: INFO: Created: latency-svc-kjr9q
May 17 17:58:21.729: INFO: Got endpoints: latency-svc-tgr98 [747.516661ms]
May 17 17:58:21.742: INFO: Created: latency-svc-g6s7p
May 17 17:58:21.780: INFO: Got endpoints: latency-svc-ztlsb [749.748821ms]
May 17 17:58:21.796: INFO: Created: latency-svc-tj5h4
May 17 17:58:21.829: INFO: Got endpoints: latency-svc-zrl9f [747.88041ms]
May 17 17:58:21.839: INFO: Created: latency-svc-6xrc6
May 17 17:58:21.877: INFO: Got endpoints: latency-svc-rqkcj [725.139697ms]
May 17 17:58:21.904: INFO: Created: latency-svc-ffr2c
May 17 17:58:21.931: INFO: Got endpoints: latency-svc-tfszs [748.399785ms]
May 17 17:58:21.959: INFO: Created: latency-svc-zqsvm
May 17 17:58:21.978: INFO: Got endpoints: latency-svc-nbjsg [751.870868ms]
May 17 17:58:21.990: INFO: Created: latency-svc-77b2z
May 17 17:58:22.031: INFO: Got endpoints: latency-svc-g6m7w [753.446153ms]
May 17 17:58:22.041: INFO: Created: latency-svc-psx8v
May 17 17:58:22.077: INFO: Got endpoints: latency-svc-qm8gf [744.59913ms]
May 17 17:58:22.088: INFO: Created: latency-svc-f4k9c
May 17 17:58:22.139: INFO: Got endpoints: latency-svc-fx5np [757.055665ms]
May 17 17:58:22.154: INFO: Created: latency-svc-mhbms
May 17 17:58:22.177: INFO: Got endpoints: latency-svc-j26df [746.411262ms]
May 17 17:58:22.189: INFO: Created: latency-svc-p4whz
May 17 17:58:22.230: INFO: Got endpoints: latency-svc-cd8b6 [751.682863ms]
May 17 17:58:22.243: INFO: Created: latency-svc-6lt9k
May 17 17:58:22.278: INFO: Got endpoints: latency-svc-kml6l [749.964302ms]
May 17 17:58:22.290: INFO: Created: latency-svc-wd74f
May 17 17:58:22.335: INFO: Got endpoints: latency-svc-6pcdm [756.994135ms]
May 17 17:58:22.343: INFO: Created: latency-svc-tjqng
May 17 17:58:22.388: INFO: Got endpoints: latency-svc-rzbj5 [758.006215ms]
May 17 17:58:22.399: INFO: Created: latency-svc-hqrc2
May 17 17:58:22.429: INFO: Got endpoints: latency-svc-kjr9q [754.007819ms]
May 17 17:58:22.439: INFO: Created: latency-svc-m4rnx
May 17 17:58:22.476: INFO: Got endpoints: latency-svc-g6s7p [747.039546ms]
May 17 17:58:22.487: INFO: Created: latency-svc-jvl2s
May 17 17:58:22.528: INFO: Got endpoints: latency-svc-tj5h4 [748.556825ms]
May 17 17:58:22.539: INFO: Created: latency-svc-npg9s
May 17 17:58:22.581: INFO: Got endpoints: latency-svc-6xrc6 [750.981136ms]
May 17 17:58:22.590: INFO: Created: latency-svc-ghgxk
May 17 17:58:22.626: INFO: Got endpoints: latency-svc-ffr2c [748.513979ms]
May 17 17:58:22.636: INFO: Created: latency-svc-s89nm
May 17 17:58:22.675: INFO: Got endpoints: latency-svc-zqsvm [744.690989ms]
May 17 17:58:22.685: INFO: Created: latency-svc-8xpvs
May 17 17:58:22.731: INFO: Got endpoints: latency-svc-77b2z [752.344738ms]
May 17 17:58:22.746: INFO: Created: latency-svc-q2fgc
May 17 17:58:22.779: INFO: Got endpoints: latency-svc-psx8v [747.812142ms]
May 17 17:58:22.788: INFO: Created: latency-svc-cngrp
May 17 17:58:22.842: INFO: Got endpoints: latency-svc-f4k9c [764.204047ms]
May 17 17:58:22.878: INFO: Got endpoints: latency-svc-mhbms [738.179914ms]
May 17 17:58:22.942: INFO: Got endpoints: latency-svc-p4whz [765.211278ms]
May 17 17:58:22.980: INFO: Got endpoints: latency-svc-6lt9k [749.196306ms]
May 17 17:58:23.028: INFO: Got endpoints: latency-svc-wd74f [749.616407ms]
May 17 17:58:23.078: INFO: Got endpoints: latency-svc-tjqng [743.359315ms]
May 17 17:58:23.127: INFO: Got endpoints: latency-svc-hqrc2 [739.299007ms]
May 17 17:58:23.180: INFO: Got endpoints: latency-svc-m4rnx [751.155102ms]
May 17 17:58:23.228: INFO: Got endpoints: latency-svc-jvl2s [752.10907ms]
May 17 17:58:23.276: INFO: Got endpoints: latency-svc-npg9s [747.571264ms]
May 17 17:58:23.329: INFO: Got endpoints: latency-svc-ghgxk [748.255427ms]
May 17 17:58:23.376: INFO: Got endpoints: latency-svc-s89nm [750.421344ms]
May 17 17:58:23.430: INFO: Got endpoints: latency-svc-8xpvs [754.096323ms]
May 17 17:58:23.477: INFO: Got endpoints: latency-svc-q2fgc [745.549047ms]
May 17 17:58:23.526: INFO: Got endpoints: latency-svc-cngrp [747.634795ms]
May 17 17:58:23.526: INFO: Latencies: [41.06099ms 48.775601ms 77.743899ms 99.132827ms 127.465951ms 140.738265ms 149.098768ms 154.747405ms 155.068049ms 160.726466ms 166.667268ms 177.679326ms 178.111554ms 181.431348ms 183.176385ms 183.682378ms 189.19707ms 189.521019ms 190.72752ms 192.611177ms 200.4597ms 203.387207ms 207.17336ms 208.28467ms 213.810679ms 220.801197ms 226.875532ms 228.184241ms 232.416905ms 232.945898ms 234.864744ms 236.144067ms 241.38108ms 245.479642ms 251.146204ms 264.078983ms 265.043629ms 268.592944ms 273.018133ms 273.527003ms 276.789243ms 276.861622ms 278.313225ms 294.607747ms 314.222301ms 347.652162ms 371.841221ms 417.289337ms 439.597718ms 463.306898ms 514.632918ms 554.165119ms 598.534027ms 627.999821ms 679.113974ms 723.762876ms 725.139697ms 734.376798ms 738.179914ms 739.299007ms 741.219844ms 742.443466ms 742.684623ms 743.359315ms 743.626156ms 743.630892ms 744.02029ms 744.086159ms 744.233719ms 744.303699ms 744.59913ms 744.690989ms 745.261928ms 745.324362ms 745.371914ms 745.549047ms 745.629504ms 745.90916ms 745.947503ms 745.988842ms 746.258751ms 746.404421ms 746.411262ms 746.413509ms 746.459492ms 746.47298ms 746.619246ms 746.722864ms 746.940604ms 747.039546ms 747.051497ms 747.345109ms 747.509152ms 747.516661ms 747.571264ms 747.577352ms 747.634795ms 747.724212ms 747.735065ms 747.812142ms 747.88041ms 747.932819ms 748.046887ms 748.255427ms 748.334465ms 748.399785ms 748.399891ms 748.412684ms 748.431308ms 748.448429ms 748.492064ms 748.513979ms 748.528053ms 748.547376ms 748.556825ms 748.876559ms 748.897343ms 749.049278ms 749.107837ms 749.129002ms 749.144891ms 749.163783ms 749.196306ms 749.389385ms 749.553669ms 749.609166ms 749.616407ms 749.632823ms 749.636386ms 749.717995ms 749.748821ms 749.770308ms 749.878053ms 749.915444ms 749.917138ms 749.964302ms 750.401862ms 750.403666ms 750.421344ms 750.428764ms 750.469498ms 750.472595ms 750.485231ms 750.578344ms 750.643382ms 750.674243ms 750.869375ms 750.920417ms 750.937164ms 750.946909ms 750.981136ms 750.990327ms 750.992235ms 751.046089ms 751.053038ms 751.074577ms 751.136622ms 751.152339ms 751.155102ms 751.43435ms 751.453815ms 751.594176ms 751.674967ms 751.682863ms 751.820875ms 751.870868ms 751.902162ms 752.106819ms 752.10907ms 752.187991ms 752.344738ms 752.551348ms 752.858384ms 752.917366ms 753.009215ms 753.036406ms 753.22107ms 753.271796ms 753.296199ms 753.321925ms 753.429473ms 753.446153ms 753.927001ms 753.982107ms 754.007819ms 754.096323ms 754.259889ms 754.547196ms 754.827814ms 756.994135ms 757.055665ms 757.456709ms 757.833268ms 758.006215ms 758.453889ms 760.288505ms 760.584285ms 764.204047ms 765.211278ms 774.529314ms]
May 17 17:58:23.527: INFO: 50 %ile: 747.88041ms
May 17 17:58:23.527: INFO: 90 %ile: 753.429473ms
May 17 17:58:23.527: INFO: 99 %ile: 765.211278ms
May 17 17:58:23.527: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:58:23.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9478" for this suite.

• [SLOW TEST:11.083 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":165,"skipped":2651,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:58:23.545: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4021
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 17 17:58:23.598: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 17:58:23.668: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 17:58:25.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:27.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:29.681: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:31.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:33.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:35.674: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:37.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 17:58:39.674: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 17 17:58:39.682: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 17 17:58:41.687: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 17 17:58:43.688: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 17 17:58:43.692: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 17 17:58:45.733: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 17:58:45.733: INFO: Going to poll 172.20.227.234 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 17 17:58:45.734: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.227.234:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4021 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:58:45.735: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:58:45.828: INFO: Found all 1 expected endpoints: [netserver-0]
May 17 17:58:45.828: INFO: Going to poll 172.20.206.222 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 17 17:58:45.832: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.206.222:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4021 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:58:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:58:45.927: INFO: Found all 1 expected endpoints: [netserver-1]
May 17 17:58:45.927: INFO: Going to poll 172.20.150.21 on port 8080 at least 0 times, with a maximum of 39 tries before failing
May 17 17:58:45.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.150.21:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4021 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 17:58:45.930: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 17:58:46.001: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:58:46.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4021" for this suite.

• [SLOW TEST:22.466 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2656,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:58:46.011: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 17:58:46.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f" in namespace "downward-api-3470" to be "Succeeded or Failed"
May 17 17:58:46.074: INFO: Pod "downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.664752ms
May 17 17:58:48.080: INFO: Pod "downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016004496s
May 17 17:58:50.084: INFO: Pod "downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019870861s
STEP: Saw pod success
May 17 17:58:50.084: INFO: Pod "downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f" satisfied condition "Succeeded or Failed"
May 17 17:58:50.086: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f container client-container: <nil>
STEP: delete the pod
May 17 17:58:50.107: INFO: Waiting for pod downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f to disappear
May 17 17:58:50.110: INFO: Pod downwardapi-volume-13462937-8ca1-49a7-88a8-c021b1fc881f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:58:50.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3470" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2659,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:58:50.122: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0517 17:58:51.705008      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 17:59:53.719: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 17:59:53.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6731" for this suite.

• [SLOW TEST:63.611 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":168,"skipped":2665,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 17:59:53.733: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 17:59:54.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 17:59:56.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871194, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871194, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871194, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871194, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 17:59:59.415: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 17 18:00:01.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=webhook-2741 attach --namespace=webhook-2741 to-be-attached-pod -i -c=container1'
May 17 18:00:01.604: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:00:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2741" for this suite.
STEP: Destroying namespace "webhook-2741-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.966 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":169,"skipped":2669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:00:01.703: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:00:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6862" for this suite.
STEP: Destroying namespace "nsdeletetest-8602" for this suite.
May 17 18:00:07.902: INFO: Namespace nsdeletetest-8602 was already deleted
STEP: Destroying namespace "nsdeletetest-8951" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":170,"skipped":2725,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:00:07.918: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 17 18:00:07.968: INFO: Waiting up to 5m0s for pod "pod-54553575-0262-4f9d-980c-8fc60d7b04a4" in namespace "emptydir-1340" to be "Succeeded or Failed"
May 17 18:00:07.983: INFO: Pod "pod-54553575-0262-4f9d-980c-8fc60d7b04a4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.099316ms
May 17 18:00:09.987: INFO: Pod "pod-54553575-0262-4f9d-980c-8fc60d7b04a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018888082s
STEP: Saw pod success
May 17 18:00:09.987: INFO: Pod "pod-54553575-0262-4f9d-980c-8fc60d7b04a4" satisfied condition "Succeeded or Failed"
May 17 18:00:09.990: INFO: Trying to get logs from node 20test-worker-3 pod pod-54553575-0262-4f9d-980c-8fc60d7b04a4 container test-container: <nil>
STEP: delete the pod
May 17 18:00:10.009: INFO: Waiting for pod pod-54553575-0262-4f9d-980c-8fc60d7b04a4 to disappear
May 17 18:00:10.011: INFO: Pod pod-54553575-0262-4f9d-980c-8fc60d7b04a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:00:10.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1340" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2737,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:00:10.023: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0517 18:00:50.209301      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 18:01:52.220: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 17 18:01:52.221: INFO: Deleting pod "simpletest.rc-25ps9" in namespace "gc-3219"
May 17 18:01:52.233: INFO: Deleting pod "simpletest.rc-54gc8" in namespace "gc-3219"
May 17 18:01:52.252: INFO: Deleting pod "simpletest.rc-5h9zm" in namespace "gc-3219"
May 17 18:01:52.270: INFO: Deleting pod "simpletest.rc-pzzbw" in namespace "gc-3219"
May 17 18:01:52.283: INFO: Deleting pod "simpletest.rc-qvlrv" in namespace "gc-3219"
May 17 18:01:52.302: INFO: Deleting pod "simpletest.rc-rfhv5" in namespace "gc-3219"
May 17 18:01:52.336: INFO: Deleting pod "simpletest.rc-sxxpw" in namespace "gc-3219"
May 17 18:01:52.357: INFO: Deleting pod "simpletest.rc-tmwgp" in namespace "gc-3219"
May 17 18:01:52.373: INFO: Deleting pod "simpletest.rc-wvtm5" in namespace "gc-3219"
May 17 18:01:52.387: INFO: Deleting pod "simpletest.rc-zlsfh" in namespace "gc-3219"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:01:52.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3219" for this suite.

• [SLOW TEST:102.388 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":172,"skipped":2738,"failed":0}
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:01:52.414: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-5037
STEP: creating service affinity-nodeport-transition in namespace services-5037
STEP: creating replication controller affinity-nodeport-transition in namespace services-5037
I0517 18:01:52.500896      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5037, replica count: 3
I0517 18:01:55.708583      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 18:01:55.717: INFO: Creating new exec pod
May 17 18:02:00.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 17 18:02:01.037: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 17 18:02:01.037: INFO: stdout: ""
May 17 18:02:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c nc -zv -t -w 2 172.30.81.165 80'
May 17 18:02:01.239: INFO: stderr: "+ nc -zv -t -w 2 172.30.81.165 80\nConnection to 172.30.81.165 80 port [tcp/http] succeeded!\n"
May 17 18:02:01.239: INFO: stdout: ""
May 17 18:02:01.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.187 31735'
May 17 18:02:01.462: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.187 31735\nConnection to 10.30.20.187 31735 port [tcp/31735] succeeded!\n"
May 17 18:02:01.462: INFO: stdout: ""
May 17 18:02:01.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.193 31735'
May 17 18:02:01.637: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.193 31735\nConnection to 10.30.20.193 31735 port [tcp/31735] succeeded!\n"
May 17 18:02:01.637: INFO: stdout: ""
May 17 18:02:01.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.30.20.193:31735/ ; done'
May 17 18:02:02.054: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n"
May 17 18:02:02.055: INFO: stdout: "\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-zwvp6\naffinity-nodeport-transition-44pzl\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-44pzl\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-zwvp6\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-zwvp6\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-zwvp6\naffinity-nodeport-transition-44pzl\naffinity-nodeport-transition-44pzl\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-zwvp6\naffinity-nodeport-transition-f58xj"
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-zwvp6
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-44pzl
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-44pzl
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-zwvp6
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-zwvp6
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-zwvp6
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-44pzl
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-44pzl
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-zwvp6
May 17 18:02:02.055: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-5037 exec execpod-affinity2q895 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.30.20.193:31735/ ; done'
May 17 18:02:02.386: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.30.20.193:31735/\n"
May 17 18:02:02.386: INFO: stdout: "\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj\naffinity-nodeport-transition-f58xj"
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Received response from host: affinity-nodeport-transition-f58xj
May 17 18:02:02.386: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5037, will wait for the garbage collector to delete the pods
May 17 18:02:02.467: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.388439ms
May 17 18:02:03.467: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 1.000170238s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:02:15.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5037" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.823 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":173,"skipped":2738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:02:15.238: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 17 18:02:17.347: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:02:17.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8146" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":2760,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:02:17.374: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 in namespace container-probe-4277
May 17 18:02:19.430: INFO: Started pod liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 in namespace container-probe-4277
STEP: checking the pod's current state and verifying that restartCount is present
May 17 18:02:19.434: INFO: Initial restart count of pod liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is 0
May 17 18:02:37.555: INFO: Restart count of pod container-probe-4277/liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is now 1 (18.119623426s elapsed)
May 17 18:02:57.595: INFO: Restart count of pod container-probe-4277/liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is now 2 (38.159927238s elapsed)
May 17 18:03:17.641: INFO: Restart count of pod container-probe-4277/liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is now 3 (58.205852024s elapsed)
May 17 18:03:37.689: INFO: Restart count of pod container-probe-4277/liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is now 4 (1m18.253780208s elapsed)
May 17 18:04:37.823: INFO: Restart count of pod container-probe-4277/liveness-ea6dd944-a05c-4e1b-b940-d0298156c0d3 is now 5 (2m18.387919764s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:04:37.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4277" for this suite.

• [SLOW TEST:140.497 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2761,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:04:37.872: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 17 18:04:37.934: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 18:05:38.026: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 17 18:05:38.064: INFO: Created pod: pod0-sched-preemption-low-priority
May 17 18:05:38.376: INFO: Created pod: pod1-sched-preemption-medium-priority
May 17 18:05:38.402: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:00.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5790" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:82.775 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":176,"skipped":2782,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:00.648: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-5141
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5141
STEP: Deleting pre-stop pod
May 17 18:06:13.743: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:13.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5141" for this suite.

• [SLOW TEST:13.125 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":177,"skipped":2784,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:13.777: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:29.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1976" for this suite.

• [SLOW TEST:16.147 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":178,"skipped":2787,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:29.928: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-8a12967b-94fe-4564-8271-592c96e43cec
STEP: Creating a pod to test consume configMaps
May 17 18:06:29.986: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d" in namespace "projected-9332" to be "Succeeded or Failed"
May 17 18:06:29.995: INFO: Pod "pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.288155ms
May 17 18:06:31.999: INFO: Pod "pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012290115s
STEP: Saw pod success
May 17 18:06:31.999: INFO: Pod "pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d" satisfied condition "Succeeded or Failed"
May 17 18:06:32.001: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d container agnhost-container: <nil>
STEP: delete the pod
May 17 18:06:32.033: INFO: Waiting for pod pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d to disappear
May 17 18:06:32.038: INFO: Pod pod-projected-configmaps-e65ee213-dfe0-4383-a75e-c03b68bb401d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:32.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9332" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:32.050: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4540" for this suite.

• [SLOW TEST:11.109 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":180,"skipped":2815,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:43.159: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-0e73032c-0390-49dd-8a69-87649d8c415d
STEP: Creating a pod to test consume configMaps
May 17 18:06:43.225: INFO: Waiting up to 5m0s for pod "pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c" in namespace "configmap-4300" to be "Succeeded or Failed"
May 17 18:06:43.238: INFO: Pod "pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.47569ms
May 17 18:06:45.243: INFO: Pod "pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018247385s
STEP: Saw pod success
May 17 18:06:45.243: INFO: Pod "pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c" satisfied condition "Succeeded or Failed"
May 17 18:06:45.245: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c container agnhost-container: <nil>
STEP: delete the pod
May 17 18:06:45.291: INFO: Waiting for pod pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c to disappear
May 17 18:06:45.295: INFO: Pod pod-configmaps-64ccc784-181c-4ee2-b385-31d42f498b8c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:45.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4300" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":2843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 17 18:06:45.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-4743 api-versions'
May 17 18:06:45.462: INFO: stderr: ""
May 17 18:06:45.462: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nceph.rook.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nobjectbucket.io/v1alpha1\noperator.tigera.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrook.io/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:45.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4743" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":182,"skipped":2869,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:45.472: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 17 18:06:45.532: INFO: observed Pod pod-test in namespace pods-7146 in phase Pending conditions []
May 17 18:06:45.535: INFO: observed Pod pod-test in namespace pods-7146 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC  }]
May 17 18:06:45.547: INFO: observed Pod pod-test in namespace pods-7146 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC  }]
May 17 18:06:46.244: INFO: observed Pod pod-test in namespace pods-7146 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-17 18:06:45 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 17 18:06:46.903: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 17 18:06:46.940: INFO: observed event type ADDED
May 17 18:06:46.940: INFO: observed event type MODIFIED
May 17 18:06:46.940: INFO: observed event type MODIFIED
May 17 18:06:46.940: INFO: observed event type MODIFIED
May 17 18:06:46.940: INFO: observed event type MODIFIED
May 17 18:06:46.940: INFO: observed event type MODIFIED
May 17 18:06:46.941: INFO: observed event type MODIFIED
May 17 18:06:46.941: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:46.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7146" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":183,"skipped":2874,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:46.954: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-1aa02899-e3d1-4615-ba4b-e430e826e047
STEP: Creating a pod to test consume configMaps
May 17 18:06:47.009: INFO: Waiting up to 5m0s for pod "pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605" in namespace "configmap-2027" to be "Succeeded or Failed"
May 17 18:06:47.018: INFO: Pod "pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605": Phase="Pending", Reason="", readiness=false. Elapsed: 7.030689ms
May 17 18:06:49.022: INFO: Pod "pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011605286s
STEP: Saw pod success
May 17 18:06:49.023: INFO: Pod "pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605" satisfied condition "Succeeded or Failed"
May 17 18:06:49.025: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605 container agnhost-container: <nil>
STEP: delete the pod
May 17 18:06:49.046: INFO: Waiting for pod pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605 to disappear
May 17 18:06:49.048: INFO: Pod pod-configmaps-0fbd8604-997b-4fc9-a23c-af33b0e49605 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:49.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2027" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":2878,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:49.061: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-dcf46de9-47e0-4007-b6f3-a91f0985d5a8
STEP: Creating a pod to test consume secrets
May 17 18:06:49.142: INFO: Waiting up to 5m0s for pod "pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e" in namespace "secrets-5804" to be "Succeeded or Failed"
May 17 18:06:49.157: INFO: Pod "pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.086309ms
May 17 18:06:51.162: INFO: Pod "pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019501075s
STEP: Saw pod success
May 17 18:06:51.162: INFO: Pod "pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e" satisfied condition "Succeeded or Failed"
May 17 18:06:51.164: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:06:51.189: INFO: Waiting for pod pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e to disappear
May 17 18:06:51.192: INFO: Pod pod-secrets-e684c8b1-6948-4481-aa70-e7ada8c46e5e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:51.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5804" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":2882,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:51.205: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:06:51.243: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:06:59.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1922" for this suite.

• [SLOW TEST:8.763 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":186,"skipped":2899,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:06:59.969: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 17 18:07:00.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 create -f -'
May 17 18:07:03.505: INFO: stderr: ""
May 17 18:07:03.505: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 17 18:07:03.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 17 18:07:03.698: INFO: stderr: ""
May 17 18:07:03.698: INFO: stdout: "update-demo-nautilus-5v4bz update-demo-nautilus-6xr59 "
May 17 18:07:03.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods update-demo-nautilus-5v4bz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 18:07:06.467: INFO: stderr: ""
May 17 18:07:06.467: INFO: stdout: "true"
May 17 18:07:06.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods update-demo-nautilus-5v4bz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 18:07:06.608: INFO: stderr: ""
May 17 18:07:06.608: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 18:07:06.608: INFO: validating pod update-demo-nautilus-5v4bz
May 17 18:07:06.612: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 18:07:06.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 18:07:06.612: INFO: update-demo-nautilus-5v4bz is verified up and running
May 17 18:07:06.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods update-demo-nautilus-6xr59 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 17 18:07:06.705: INFO: stderr: ""
May 17 18:07:06.705: INFO: stdout: "true"
May 17 18:07:06.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods update-demo-nautilus-6xr59 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 17 18:07:06.809: INFO: stderr: ""
May 17 18:07:06.809: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 17 18:07:06.809: INFO: validating pod update-demo-nautilus-6xr59
May 17 18:07:06.813: INFO: got data: {
  "image": "nautilus.jpg"
}

May 17 18:07:06.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 17 18:07:06.813: INFO: update-demo-nautilus-6xr59 is verified up and running
STEP: using delete to clean up resources
May 17 18:07:06.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 delete --grace-period=0 --force -f -'
May 17 18:07:06.945: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 18:07:06.945: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 17 18:07:06.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get rc,svc -l name=update-demo --no-headers'
May 17 18:07:07.048: INFO: stderr: "No resources found in kubectl-978 namespace.\n"
May 17 18:07:07.048: INFO: stdout: ""
May 17 18:07:07.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 18:07:07.136: INFO: stderr: ""
May 17 18:07:07.136: INFO: stdout: "update-demo-nautilus-5v4bz\nupdate-demo-nautilus-6xr59\n"
May 17 18:07:07.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get rc,svc -l name=update-demo --no-headers'
May 17 18:07:07.784: INFO: stderr: "No resources found in kubectl-978 namespace.\n"
May 17 18:07:07.784: INFO: stdout: ""
May 17 18:07:07.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-978 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 18:07:07.904: INFO: stderr: ""
May 17 18:07:07.904: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:07.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-978" for this suite.

• [SLOW TEST:7.944 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":187,"skipped":2901,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:07.913: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:07:07.957: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1813" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":188,"skipped":2907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:10.358: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-24fc9210-4f46-4a7b-bae7-7a75d21f44bb
STEP: Creating a pod to test consume secrets
May 17 18:07:10.407: INFO: Waiting up to 5m0s for pod "pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4" in namespace "secrets-7283" to be "Succeeded or Failed"
May 17 18:07:10.410: INFO: Pod "pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.155656ms
May 17 18:07:12.414: INFO: Pod "pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007315153s
STEP: Saw pod success
May 17 18:07:12.414: INFO: Pod "pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4" satisfied condition "Succeeded or Failed"
May 17 18:07:12.416: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4 container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:07:12.436: INFO: Waiting for pod pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4 to disappear
May 17 18:07:12.439: INFO: Pod pod-secrets-7ae2d1f3-ab86-4479-87ea-47bded1cf4c4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:12.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7283" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":2954,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:12.453: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:07:13.964: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 18:07:15.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871633, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871633, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871634, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871633, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:07:18.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:19.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7791" for this suite.
STEP: Destroying namespace "webhook-7791-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.702 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":190,"skipped":2970,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:19.155: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:07:19.252: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941" in namespace "projected-2825" to be "Succeeded or Failed"
May 17 18:07:19.254: INFO: Pod "downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.796526ms
May 17 18:07:21.258: INFO: Pod "downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006805994s
STEP: Saw pod success
May 17 18:07:21.258: INFO: Pod "downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941" satisfied condition "Succeeded or Failed"
May 17 18:07:21.260: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941 container client-container: <nil>
STEP: delete the pod
May 17 18:07:21.296: INFO: Waiting for pod downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941 to disappear
May 17 18:07:21.301: INFO: Pod downwardapi-volume-3164d240-29e6-4660-88b9-a98a025aa941 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:21.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2825" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":191,"skipped":2986,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 17 18:07:21.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 create -f -'
May 17 18:07:22.100: INFO: stderr: ""
May 17 18:07:22.100: INFO: stdout: "pod/pause created\n"
May 17 18:07:22.100: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 17 18:07:22.100: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7411" to be "running and ready"
May 17 18:07:22.105: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.932539ms
May 17 18:07:24.140: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.040603056s
May 17 18:07:24.141: INFO: Pod "pause" satisfied condition "running and ready"
May 17 18:07:24.141: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 17 18:07:24.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 label pods pause testing-label=testing-label-value'
May 17 18:07:24.620: INFO: stderr: ""
May 17 18:07:24.621: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 17 18:07:24.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 get pod pause -L testing-label'
May 17 18:07:24.741: INFO: stderr: ""
May 17 18:07:24.741: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 17 18:07:24.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 label pods pause testing-label-'
May 17 18:07:24.914: INFO: stderr: ""
May 17 18:07:24.914: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 17 18:07:24.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 get pod pause -L testing-label'
May 17 18:07:25.016: INFO: stderr: ""
May 17 18:07:25.016: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 17 18:07:25.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 delete --grace-period=0 --force -f -'
May 17 18:07:25.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 17 18:07:25.124: INFO: stdout: "pod \"pause\" force deleted\n"
May 17 18:07:25.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 get rc,svc -l name=pause --no-headers'
May 17 18:07:25.232: INFO: stderr: "No resources found in kubectl-7411 namespace.\n"
May 17 18:07:25.232: INFO: stdout: ""
May 17 18:07:25.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7411 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 17 18:07:25.335: INFO: stderr: ""
May 17 18:07:25.335: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:25.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7411" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":192,"skipped":2991,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:25.343: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-3383
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3383 to expose endpoints map[]
May 17 18:07:25.409: INFO: successfully validated that service endpoint-test2 in namespace services-3383 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3383
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3383 to expose endpoints map[pod1:[80]]
May 17 18:07:27.450: INFO: successfully validated that service endpoint-test2 in namespace services-3383 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3383
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3383 to expose endpoints map[pod1:[80] pod2:[80]]
May 17 18:07:29.478: INFO: successfully validated that service endpoint-test2 in namespace services-3383 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3383
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3383 to expose endpoints map[pod2:[80]]
May 17 18:07:29.513: INFO: successfully validated that service endpoint-test2 in namespace services-3383 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3383
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3383 to expose endpoints map[]
May 17 18:07:29.539: INFO: successfully validated that service endpoint-test2 in namespace services-3383 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:29.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3383" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":193,"skipped":3000,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 17 18:07:29.649: INFO: Waiting up to 5m0s for pod "pod-484a2511-81a6-4d39-8f4b-1e133baeebc6" in namespace "emptydir-3034" to be "Succeeded or Failed"
May 17 18:07:29.657: INFO: Pod "pod-484a2511-81a6-4d39-8f4b-1e133baeebc6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481476ms
May 17 18:07:31.661: INFO: Pod "pod-484a2511-81a6-4d39-8f4b-1e133baeebc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012339752s
STEP: Saw pod success
May 17 18:07:31.661: INFO: Pod "pod-484a2511-81a6-4d39-8f4b-1e133baeebc6" satisfied condition "Succeeded or Failed"
May 17 18:07:31.663: INFO: Trying to get logs from node 20test-worker-3 pod pod-484a2511-81a6-4d39-8f4b-1e133baeebc6 container test-container: <nil>
STEP: delete the pod
May 17 18:07:31.681: INFO: Waiting for pod pod-484a2511-81a6-4d39-8f4b-1e133baeebc6 to disappear
May 17 18:07:31.683: INFO: Pod pod-484a2511-81a6-4d39-8f4b-1e133baeebc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3034" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:31.691: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:34.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8146" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":195,"skipped":3054,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:34.824: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-1393
STEP: creating replication controller nodeport-test in namespace services-1393
I0517 18:07:34.940319      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1393, replica count: 2
I0517 18:07:37.997128      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 18:07:37.997: INFO: Creating new exec pod
May 17 18:07:41.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-1393 exec execpodvcb7r -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 17 18:07:41.290: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 17 18:07:41.290: INFO: stdout: ""
May 17 18:07:41.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-1393 exec execpodvcb7r -- /bin/sh -x -c nc -zv -t -w 2 172.30.91.230 80'
May 17 18:07:41.499: INFO: stderr: "+ nc -zv -t -w 2 172.30.91.230 80\nConnection to 172.30.91.230 80 port [tcp/http] succeeded!\n"
May 17 18:07:41.499: INFO: stdout: ""
May 17 18:07:41.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-1393 exec execpodvcb7r -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.161 32566'
May 17 18:07:41.676: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.161 32566\nConnection to 10.30.20.161 32566 port [tcp/32566] succeeded!\n"
May 17 18:07:41.676: INFO: stdout: ""
May 17 18:07:41.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-1393 exec execpodvcb7r -- /bin/sh -x -c nc -zv -t -w 2 10.30.20.187 32566'
May 17 18:07:41.855: INFO: stderr: "+ nc -zv -t -w 2 10.30.20.187 32566\nConnection to 10.30.20.187 32566 port [tcp/32566] succeeded!\n"
May 17 18:07:41.855: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:07:41.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1393" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.041 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":196,"skipped":3060,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:07:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 17 18:09:42.452: INFO: Successfully updated pod "var-expansion-0777da53-ebe7-4997-9e35-d176fc0b7c23"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 17 18:09:44.459: INFO: Deleting pod "var-expansion-0777da53-ebe7-4997-9e35-d176fc0b7c23" in namespace "var-expansion-4039"
May 17 18:09:44.470: INFO: Wait up to 5m0s for pod "var-expansion-0777da53-ebe7-4997-9e35-d176fc0b7c23" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:10:26.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4039" for this suite.

• [SLOW TEST:164.620 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":197,"skipped":3064,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:10:26.490: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 17 18:10:26.526: INFO: PodSpec: initContainers in spec.initContainers
May 17 18:11:12.717: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9db5b0ac-ac77-47d0-989e-0af25fd6d006", GenerateName:"", Namespace:"init-container-7363", SelfLink:"", UID:"bafd60e1-9636-4863-b6e8-3b6fb783878d", ResourceVersion:"36772", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63756871826, loc:(*time.Location)(0x7962e20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"526686629"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.20.150.14/32", "cni.projectcalico.org/podIPs":"172.20.150.14/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002aba0c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002aba0e0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002aba100), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002aba120)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002aba160), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002aba1a0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-jf7fj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002c7e080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jf7fj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jf7fj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-jf7fj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003de00e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"20test-worker-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0034e0000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003de0190)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003de01b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003de01b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003de01bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002594030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871826, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871826, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871826, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756871826, loc:(*time.Location)(0x7962e20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.30.20.187", PodIP:"172.20.150.14", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.20.150.14"}}, StartTime:(*v1.Time)(0xc002aba1e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e0150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0034e01c0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://7bb296ad92836282b5681bf3a9e44b676bf5b7d5e9da115adc1f20317651c5ec", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002aba220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002aba200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc003de026f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:12.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7363" for this suite.

• [SLOW TEST:46.254 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":198,"skipped":3066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:12.746: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:12.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2444" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":199,"skipped":3141,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:12.844: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:11:12.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3" in namespace "projected-8998" to be "Succeeded or Failed"
May 17 18:11:12.885: INFO: Pod "downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.00542ms
May 17 18:11:14.892: INFO: Pod "downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009166555s
STEP: Saw pod success
May 17 18:11:14.892: INFO: Pod "downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3" satisfied condition "Succeeded or Failed"
May 17 18:11:14.899: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3 container client-container: <nil>
STEP: delete the pod
May 17 18:11:14.924: INFO: Waiting for pod downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3 to disappear
May 17 18:11:14.928: INFO: Pod downwardapi-volume-00718e12-af63-4978-81e5-f671e1511da3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:14.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8998" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-eaa371bc-9fe6-41b3-ad29-1e55a6e3494d
STEP: Creating a pod to test consume secrets
May 17 18:11:14.979: INFO: Waiting up to 5m0s for pod "pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7" in namespace "secrets-2302" to be "Succeeded or Failed"
May 17 18:11:14.984: INFO: Pod "pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285856ms
May 17 18:11:16.988: INFO: Pod "pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009203313s
STEP: Saw pod success
May 17 18:11:16.988: INFO: Pod "pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7" satisfied condition "Succeeded or Failed"
May 17 18:11:16.992: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7 container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:11:17.007: INFO: Waiting for pod pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7 to disappear
May 17 18:11:17.009: INFO: Pod pod-secrets-5757958b-2d2a-4c01-8a9d-d3336e7249d7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:17.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2302" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:17.021: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 17 18:11:17.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7368 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 17 18:11:17.219: INFO: stderr: ""
May 17 18:11:17.219: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 17 18:11:22.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7368 get pod e2e-test-httpd-pod -o json'
May 17 18:11:22.420: INFO: stderr: ""
May 17 18:11:22.420: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.20.150.15/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.20.150.15/32\"\n        },\n        \"creationTimestamp\": \"2021-05-17T18:11:17Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-17T18:11:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-17T18:11:17Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.20.150.15\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-17T18:11:18Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7368\",\n        \"resourceVersion\": \"36896\",\n        \"uid\": \"824b1b40-d971-414b-9284-e6a6b1a72270\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-jnq72\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"20test-worker-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-jnq72\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-jnq72\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-17T18:11:17Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-17T18:11:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-17T18:11:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-17T18:11:17Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b5cb804cfcfe1e4318c9b4465ef316776b7d66070ac0732f07c46eb027f692e9\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-17T18:11:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.30.20.187\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.150.15\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.20.150.15\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-17T18:11:17Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 17 18:11:22.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7368 replace -f -'
May 17 18:11:22.761: INFO: stderr: ""
May 17 18:11:22.761: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 17 18:11:22.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-7368 delete pods e2e-test-httpd-pod'
May 17 18:11:35.104: INFO: stderr: ""
May 17 18:11:35.104: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:35.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7368" for this suite.

• [SLOW TEST:18.093 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":202,"skipped":3199,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:35.115: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 17 18:11:37.730: INFO: Successfully updated pod "annotationupdate401f5670-166f-4c80-98b7-8bde97e5aa43"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:39.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9556" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3206,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:39.754: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:11:50.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5531" for this suite.

• [SLOW TEST:11.168 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":204,"skipped":3222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:11:50.935: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 17 18:11:51.029: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:51.047: INFO: Number of nodes with available pods: 0
May 17 18:11:51.047: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:52.056: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:52.061: INFO: Number of nodes with available pods: 0
May 17 18:11:52.061: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:53.053: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:53.056: INFO: Number of nodes with available pods: 3
May 17 18:11:53.057: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 17 18:11:53.079: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:53.083: INFO: Number of nodes with available pods: 2
May 17 18:11:53.083: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:54.091: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:54.094: INFO: Number of nodes with available pods: 2
May 17 18:11:54.094: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:55.088: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:55.097: INFO: Number of nodes with available pods: 2
May 17 18:11:55.097: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:56.108: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:56.125: INFO: Number of nodes with available pods: 2
May 17 18:11:56.125: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:57.089: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:57.092: INFO: Number of nodes with available pods: 2
May 17 18:11:57.092: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:58.089: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:58.093: INFO: Number of nodes with available pods: 2
May 17 18:11:58.093: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:11:59.091: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:11:59.095: INFO: Number of nodes with available pods: 2
May 17 18:11:59.095: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:12:00.090: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:12:00.095: INFO: Number of nodes with available pods: 2
May 17 18:12:00.095: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:12:01.090: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:12:01.094: INFO: Number of nodes with available pods: 3
May 17 18:12:01.095: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-297, will wait for the garbage collector to delete the pods
May 17 18:12:01.171: INFO: Deleting DaemonSet.extensions daemon-set took: 19.934589ms
May 17 18:12:02.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.368954ms
May 17 18:12:12.876: INFO: Number of nodes with available pods: 0
May 17 18:12:12.877: INFO: Number of running nodes: 0, number of available pods: 0
May 17 18:12:12.879: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37330"},"items":null}

May 17 18:12:12.881: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37330"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:12.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-297" for this suite.

• [SLOW TEST:21.967 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":205,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:12.903: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:23.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4066" for this suite.

• [SLOW TEST:11.095 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":206,"skipped":3294,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:24.007: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-c19faeae-ae1f-4008-b6b0-3d717d66591b
STEP: Creating a pod to test consume configMaps
May 17 18:12:24.073: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc" in namespace "projected-1618" to be "Succeeded or Failed"
May 17 18:12:24.080: INFO: Pod "pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108268ms
May 17 18:12:26.084: INFO: Pod "pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009246363s
STEP: Saw pod success
May 17 18:12:26.084: INFO: Pod "pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc" satisfied condition "Succeeded or Failed"
May 17 18:12:26.088: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc container agnhost-container: <nil>
STEP: delete the pod
May 17 18:12:26.107: INFO: Waiting for pod pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc to disappear
May 17 18:12:26.111: INFO: Pod pod-projected-configmaps-6b451a0d-2024-4695-9b7f-235ffc7b0fbc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:26.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1618" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3385,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:26.129: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:26.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7177" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":208,"skipped":3388,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:26.178: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 17 18:12:26.222: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 17 18:12:26.228: INFO: Waiting for terminating namespaces to be deleted...
May 17 18:12:26.230: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-1 before test
May 17 18:12:26.247: INFO: calico-node-fjsxq from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container calico-node ready: true, restart count 0
May 17 18:12:26.247: INFO: calico-typha-954b59468-864hr from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container calico-typha ready: true, restart count 0
May 17 18:12:26.247: INFO: kube-proxy-d7hzd from kube-system started at 2021-05-17 16:45:38 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 18:12:26.247: INFO: fluent-bit-vm2vm from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 18:12:26.247: INFO: alertmanager-main-0 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container alertmanager ready: true, restart count 0
May 17 18:12:26.247: INFO: 	Container config-reloader ready: true, restart count 0
May 17 18:12:26.247: INFO: grafana-f8cd57fcf-p45c2 from monitoring started at 2021-05-17 16:46:48 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container grafana ready: true, restart count 0
May 17 18:12:26.247: INFO: kube-state-metrics-587bfd4f97-6qs5t from monitoring started at 2021-05-17 16:46:48 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.247: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 17 18:12:26.247: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 17 18:12:26.247: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 17 18:12:26.247: INFO: node-exporter-5dkbc from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container node-exporter ready: true, restart count 0
May 17 18:12:26.248: INFO: prometheus-operator-7649c7454f-5xts8 from monitoring started at 2021-05-17 16:46:39 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container prometheus-operator ready: true, restart count 0
May 17 18:12:26.248: INFO: csi-cephfsplugin-cgwrr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.248: INFO: csi-cephfsplugin-provisioner-8658f67749-mj486 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.248: INFO: csi-rbdplugin-6jrsr from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.248: INFO: csi-rbdplugin-provisioner-94f699d86-r49r5 from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (6 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 18:12:26.248: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.248: INFO: rook-ceph-crashcollector-20test-worker-1-64f79c894f-47rch from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 18:12:26.248: INFO: rook-ceph-mgr-a-5db4bfbc6-fnj5z from rook-ceph started at 2021-05-17 16:49:56 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container mgr ready: true, restart count 0
May 17 18:12:26.248: INFO: rook-ceph-mon-c-545cb7776f-tsblp from rook-ceph started at 2021-05-17 16:49:42 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container mon ready: true, restart count 0
May 17 18:12:26.248: INFO: rook-ceph-osd-2-96fd479f5-88zbz from rook-ceph started at 2021-05-17 16:50:10 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container osd ready: true, restart count 0
May 17 18:12:26.248: INFO: rook-ceph-osd-prepare-20test-worker-1-qb5xq from rook-ceph started at 2021-05-17 17:14:22 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container provision ready: false, restart count 0
May 17 18:12:26.248: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-wzzjb from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.248: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 17 18:12:26.248: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 18:12:26.248: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-2 before test
May 17 18:12:26.265: INFO: calico-kube-controllers-5fb9f7f6d8-gpg2w from calico-system started at 2021-05-17 16:46:37 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 17 18:12:26.265: INFO: calico-node-sxlpp from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container calico-node ready: true, restart count 0
May 17 18:12:26.265: INFO: calico-typha-954b59468-jks4v from calico-system started at 2021-05-17 16:48:15 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container calico-typha ready: true, restart count 0
May 17 18:12:26.265: INFO: coredns-74ff55c5b-hjgsp from kube-system started at 2021-05-17 16:46:42 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container coredns ready: true, restart count 0
May 17 18:12:26.265: INFO: kube-proxy-wmdvj from kube-system started at 2021-05-17 16:45:52 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 18:12:26.265: INFO: fluent-bit-mjvsf from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 18:12:26.265: INFO: alertmanager-main-2 from monitoring started at 2021-05-17 16:46:45 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container alertmanager ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container config-reloader ready: true, restart count 0
May 17 18:12:26.265: INFO: node-exporter-gcj5f from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container node-exporter ready: true, restart count 0
May 17 18:12:26.265: INFO: prometheus-adapter-69b8496df6-tns5m from monitoring started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 17 18:12:26.265: INFO: prometheus-k8s-0 from monitoring started at 2021-05-17 16:47:03 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container config-reloader ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container prometheus ready: true, restart count 1
May 17 18:12:26.265: INFO: csi-cephfsplugin-8lpms from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.265: INFO: csi-cephfsplugin-provisioner-8658f67749-vcx69 from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.265: INFO: csi-rbdplugin-7rpck from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.265: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.265: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.265: INFO: csi-rbdplugin-provisioner-94f699d86-xznpd from rook-ceph started at 2021-05-17 16:48:45 +0000 UTC (6 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container csi-attacher ready: true, restart count 0
May 17 18:12:26.266: INFO: 	Container csi-provisioner ready: true, restart count 0
May 17 18:12:26.266: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 18:12:26.266: INFO: 	Container csi-resizer ready: true, restart count 0
May 17 18:12:26.266: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 17 18:12:26.266: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.266: INFO: rook-ceph-crashcollector-20test-worker-2-98db6957b-n4mg6 from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 18:12:26.266: INFO: rook-ceph-mon-b-589bbc6556-w592m from rook-ceph started at 2021-05-17 16:49:28 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container mon ready: true, restart count 0
May 17 18:12:26.266: INFO: rook-ceph-operator-5f6ffc46c7-hj2ml from rook-ceph started at 2021-05-17 17:13:48 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container rook-ceph-operator ready: true, restart count 0
May 17 18:12:26.266: INFO: rook-ceph-osd-0-7896cc545c-psfck from rook-ceph started at 2021-05-17 16:50:08 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container osd ready: true, restart count 0
May 17 18:12:26.266: INFO: rook-ceph-osd-prepare-20test-worker-2-lhtcq from rook-ceph started at 2021-05-17 17:14:24 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container provision ready: false, restart count 0
May 17 18:12:26.266: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-hn7cn from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 17 18:12:26.266: INFO: 	Container systemd-logs ready: true, restart count 0
May 17 18:12:26.266: INFO: tigera-operator-657cc89589-qhpft from tigera-operator started at 2021-05-17 16:46:10 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.266: INFO: 	Container tigera-operator ready: true, restart count 0
May 17 18:12:26.266: INFO: 
Logging pods the apiserver thinks is on node 20test-worker-3 before test
May 17 18:12:26.280: INFO: calico-node-rb678 from calico-system started at 2021-05-17 16:46:17 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container calico-node ready: true, restart count 0
May 17 18:12:26.280: INFO: calico-typha-954b59468-v2z88 from calico-system started at 2021-05-17 16:46:16 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container calico-typha ready: true, restart count 0
May 17 18:12:26.280: INFO: kube-proxy-n6xv8 from kube-system started at 2021-05-17 16:45:25 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container kube-proxy ready: true, restart count 0
May 17 18:12:26.280: INFO: fluent-bit-hg8tt from logging started at 2021-05-17 16:47:04 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container fluent-bit ready: true, restart count 0
May 17 18:12:26.280: INFO: alertmanager-main-1 from monitoring started at 2021-05-17 17:14:35 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container alertmanager ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container config-reloader ready: true, restart count 0
May 17 18:12:26.280: INFO: node-exporter-ccd9w from monitoring started at 2021-05-17 16:46:51 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container node-exporter ready: true, restart count 0
May 17 18:12:26.280: INFO: prometheus-k8s-1 from monitoring started at 2021-05-17 17:14:37 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container config-reloader ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container prometheus ready: true, restart count 1
May 17 18:12:26.280: INFO: csi-cephfsplugin-9hmwc from rook-ceph started at 2021-05-17 17:14:36 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.280: INFO: csi-rbdplugin-9hr4g from rook-ceph started at 2021-05-17 17:14:38 +0000 UTC (3 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container csi-rbdplugin ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container driver-registrar ready: true, restart count 0
May 17 18:12:26.280: INFO: 	Container liveness-prometheus ready: true, restart count 0
May 17 18:12:26.280: INFO: rook-ceph-crashcollector-20test-worker-3-59976d65b7-8djwm from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container ceph-crash ready: true, restart count 0
May 17 18:12:26.280: INFO: rook-ceph-mon-a-6947dc47f-8wtqd from rook-ceph started at 2021-05-17 17:13:50 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container mon ready: true, restart count 0
May 17 18:12:26.280: INFO: rook-ceph-osd-1-6c8669bc9-89l9s from rook-ceph started at 2021-05-17 17:13:51 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container osd ready: true, restart count 0
May 17 18:12:26.280: INFO: rook-ceph-osd-prepare-20test-worker-3-cqpxz from rook-ceph started at 2021-05-17 17:14:26 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container provision ready: false, restart count 0
May 17 18:12:26.280: INFO: sonobuoy from sonobuoy started at 2021-05-17 16:59:52 +0000 UTC (1 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 17 18:12:26.280: INFO: sonobuoy-systemd-logs-daemon-set-cb2db5bc738f4caf-42p4c from sonobuoy started at 2021-05-17 16:59:57 +0000 UTC (2 container statuses recorded)
May 17 18:12:26.280: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 17 18:12:26.280: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b8e577de-4c5e-4e72-bbcb-0257eabb4560 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b8e577de-4c5e-4e72-bbcb-0257eabb4560 off the node 20test-worker-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b8e577de-4c5e-4e72-bbcb-0257eabb4560
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:30.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9830" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":209,"skipped":3396,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:30.413: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-633
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 17 18:12:30.446: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 17 18:12:30.488: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 18:12:32.492: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 17 18:12:34.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:36.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:38.505: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:40.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:42.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:44.492: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:46.493: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 17 18:12:48.496: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 17 18:12:48.500: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 17 18:12:48.503: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 17 18:12:50.529: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 17 18:12:50.529: INFO: Breadth first check of 172.20.227.239 on host 10.30.20.193...
May 17 18:12:50.531: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.26:9080/dial?request=hostname&protocol=udp&host=172.20.227.239&port=8081&tries=1'] Namespace:pod-network-test-633 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:12:50.531: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:12:50.611: INFO: Waiting for responses: map[]
May 17 18:12:50.611: INFO: reached 172.20.227.239 after 0/1 tries
May 17 18:12:50.611: INFO: Breadth first check of 172.20.206.225 on host 10.30.20.161...
May 17 18:12:50.614: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.26:9080/dial?request=hostname&protocol=udp&host=172.20.206.225&port=8081&tries=1'] Namespace:pod-network-test-633 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:12:50.614: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:12:50.679: INFO: Waiting for responses: map[]
May 17 18:12:50.679: INFO: reached 172.20.206.225 after 0/1 tries
May 17 18:12:50.679: INFO: Breadth first check of 172.20.150.21 on host 10.30.20.187...
May 17 18:12:50.682: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.150.26:9080/dial?request=hostname&protocol=udp&host=172.20.150.21&port=8081&tries=1'] Namespace:pod-network-test-633 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:12:50.682: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:12:50.765: INFO: Waiting for responses: map[]
May 17 18:12:50.765: INFO: reached 172.20.150.21 after 0/1 tries
May 17 18:12:50.765: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:50.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-633" for this suite.

• [SLOW TEST:20.362 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":210,"skipped":3419,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:50.776: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 17 18:12:50.850: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 17 18:12:50.852: INFO: starting watch
STEP: patching
STEP: updating
May 17 18:12:50.862: INFO: waiting for watch events with expected annotations
May 17 18:12:50.862: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:12:50.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9302" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":211,"skipped":3423,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:12:50.925: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-c6c23cb5-f166-4454-bc5a-00f021c66c13 in namespace container-probe-5873
May 17 18:12:54.989: INFO: Started pod busybox-c6c23cb5-f166-4454-bc5a-00f021c66c13 in namespace container-probe-5873
STEP: checking the pod's current state and verifying that restartCount is present
May 17 18:12:54.992: INFO: Initial restart count of pod busybox-c6c23cb5-f166-4454-bc5a-00f021c66c13 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:16:55.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5873" for this suite.

• [SLOW TEST:244.768 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:16:55.694: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 17 18:16:56.565: INFO: starting watch
STEP: patching
STEP: updating
May 17 18:16:56.576: INFO: waiting for watch events with expected annotations
May 17 18:16:56.576: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:16:56.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6543" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":213,"skipped":3465,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:16:56.645: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:16:57.474: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 18:16:59.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872217, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872217, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872217, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872217, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:17:02.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:17:02.516: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:17:03.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8540" for this suite.
STEP: Destroying namespace "webhook-8540-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.374 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":214,"skipped":3473,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:17:04.019: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 17 18:17:04.909: INFO: Pod name wrapped-volume-race-6e6eada9-875b-4b3e-993a-1301dae1ed3e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6e6eada9-875b-4b3e-993a-1301dae1ed3e in namespace emptydir-wrapper-1165, will wait for the garbage collector to delete the pods
May 17 18:17:21.014: INFO: Deleting ReplicationController wrapped-volume-race-6e6eada9-875b-4b3e-993a-1301dae1ed3e took: 26.629295ms
May 17 18:17:21.918: INFO: Terminating ReplicationController wrapped-volume-race-6e6eada9-875b-4b3e-993a-1301dae1ed3e pods took: 904.309641ms
STEP: Creating RC which spawns configmap-volume pods
May 17 18:18:02.950: INFO: Pod name wrapped-volume-race-1657d327-3f9e-467f-a90c-0c5bcaee6552: Found 0 pods out of 5
May 17 18:18:07.957: INFO: Pod name wrapped-volume-race-1657d327-3f9e-467f-a90c-0c5bcaee6552: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1657d327-3f9e-467f-a90c-0c5bcaee6552 in namespace emptydir-wrapper-1165, will wait for the garbage collector to delete the pods
May 17 18:18:18.035: INFO: Deleting ReplicationController wrapped-volume-race-1657d327-3f9e-467f-a90c-0c5bcaee6552 took: 7.493925ms
May 17 18:18:19.035: INFO: Terminating ReplicationController wrapped-volume-race-1657d327-3f9e-467f-a90c-0c5bcaee6552 pods took: 1.000345103s
STEP: Creating RC which spawns configmap-volume pods
May 17 18:18:25.427: INFO: Pod name wrapped-volume-race-a92a8a4a-502b-4b6b-a4d2-c9cd22be6ac6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a92a8a4a-502b-4b6b-a4d2-c9cd22be6ac6 in namespace emptydir-wrapper-1165, will wait for the garbage collector to delete the pods
May 17 18:18:41.514: INFO: Deleting ReplicationController wrapped-volume-race-a92a8a4a-502b-4b6b-a4d2-c9cd22be6ac6 took: 6.660281ms
May 17 18:18:41.615: INFO: Terminating ReplicationController wrapped-volume-race-a92a8a4a-502b-4b6b-a4d2-c9cd22be6ac6 pods took: 100.409179ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:03.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1165" for this suite.

• [SLOW TEST:119.044 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":215,"skipped":3475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 17 18:19:03.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1704  1f6457cd-9d29-4d13-ba02-f167cb17e055 40050 0 2021-05-17 18:19:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-17 18:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:19:03.124: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1704  1f6457cd-9d29-4d13-ba02-f167cb17e055 40051 0 2021-05-17 18:19:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-17 18:19:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 17 18:19:03.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1704  1f6457cd-9d29-4d13-ba02-f167cb17e055 40052 0 2021-05-17 18:19:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-17 18:19:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:19:03.135: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1704  1f6457cd-9d29-4d13-ba02-f167cb17e055 40053 0 2021-05-17 18:19:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-17 18:19:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:03.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1704" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":216,"skipped":3554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:03.143: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563
May 17 18:19:03.199: INFO: Pod name my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563: Found 1 pods out of 1
May 17 18:19:03.199: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563" are running
May 17 18:19:05.225: INFO: Pod "my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563-w4rkk" is running (conditions: [])
May 17 18:19:05.225: INFO: Trying to dial the pod
May 17 18:19:10.235: INFO: Controller my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563: Got expected result from replica 1 [my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563-w4rkk]: "my-hostname-basic-9441d61d-014f-4856-966e-b7fa3514e563-w4rkk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:10.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-534" for this suite.

• [SLOW TEST:7.105 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":217,"skipped":3576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3016.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3016.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3016.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3016.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3016.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3016.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 18:19:14.376: INFO: DNS probes using dns-3016/dns-test-23daab7f-64e1-4829-b689-57de507bf586 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:14.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3016" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":218,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:14.431: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c877d035-256f-4ab1-8853-9633747a52fb
STEP: Creating a pod to test consume configMaps
May 17 18:19:14.496: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48" in namespace "configmap-2945" to be "Succeeded or Failed"
May 17 18:19:14.500: INFO: Pod "pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664763ms
May 17 18:19:16.507: INFO: Pod "pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010982004s
STEP: Saw pod success
May 17 18:19:16.507: INFO: Pod "pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48" satisfied condition "Succeeded or Failed"
May 17 18:19:16.511: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48 container agnhost-container: <nil>
STEP: delete the pod
May 17 18:19:16.551: INFO: Waiting for pod pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48 to disappear
May 17 18:19:16.553: INFO: Pod pod-configmaps-8ddf0b6d-bef5-40a4-92e9-d23ec4a2cd48 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:16.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2945" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3695,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:16.565: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:19:17.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 18:19:19.754: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872357, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872357, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872357, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756872357, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:19:22.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:19:22.849: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5860-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:24.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1954" for this suite.
STEP: Destroying namespace "webhook-1954-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":220,"skipped":3695,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:24.417: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 17 18:19:24.486: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:29.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4888" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":221,"skipped":3701,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:29.143: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-c0d7fa40-4af1-4380-83ab-8b4617d9b654
STEP: Creating a pod to test consume configMaps
May 17 18:19:29.193: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a" in namespace "projected-4066" to be "Succeeded or Failed"
May 17 18:19:29.204: INFO: Pod "pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.409723ms
May 17 18:19:31.207: INFO: Pod "pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013812898s
STEP: Saw pod success
May 17 18:19:31.207: INFO: Pod "pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a" satisfied condition "Succeeded or Failed"
May 17 18:19:31.209: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 17 18:19:31.221: INFO: Waiting for pod pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a to disappear
May 17 18:19:31.225: INFO: Pod pod-projected-configmaps-f8d5b495-dc02-47e2-9449-6e8824c5c86a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:19:31.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4066" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3709,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:19:31.236: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-r2gpq in namespace proxy-5027
I0517 18:19:31.291642      19 runners.go:190] Created replication controller with name: proxy-service-r2gpq, namespace: proxy-5027, replica count: 1
I0517 18:19:32.346017      19 runners.go:190] proxy-service-r2gpq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0517 18:19:33.346252      19 runners.go:190] proxy-service-r2gpq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0517 18:19:34.346762      19 runners.go:190] proxy-service-r2gpq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0517 18:19:35.352961      19 runners.go:190] proxy-service-r2gpq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 17 18:19:35.365: INFO: setup took 4.095552646s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 17 18:19:35.405: INFO: (0) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 36.124006ms)
May 17 18:19:35.405: INFO: (0) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 39.516248ms)
May 17 18:19:35.405: INFO: (0) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 40.313842ms)
May 17 18:19:35.405: INFO: (0) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 39.954135ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 39.844342ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 39.588584ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 37.987953ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 40.315454ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 38.655645ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 38.257001ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 38.519134ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 36.776096ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 38.157686ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 36.5766ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 36.72839ms)
May 17 18:19:35.406: INFO: (0) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 36.809899ms)
May 17 18:19:35.412: INFO: (1) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 5.763705ms)
May 17 18:19:35.432: INFO: (1) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 22.12704ms)
May 17 18:19:35.433: INFO: (1) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 23.106613ms)
May 17 18:19:35.433: INFO: (1) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 23.205646ms)
May 17 18:19:35.434: INFO: (1) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 23.071775ms)
May 17 18:19:35.434: INFO: (1) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 23.734898ms)
May 17 18:19:35.434: INFO: (1) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 23.534736ms)
May 17 18:19:35.434: INFO: (1) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 23.970061ms)
May 17 18:19:35.435: INFO: (1) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 25.041889ms)
May 17 18:19:35.435: INFO: (1) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 25.311314ms)
May 17 18:19:35.436: INFO: (1) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 25.294339ms)
May 17 18:19:35.436: INFO: (1) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 25.266495ms)
May 17 18:19:35.436: INFO: (1) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 26.283848ms)
May 17 18:19:35.437: INFO: (1) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 26.426784ms)
May 17 18:19:35.437: INFO: (1) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 26.935591ms)
May 17 18:19:35.437: INFO: (1) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 27.115073ms)
May 17 18:19:35.455: INFO: (2) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 7.145607ms)
May 17 18:19:35.468: INFO: (2) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 28.758651ms)
May 17 18:19:35.468: INFO: (2) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 30.472349ms)
May 17 18:19:35.468: INFO: (2) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 27.89095ms)
May 17 18:19:35.469: INFO: (2) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 20.398755ms)
May 17 18:19:35.469: INFO: (2) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 20.895781ms)
May 17 18:19:35.469: INFO: (2) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 20.647305ms)
May 17 18:19:35.469: INFO: (2) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 20.586353ms)
May 17 18:19:35.472: INFO: (2) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 33.806108ms)
May 17 18:19:35.480: INFO: (2) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 31.298342ms)
May 17 18:19:35.480: INFO: (2) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 31.532353ms)
May 17 18:19:35.480: INFO: (2) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 31.761536ms)
May 17 18:19:35.484: INFO: (2) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 35.76738ms)
May 17 18:19:35.484: INFO: (2) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 35.920114ms)
May 17 18:19:35.485: INFO: (2) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 36.420308ms)
May 17 18:19:35.485: INFO: (2) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 36.856937ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 7.546414ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 24.485128ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 23.975738ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 24.190732ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 24.3161ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 24.439966ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 24.164318ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 22.931152ms)
May 17 18:19:35.510: INFO: (3) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 22.853667ms)
May 17 18:19:35.514: INFO: (3) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 7.394693ms)
May 17 18:19:35.515: INFO: (3) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 6.548369ms)
May 17 18:19:35.515: INFO: (3) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 11.191418ms)
May 17 18:19:35.515: INFO: (3) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 12.161964ms)
May 17 18:19:35.515: INFO: (3) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 7.168025ms)
May 17 18:19:35.516: INFO: (3) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 9.001965ms)
May 17 18:19:35.517: INFO: (3) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 14.086933ms)
May 17 18:19:35.547: INFO: (4) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 29.333411ms)
May 17 18:19:35.547: INFO: (4) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 28.146567ms)
May 17 18:19:35.548: INFO: (4) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 28.729948ms)
May 17 18:19:35.548: INFO: (4) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 30.237027ms)
May 17 18:19:35.548: INFO: (4) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 25.196602ms)
May 17 18:19:35.548: INFO: (4) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 24.437603ms)
May 17 18:19:35.548: INFO: (4) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 28.199343ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 22.557938ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 20.205942ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 19.977792ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 20.232224ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 20.707981ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 20.34041ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 20.295488ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 20.494338ms)
May 17 18:19:35.549: INFO: (4) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 20.498664ms)
May 17 18:19:35.554: INFO: (5) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 4.693663ms)
May 17 18:19:35.554: INFO: (5) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 4.791627ms)
May 17 18:19:35.554: INFO: (5) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 4.951187ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 10.454671ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 10.696896ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 13.227979ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 11.001246ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 10.943186ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 9.262004ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 11.156679ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 11.274541ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 10.746832ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 9.109678ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 9.028542ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 9.258055ms)
May 17 18:19:35.563: INFO: (5) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 8.991581ms)
May 17 18:19:35.566: INFO: (6) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 2.762675ms)
May 17 18:19:35.573: INFO: (6) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 7.750145ms)
May 17 18:19:35.577: INFO: (6) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 11.745746ms)
May 17 18:19:35.583: INFO: (6) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 17.812278ms)
May 17 18:19:35.584: INFO: (6) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 18.25125ms)
May 17 18:19:35.584: INFO: (6) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 18.553193ms)
May 17 18:19:35.584: INFO: (6) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 18.919ms)
May 17 18:19:35.584: INFO: (6) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 19.250142ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 19.677645ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 19.720887ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 19.788689ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 19.768827ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 19.736587ms)
May 17 18:19:35.585: INFO: (6) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 19.815872ms)
May 17 18:19:35.586: INFO: (6) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 20.193406ms)
May 17 18:19:35.589: INFO: (6) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 25.905554ms)
May 17 18:19:35.598: INFO: (7) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 8.475786ms)
May 17 18:19:35.598: INFO: (7) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 8.451634ms)
May 17 18:19:35.598: INFO: (7) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 8.781719ms)
May 17 18:19:35.602: INFO: (7) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 11.2064ms)
May 17 18:19:35.604: INFO: (7) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 14.132916ms)
May 17 18:19:35.604: INFO: (7) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 13.822317ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 14.382214ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 14.662778ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 14.963385ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 14.315268ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 14.637689ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 14.584904ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 14.223176ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 15.000398ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 14.802505ms)
May 17 18:19:35.605: INFO: (7) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 15.272481ms)
May 17 18:19:35.610: INFO: (8) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 3.928343ms)
May 17 18:19:35.611: INFO: (8) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 5.081802ms)
May 17 18:19:35.612: INFO: (8) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 6.357193ms)
May 17 18:19:35.612: INFO: (8) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 6.182282ms)
May 17 18:19:35.612: INFO: (8) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 6.320848ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 11.125164ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 10.98479ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 11.244533ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 14.841126ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 11.423274ms)
May 17 18:19:35.621: INFO: (8) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 11.58827ms)
May 17 18:19:35.622: INFO: (8) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 11.954462ms)
May 17 18:19:35.622: INFO: (8) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 11.906621ms)
May 17 18:19:35.622: INFO: (8) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 11.969173ms)
May 17 18:19:35.622: INFO: (8) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 11.935924ms)
May 17 18:19:35.622: INFO: (8) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 12.57127ms)
May 17 18:19:35.640: INFO: (9) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 17.77589ms)
May 17 18:19:35.641: INFO: (9) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 17.92754ms)
May 17 18:19:35.641: INFO: (9) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 17.165022ms)
May 17 18:19:35.641: INFO: (9) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 15.933123ms)
May 17 18:19:35.641: INFO: (9) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 17.42987ms)
May 17 18:19:35.641: INFO: (9) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 17.533099ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 17.628061ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 16.644679ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 16.73256ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 16.592456ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 16.754552ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 19.298421ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 18.609309ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 18.062774ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 17.924621ms)
May 17 18:19:35.642: INFO: (9) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 16.773094ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 11.243012ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 12.106444ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 12.168155ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 12.037625ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 12.145488ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 12.457457ms)
May 17 18:19:35.655: INFO: (10) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 12.60905ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 12.798741ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 10.896958ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 13.066966ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 13.035139ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 12.268473ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 13.18979ms)
May 17 18:19:35.656: INFO: (10) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 11.54975ms)
May 17 18:19:35.657: INFO: (10) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 11.973765ms)
May 17 18:19:35.657: INFO: (10) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 13.164911ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 12.690852ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 13.368135ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 12.902105ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 13.034633ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 12.345566ms)
May 17 18:19:35.670: INFO: (11) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 13.471856ms)
May 17 18:19:35.671: INFO: (11) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 13.051809ms)
May 17 18:19:35.671: INFO: (11) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 14.04965ms)
May 17 18:19:35.671: INFO: (11) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 5.357723ms)
May 17 18:19:35.671: INFO: (11) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 13.787573ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 9.786074ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 14.08391ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 14.384374ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 6.231195ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 10.114018ms)
May 17 18:19:35.672: INFO: (11) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 13.922961ms)
May 17 18:19:35.680: INFO: (12) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 8.084646ms)
May 17 18:19:35.681: INFO: (12) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 7.83069ms)
May 17 18:19:35.681: INFO: (12) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 8.615942ms)
May 17 18:19:35.687: INFO: (12) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 12.497772ms)
May 17 18:19:35.687: INFO: (12) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 14.863341ms)
May 17 18:19:35.687: INFO: (12) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 14.538142ms)
May 17 18:19:35.687: INFO: (12) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 14.409447ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 14.188817ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 15.453296ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 14.286582ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 11.483323ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 14.000328ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 11.8675ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 11.941981ms)
May 17 18:19:35.688: INFO: (12) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 14.802674ms)
May 17 18:19:35.689: INFO: (12) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 14.471175ms)
May 17 18:19:35.692: INFO: (13) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 2.74659ms)
May 17 18:19:35.696: INFO: (13) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 5.080896ms)
May 17 18:19:35.697: INFO: (13) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 7.396921ms)
May 17 18:19:35.697: INFO: (13) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 5.163344ms)
May 17 18:19:35.697: INFO: (13) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 5.117618ms)
May 17 18:19:35.697: INFO: (13) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 5.857564ms)
May 17 18:19:35.699: INFO: (13) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 8.989714ms)
May 17 18:19:35.701: INFO: (13) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 5.071495ms)
May 17 18:19:35.703: INFO: (13) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 11.148049ms)
May 17 18:19:35.704: INFO: (13) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 7.513238ms)
May 17 18:19:35.706: INFO: (13) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 9.156946ms)
May 17 18:19:35.706: INFO: (13) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 16.542042ms)
May 17 18:19:35.709: INFO: (13) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 10.098696ms)
May 17 18:19:35.709: INFO: (13) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 10.060961ms)
May 17 18:19:35.710: INFO: (13) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 9.939289ms)
May 17 18:19:35.710: INFO: (13) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 10.650427ms)
May 17 18:19:35.719: INFO: (14) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 5.404616ms)
May 17 18:19:35.719: INFO: (14) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 6.703032ms)
May 17 18:19:35.719: INFO: (14) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 5.838456ms)
May 17 18:19:35.719: INFO: (14) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 5.382065ms)
May 17 18:19:35.719: INFO: (14) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 8.805018ms)
May 17 18:19:35.723: INFO: (14) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 9.245354ms)
May 17 18:19:35.723: INFO: (14) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 9.832408ms)
May 17 18:19:35.729: INFO: (14) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 10.37593ms)
May 17 18:19:35.730: INFO: (14) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 11.123995ms)
May 17 18:19:35.730: INFO: (14) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 15.960392ms)
May 17 18:19:35.730: INFO: (14) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 11.662427ms)
May 17 18:19:35.730: INFO: (14) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 11.90172ms)
May 17 18:19:35.731: INFO: (14) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 12.0172ms)
May 17 18:19:35.731: INFO: (14) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 17.206636ms)
May 17 18:19:35.731: INFO: (14) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 12.49308ms)
May 17 18:19:35.731: INFO: (14) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 12.820651ms)
May 17 18:19:35.769: INFO: (15) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 35.766754ms)
May 17 18:19:35.772: INFO: (15) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 39.808544ms)
May 17 18:19:35.772: INFO: (15) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 39.476007ms)
May 17 18:19:35.774: INFO: (15) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 41.698936ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 41.486118ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 42.006212ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 42.706788ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 41.917882ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 42.389518ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 42.548267ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 42.740079ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 42.714099ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 42.409099ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 42.570667ms)
May 17 18:19:35.775: INFO: (15) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 42.480045ms)
May 17 18:19:35.776: INFO: (15) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 42.626668ms)
May 17 18:19:35.794: INFO: (16) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 15.852304ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 16.031885ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 15.815768ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 15.838495ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 15.878998ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 16.12609ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 16.018849ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 16.057942ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 16.122673ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 16.18208ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 16.090655ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 16.190187ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 18.464132ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 16.40689ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 16.320722ms)
May 17 18:19:35.795: INFO: (16) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 16.43101ms)
May 17 18:19:35.813: INFO: (17) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 14.761288ms)
May 17 18:19:35.814: INFO: (17) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 15.756786ms)
May 17 18:19:35.814: INFO: (17) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 15.52784ms)
May 17 18:19:35.814: INFO: (17) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 15.290282ms)
May 17 18:19:35.814: INFO: (17) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 15.565026ms)
May 17 18:19:35.816: INFO: (17) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 17.208578ms)
May 17 18:19:35.816: INFO: (17) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 17.411659ms)
May 17 18:19:35.817: INFO: (17) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 18.033984ms)
May 17 18:19:35.817: INFO: (17) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 18.146539ms)
May 17 18:19:35.818: INFO: (17) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 18.111003ms)
May 17 18:19:35.818: INFO: (17) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 18.113543ms)
May 17 18:19:35.819: INFO: (17) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 19.440706ms)
May 17 18:19:35.819: INFO: (17) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 20.300184ms)
May 17 18:19:35.820: INFO: (17) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 20.75152ms)
May 17 18:19:35.820: INFO: (17) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 21.163619ms)
May 17 18:19:35.821: INFO: (17) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 22.172421ms)
May 17 18:19:35.830: INFO: (18) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 8.610872ms)
May 17 18:19:35.830: INFO: (18) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 8.926857ms)
May 17 18:19:35.850: INFO: (18) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 21.905527ms)
May 17 18:19:35.851: INFO: (18) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 26.736383ms)
May 17 18:19:35.852: INFO: (18) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 27.578885ms)
May 17 18:19:35.853: INFO: (18) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 28.866524ms)
May 17 18:19:35.853: INFO: (18) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 28.856075ms)
May 17 18:19:35.854: INFO: (18) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 29.912435ms)
May 17 18:19:35.855: INFO: (18) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 33.059252ms)
May 17 18:19:35.856: INFO: (18) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 33.747119ms)
May 17 18:19:35.856: INFO: (18) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 31.221242ms)
May 17 18:19:35.856: INFO: (18) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 31.109009ms)
May 17 18:19:35.857: INFO: (18) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 32.668795ms)
May 17 18:19:35.858: INFO: (18) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 33.796082ms)
May 17 18:19:35.858: INFO: (18) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 36.026826ms)
May 17 18:19:35.859: INFO: (18) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 36.477936ms)
May 17 18:19:35.867: INFO: (19) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 5.328783ms)
May 17 18:19:35.872: INFO: (19) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">... (200; 13.503642ms)
May 17 18:19:35.873: INFO: (19) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname1/proxy/: tls baz (200; 14.0118ms)
May 17 18:19:35.876: INFO: (19) /api/v1/namespaces/proxy-5027/pods/http:proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 13.656608ms)
May 17 18:19:35.883: INFO: (19) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:1080/proxy/rewriteme">test<... (200; 20.228008ms)
May 17 18:19:35.883: INFO: (19) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj/proxy/rewriteme">test</a> (200; 20.733367ms)
May 17 18:19:35.883: INFO: (19) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:460/proxy/: tls baz (200; 20.695833ms)
May 17 18:19:35.883: INFO: (19) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname1/proxy/: foo (200; 22.035099ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:160/proxy/: foo (200; 20.569602ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:462/proxy/: tls qux (200; 20.690446ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/: <a href="/api/v1/namespaces/proxy-5027/pods/https:proxy-service-r2gpq-wzxcj:443/proxy/tlsrewritem... (200; 20.917622ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/pods/proxy-service-r2gpq-wzxcj:162/proxy/: bar (200; 20.857983ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/services/proxy-service-r2gpq:portname2/proxy/: bar (200; 9.958332ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname1/proxy/: foo (200; 10.745132ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/services/https:proxy-service-r2gpq:tlsportname2/proxy/: tls qux (200; 24.979485ms)
May 17 18:19:35.884: INFO: (19) /api/v1/namespaces/proxy-5027/services/http:proxy-service-r2gpq:portname2/proxy/: bar (200; 7.971488ms)
STEP: deleting ReplicationController proxy-service-r2gpq in namespace proxy-5027, will wait for the garbage collector to delete the pods
May 17 18:19:35.951: INFO: Deleting ReplicationController proxy-service-r2gpq took: 13.7177ms
May 17 18:19:36.053: INFO: Terminating ReplicationController proxy-service-r2gpq pods took: 101.421503ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:20:25.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5027" for this suite.

• [SLOW TEST:53.926 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":223,"skipped":3729,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:20:25.166: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-7f7ad459-a96c-4303-adef-384475a10972
STEP: Creating a pod to test consume configMaps
May 17 18:20:25.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931" in namespace "projected-7855" to be "Succeeded or Failed"
May 17 18:20:25.214: INFO: Pod "pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931": Phase="Pending", Reason="", readiness=false. Elapsed: 1.97588ms
May 17 18:20:27.221: INFO: Pod "pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008268451s
May 17 18:20:29.224: INFO: Pod "pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01146725s
STEP: Saw pod success
May 17 18:20:29.225: INFO: Pod "pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931" satisfied condition "Succeeded or Failed"
May 17 18:20:29.226: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931 container agnhost-container: <nil>
STEP: delete the pod
May 17 18:20:29.244: INFO: Waiting for pod pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931 to disappear
May 17 18:20:29.246: INFO: Pod pod-projected-configmaps-b9217fc8-0452-4097-9bb1-42baba4aa931 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:20:29.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7855" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":224,"skipped":3742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:20:29.261: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 17 18:20:29.312: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 18:21:29.380: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:21:29.383: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:21:29.451: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 17 18:21:29.453: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:21:29.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8405" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:21:29.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6054" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.296 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":225,"skipped":3777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:21:29.561: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:21:29.606: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b" in namespace "projected-6387" to be "Succeeded or Failed"
May 17 18:21:29.624: INFO: Pod "downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.738891ms
May 17 18:21:31.628: INFO: Pod "downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022758482s
STEP: Saw pod success
May 17 18:21:31.628: INFO: Pod "downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b" satisfied condition "Succeeded or Failed"
May 17 18:21:31.630: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b container client-container: <nil>
STEP: delete the pod
May 17 18:21:31.648: INFO: Waiting for pod downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b to disappear
May 17 18:21:31.652: INFO: Pod downwardapi-volume-a1bd16cb-a8b9-40f1-b35c-85ac3f1d786b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:21:31.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6387" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3804,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:21:31.668: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 17 18:21:31.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-6226 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 17 18:21:32.222: INFO: stderr: ""
May 17 18:21:32.222: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 17 18:21:32.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=kubectl-6226 delete pods e2e-test-httpd-pod'
May 17 18:21:45.108: INFO: stderr: ""
May 17 18:21:45.108: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:21:45.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6226" for this suite.

• [SLOW TEST:13.461 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":227,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:21:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:21:45.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5" in namespace "projected-4080" to be "Succeeded or Failed"
May 17 18:21:45.208: INFO: Pod "downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.17164ms
May 17 18:21:47.237: INFO: Pod "downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5": Phase="Running", Reason="", readiness=true. Elapsed: 2.038488014s
May 17 18:21:49.242: INFO: Pod "downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043465732s
STEP: Saw pod success
May 17 18:21:49.242: INFO: Pod "downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5" satisfied condition "Succeeded or Failed"
May 17 18:21:49.245: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5 container client-container: <nil>
STEP: delete the pod
May 17 18:21:49.289: INFO: Waiting for pod downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5 to disappear
May 17 18:21:49.293: INFO: Pod downwardapi-volume-e9b2981c-0f3e-4cd5-8a9f-88d27c689aa5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:21:49.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4080" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3856,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:21:49.305: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 17 18:21:49.352: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 17 18:22:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:22:16.663: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:22:38.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9274" for this suite.

• [SLOW TEST:49.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":229,"skipped":3876,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:22:38.440: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 17 18:22:38.479: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 18:23:38.550: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:23:38.553: INFO: Starting informer...
STEP: Starting pods...
May 17 18:23:38.775: INFO: Pod1 is running on 20test-worker-3. Tainting Node
May 17 18:23:40.798: INFO: Pod2 is running on 20test-worker-3. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 17 18:23:55.099: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 17 18:24:15.099: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:15.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7384" for this suite.

• [SLOW TEST:96.805 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":230,"skipped":3885,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:15.245: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 17 18:24:15.347: INFO: Waiting up to 5m0s for pod "downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986" in namespace "downward-api-2359" to be "Succeeded or Failed"
May 17 18:24:15.350: INFO: Pod "downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986": Phase="Pending", Reason="", readiness=false. Elapsed: 2.894599ms
May 17 18:24:17.353: INFO: Pod "downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006194617s
STEP: Saw pod success
May 17 18:24:17.353: INFO: Pod "downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986" satisfied condition "Succeeded or Failed"
May 17 18:24:17.356: INFO: Trying to get logs from node 20test-worker-3 pod downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986 container dapi-container: <nil>
STEP: delete the pod
May 17 18:24:17.386: INFO: Waiting for pod downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986 to disappear
May 17 18:24:17.388: INFO: Pod downward-api-8fad791f-6851-4020-bcc5-9e2b2709a986 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:17.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2359" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3890,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:17.398: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 17 18:24:17.449: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:48.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7250" for this suite.

• [SLOW TEST:30.964 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":232,"skipped":3918,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:48.369: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 17 18:24:48.424: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:51.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8462" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":233,"skipped":3929,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:51.028: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-2fcfd3ef-f0a5-4a65-b7a3-74a1f81f4963
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2fcfd3ef-f0a5-4a65-b7a3-74a1f81f4963
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:55.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4053" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":3941,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:55.148: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-f64fbadf-4197-460d-93e0-3cbc0dd067a5
STEP: Creating a pod to test consume configMaps
May 17 18:24:55.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e" in namespace "configmap-6075" to be "Succeeded or Failed"
May 17 18:24:55.196: INFO: Pod "pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.871535ms
May 17 18:24:57.200: INFO: Pod "pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011265266s
STEP: Saw pod success
May 17 18:24:57.200: INFO: Pod "pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e" satisfied condition "Succeeded or Failed"
May 17 18:24:57.205: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e container agnhost-container: <nil>
STEP: delete the pod
May 17 18:24:57.223: INFO: Waiting for pod pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e to disappear
May 17 18:24:57.225: INFO: Pod pod-configmaps-1ec2eee5-47a4-4d86-93df-8b72a6b3b53e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:57.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6075" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":3944,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-dbe06ece-ccd8-4138-8060-5a1c2fc96e4b
STEP: Creating a pod to test consume configMaps
May 17 18:24:57.285: INFO: Waiting up to 5m0s for pod "pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4" in namespace "configmap-6916" to be "Succeeded or Failed"
May 17 18:24:57.290: INFO: Pod "pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064303ms
May 17 18:24:59.292: INFO: Pod "pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006771411s
STEP: Saw pod success
May 17 18:24:59.292: INFO: Pod "pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4" satisfied condition "Succeeded or Failed"
May 17 18:24:59.294: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4 container agnhost-container: <nil>
STEP: delete the pod
May 17 18:24:59.311: INFO: Waiting for pod pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4 to disappear
May 17 18:24:59.313: INFO: Pod pod-configmaps-83de893e-f9ec-41eb-941c-605e630e76b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:59.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6916" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":3955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:59.334: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 17 18:24:59.422: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 17 18:24:59.451: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:24:59.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1734" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":237,"skipped":3990,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:24:59.484: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7908
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 17 18:24:59.547: INFO: Found 0 stateful pods, waiting for 3
May 17 18:25:09.552: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:25:09.552: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:25:09.552: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 17 18:25:09.583: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 17 18:25:19.620: INFO: Updating stateful set ss2
May 17 18:25:19.649: INFO: Waiting for Pod statefulset-7908/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 17 18:25:29.739: INFO: Found 2 stateful pods, waiting for 3
May 17 18:25:39.743: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:25:39.743: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:25:39.743: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 17 18:25:39.766: INFO: Updating stateful set ss2
May 17 18:25:39.782: INFO: Waiting for Pod statefulset-7908/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:25:49.789: INFO: Waiting for Pod statefulset-7908/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:25:59.790: INFO: Waiting for Pod statefulset-7908/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:09.788: INFO: Waiting for Pod statefulset-7908/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:19.789: INFO: Waiting for Pod statefulset-7908/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:29.832: INFO: Updating stateful set ss2
May 17 18:26:29.839: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:26:29.839: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:39.846: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:26:39.847: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:49.847: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:26:49.847: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:26:59.845: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:26:59.845: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:27:09.846: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:27:09.846: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:27:19.846: INFO: Waiting for StatefulSet statefulset-7908/ss2 to complete update
May 17 18:27:19.846: INFO: Waiting for Pod statefulset-7908/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 18:27:29.845: INFO: Deleting all statefulset in ns statefulset-7908
May 17 18:27:29.847: INFO: Scaling statefulset ss2 to 0
May 17 18:29:29.862: INFO: Waiting for statefulset status.replicas updated to 0
May 17 18:29:29.865: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:29:29.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7908" for this suite.

• [SLOW TEST:270.410 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":238,"skipped":4006,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:29:29.899: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:29:29.934: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9f45fe1c-7091-4fb0-be33-be6d758d9d8f" in namespace "security-context-test-659" to be "Succeeded or Failed"
May 17 18:29:29.937: INFO: Pod "busybox-user-65534-9f45fe1c-7091-4fb0-be33-be6d758d9d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479315ms
May 17 18:29:31.941: INFO: Pod "busybox-user-65534-9f45fe1c-7091-4fb0-be33-be6d758d9d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006276884s
May 17 18:29:31.941: INFO: Pod "busybox-user-65534-9f45fe1c-7091-4fb0-be33-be6d758d9d8f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:29:31.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-659" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:29:31.957: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:30:01.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3338" for this suite.
STEP: Destroying namespace "nsdeletetest-5961" for this suite.
May 17 18:30:01.123: INFO: Namespace nsdeletetest-5961 was already deleted
STEP: Destroying namespace "nsdeletetest-5653" for this suite.

• [SLOW TEST:29.169 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":240,"skipped":4058,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:30:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-g6m9
STEP: Creating a pod to test atomic-volume-subpath
May 17 18:30:01.213: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g6m9" in namespace "subpath-6911" to be "Succeeded or Failed"
May 17 18:30:01.222: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.922159ms
May 17 18:30:03.227: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013995552s
May 17 18:30:05.232: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 4.018854658s
May 17 18:30:07.236: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 6.02256696s
May 17 18:30:09.243: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 8.029026012s
May 17 18:30:11.247: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 10.033395724s
May 17 18:30:13.251: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 12.037778969s
May 17 18:30:15.257: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 14.043209645s
May 17 18:30:17.261: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 16.047687274s
May 17 18:30:19.266: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 18.052540497s
May 17 18:30:21.271: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Running", Reason="", readiness=true. Elapsed: 20.057918068s
May 17 18:30:23.275: INFO: Pod "pod-subpath-test-configmap-g6m9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061706811s
STEP: Saw pod success
May 17 18:30:23.275: INFO: Pod "pod-subpath-test-configmap-g6m9" satisfied condition "Succeeded or Failed"
May 17 18:30:23.279: INFO: Trying to get logs from node 20test-worker-3 pod pod-subpath-test-configmap-g6m9 container test-container-subpath-configmap-g6m9: <nil>
STEP: delete the pod
May 17 18:30:23.312: INFO: Waiting for pod pod-subpath-test-configmap-g6m9 to disappear
May 17 18:30:23.316: INFO: Pod pod-subpath-test-configmap-g6m9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g6m9
May 17 18:30:23.316: INFO: Deleting pod "pod-subpath-test-configmap-g6m9" in namespace "subpath-6911"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:30:23.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6911" for this suite.

• [SLOW TEST:22.206 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":241,"skipped":4097,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:30:23.334: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-587fcf12-ef63-4bf6-aae3-132e4749ba46
STEP: Creating a pod to test consume secrets
May 17 18:30:23.383: INFO: Waiting up to 5m0s for pod "pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd" in namespace "secrets-6696" to be "Succeeded or Failed"
May 17 18:30:23.391: INFO: Pod "pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.806321ms
May 17 18:30:25.395: INFO: Pod "pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0126577s
STEP: Saw pod success
May 17 18:30:25.396: INFO: Pod "pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd" satisfied condition "Succeeded or Failed"
May 17 18:30:25.398: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd container secret-env-test: <nil>
STEP: delete the pod
May 17 18:30:25.418: INFO: Waiting for pod pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd to disappear
May 17 18:30:25.421: INFO: Pod pod-secrets-e3716ceb-aa69-42b0-849c-12edb0de4dfd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:30:25.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6696" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4103,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:30:25.430: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:25.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2623" for this suite.

• [SLOW TEST:60.103 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4117,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:25.535: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-2172/configmap-test-ed05684d-fced-44dd-b435-5642e6417e66
STEP: Creating a pod to test consume configMaps
May 17 18:31:25.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4" in namespace "configmap-2172" to be "Succeeded or Failed"
May 17 18:31:25.647: INFO: Pod "pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.774833ms
May 17 18:31:27.721: INFO: Pod "pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082778476s
May 17 18:31:29.726: INFO: Pod "pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087798246s
STEP: Saw pod success
May 17 18:31:29.726: INFO: Pod "pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4" satisfied condition "Succeeded or Failed"
May 17 18:31:29.729: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4 container env-test: <nil>
STEP: delete the pod
May 17 18:31:29.745: INFO: Waiting for pod pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4 to disappear
May 17 18:31:29.747: INFO: Pod pod-configmaps-1141f769-7eb8-4dd9-a8bc-59bd08c700f4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:29.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2172" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4119,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:29.756: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:31:29.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133" in namespace "projected-3460" to be "Succeeded or Failed"
May 17 18:31:29.806: INFO: Pod "downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133": Phase="Pending", Reason="", readiness=false. Elapsed: 6.828387ms
May 17 18:31:31.820: INFO: Pod "downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02130627s
May 17 18:31:33.826: INFO: Pod "downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026980742s
STEP: Saw pod success
May 17 18:31:33.826: INFO: Pod "downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133" satisfied condition "Succeeded or Failed"
May 17 18:31:33.828: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133 container client-container: <nil>
STEP: delete the pod
May 17 18:31:33.863: INFO: Waiting for pod downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133 to disappear
May 17 18:31:33.866: INFO: Pod downwardapi-volume-a37935f8-0d20-497e-9f24-cabb19b06133 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:33.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3460" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4120,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:33.873: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-7977
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7977 to expose endpoints map[]
May 17 18:31:33.937: INFO: successfully validated that service multi-endpoint-test in namespace services-7977 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7977
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7977 to expose endpoints map[pod1:[100]]
May 17 18:31:36.967: INFO: successfully validated that service multi-endpoint-test in namespace services-7977 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7977
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7977 to expose endpoints map[pod1:[100] pod2:[101]]
May 17 18:31:40.010: INFO: successfully validated that service multi-endpoint-test in namespace services-7977 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-7977
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7977 to expose endpoints map[pod2:[101]]
May 17 18:31:40.055: INFO: successfully validated that service multi-endpoint-test in namespace services-7977 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7977
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7977 to expose endpoints map[]
May 17 18:31:40.094: INFO: successfully validated that service multi-endpoint-test in namespace services-7977 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:40.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7977" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.264 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":246,"skipped":4140,"failed":0}
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:40.139: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 17 18:31:40.182: INFO: created test-podtemplate-1
May 17 18:31:40.186: INFO: created test-podtemplate-2
May 17 18:31:40.190: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 17 18:31:40.192: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 17 18:31:40.210: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:40.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3015" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":247,"skipped":4140,"failed":0}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:40.231: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:31:40.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5463" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":248,"skipped":4141,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:31:40.426: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 17 18:31:40.464: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:31:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:32:10.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8485" for this suite.

• [SLOW TEST:29.853 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":249,"skipped":4175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:32:10.281: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 17 18:32:12.373: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:32:12.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3491" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4238,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:32:12.401: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-bec37929-9053-497f-a550-b1cbda2e703f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:32:12.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-873" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":251,"skipped":4243,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:32:12.463: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:32:14.542: INFO: Deleting pod "var-expansion-791d554c-7229-4acf-ab2d-c5d96aa7bfef" in namespace "var-expansion-7513"
May 17 18:32:14.549: INFO: Wait up to 5m0s for pod "var-expansion-791d554c-7229-4acf-ab2d-c5d96aa7bfef" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:32:36.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7513" for this suite.

• [SLOW TEST:24.105 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":252,"skipped":4262,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:32:36.572: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:33:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9499" for this suite.

• [SLOW TEST:28.092 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":253,"skipped":4269,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:33:04.664: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 17 18:33:04.735: INFO: Waiting up to 5m0s for pod "downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660" in namespace "downward-api-4731" to be "Succeeded or Failed"
May 17 18:33:04.766: INFO: Pod "downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660": Phase="Pending", Reason="", readiness=false. Elapsed: 30.714437ms
May 17 18:33:06.769: INFO: Pod "downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033860086s
STEP: Saw pod success
May 17 18:33:06.769: INFO: Pod "downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660" satisfied condition "Succeeded or Failed"
May 17 18:33:06.772: INFO: Trying to get logs from node 20test-worker-3 pod downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660 container dapi-container: <nil>
STEP: delete the pod
May 17 18:33:06.808: INFO: Waiting for pod downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660 to disappear
May 17 18:33:06.810: INFO: Pod downward-api-d196efe3-fa82-49b5-aeb7-6a06161e6660 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:33:06.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4731" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:33:06.820: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:33:08.895: INFO: Deleting pod "var-expansion-898aec17-4fbc-43c3-9a48-56bdf97b55e7" in namespace "var-expansion-5776"
May 17 18:33:08.902: INFO: Wait up to 5m0s for pod "var-expansion-898aec17-4fbc-43c3-9a48-56bdf97b55e7" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:33:36.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5776" for this suite.

• [SLOW TEST:30.098 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":255,"skipped":4304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:33:36.920: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 17 18:33:36.977: INFO: Waiting up to 5m0s for pod "var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf" in namespace "var-expansion-7012" to be "Succeeded or Failed"
May 17 18:33:36.983: INFO: Pod "var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.647564ms
May 17 18:33:38.989: INFO: Pod "var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011422907s
STEP: Saw pod success
May 17 18:33:38.989: INFO: Pod "var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf" satisfied condition "Succeeded or Failed"
May 17 18:33:38.995: INFO: Trying to get logs from node 20test-worker-3 pod var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf container dapi-container: <nil>
STEP: delete the pod
May 17 18:33:39.011: INFO: Waiting for pod var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf to disappear
May 17 18:33:39.014: INFO: Pod var-expansion-1a79fe20-d850-4be8-ac8b-9d09a6f2daaf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:33:39.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7012" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4337,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:33:39.025: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-ngwj
STEP: Creating a pod to test atomic-volume-subpath
May 17 18:33:39.066: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ngwj" in namespace "subpath-9899" to be "Succeeded or Failed"
May 17 18:33:39.073: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.735259ms
May 17 18:33:41.076: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009699035s
May 17 18:33:43.080: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013772248s
May 17 18:33:45.085: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 6.018288325s
May 17 18:33:47.090: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 8.023600622s
May 17 18:33:49.094: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 10.027151749s
May 17 18:33:51.098: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 12.031649575s
May 17 18:33:53.111: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 14.044464094s
May 17 18:33:55.116: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 16.049365152s
May 17 18:33:57.121: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 18.054720069s
May 17 18:33:59.128: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Running", Reason="", readiness=true. Elapsed: 20.061163131s
May 17 18:34:01.131: INFO: Pod "pod-subpath-test-secret-ngwj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064419252s
STEP: Saw pod success
May 17 18:34:01.131: INFO: Pod "pod-subpath-test-secret-ngwj" satisfied condition "Succeeded or Failed"
May 17 18:34:01.133: INFO: Trying to get logs from node 20test-worker-3 pod pod-subpath-test-secret-ngwj container test-container-subpath-secret-ngwj: <nil>
STEP: delete the pod
May 17 18:34:01.150: INFO: Waiting for pod pod-subpath-test-secret-ngwj to disappear
May 17 18:34:01.152: INFO: Pod pod-subpath-test-secret-ngwj no longer exists
STEP: Deleting pod pod-subpath-test-secret-ngwj
May 17 18:34:01.153: INFO: Deleting pod "pod-subpath-test-secret-ngwj" in namespace "subpath-9899"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:34:01.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9899" for this suite.

• [SLOW TEST:22.138 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":257,"skipped":4339,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:34:01.166: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6181
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 17 18:34:01.224: INFO: Found 0 stateful pods, waiting for 3
May 17 18:34:11.228: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:34:11.228: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:34:11.228: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 17 18:34:11.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-6181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 18:34:11.733: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 18:34:11.733: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 18:34:11.733: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 17 18:34:21.761: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 17 18:34:31.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-6181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 18:34:31.953: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 18:34:31.953: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 18:34:31.953: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 18:34:41.977: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:34:41.977: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:34:41.977: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:34:41.977: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:34:51.983: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:34:51.983: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:34:51.983: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:34:51.983: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:01.985: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:01.985: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:01.985: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:01.985: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:11.983: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:11.983: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:11.983: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:11.984: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:21.984: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:21.985: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:21.985: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:21.985: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:31.986: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:31.986: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:31.986: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:31.986: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:41.983: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:41.983: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:41.983: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 17 18:35:51.985: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:35:51.986: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 17 18:36:01.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-6181 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 17 18:36:02.192: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 17 18:36:02.192: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 17 18:36:02.192: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 17 18:36:12.240: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 17 18:36:12.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=statefulset-6181 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 17 18:36:12.445: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 17 18:36:12.445: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 17 18:36:12.445: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 17 18:36:22.463: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:36:22.464: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:22.464: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:22.464: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:32.472: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:36:32.473: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:32.473: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:32.473: INFO: Waiting for Pod statefulset-6181/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:42.472: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:36:42.472: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:42.472: INFO: Waiting for Pod statefulset-6181/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:36:52.472: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:36:52.472: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:37:02.473: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:37:02.473: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:37:12.470: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:37:12.470: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:37:22.474: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:37:22.474: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
May 17 18:37:32.471: INFO: Waiting for StatefulSet statefulset-6181/ss2 to complete update
May 17 18:37:32.472: INFO: Waiting for Pod statefulset-6181/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 18:37:42.481: INFO: Deleting all statefulset in ns statefulset-6181
May 17 18:37:42.485: INFO: Scaling statefulset ss2 to 0
May 17 18:39:02.545: INFO: Waiting for statefulset status.replicas updated to 0
May 17 18:39:02.546: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:39:02.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6181" for this suite.

• [SLOW TEST:301.414 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":258,"skipped":4348,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:39:02.580: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 17 18:39:02.619: INFO: Waiting up to 5m0s for pod "pod-b24c0500-7ca6-4561-8f2e-73ded0007e36" in namespace "emptydir-6048" to be "Succeeded or Failed"
May 17 18:39:02.634: INFO: Pod "pod-b24c0500-7ca6-4561-8f2e-73ded0007e36": Phase="Pending", Reason="", readiness=false. Elapsed: 15.013396ms
May 17 18:39:04.638: INFO: Pod "pod-b24c0500-7ca6-4561-8f2e-73ded0007e36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019053334s
STEP: Saw pod success
May 17 18:39:04.638: INFO: Pod "pod-b24c0500-7ca6-4561-8f2e-73ded0007e36" satisfied condition "Succeeded or Failed"
May 17 18:39:04.640: INFO: Trying to get logs from node 20test-worker-3 pod pod-b24c0500-7ca6-4561-8f2e-73ded0007e36 container test-container: <nil>
STEP: delete the pod
May 17 18:39:04.670: INFO: Waiting for pod pod-b24c0500-7ca6-4561-8f2e-73ded0007e36 to disappear
May 17 18:39:04.673: INFO: Pod pod-b24c0500-7ca6-4561-8f2e-73ded0007e36 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:39:04.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6048" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4362,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:39:04.686: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 17 18:39:04.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-925  fdd6d086-6feb-4e13-84d5-cd01823180f1 47397 0 2021-05-17 18:39:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-17 18:39:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:39:04.749: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-925  fdd6d086-6feb-4e13-84d5-cd01823180f1 47398 0 2021-05-17 18:39:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-17 18:39:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:39:04.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-925" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":260,"skipped":4377,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:39:04.759: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 17 18:39:06.834: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:39:06.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2637" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4384,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:39:07.005: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:39:07.065: INFO: Create a RollingUpdate DaemonSet
May 17 18:39:07.074: INFO: Check that daemon pods launch on every node of the cluster
May 17 18:39:07.087: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:07.097: INFO: Number of nodes with available pods: 0
May 17 18:39:07.098: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:39:08.103: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:08.119: INFO: Number of nodes with available pods: 0
May 17 18:39:08.120: INFO: Node 20test-worker-1 is running more than one daemon pod
May 17 18:39:09.102: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:09.105: INFO: Number of nodes with available pods: 1
May 17 18:39:09.105: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:39:10.103: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:10.107: INFO: Number of nodes with available pods: 2
May 17 18:39:10.107: INFO: Node 20test-worker-3 is running more than one daemon pod
May 17 18:39:11.104: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:11.107: INFO: Number of nodes with available pods: 3
May 17 18:39:11.107: INFO: Number of running nodes: 3, number of available pods: 3
May 17 18:39:11.107: INFO: Update the DaemonSet to trigger a rollout
May 17 18:39:11.114: INFO: Updating DaemonSet daemon-set
May 17 18:39:19.132: INFO: Roll back the DaemonSet before rollout is complete
May 17 18:39:19.139: INFO: Updating DaemonSet daemon-set
May 17 18:39:19.139: INFO: Make sure DaemonSet rollback is complete
May 17 18:39:19.145: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:19.145: INFO: Pod daemon-set-788tg is not available
May 17 18:39:19.150: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:20.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:20.155: INFO: Pod daemon-set-788tg is not available
May 17 18:39:20.158: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:21.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:21.155: INFO: Pod daemon-set-788tg is not available
May 17 18:39:21.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:22.153: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:22.153: INFO: Pod daemon-set-788tg is not available
May 17 18:39:22.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:23.153: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:23.153: INFO: Pod daemon-set-788tg is not available
May 17 18:39:23.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:24.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:24.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:24.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:25.160: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:25.160: INFO: Pod daemon-set-788tg is not available
May 17 18:39:25.164: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:26.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:26.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:26.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:27.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:27.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:27.160: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:28.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:28.155: INFO: Pod daemon-set-788tg is not available
May 17 18:39:28.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:29.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:29.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:29.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:30.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:30.155: INFO: Pod daemon-set-788tg is not available
May 17 18:39:30.158: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:31.157: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:31.157: INFO: Pod daemon-set-788tg is not available
May 17 18:39:31.160: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:32.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:32.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:32.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:33.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:33.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:33.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:34.157: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:34.157: INFO: Pod daemon-set-788tg is not available
May 17 18:39:34.163: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:35.156: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:35.156: INFO: Pod daemon-set-788tg is not available
May 17 18:39:35.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:36.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:36.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:36.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:37.153: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:37.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:37.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:38.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:38.155: INFO: Pod daemon-set-788tg is not available
May 17 18:39:38.160: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:39.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:39.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:39.158: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:40.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:40.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:40.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:41.156: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:41.156: INFO: Pod daemon-set-788tg is not available
May 17 18:39:41.159: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:42.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:42.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:42.158: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:43.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:43.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:43.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:44.268: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:44.268: INFO: Pod daemon-set-788tg is not available
May 17 18:39:44.271: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:45.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:45.156: INFO: Pod daemon-set-788tg is not available
May 17 18:39:45.186: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:46.153: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:46.153: INFO: Pod daemon-set-788tg is not available
May 17 18:39:46.157: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:47.155: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:47.156: INFO: Pod daemon-set-788tg is not available
May 17 18:39:47.160: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:48.154: INFO: Wrong image for pod: daemon-set-788tg. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 17 18:39:48.154: INFO: Pod daemon-set-788tg is not available
May 17 18:39:48.158: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 17 18:39:49.154: INFO: Pod daemon-set-5bkbm is not available
May 17 18:39:49.156: INFO: DaemonSet pods can't tolerate node 20test-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2305, will wait for the garbage collector to delete the pods
May 17 18:39:49.220: INFO: Deleting DaemonSet.extensions daemon-set took: 5.481602ms
May 17 18:39:50.120: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.344932ms
May 17 18:40:35.128: INFO: Number of nodes with available pods: 0
May 17 18:40:35.128: INFO: Number of running nodes: 0, number of available pods: 0
May 17 18:40:35.129: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47981"},"items":null}

May 17 18:40:35.131: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47981"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:35.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2305" for this suite.

• [SLOW TEST:88.146 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":262,"skipped":4395,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:35.152: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 17 18:40:39.238: INFO: DNS probes using dns-8481/dns-test-40cd6825-95bf-431e-a5c4-3ba04df80c5e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8481" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":263,"skipped":4403,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:39.277: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 17 18:40:39.335: INFO: Pod name pod-release: Found 0 pods out of 1
May 17 18:40:44.338: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:44.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8983" for this suite.

• [SLOW TEST:5.142 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":264,"skipped":4415,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:44.420: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-86069fd2-5586-41c0-b6fb-cf69bc7afe87
STEP: Creating a pod to test consume configMaps
May 17 18:40:44.497: INFO: Waiting up to 5m0s for pod "pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc" in namespace "configmap-8139" to be "Succeeded or Failed"
May 17 18:40:44.511: INFO: Pod "pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.45873ms
May 17 18:40:46.516: INFO: Pod "pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018579258s
STEP: Saw pod success
May 17 18:40:46.516: INFO: Pod "pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc" satisfied condition "Succeeded or Failed"
May 17 18:40:46.519: INFO: Trying to get logs from node 20test-worker-3 pod pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc container configmap-volume-test: <nil>
STEP: delete the pod
May 17 18:40:46.567: INFO: Waiting for pod pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc to disappear
May 17 18:40:46.569: INFO: Pod pod-configmaps-641cfd50-5683-44d2-8203-3f1f3c80e3bc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:46.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8139" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:46.587: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:46.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8642" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":266,"skipped":4485,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:46.667: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 17 18:40:46.746: INFO: Waiting up to 5m0s for pod "pod-ce49ecc1-73ee-4249-a404-743e7c528d4c" in namespace "emptydir-9783" to be "Succeeded or Failed"
May 17 18:40:46.755: INFO: Pod "pod-ce49ecc1-73ee-4249-a404-743e7c528d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.831937ms
May 17 18:40:48.758: INFO: Pod "pod-ce49ecc1-73ee-4249-a404-743e7c528d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012257751s
STEP: Saw pod success
May 17 18:40:48.758: INFO: Pod "pod-ce49ecc1-73ee-4249-a404-743e7c528d4c" satisfied condition "Succeeded or Failed"
May 17 18:40:48.760: INFO: Trying to get logs from node 20test-worker-3 pod pod-ce49ecc1-73ee-4249-a404-743e7c528d4c container test-container: <nil>
STEP: delete the pod
May 17 18:40:48.774: INFO: Waiting for pod pod-ce49ecc1-73ee-4249-a404-743e7c528d4c to disappear
May 17 18:40:48.776: INFO: Pod pod-ce49ecc1-73ee-4249-a404-743e7c528d4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:48.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9783" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4488,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:48.788: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:40:50.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1667" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4509,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:40:50.856: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 17 18:40:54.957: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:40:54.965: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:40:56.966: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:40:56.971: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:40:58.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:40:58.970: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:41:00.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:41:00.971: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:41:02.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:41:02.968: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:41:04.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:41:04.970: INFO: Pod pod-with-prestop-http-hook still exists
May 17 18:41:06.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 17 18:41:06.971: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:41:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6052" for this suite.

• [SLOW TEST:16.133 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:41:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 17 18:41:07.033: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 17 18:41:07.775: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
May 17 18:41:09.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 18:41:11.848: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 18:41:13.845: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 18:41:15.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 18:41:17.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756873667, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 17 18:41:22.076: INFO: Waited 2.203797321s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:41:23.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8070" for this suite.

• [SLOW TEST:16.217 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":270,"skipped":4588,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:41:23.214: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 17 18:41:25.855: INFO: Successfully updated pod "pod-update-80fbefcb-5f6a-43b7-a377-58423729d3cf"
STEP: verifying the updated pod is in kubernetes
May 17 18:41:25.863: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:41:25.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6029" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4597,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:41:25.875: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 17 18:41:25.930: INFO: Waiting up to 5m0s for pod "pod-ae0bcebb-4267-4064-8043-90747c0522c3" in namespace "emptydir-7901" to be "Succeeded or Failed"
May 17 18:41:25.933: INFO: Pod "pod-ae0bcebb-4267-4064-8043-90747c0522c3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.030644ms
May 17 18:41:27.936: INFO: Pod "pod-ae0bcebb-4267-4064-8043-90747c0522c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005948206s
STEP: Saw pod success
May 17 18:41:27.936: INFO: Pod "pod-ae0bcebb-4267-4064-8043-90747c0522c3" satisfied condition "Succeeded or Failed"
May 17 18:41:27.939: INFO: Trying to get logs from node 20test-worker-3 pod pod-ae0bcebb-4267-4064-8043-90747c0522c3 container test-container: <nil>
STEP: delete the pod
May 17 18:41:27.961: INFO: Waiting for pod pod-ae0bcebb-4267-4064-8043-90747c0522c3 to disappear
May 17 18:41:27.986: INFO: Pod pod-ae0bcebb-4267-4064-8043-90747c0522c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:41:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7901" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4598,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:41:28.003: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9294
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9294
STEP: creating replication controller externalsvc in namespace services-9294
I0517 18:41:28.139273      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9294, replica count: 2
I0517 18:41:31.189630      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 17 18:41:31.214: INFO: Creating new exec pod
May 17 18:41:33.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=services-9294 exec execpodmthtm -- /bin/sh -x -c nslookup nodeport-service.services-9294.svc.cluster.local'
May 17 18:41:33.533: INFO: stderr: "+ nslookup nodeport-service.services-9294.svc.cluster.local\n"
May 17 18:41:33.533: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-9294.svc.cluster.local\tcanonical name = externalsvc.services-9294.svc.cluster.local.\nName:\texternalsvc.services-9294.svc.cluster.local\nAddress: 172.30.233.230\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9294, will wait for the garbage collector to delete the pods
May 17 18:41:33.596: INFO: Deleting ReplicationController externalsvc took: 9.760273ms
May 17 18:41:33.696: INFO: Terminating ReplicationController externalsvc pods took: 100.247719ms
May 17 18:42:35.157: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:42:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9294" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:67.186 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":273,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:42:35.197: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 17 18:42:35.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49020 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:42:35.237: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49020 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 17 18:42:45.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49093 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:42:45.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49093 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 17 18:42:55.255: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49133 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:42:55.256: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49133 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 17 18:43:05.261: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49170 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:43:05.261: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1269  32b0885f-9f85-4f5c-b0b8-8d6635326712 49170 0 2021-05-17 18:42:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-17 18:42:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 17 18:43:15.268: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1269  0619cfce-e92e-4fce-b87a-cc4a6e86313b 49207 0 2021-05-17 18:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-17 18:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:43:15.268: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1269  0619cfce-e92e-4fce-b87a-cc4a6e86313b 49207 0 2021-05-17 18:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-17 18:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 17 18:43:25.274: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1269  0619cfce-e92e-4fce-b87a-cc4a6e86313b 49243 0 2021-05-17 18:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-17 18:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 17 18:43:25.275: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1269  0619cfce-e92e-4fce-b87a-cc4a6e86313b 49243 0 2021-05-17 18:43:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-17 18:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:43:35.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1269" for this suite.

• [SLOW TEST:60.090 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":274,"skipped":4623,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:43:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-4d25b55a-e023-4d5c-a7cf-c38e624d5212
STEP: Creating a pod to test consume secrets
May 17 18:43:35.355: INFO: Waiting up to 5m0s for pod "pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b" in namespace "secrets-7813" to be "Succeeded or Failed"
May 17 18:43:35.369: INFO: Pod "pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.39036ms
May 17 18:43:37.372: INFO: Pod "pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016327159s
STEP: Saw pod success
May 17 18:43:37.373: INFO: Pod "pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b" satisfied condition "Succeeded or Failed"
May 17 18:43:37.375: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:43:37.397: INFO: Waiting for pod pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b to disappear
May 17 18:43:37.398: INFO: Pod pod-secrets-fa7d7687-715c-44bd-ab70-31f6c13cdb8b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:43:37.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7813" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":275,"skipped":4642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:43:37.405: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-33829676-f0bd-48c1-b39e-75449238a666
STEP: Creating secret with name s-test-opt-upd-32e6ed39-0d69-4b18-85b4-a60aebcddabd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-33829676-f0bd-48c1-b39e-75449238a666
STEP: Updating secret s-test-opt-upd-32e6ed39-0d69-4b18-85b4-a60aebcddabd
STEP: Creating secret with name s-test-opt-create-8e445459-a05b-4bdc-8e97-646a1fb181fc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:03.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-106" for this suite.

• [SLOW TEST:86.413 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4671,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:03.819: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-1c53070f-6f95-4de4-a3c7-67c470dc732d
STEP: Creating a pod to test consume secrets
May 17 18:45:03.870: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a" in namespace "projected-3156" to be "Succeeded or Failed"
May 17 18:45:03.875: INFO: Pod "pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.146887ms
May 17 18:45:05.878: INFO: Pod "pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008079392s
STEP: Saw pod success
May 17 18:45:05.879: INFO: Pod "pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a" satisfied condition "Succeeded or Failed"
May 17 18:45:05.881: INFO: Trying to get logs from node 20test-worker-3 pod pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 17 18:45:05.903: INFO: Waiting for pod pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a to disappear
May 17 18:45:05.906: INFO: Pod pod-projected-secrets-06edc50a-4254-4c21-81c3-17bb58e3a12a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:05.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3156" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":277,"skipped":4674,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:05.915: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 17 18:45:05.967: INFO: Waiting up to 5m0s for pod "pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9" in namespace "emptydir-5561" to be "Succeeded or Failed"
May 17 18:45:05.970: INFO: Pod "pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.945206ms
May 17 18:45:07.974: INFO: Pod "pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007537294s
STEP: Saw pod success
May 17 18:45:07.975: INFO: Pod "pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9" satisfied condition "Succeeded or Failed"
May 17 18:45:07.977: INFO: Trying to get logs from node 20test-worker-3 pod pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9 container test-container: <nil>
STEP: delete the pod
May 17 18:45:07.995: INFO: Waiting for pod pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9 to disappear
May 17 18:45:07.997: INFO: Pod pod-87cc00b5-e7dd-4e2b-8fdf-1e376a3ca1f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:07.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:08.009: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 17 18:45:12.084: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2215 PodName:pod-sharedvolume-7e66f401-5727-440b-8af0-8b7e8ba037a7 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:45:12.084: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:45:12.162: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:12.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2215" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":279,"skipped":4698,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:12.172: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:45:12.231: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 17 18:45:12.241: INFO: Number of nodes with available pods: 0
May 17 18:45:12.241: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 17 18:45:12.280: INFO: Number of nodes with available pods: 0
May 17 18:45:12.280: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:13.293: INFO: Number of nodes with available pods: 0
May 17 18:45:13.293: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:14.286: INFO: Number of nodes with available pods: 1
May 17 18:45:14.286: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 17 18:45:14.310: INFO: Number of nodes with available pods: 1
May 17 18:45:14.310: INFO: Number of running nodes: 0, number of available pods: 1
May 17 18:45:15.314: INFO: Number of nodes with available pods: 0
May 17 18:45:15.314: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 17 18:45:15.351: INFO: Number of nodes with available pods: 0
May 17 18:45:15.351: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:16.358: INFO: Number of nodes with available pods: 0
May 17 18:45:16.359: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:17.358: INFO: Number of nodes with available pods: 0
May 17 18:45:17.358: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:18.355: INFO: Number of nodes with available pods: 0
May 17 18:45:18.355: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:19.356: INFO: Number of nodes with available pods: 0
May 17 18:45:19.356: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:20.355: INFO: Number of nodes with available pods: 0
May 17 18:45:20.355: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:21.356: INFO: Number of nodes with available pods: 0
May 17 18:45:21.356: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:22.355: INFO: Number of nodes with available pods: 0
May 17 18:45:22.355: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:23.355: INFO: Number of nodes with available pods: 0
May 17 18:45:23.355: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:24.355: INFO: Number of nodes with available pods: 0
May 17 18:45:24.355: INFO: Node 20test-worker-2 is running more than one daemon pod
May 17 18:45:25.354: INFO: Number of nodes with available pods: 1
May 17 18:45:25.354: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8033, will wait for the garbage collector to delete the pods
May 17 18:45:25.418: INFO: Deleting DaemonSet.extensions daemon-set took: 7.034985ms
May 17 18:45:26.318: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.370598ms
May 17 18:45:32.822: INFO: Number of nodes with available pods: 0
May 17 18:45:32.822: INFO: Number of running nodes: 0, number of available pods: 0
May 17 18:45:32.824: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49970"},"items":null}

May 17 18:45:32.826: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49970"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:32.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8033" for this suite.

• [SLOW TEST:20.685 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":280,"skipped":4703,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:32.858: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 17 18:45:32.918: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3651  fb87c0e2-27b9-473a-a04f-5ba7fb4be430 49978 0 2021-05-17 18:45:32 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-17 18:45:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n97w5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n97w5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n97w5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 17 18:45:32.928: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 17 18:45:34.934: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 17 18:45:34.936: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3651 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:45:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Verifying customized DNS server is configured on pod...
May 17 18:45:35.047: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3651 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 17 18:45:35.048: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
May 17 18:45:35.167: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:35.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3651" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":281,"skipped":4710,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:35.194: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:45:35.954: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:45:38.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:45:38.996: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8503-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:45:40.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-587" for this suite.
STEP: Destroying namespace "webhook-587-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.435 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":282,"skipped":4712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:45:40.646: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:45:40.791: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Creating first CR 
May 17 18:45:42.248: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:45:42Z]] name:name1 resourceVersion:50144 uid:d280b2db-96ce-43a6-b283-24d8489455c4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 17 18:45:52.255: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:45:52Z]] name:name2 resourceVersion:50200 uid:a532bd16-e001-4819-84d9-93ccbd6f0a23] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 17 18:46:02.261: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:46:02Z]] name:name1 resourceVersion:50237 uid:d280b2db-96ce-43a6-b283-24d8489455c4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 17 18:46:12.266: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:46:12Z]] name:name2 resourceVersion:50275 uid:a532bd16-e001-4819-84d9-93ccbd6f0a23] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 17 18:46:22.273: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:46:02Z]] name:name1 resourceVersion:50312 uid:d280b2db-96ce-43a6-b283-24d8489455c4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 17 18:46:32.282: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-17T18:45:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-17T18:46:12Z]] name:name2 resourceVersion:50349 uid:a532bd16-e001-4819-84d9-93ccbd6f0a23] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:46:42.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-1418" for this suite.

• [SLOW TEST:62.269 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":283,"skipped":4767,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:46:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 17 18:46:42.980: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 18:47:43.051: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 17 18:47:43.083: INFO: Created pod: pod0-sched-preemption-low-priority
May 17 18:47:43.115: INFO: Created pod: pod1-sched-preemption-medium-priority
May 17 18:47:43.138: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:48:51.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1536" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:128.327 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":284,"skipped":4771,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:48:51.249: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:48:51.298: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3" in namespace "projected-4398" to be "Succeeded or Failed"
May 17 18:48:51.306: INFO: Pod "downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.554576ms
May 17 18:48:53.309: INFO: Pod "downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011134729s
STEP: Saw pod success
May 17 18:48:53.310: INFO: Pod "downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3" satisfied condition "Succeeded or Failed"
May 17 18:48:53.312: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3 container client-container: <nil>
STEP: delete the pod
May 17 18:48:53.342: INFO: Waiting for pod downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3 to disappear
May 17 18:48:53.344: INFO: Pod downwardapi-volume-41703ecd-a1f6-4539-a1a8-a9caf315b9b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:48:53.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4398" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4778,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:48:53.353: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:48:53.385: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:48:54.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1831" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":286,"skipped":4791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:48:54.484: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:49:20.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6370" for this suite.

• [SLOW TEST:26.363 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4814,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:49:20.850: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 17 18:49:20.901: INFO: Waiting up to 1m0s for all nodes to be ready
May 17 18:50:20.994: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:50:20.996: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 17 18:50:23.081: INFO: found a healthy node: 20test-worker-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:50:37.224: INFO: pods created so far: [1 1 1]
May 17 18:50:37.224: INFO: length of pods created so far: 3
May 17 18:50:51.277: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:50:58.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8929" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:50:58.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-161" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:97.542 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":288,"skipped":4852,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:50:58.393: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:50:59.045: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 17 18:51:01.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874259, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874259, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874259, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874259, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:51:04.090: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:51:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9576-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:51:05.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8193" for this suite.
STEP: Destroying namespace "webhook-8193-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.582 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":289,"skipped":4853,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:51:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:51:07.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874267, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874267, loc:(*time.Location)(0x7962e20)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874267, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874267, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:51:10.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:51:11.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8810" for this suite.
STEP: Destroying namespace "webhook-8810-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":290,"skipped":4863,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:51:11.221: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:51:11.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d" in namespace "downward-api-862" to be "Succeeded or Failed"
May 17 18:51:11.365: INFO: Pod "downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.036778ms
May 17 18:51:13.372: INFO: Pod "downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020176845s
STEP: Saw pod success
May 17 18:51:13.372: INFO: Pod "downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d" satisfied condition "Succeeded or Failed"
May 17 18:51:13.385: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d container client-container: <nil>
STEP: delete the pod
May 17 18:51:13.433: INFO: Waiting for pod downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d to disappear
May 17 18:51:13.436: INFO: Pod downwardapi-volume-d9e8402b-88e7-43ef-90c7-4b1652a64e9d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:51:13.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-862" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":4895,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:51:13.455: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 17 18:51:13.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8" in namespace "downward-api-4756" to be "Succeeded or Failed"
May 17 18:51:13.579: INFO: Pod "downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.652843ms
May 17 18:51:15.583: INFO: Pod "downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042965697s
STEP: Saw pod success
May 17 18:51:15.583: INFO: Pod "downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8" satisfied condition "Succeeded or Failed"
May 17 18:51:15.586: INFO: Trying to get logs from node 20test-worker-3 pod downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8 container client-container: <nil>
STEP: delete the pod
May 17 18:51:15.606: INFO: Waiting for pod downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8 to disappear
May 17 18:51:15.608: INFO: Pod downwardapi-volume-ae252cf2-b426-4619-8e48-c2c318d52fc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:51:15.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4756" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4897,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:51:15.616: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:51:20.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-223" for this suite.

• [SLOW TEST:5.462 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":293,"skipped":4917,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:51:21.079: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0517 18:51:31.272686      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 17 18:52:33.285: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
May 17 18:52:33.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-2h2rd" in namespace "gc-8678"
May 17 18:52:33.302: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cvqc" in namespace "gc-8678"
May 17 18:52:33.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jgj7" in namespace "gc-8678"
May 17 18:52:33.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xb99" in namespace "gc-8678"
May 17 18:52:33.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-b492q" in namespace "gc-8678"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:33.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8678" for this suite.

• [SLOW TEST:72.290 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":294,"skipped":4925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:33.370: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 17 18:52:33.431: INFO: Waiting up to 5m0s for pod "pod-aebe55c6-64b7-4adf-be0c-d179e5695c15" in namespace "emptydir-4648" to be "Succeeded or Failed"
May 17 18:52:33.434: INFO: Pod "pod-aebe55c6-64b7-4adf-be0c-d179e5695c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.579815ms
May 17 18:52:35.445: INFO: Pod "pod-aebe55c6-64b7-4adf-be0c-d179e5695c15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013856503s
May 17 18:52:37.449: INFO: Pod "pod-aebe55c6-64b7-4adf-be0c-d179e5695c15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017092994s
STEP: Saw pod success
May 17 18:52:37.449: INFO: Pod "pod-aebe55c6-64b7-4adf-be0c-d179e5695c15" satisfied condition "Succeeded or Failed"
May 17 18:52:37.451: INFO: Trying to get logs from node 20test-worker-3 pod pod-aebe55c6-64b7-4adf-be0c-d179e5695c15 container test-container: <nil>
STEP: delete the pod
May 17 18:52:37.468: INFO: Waiting for pod pod-aebe55c6-64b7-4adf-be0c-d179e5695c15 to disappear
May 17 18:52:37.471: INFO: Pod pod-aebe55c6-64b7-4adf-be0c-d179e5695c15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:37.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4648" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":4968,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:37.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5121" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":296,"skipped":4981,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:37.589: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:52:39.706: INFO: Waiting up to 5m0s for pod "client-envvars-0b856744-c222-4022-b576-e78cdbc7baab" in namespace "pods-6601" to be "Succeeded or Failed"
May 17 18:52:39.729: INFO: Pod "client-envvars-0b856744-c222-4022-b576-e78cdbc7baab": Phase="Pending", Reason="", readiness=false. Elapsed: 23.752089ms
May 17 18:52:41.733: INFO: Pod "client-envvars-0b856744-c222-4022-b576-e78cdbc7baab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02714391s
STEP: Saw pod success
May 17 18:52:41.733: INFO: Pod "client-envvars-0b856744-c222-4022-b576-e78cdbc7baab" satisfied condition "Succeeded or Failed"
May 17 18:52:41.735: INFO: Trying to get logs from node 20test-worker-3 pod client-envvars-0b856744-c222-4022-b576-e78cdbc7baab container env3cont: <nil>
STEP: delete the pod
May 17 18:52:41.749: INFO: Waiting for pod client-envvars-0b856744-c222-4022-b576-e78cdbc7baab to disappear
May 17 18:52:41.752: INFO: Pod client-envvars-0b856744-c222-4022-b576-e78cdbc7baab no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:41.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6601" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5001,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:41.765: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ae9d5c2c-3b61-440f-a999-c0f47c1fde50
STEP: Creating a pod to test consume secrets
May 17 18:52:41.836: INFO: Waiting up to 5m0s for pod "pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816" in namespace "secrets-8460" to be "Succeeded or Failed"
May 17 18:52:41.853: INFO: Pod "pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816": Phase="Pending", Reason="", readiness=false. Elapsed: 16.777706ms
May 17 18:52:43.860: INFO: Pod "pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023488616s
STEP: Saw pod success
May 17 18:52:43.860: INFO: Pod "pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816" satisfied condition "Succeeded or Failed"
May 17 18:52:43.862: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816 container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:52:43.885: INFO: Waiting for pod pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816 to disappear
May 17 18:52:43.887: INFO: Pod pod-secrets-7f7717dc-d44c-4d58-bb79-907f75824816 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:43.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8460" for this suite.
STEP: Destroying namespace "secret-namespace-6303" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5085,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:43.900: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:52:43.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3604" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":299,"skipped":5090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:52:43.969: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4691
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4691
STEP: Creating statefulset with conflicting port in namespace statefulset-4691
STEP: Waiting until pod test-pod will start running in namespace statefulset-4691
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4691
May 17 18:52:48.073: INFO: Observed stateful pod in namespace: statefulset-4691, name: ss-0, uid: 1632e6a9-3f42-4f11-943f-14808d18a991, status phase: Pending. Waiting for statefulset controller to delete.
May 17 18:52:48.437: INFO: Observed stateful pod in namespace: statefulset-4691, name: ss-0, uid: 1632e6a9-3f42-4f11-943f-14808d18a991, status phase: Failed. Waiting for statefulset controller to delete.
May 17 18:52:48.468: INFO: Observed stateful pod in namespace: statefulset-4691, name: ss-0, uid: 1632e6a9-3f42-4f11-943f-14808d18a991, status phase: Failed. Waiting for statefulset controller to delete.
May 17 18:52:48.475: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4691
STEP: Removing pod with conflicting port in namespace statefulset-4691
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4691 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 17 18:52:52.504: INFO: Deleting all statefulset in ns statefulset-4691
May 17 18:52:52.506: INFO: Scaling statefulset ss to 0
May 17 18:53:12.535: INFO: Waiting for statefulset status.replicas updated to 0
May 17 18:53:12.537: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:12.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4691" for this suite.

• [SLOW TEST:28.603 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":300,"skipped":5128,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:12.575: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:53:12.612: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 17 18:53:13.638: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:14.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5745" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":301,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:14.701: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:31.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5669" for this suite.

• [SLOW TEST:17.097 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":302,"skipped":5176,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:31.798: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:53:31.837: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 17 18:53:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 create -f -'
May 17 18:53:38.245: INFO: stderr: ""
May 17 18:53:38.245: INFO: stdout: "e2e-test-crd-publish-openapi-5097-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 18:53:38.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 delete e2e-test-crd-publish-openapi-5097-crds test-foo'
May 17 18:53:38.354: INFO: stderr: ""
May 17 18:53:38.354: INFO: stdout: "e2e-test-crd-publish-openapi-5097-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 17 18:53:38.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 apply -f -'
May 17 18:53:38.682: INFO: stderr: ""
May 17 18:53:38.682: INFO: stdout: "e2e-test-crd-publish-openapi-5097-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 17 18:53:38.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 delete e2e-test-crd-publish-openapi-5097-crds test-foo'
May 17 18:53:38.782: INFO: stderr: ""
May 17 18:53:38.782: INFO: stdout: "e2e-test-crd-publish-openapi-5097-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 17 18:53:38.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 create -f -'
May 17 18:53:39.094: INFO: rc: 1
May 17 18:53:39.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 apply -f -'
May 17 18:53:39.400: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 17 18:53:39.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 create -f -'
May 17 18:53:39.670: INFO: rc: 1
May 17 18:53:39.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 apply -f -'
May 17 18:53:39.952: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 17 18:53:39.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-5097-crds'
May 17 18:53:40.261: INFO: stderr: ""
May 17 18:53:40.261: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5097-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 17 18:53:40.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-5097-crds.metadata'
May 17 18:53:40.607: INFO: stderr: ""
May 17 18:53:40.607: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5097-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 17 18:53:40.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-5097-crds.spec'
May 17 18:53:40.912: INFO: stderr: ""
May 17 18:53:40.912: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5097-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 17 18:53:40.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-5097-crds.spec.bars'
May 17 18:53:41.259: INFO: stderr: ""
May 17 18:53:41.259: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5097-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 17 18:53:41.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-5097-crds.spec.bars2'
May 17 18:53:41.562: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:47.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-105" for this suite.

• [SLOW TEST:15.364 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":303,"skipped":5205,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:47.162: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:53:47.875: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 17 18:53:49.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874427, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874427, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874427, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874427, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:53:52.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:52.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1691" for this suite.
STEP: Destroying namespace "webhook-1691-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.902 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":304,"skipped":5212,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:53.073: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 17 18:53:53.125: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:53:56.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2805" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":305,"skipped":5233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:53:56.886: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 17 18:53:56.943: INFO: Waiting up to 5m0s for pod "downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c" in namespace "downward-api-4856" to be "Succeeded or Failed"
May 17 18:53:56.952: INFO: Pod "downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.738006ms
May 17 18:53:59.078: INFO: Pod "downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134417196s
May 17 18:54:01.082: INFO: Pod "downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.137905576s
STEP: Saw pod success
May 17 18:54:01.082: INFO: Pod "downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c" satisfied condition "Succeeded or Failed"
May 17 18:54:01.083: INFO: Trying to get logs from node 20test-worker-3 pod downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c container dapi-container: <nil>
STEP: delete the pod
May 17 18:54:01.108: INFO: Waiting for pod downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c to disappear
May 17 18:54:01.111: INFO: Pod downward-api-73fdda8b-40b7-4d99-b15b-e6543bc4a68c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:54:01.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4856" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5279,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:54:01.120: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-e0676937-1950-4fed-ad05-85e9dee1f8c4
STEP: Creating a pod to test consume secrets
May 17 18:54:01.168: INFO: Waiting up to 5m0s for pod "pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246" in namespace "secrets-8677" to be "Succeeded or Failed"
May 17 18:54:01.171: INFO: Pod "pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246": Phase="Pending", Reason="", readiness=false. Elapsed: 3.207958ms
May 17 18:54:03.181: INFO: Pod "pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012722741s
STEP: Saw pod success
May 17 18:54:03.182: INFO: Pod "pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246" satisfied condition "Succeeded or Failed"
May 17 18:54:03.189: INFO: Trying to get logs from node 20test-worker-3 pod pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246 container secret-volume-test: <nil>
STEP: delete the pod
May 17 18:54:03.218: INFO: Waiting for pod pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246 to disappear
May 17 18:54:03.222: INFO: Pod pod-secrets-2e84cf10-cbc9-4e7b-a833-c9e27f4ec246 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:54:03.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8677" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":307,"skipped":5285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:54:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 17 18:54:03.786: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 17 18:54:05.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874443, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874443, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874443, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63756874443, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 17 18:54:08.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 17 18:54:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:54:08.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6941" for this suite.
STEP: Destroying namespace "webhook-6941-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.689 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":308,"skipped":5310,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:54:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 17 18:54:11.151: INFO: Pod pod-hostip-911fea3d-e7a1-421f-8d06-ff93e4500544 has hostIP: 10.30.20.187
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:54:11.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1520" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5312,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:54:11.182: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 17 18:54:11.215: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 17 18:54:17.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-9793 --namespace=crd-publish-openapi-9793 create -f -'
May 17 18:54:18.128: INFO: stderr: ""
May 17 18:54:18.128: INFO: stdout: "e2e-test-crd-publish-openapi-7285-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 18:54:18.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-9793 --namespace=crd-publish-openapi-9793 delete e2e-test-crd-publish-openapi-7285-crds test-cr'
May 17 18:54:18.238: INFO: stderr: ""
May 17 18:54:18.238: INFO: stdout: "e2e-test-crd-publish-openapi-7285-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 17 18:54:18.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-9793 --namespace=crd-publish-openapi-9793 apply -f -'
May 17 18:54:18.526: INFO: stderr: ""
May 17 18:54:18.526: INFO: stdout: "e2e-test-crd-publish-openapi-7285-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 17 18:54:18.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-9793 --namespace=crd-publish-openapi-9793 delete e2e-test-crd-publish-openapi-7285-crds test-cr'
May 17 18:54:18.633: INFO: stderr: ""
May 17 18:54:18.633: INFO: stdout: "e2e-test-crd-publish-openapi-7285-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 17 18:54:18.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-261591685 --namespace=crd-publish-openapi-9793 explain e2e-test-crd-publish-openapi-7285-crds'
May 17 18:54:18.917: INFO: stderr: ""
May 17 18:54:18.917: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7285-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:54:24.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9793" for this suite.

• [SLOW TEST:13.731 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":310,"skipped":5317,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 17 18:54:24.915: INFO: >>> kubeConfig: /tmp/kubeconfig-261591685
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-e543b7f0-407c-493c-ac2d-e2e2c2995ae2
STEP: Creating configMap with name cm-test-opt-upd-20904117-bd93-4dab-bd38-4995c2a32c39
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e543b7f0-407c-493c-ac2d-e2e2c2995ae2
STEP: Updating configmap cm-test-opt-upd-20904117-bd93-4dab-bd38-4995c2a32c39
STEP: Creating configMap with name cm-test-opt-create-66756d44-ac57-45aa-98f1-5efe381be99f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 17 18:55:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4961" for this suite.

• [SLOW TEST:90.499 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":311,"skipped":5337,"failed":0}
SSSSSSSSSSSSSSSSSSSMay 17 18:55:55.416: INFO: Running AfterSuite actions on all nodes
May 17 18:55:55.416: INFO: Running AfterSuite actions on node 1
May 17 18:55:55.417: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6928.153 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h55m30.217901494s
Test Suite Passed
