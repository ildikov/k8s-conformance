I0923 08:25:26.660065      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-977129552
I0923 08:25:26.660094      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0923 08:25:26.660175      25 e2e.go:129] Starting e2e run "c4ecebf6-fdbc-4981-a198-1ded3cf4d60d" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1632385525 - Will randomize all specs
Will run 311 of 5667 specs

Sep 23 08:25:26.678: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:25:26.680: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 23 08:25:26.693: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 23 08:25:26.719: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 23 08:25:26.719: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Sep 23 08:25:26.719: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 23 08:25:26.727: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep 23 08:25:26.727: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-qingcloud-node' (0 seconds elapsed)
Sep 23 08:25:26.727: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 23 08:25:26.727: INFO: e2e test version: v1.20.6
Sep 23 08:25:26.728: INFO: kube-apiserver version: v1.20.6
Sep 23 08:25:26.728: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:25:26.732: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:25:26.732: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubelet-test
Sep 23 08:25:26.753: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:25:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3377" for this suite.

• [SLOW TEST:10.060 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":6,"failed":0}
SSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:25:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 23 08:25:36.823: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 08:26:36.858: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:26:36.860: INFO: Starting informer...
STEP: Starting pod...
Sep 23 08:26:37.068: INFO: Pod is running on worker-s002. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 23 08:26:37.085: INFO: Pod wasn't evicted. Proceeding
Sep 23 08:26:37.085: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 23 08:27:52.121: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:27:52.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6022" for this suite.

• [SLOW TEST:135.333 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":2,"skipped":9,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:27:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-86156929-b524-4b67-9c44-07ba8da4b09a
STEP: Creating a pod to test consume configMaps
Sep 23 08:27:52.157: INFO: Waiting up to 5m0s for pod "pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544" in namespace "configmap-750" to be "Succeeded or Failed"
Sep 23 08:27:52.161: INFO: Pod "pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 4.082755ms
Sep 23 08:27:54.165: INFO: Pod "pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007302912s
Sep 23 08:27:56.167: INFO: Pod "pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009909716s
STEP: Saw pod success
Sep 23 08:27:56.167: INFO: Pod "pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544" satisfied condition "Succeeded or Failed"
Sep 23 08:27:56.169: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:27:56.193: INFO: Waiting for pod pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544 to disappear
Sep 23 08:27:56.200: INFO: Pod pod-configmaps-dfad3ff8-df7a-4096-bc7b-ec51ff2f1544 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:27:56.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-750" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":23,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:27:56.205: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:27:56.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4371" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":4,"skipped":79,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:27:56.294: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:27:56.330: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 23 08:28:01.335: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 08:28:01.335: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 23 08:28:03.340: INFO: Creating deployment "test-rollover-deployment"
Sep 23 08:28:03.345: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 23 08:28:05.366: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 23 08:28:05.369: INFO: Ensure that both replica sets have 1 created replica
Sep 23 08:28:05.372: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 23 08:28:05.379: INFO: Updating deployment test-rollover-deployment
Sep 23 08:28:05.379: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 23 08:28:07.389: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 23 08:28:07.393: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 23 08:28:07.395: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:07.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982485, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:09.400: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:09.400: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982487, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:11.401: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:11.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982487, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:13.401: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:13.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982487, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:15.401: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:15.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982487, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:17.409: INFO: all replica sets need to contain the pod-template-hash label
Sep 23 08:28:17.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982487, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 08:28:19.401: INFO: 
Sep 23 08:28:19.401: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 08:28:19.405: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7028  4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04 65407 2 2021-09-23 08:28:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-23 08:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-23 08:28:03 +0000 UTC,LastTransitionTime:2021-09-23 08:28:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-09-23 08:28:17 +0000 UTC,LastTransitionTime:2021-09-23 08:28:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 23 08:28:19.407: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-7028  9bfd61b6-5355-484c-a823-795c7858315a 65396 2 2021-09-23 08:28:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04 0xc002aaf797 0xc002aaf798}] []  [{kube-controller-manager Update apps/v1 2021-09-23 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 23 08:28:19.407: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 23 08:28:19.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7028  c6f42f6a-95fb-4c0b-a319-cb7de3d498d2 65405 2 2021-09-23 08:27:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04 0xc002aaf687 0xc002aaf688}] []  [{e2e.test Update apps/v1 2021-09-23 08:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 08:28:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002aaf728 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 08:28:19.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7028  fae22bae-e745-4c22-956f-c1e6241668c7 65329 2 2021-09-23 08:28:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04 0xc002aaf897 0xc002aaf898}] []  [{kube-controller-manager Update apps/v1 2021-09-23 08:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb1bc8e-8648-4e3f-b2b3-7e0ec555dc04\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aaf928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 08:28:19.408: INFO: Pod "test-rollover-deployment-668db69979-6vbd8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-6vbd8 test-rollover-deployment-668db69979- deployment-7028  3b943c6c-a4f2-4a6b-a595-e8aa17d9d241 65356 0 2021-09-23 08:28:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:10.10.131.161/32 cni.projectcalico.org/podIPs:10.10.131.161/32] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 9bfd61b6-5355-484c-a823-795c7858315a 0xc002aafe27 0xc002aafe28}] []  [{kube-controller-manager Update v1 2021-09-23 08:28:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bfd61b6-5355-484c-a823-795c7858315a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 08:28:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 08:28:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tnkfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tnkfv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tnkfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 08:28:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 08:28:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 08:28:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 08:28:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.161,StartTime:2021-09-23 08:28:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 08:28:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://0760781b4abfc04894ca44b094a3542434a8c511a60bdd98a3e140410fff6d66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7028" for this suite.

• [SLOW TEST:23.120 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":5,"skipped":80,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:28:19.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 23 08:28:21.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982499, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982499, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982499, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982499, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:28:24.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 23 08:28:24.922: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:24.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9317" for this suite.
STEP: Destroying namespace "webhook-9317-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.554 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":6,"skipped":89,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:28:25.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 08:28:27.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982505, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982505, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767982505, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:28:30.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 23 08:28:32.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=webhook-3384 attach --namespace=webhook-3384 to-be-attached-pod -i -c=container1'
Sep 23 08:28:32.757: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:32.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3384" for this suite.
STEP: Destroying namespace "webhook-3384-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.839 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":7,"skipped":95,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:32.807: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-1c29b2fc-9f3e-4563-8ecb-23ce335e7a3f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:32.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5410" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":8,"skipped":132,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:32.841: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 08:28:32.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958" in namespace "downward-api-454" to be "Succeeded or Failed"
Sep 23 08:28:32.873: INFO: Pod "downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00104ms
Sep 23 08:28:34.877: INFO: Pod "downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00805682s
STEP: Saw pod success
Sep 23 08:28:34.877: INFO: Pod "downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958" satisfied condition "Succeeded or Failed"
Sep 23 08:28:34.879: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958 container client-container: <nil>
STEP: delete the pod
Sep 23 08:28:34.889: INFO: Waiting for pod downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958 to disappear
Sep 23 08:28:34.891: INFO: Pod downwardapi-volume-ee3949c1-9f77-4633-950c-a6985e236958 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:34.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-454" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":156,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:34.896: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 08:28:34.942: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:34.943: INFO: Number of nodes with available pods: 0
Sep 23 08:28:34.943: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:35.955: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:35.957: INFO: Number of nodes with available pods: 0
Sep 23 08:28:35.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:36.947: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:36.949: INFO: Number of nodes with available pods: 1
Sep 23 08:28:36.949: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:37.953: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:37.955: INFO: Number of nodes with available pods: 2
Sep 23 08:28:37.955: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 23 08:28:37.973: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:37.978: INFO: Number of nodes with available pods: 1
Sep 23 08:28:37.978: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:38.982: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:38.984: INFO: Number of nodes with available pods: 1
Sep 23 08:28:38.984: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:39.982: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:39.983: INFO: Number of nodes with available pods: 1
Sep 23 08:28:39.983: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:28:40.982: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:28:40.984: INFO: Number of nodes with available pods: 2
Sep 23 08:28:40.984: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-909, will wait for the garbage collector to delete the pods
Sep 23 08:28:41.057: INFO: Deleting DaemonSet.extensions daemon-set took: 19.143499ms
Sep 23 08:28:41.157: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.180386ms
Sep 23 08:28:44.259: INFO: Number of nodes with available pods: 0
Sep 23 08:28:44.260: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 08:28:44.262: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65877"},"items":null}

Sep 23 08:28:44.264: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65877"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:44.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-909" for this suite.

• [SLOW TEST:9.383 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":10,"skipped":160,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:44.280: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:28:44.327: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 23 08:28:48.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 create -f -'
Sep 23 08:28:49.150: INFO: stderr: ""
Sep 23 08:28:49.150: INFO: stdout: "e2e-test-crd-publish-openapi-5322-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 23 08:28:49.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 delete e2e-test-crd-publish-openapi-5322-crds test-foo'
Sep 23 08:28:49.212: INFO: stderr: ""
Sep 23 08:28:49.212: INFO: stdout: "e2e-test-crd-publish-openapi-5322-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 23 08:28:49.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 apply -f -'
Sep 23 08:28:49.422: INFO: stderr: ""
Sep 23 08:28:49.422: INFO: stdout: "e2e-test-crd-publish-openapi-5322-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 23 08:28:49.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 delete e2e-test-crd-publish-openapi-5322-crds test-foo'
Sep 23 08:28:49.499: INFO: stderr: ""
Sep 23 08:28:49.499: INFO: stdout: "e2e-test-crd-publish-openapi-5322-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 23 08:28:49.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 create -f -'
Sep 23 08:28:49.691: INFO: rc: 1
Sep 23 08:28:49.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 apply -f -'
Sep 23 08:28:49.874: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 23 08:28:49.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 create -f -'
Sep 23 08:28:50.058: INFO: rc: 1
Sep 23 08:28:50.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 --namespace=crd-publish-openapi-7978 apply -f -'
Sep 23 08:28:50.266: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 23 08:28:50.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 explain e2e-test-crd-publish-openapi-5322-crds'
Sep 23 08:28:50.468: INFO: stderr: ""
Sep 23 08:28:50.468: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5322-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 23 08:28:50.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 explain e2e-test-crd-publish-openapi-5322-crds.metadata'
Sep 23 08:28:50.673: INFO: stderr: ""
Sep 23 08:28:50.673: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5322-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 23 08:28:50.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 explain e2e-test-crd-publish-openapi-5322-crds.spec'
Sep 23 08:28:50.892: INFO: stderr: ""
Sep 23 08:28:50.892: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5322-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 23 08:28:50.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 explain e2e-test-crd-publish-openapi-5322-crds.spec.bars'
Sep 23 08:28:51.100: INFO: stderr: ""
Sep 23 08:28:51.100: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5322-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 23 08:28:51.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-7978 explain e2e-test-crd-publish-openapi-5322-crds.spec.bars2'
Sep 23 08:28:51.328: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:55.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7978" for this suite.

• [SLOW TEST:10.917 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":11,"skipped":179,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:55.196: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-f9463c16-c46e-4d20-91e8-8e66bce074c0
STEP: Creating a pod to test consume secrets
Sep 23 08:28:55.244: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653" in namespace "projected-1145" to be "Succeeded or Failed"
Sep 23 08:28:55.247: INFO: Pod "pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653": Phase="Pending", Reason="", readiness=false. Elapsed: 2.561856ms
Sep 23 08:28:57.249: INFO: Pod "pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004878531s
STEP: Saw pod success
Sep 23 08:28:57.249: INFO: Pod "pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653" satisfied condition "Succeeded or Failed"
Sep 23 08:28:57.251: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 08:28:57.263: INFO: Waiting for pod pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653 to disappear
Sep 23 08:28:57.266: INFO: Pod pod-projected-secrets-58e4bbd3-2ea3-4faa-a20e-2a13c4782653 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:28:57.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1145" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":194,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:28:57.271: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 23 08:29:01.822: INFO: Successfully updated pod "pod-update-fb21e5b6-e911-4407-bfd5-0e39edfc1cbc"
STEP: verifying the updated pod is in kubernetes
Sep 23 08:29:01.827: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:29:01.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-167" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:29:01.832: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 08:29:01.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e" in namespace "downward-api-5354" to be "Succeeded or Failed"
Sep 23 08:29:01.873: INFO: Pod "downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155477ms
Sep 23 08:29:03.877: INFO: Pod "downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00640606s
STEP: Saw pod success
Sep 23 08:29:03.877: INFO: Pod "downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e" satisfied condition "Succeeded or Failed"
Sep 23 08:29:03.879: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e container client-container: <nil>
STEP: delete the pod
Sep 23 08:29:03.894: INFO: Waiting for pod downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e to disappear
Sep 23 08:29:03.897: INFO: Pod downwardapi-volume-9c8164da-2627-4ed8-9124-4c6a058d861e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:29:03.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5354" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":14,"skipped":247,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:29:03.902: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 23 08:31:04.455: INFO: Successfully updated pod "var-expansion-0b3b99cb-236d-4c68-9140-d28b54cb3450"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 23 08:31:06.461: INFO: Deleting pod "var-expansion-0b3b99cb-236d-4c68-9140-d28b54cb3450" in namespace "var-expansion-3179"
Sep 23 08:31:06.463: INFO: Wait up to 5m0s for pod "var-expansion-0b3b99cb-236d-4c68-9140-d28b54cb3450" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:38.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3179" for this suite.

• [SLOW TEST:154.588 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":15,"skipped":253,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:38.491: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:31:40.642: INFO: Deleting pod "var-expansion-e9dc1114-698b-45dc-a76f-e973b0832bd4" in namespace "var-expansion-7391"
Sep 23 08:31:40.645: INFO: Wait up to 5m0s for pod "var-expansion-e9dc1114-698b-45dc-a76f-e973b0832bd4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:44.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7391" for this suite.

• [SLOW TEST:6.164 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":16,"skipped":257,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-499fa387-2c9a-4d93-a2d3-87ef6442f4fd
STEP: Creating a pod to test consume secrets
Sep 23 08:31:44.683: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af617082-1139-4680-a710-396a075e713f" in namespace "projected-6271" to be "Succeeded or Failed"
Sep 23 08:31:44.686: INFO: Pod "pod-projected-secrets-af617082-1139-4680-a710-396a075e713f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755755ms
Sep 23 08:31:46.689: INFO: Pod "pod-projected-secrets-af617082-1139-4680-a710-396a075e713f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005555358s
STEP: Saw pod success
Sep 23 08:31:46.689: INFO: Pod "pod-projected-secrets-af617082-1139-4680-a710-396a075e713f" satisfied condition "Succeeded or Failed"
Sep 23 08:31:46.691: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-af617082-1139-4680-a710-396a075e713f container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 08:31:46.720: INFO: Waiting for pod pod-projected-secrets-af617082-1139-4680-a710-396a075e713f to disappear
Sep 23 08:31:46.722: INFO: Pod pod-projected-secrets-af617082-1139-4680-a710-396a075e713f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:46.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6271" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":17,"skipped":258,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:46.727: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:46.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2510" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":18,"skipped":269,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 23 08:31:46.799: INFO: Waiting up to 5m0s for pod "pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1" in namespace "emptydir-7423" to be "Succeeded or Failed"
Sep 23 08:31:46.802: INFO: Pod "pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.278541ms
Sep 23 08:31:48.807: INFO: Pod "pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007954282s
STEP: Saw pod success
Sep 23 08:31:48.807: INFO: Pod "pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1" satisfied condition "Succeeded or Failed"
Sep 23 08:31:48.811: INFO: Trying to get logs from node worker-s002 pod pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1 container test-container: <nil>
STEP: delete the pod
Sep 23 08:31:48.830: INFO: Waiting for pod pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1 to disappear
Sep 23 08:31:48.835: INFO: Pod pod-7c44f594-5665-4113-b5b6-f92d0fe7d2c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:48.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7423" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":277,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:48.841: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:31:48.868: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:31:50.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4256" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:31:50.979: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-916
STEP: creating service affinity-clusterip in namespace services-916
STEP: creating replication controller affinity-clusterip in namespace services-916
I0923 08:31:51.040112      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-916, replica count: 3
I0923 08:31:54.090407      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:31:57.090586      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:00.090766      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:03.090945      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:06.091098      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:09.091267      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:12.091445      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:15.091629      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:18.091798      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:21.091979      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:24.092145      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:27.092304      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:30.092469      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:33.092632      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:32:36.092795      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 08:32:36.097: INFO: Creating new exec pod
Sep 23 08:32:39.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-916 exec execpod-affinitysmzmm -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep 23 08:32:39.311: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 23 08:32:39.311: INFO: stdout: ""
Sep 23 08:32:39.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-916 exec execpod-affinitysmzmm -- /bin/sh -x -c nc -zv -t -w 2 10.96.106.65 80'
Sep 23 08:32:39.448: INFO: stderr: "+ nc -zv -t -w 2 10.96.106.65 80\nConnection to 10.96.106.65 80 port [tcp/http] succeeded!\n"
Sep 23 08:32:39.448: INFO: stdout: ""
Sep 23 08:32:39.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-916 exec execpod-affinitysmzmm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.106.65:80/ ; done'
Sep 23 08:32:39.645: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.106.65:80/\n"
Sep 23 08:32:39.645: INFO: stdout: "\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl\naffinity-clusterip-hl7bl"
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Received response from host: affinity-clusterip-hl7bl
Sep 23 08:32:39.645: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-916, will wait for the garbage collector to delete the pods
Sep 23 08:32:39.714: INFO: Deleting ReplicationController affinity-clusterip took: 3.209966ms
Sep 23 08:32:41.214: INFO: Terminating ReplicationController affinity-clusterip pods took: 1.500115817s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:32:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-916" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:63.050 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":21,"skipped":311,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:32:54.030: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 23 08:32:54.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1173 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep 23 08:32:54.133: INFO: stderr: ""
Sep 23 08:32:54.133: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 23 08:32:54.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1173 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Sep 23 08:32:54.413: INFO: stderr: ""
Sep 23 08:32:54.413: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Sep 23 08:32:54.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1173 delete pods e2e-test-httpd-pod'
Sep 23 08:32:56.142: INFO: stderr: ""
Sep 23 08:32:56.142: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:32:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1173" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":22,"skipped":313,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:32:56.150: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:32:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:32:56.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2113" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":23,"skipped":315,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:32:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep 23 08:32:56.729: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 08:32:56.733: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 08:32:56.736: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Sep 23 08:32:56.741: INFO: calico-node-gfgx9 from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 08:32:56.741: INFO: csi-qingcloud-controller-6b58955cdd-l56st from kube-system started at 2021-09-23 03:58:05 +0000 UTC (5 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 23 08:32:56.741: INFO: csi-qingcloud-node-xqq5p from kube-system started at 2021-09-23 03:58:05 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 08:32:56.741: INFO: kube-proxy-8nnxr from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 08:32:56.741: INFO: metrics-server-57bcd9bccd-hbn8w from kube-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container metrics-server ready: true, restart count 0
Sep 23 08:32:56.741: INFO: kubectl-admin-776b98f44f-hftkv from kubesphere-controls-system started at 2021-09-23 04:02:15 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container kubectl ready: true, restart count 0
Sep 23 08:32:56.741: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-09-23 04:01:40 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 08:32:56.741: INFO: kube-state-metrics-67588479db-tvwbm from kubesphere-monitoring-system started at 2021-09-23 04:01:32 +0000 UTC (3 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 23 08:32:56.741: INFO: node-exporter-t9zb4 from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 08:32:56.741: INFO: notification-manager-deployment-7bd887ffb4-n9rvc from kubesphere-monitoring-system started at 2021-09-23 04:02:07 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 08:32:56.741: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-09-23 04:02:27 +0000 UTC (3 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 08:32:56.741: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 08:32:56.741: INFO: ks-installer-f58dbc4cc-q6zkt from kubesphere-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container installer ready: true, restart count 1
Sep 23 08:32:56.741: INFO: sonobuoy from sonobuoy started at 2021-09-23 08:25:22 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 08:32:56.741: INFO: sonobuoy-e2e-job-57e5d9d7846a4ccc from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container e2e ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 08:32:56.741: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 08:32:56.741: INFO: ss2-0 from statefulset-7832 started at 2021-09-23 08:23:22 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.741: INFO: 	Container webserver ready: true, restart count 0
Sep 23 08:32:56.741: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Sep 23 08:32:56.745: INFO: calico-node-f5cnk from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 08:32:56.745: INFO: csi-qingcloud-node-xhrfx from kube-system started at 2021-09-23 08:26:46 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 08:32:56.745: INFO: kube-proxy-fn894 from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 08:32:56.745: INFO: snapshot-controller-0 from kube-system started at 2021-09-23 08:26:52 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 23 08:32:56.745: INFO: default-http-backend-76d9fb4bb7-6vdjv from kubesphere-controls-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container default-http-backend ready: true, restart count 0
Sep 23 08:32:56.745: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-09-23 08:26:47 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 08:32:56.745: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-09-23 08:26:50 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 08:32:56.745: INFO: node-exporter-6n95k from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 08:32:56.745: INFO: notification-manager-deployment-7bd887ffb4-8frpr from kubesphere-monitoring-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 08:32:56.745: INFO: notification-manager-operator-78595d8666-n29fc from kubesphere-monitoring-system started at 2021-09-23 08:26:37 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container notification-manager-operator ready: true, restart count 0
Sep 23 08:32:56.745: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-09-23 08:27:03 +0000 UTC (3 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 08:32:56.745: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 08:32:56.745: INFO: prometheus-operator-d7fdfccbf-m4btn from kubesphere-monitoring-system started at 2021-09-23 08:26:37 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 23 08:32:56.745: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 08:32:56.745: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 08:32:56.745: INFO: ss2-1 from statefulset-7832 started at 2021-09-23 08:26:48 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container webserver ready: true, restart count 0
Sep 23 08:32:56.745: INFO: ss2-2 from statefulset-7832 started at 2021-09-23 08:26:55 +0000 UTC (1 container statuses recorded)
Sep 23 08:32:56.745: INFO: 	Container webserver ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node worker-s001
STEP: verifying the node has the label node worker-s002
Sep 23 08:32:56.794: INFO: Pod calico-node-f5cnk requesting resource cpu=250m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod calico-node-gfgx9 requesting resource cpu=250m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod csi-qingcloud-controller-6b58955cdd-l56st requesting resource cpu=250m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod csi-qingcloud-node-xhrfx requesting resource cpu=60m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod csi-qingcloud-node-xqq5p requesting resource cpu=60m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod kube-proxy-8nnxr requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod kube-proxy-fn894 requesting resource cpu=0m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod metrics-server-57bcd9bccd-hbn8w requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod snapshot-controller-0 requesting resource cpu=0m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod default-http-backend-76d9fb4bb7-6vdjv requesting resource cpu=10m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod kubectl-admin-776b98f44f-hftkv requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod alertmanager-main-0 requesting resource cpu=20m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod alertmanager-main-1 requesting resource cpu=20m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod alertmanager-main-2 requesting resource cpu=20m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod kube-state-metrics-67588479db-tvwbm requesting resource cpu=120m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod node-exporter-6n95k requesting resource cpu=112m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod node-exporter-t9zb4 requesting resource cpu=112m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod notification-manager-deployment-7bd887ffb4-8frpr requesting resource cpu=5m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod notification-manager-deployment-7bd887ffb4-n9rvc requesting resource cpu=5m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod notification-manager-operator-78595d8666-n29fc requesting resource cpu=10m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod prometheus-k8s-0 requesting resource cpu=200m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod prometheus-k8s-1 requesting resource cpu=200m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod prometheus-operator-d7fdfccbf-m4btn requesting resource cpu=110m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod ks-installer-f58dbc4cc-q6zkt requesting resource cpu=20m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod sonobuoy-e2e-job-57e5d9d7846a4ccc requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 requesting resource cpu=0m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod ss2-0 requesting resource cpu=0m on Node worker-s001
Sep 23 08:32:56.794: INFO: Pod ss2-1 requesting resource cpu=0m on Node worker-s002
Sep 23 08:32:56.794: INFO: Pod ss2-2 requesting resource cpu=0m on Node worker-s002
STEP: Starting Pods to consume most of the cluster CPU.
Sep 23 08:32:56.794: INFO: Creating a pod which consumes cpu=4594m on Node worker-s001
Sep 23 08:32:56.799: INFO: Creating a pod which consumes cpu=4762m on Node worker-s002
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf.16a76665ac08f64e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4855/filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf to worker-s001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf.16a76665eb661064], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.2"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf.16a76666464a4bbc], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.2" in 1.524895106s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf.16a766664970d067], Reason = [Created], Message = [Created container filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf.16a766665499e0a3], Reason = [Started], Message = [Started container filler-pod-9d226112-77d4-404d-a6bb-9fc890664abf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124.16a76665aca9446d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4855/filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124 to worker-s002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124.16a76665ec5d815f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124.16a76665f0fef25a], Reason = [Created], Message = [Created container filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124.16a76665ffe57f88], Reason = [Started], Message = [Started container filler-pod-b060ca99-777c-4138-a3fa-d0559cb05124]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a766669bad9c17], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a766669c48a536], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node worker-s002
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker-s001
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:01.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4855" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.150 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":24,"skipped":326,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:01.849: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:33:01.891: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"47f3e813-3068-4110-8b87-987dd4bcbfb4", Controller:(*bool)(0xc000acf53e), BlockOwnerDeletion:(*bool)(0xc000acf53f)}}
Sep 23 08:33:01.897: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"58e4c0e5-216e-4de8-9bbe-d95eed8fe43f", Controller:(*bool)(0xc000acf9f6), BlockOwnerDeletion:(*bool)(0xc000acf9f7)}}
Sep 23 08:33:01.901: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8c6d64ce-ca5e-40dc-8e33-0aeecc009342", Controller:(*bool)(0xc003905716), BlockOwnerDeletion:(*bool)(0xc003905717)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:06.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2428" for this suite.

• [SLOW TEST:5.065 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":25,"skipped":344,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Sep 23 08:33:06.943: INFO: Waiting up to 5m0s for pod "var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293" in namespace "var-expansion-5762" to be "Succeeded or Failed"
Sep 23 08:33:06.945: INFO: Pod "var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165659ms
Sep 23 08:33:08.950: INFO: Pod "var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006949859s
Sep 23 08:33:10.952: INFO: Pod "var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009677s
STEP: Saw pod success
Sep 23 08:33:10.952: INFO: Pod "var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293" satisfied condition "Succeeded or Failed"
Sep 23 08:33:10.954: INFO: Trying to get logs from node worker-s002 pod var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293 container dapi-container: <nil>
STEP: delete the pod
Sep 23 08:33:10.966: INFO: Waiting for pod var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293 to disappear
Sep 23 08:33:10.968: INFO: Pod var-expansion-2841c4ab-e589-4ad7-9001-4c2d72191293 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:10.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5762" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":345,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Sep 23 08:33:11.521: INFO: created pod pod-service-account-defaultsa
Sep 23 08:33:11.521: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 23 08:33:11.529: INFO: created pod pod-service-account-mountsa
Sep 23 08:33:11.529: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 23 08:33:11.534: INFO: created pod pod-service-account-nomountsa
Sep 23 08:33:11.534: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 23 08:33:11.548: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 23 08:33:11.548: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 23 08:33:11.559: INFO: created pod pod-service-account-mountsa-mountspec
Sep 23 08:33:11.559: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 23 08:33:11.566: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 23 08:33:11.566: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 23 08:33:11.576: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 23 08:33:11.576: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 23 08:33:11.590: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 23 08:33:11.590: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 23 08:33:11.594: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 23 08:33:11.594: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:11.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6973" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":27,"skipped":357,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Sep 23 08:33:11.655: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Sep 23 08:33:11.664: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2983" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":28,"skipped":360,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:11.685: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep 23 08:33:11.715: INFO: Waiting up to 5m0s for pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00" in namespace "downward-api-7828" to be "Succeeded or Failed"
Sep 23 08:33:11.719: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201538ms
Sep 23 08:33:13.721: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006895731s
Sep 23 08:33:15.728: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013139025s
Sep 23 08:33:17.730: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015100381s
Sep 23 08:33:19.732: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017178091s
Sep 23 08:33:21.734: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.019222351s
STEP: Saw pod success
Sep 23 08:33:21.734: INFO: Pod "downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00" satisfied condition "Succeeded or Failed"
Sep 23 08:33:21.735: INFO: Trying to get logs from node worker-s001 pod downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00 container dapi-container: <nil>
STEP: delete the pod
Sep 23 08:33:21.758: INFO: Waiting for pod downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00 to disappear
Sep 23 08:33:21.760: INFO: Pod downward-api-13457c28-f8ce-4177-90a9-1c19a26fde00 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:21.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7828" for this suite.

• [SLOW TEST:10.079 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:21.764: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:27.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-836" for this suite.
STEP: Destroying namespace "nsdeletetest-6887" for this suite.
Sep 23 08:33:27.857: INFO: Namespace nsdeletetest-6887 was already deleted
STEP: Destroying namespace "nsdeletetest-4963" for this suite.

• [SLOW TEST:6.096 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":30,"skipped":401,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-7ccfd5c9-cdb7-4bb5-ab9a-26ec4a1ce294
STEP: Creating a pod to test consume configMaps
Sep 23 08:33:27.886: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08" in namespace "projected-1726" to be "Succeeded or Failed"
Sep 23 08:33:27.888: INFO: Pod "pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08": Phase="Pending", Reason="", readiness=false. Elapsed: 1.559085ms
Sep 23 08:33:29.892: INFO: Pod "pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005344808s
STEP: Saw pod success
Sep 23 08:33:29.892: INFO: Pod "pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08" satisfied condition "Succeeded or Failed"
Sep 23 08:33:29.895: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:33:29.920: INFO: Waiting for pod pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08 to disappear
Sep 23 08:33:29.922: INFO: Pod pod-projected-configmaps-e8466501-65db-4b90-8f8e-d4988a80af08 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1726" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":404,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:29.928: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 23 08:33:30.556: INFO: starting watch
STEP: patching
STEP: updating
Sep 23 08:33:30.562: INFO: waiting for watch events with expected annotations
Sep 23 08:33:30.562: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:30.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8909" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":32,"skipped":455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:30.600: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:33:30.630: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6428
I0923 08:33:30.643815      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6428, replica count: 1
I0923 08:33:31.694130      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:33:32.694231      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 08:33:33.694413      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 08:33:33.802: INFO: Created: latency-svc-jm55q
Sep 23 08:33:33.809: INFO: Got endpoints: latency-svc-jm55q [14.560236ms]
Sep 23 08:33:33.816: INFO: Created: latency-svc-vbrzm
Sep 23 08:33:33.819: INFO: Created: latency-svc-wwc77
Sep 23 08:33:33.824: INFO: Got endpoints: latency-svc-wwc77 [15.416117ms]
Sep 23 08:33:33.824: INFO: Got endpoints: latency-svc-vbrzm [15.474697ms]
Sep 23 08:33:33.825: INFO: Created: latency-svc-grpz2
Sep 23 08:33:33.830: INFO: Got endpoints: latency-svc-grpz2 [21.43291ms]
Sep 23 08:33:33.834: INFO: Created: latency-svc-9zdrm
Sep 23 08:33:33.836: INFO: Got endpoints: latency-svc-9zdrm [26.797064ms]
Sep 23 08:33:33.843: INFO: Created: latency-svc-r5rx6
Sep 23 08:33:33.848: INFO: Created: latency-svc-ncpvz
Sep 23 08:33:33.851: INFO: Got endpoints: latency-svc-r5rx6 [42.220522ms]
Sep 23 08:33:33.853: INFO: Got endpoints: latency-svc-ncpvz [44.585221ms]
Sep 23 08:33:33.854: INFO: Created: latency-svc-f2qcp
Sep 23 08:33:33.858: INFO: Created: latency-svc-67sm2
Sep 23 08:33:33.860: INFO: Got endpoints: latency-svc-f2qcp [50.981315ms]
Sep 23 08:33:33.863: INFO: Created: latency-svc-t8qn2
Sep 23 08:33:33.863: INFO: Got endpoints: latency-svc-67sm2 [54.182337ms]
Sep 23 08:33:33.868: INFO: Got endpoints: latency-svc-t8qn2 [58.955658ms]
Sep 23 08:33:33.868: INFO: Created: latency-svc-nhdbw
Sep 23 08:33:33.872: INFO: Created: latency-svc-7b5cs
Sep 23 08:33:33.873: INFO: Got endpoints: latency-svc-nhdbw [64.497891ms]
Sep 23 08:33:33.875: INFO: Created: latency-svc-ncbf5
Sep 23 08:33:33.879: INFO: Created: latency-svc-nzbgm
Sep 23 08:33:33.884: INFO: Got endpoints: latency-svc-ncbf5 [74.618738ms]
Sep 23 08:33:33.884: INFO: Got endpoints: latency-svc-7b5cs [74.771705ms]
Sep 23 08:33:33.884: INFO: Created: latency-svc-5lwll
Sep 23 08:33:33.886: INFO: Got endpoints: latency-svc-nzbgm [77.252409ms]
Sep 23 08:33:33.888: INFO: Created: latency-svc-gx92s
Sep 23 08:33:33.889: INFO: Got endpoints: latency-svc-5lwll [79.800463ms]
Sep 23 08:33:33.892: INFO: Got endpoints: latency-svc-gx92s [83.327157ms]
Sep 23 08:33:33.895: INFO: Created: latency-svc-lfmbk
Sep 23 08:33:33.899: INFO: Got endpoints: latency-svc-lfmbk [74.969094ms]
Sep 23 08:33:33.903: INFO: Created: latency-svc-lc45x
Sep 23 08:33:33.907: INFO: Created: latency-svc-hb5mc
Sep 23 08:33:33.908: INFO: Got endpoints: latency-svc-lc45x [19.214359ms]
Sep 23 08:33:33.910: INFO: Created: latency-svc-6wxm7
Sep 23 08:33:33.911: INFO: Got endpoints: latency-svc-hb5mc [87.219651ms]
Sep 23 08:33:33.913: INFO: Got endpoints: latency-svc-6wxm7 [82.436796ms]
Sep 23 08:33:33.917: INFO: Created: latency-svc-k7mjh
Sep 23 08:33:33.920: INFO: Got endpoints: latency-svc-k7mjh [84.393262ms]
Sep 23 08:33:33.923: INFO: Created: latency-svc-rb5q4
Sep 23 08:33:33.925: INFO: Created: latency-svc-hm8b9
Sep 23 08:33:33.929: INFO: Got endpoints: latency-svc-rb5q4 [77.78338ms]
Sep 23 08:33:33.929: INFO: Got endpoints: latency-svc-hm8b9 [75.748955ms]
Sep 23 08:33:33.930: INFO: Created: latency-svc-v5dc2
Sep 23 08:33:33.932: INFO: Got endpoints: latency-svc-v5dc2 [72.177571ms]
Sep 23 08:33:33.934: INFO: Created: latency-svc-2c4pv
Sep 23 08:33:33.937: INFO: Created: latency-svc-7rqfx
Sep 23 08:33:33.937: INFO: Got endpoints: latency-svc-2c4pv [74.120072ms]
Sep 23 08:33:33.943: INFO: Got endpoints: latency-svc-7rqfx [75.411173ms]
Sep 23 08:33:33.943: INFO: Created: latency-svc-l6khv
Sep 23 08:33:33.947: INFO: Got endpoints: latency-svc-l6khv [74.042266ms]
Sep 23 08:33:33.949: INFO: Created: latency-svc-qn9k9
Sep 23 08:33:33.952: INFO: Got endpoints: latency-svc-qn9k9 [68.405933ms]
Sep 23 08:33:33.953: INFO: Created: latency-svc-sl4wd
Sep 23 08:33:33.957: INFO: Created: latency-svc-dt75n
Sep 23 08:33:33.958: INFO: Got endpoints: latency-svc-sl4wd [74.451417ms]
Sep 23 08:33:33.963: INFO: Got endpoints: latency-svc-dt75n [76.80832ms]
Sep 23 08:33:33.964: INFO: Created: latency-svc-fcl9t
Sep 23 08:33:33.967: INFO: Created: latency-svc-pjgdj
Sep 23 08:33:33.968: INFO: Got endpoints: latency-svc-fcl9t [75.205166ms]
Sep 23 08:33:33.970: INFO: Got endpoints: latency-svc-pjgdj [70.404583ms]
Sep 23 08:33:33.972: INFO: Created: latency-svc-pnfxb
Sep 23 08:33:33.979: INFO: Got endpoints: latency-svc-pnfxb [71.292016ms]
Sep 23 08:33:33.979: INFO: Created: latency-svc-9psnp
Sep 23 08:33:33.981: INFO: Created: latency-svc-mkm6h
Sep 23 08:33:33.987: INFO: Created: latency-svc-mpb9f
Sep 23 08:33:33.990: INFO: Created: latency-svc-rkg2n
Sep 23 08:33:33.994: INFO: Created: latency-svc-49z5h
Sep 23 08:33:34.008: INFO: Created: latency-svc-n7mf5
Sep 23 08:33:34.008: INFO: Got endpoints: latency-svc-9psnp [95.156142ms]
Sep 23 08:33:34.017: INFO: Created: latency-svc-qbthz
Sep 23 08:33:34.026: INFO: Created: latency-svc-kcz9b
Sep 23 08:33:34.034: INFO: Created: latency-svc-bg8gb
Sep 23 08:33:34.038: INFO: Created: latency-svc-m9jv7
Sep 23 08:33:34.042: INFO: Created: latency-svc-knmht
Sep 23 08:33:34.048: INFO: Created: latency-svc-nhcvg
Sep 23 08:33:34.055: INFO: Created: latency-svc-ppzs7
Sep 23 08:33:34.060: INFO: Got endpoints: latency-svc-mkm6h [148.067904ms]
Sep 23 08:33:34.067: INFO: Created: latency-svc-cbdkd
Sep 23 08:33:34.073: INFO: Created: latency-svc-rcc5c
Sep 23 08:33:34.077: INFO: Created: latency-svc-l9tq5
Sep 23 08:33:34.081: INFO: Created: latency-svc-62hts
Sep 23 08:33:34.108: INFO: Got endpoints: latency-svc-mpb9f [187.85415ms]
Sep 23 08:33:34.124: INFO: Created: latency-svc-4z5tf
Sep 23 08:33:34.155: INFO: Got endpoints: latency-svc-rkg2n [225.879257ms]
Sep 23 08:33:34.165: INFO: Created: latency-svc-64p4q
Sep 23 08:33:34.204: INFO: Got endpoints: latency-svc-49z5h [275.088977ms]
Sep 23 08:33:34.213: INFO: Created: latency-svc-zgj77
Sep 23 08:33:34.257: INFO: Got endpoints: latency-svc-n7mf5 [324.96778ms]
Sep 23 08:33:34.262: INFO: Created: latency-svc-2g4nr
Sep 23 08:33:34.306: INFO: Got endpoints: latency-svc-qbthz [368.410472ms]
Sep 23 08:33:34.312: INFO: Created: latency-svc-xjnx7
Sep 23 08:33:34.358: INFO: Got endpoints: latency-svc-kcz9b [414.297403ms]
Sep 23 08:33:34.364: INFO: Created: latency-svc-9cf55
Sep 23 08:33:34.405: INFO: Got endpoints: latency-svc-bg8gb [457.909493ms]
Sep 23 08:33:34.411: INFO: Created: latency-svc-vncsh
Sep 23 08:33:34.455: INFO: Got endpoints: latency-svc-m9jv7 [502.519168ms]
Sep 23 08:33:34.460: INFO: Created: latency-svc-ln7ks
Sep 23 08:33:34.505: INFO: Got endpoints: latency-svc-knmht [546.406842ms]
Sep 23 08:33:34.511: INFO: Created: latency-svc-2rf92
Sep 23 08:33:34.560: INFO: Got endpoints: latency-svc-nhcvg [596.905206ms]
Sep 23 08:33:34.568: INFO: Created: latency-svc-zms9x
Sep 23 08:33:34.607: INFO: Got endpoints: latency-svc-ppzs7 [638.954468ms]
Sep 23 08:33:34.617: INFO: Created: latency-svc-mh4gn
Sep 23 08:33:34.655: INFO: Got endpoints: latency-svc-cbdkd [684.818993ms]
Sep 23 08:33:34.663: INFO: Created: latency-svc-hkjph
Sep 23 08:33:34.710: INFO: Got endpoints: latency-svc-rcc5c [730.686803ms]
Sep 23 08:33:34.717: INFO: Created: latency-svc-pvrm2
Sep 23 08:33:34.757: INFO: Got endpoints: latency-svc-l9tq5 [749.146321ms]
Sep 23 08:33:34.762: INFO: Created: latency-svc-5x2qs
Sep 23 08:33:34.805: INFO: Got endpoints: latency-svc-62hts [745.0325ms]
Sep 23 08:33:34.817: INFO: Created: latency-svc-htmns
Sep 23 08:33:34.856: INFO: Got endpoints: latency-svc-4z5tf [747.704524ms]
Sep 23 08:33:34.862: INFO: Created: latency-svc-4gpwn
Sep 23 08:33:34.907: INFO: Got endpoints: latency-svc-64p4q [752.292147ms]
Sep 23 08:33:34.914: INFO: Created: latency-svc-65g9g
Sep 23 08:33:34.955: INFO: Got endpoints: latency-svc-zgj77 [750.139295ms]
Sep 23 08:33:34.962: INFO: Created: latency-svc-n4bcx
Sep 23 08:33:35.016: INFO: Got endpoints: latency-svc-2g4nr [759.318612ms]
Sep 23 08:33:35.022: INFO: Created: latency-svc-tz2b9
Sep 23 08:33:35.055: INFO: Got endpoints: latency-svc-xjnx7 [749.156281ms]
Sep 23 08:33:35.062: INFO: Created: latency-svc-r4rl8
Sep 23 08:33:35.119: INFO: Got endpoints: latency-svc-9cf55 [761.575821ms]
Sep 23 08:33:35.133: INFO: Created: latency-svc-dg66w
Sep 23 08:33:35.157: INFO: Got endpoints: latency-svc-vncsh [751.58118ms]
Sep 23 08:33:35.168: INFO: Created: latency-svc-7z5ng
Sep 23 08:33:35.205: INFO: Got endpoints: latency-svc-ln7ks [750.069444ms]
Sep 23 08:33:35.211: INFO: Created: latency-svc-lgrl9
Sep 23 08:33:35.255: INFO: Got endpoints: latency-svc-2rf92 [749.642321ms]
Sep 23 08:33:35.263: INFO: Created: latency-svc-7hfrh
Sep 23 08:33:35.305: INFO: Got endpoints: latency-svc-zms9x [745.222734ms]
Sep 23 08:33:35.314: INFO: Created: latency-svc-hx9s7
Sep 23 08:33:35.355: INFO: Got endpoints: latency-svc-mh4gn [748.712616ms]
Sep 23 08:33:35.365: INFO: Created: latency-svc-dfwn2
Sep 23 08:33:35.408: INFO: Got endpoints: latency-svc-hkjph [753.089055ms]
Sep 23 08:33:35.414: INFO: Created: latency-svc-6dwdp
Sep 23 08:33:35.465: INFO: Got endpoints: latency-svc-pvrm2 [754.882197ms]
Sep 23 08:33:35.471: INFO: Created: latency-svc-rmmmf
Sep 23 08:33:35.505: INFO: Got endpoints: latency-svc-5x2qs [747.779423ms]
Sep 23 08:33:35.512: INFO: Created: latency-svc-zdlpd
Sep 23 08:33:35.555: INFO: Got endpoints: latency-svc-htmns [750.658561ms]
Sep 23 08:33:35.561: INFO: Created: latency-svc-42zxm
Sep 23 08:33:35.619: INFO: Got endpoints: latency-svc-4gpwn [763.052175ms]
Sep 23 08:33:35.633: INFO: Created: latency-svc-rqhp9
Sep 23 08:33:35.655: INFO: Got endpoints: latency-svc-65g9g [747.917012ms]
Sep 23 08:33:35.662: INFO: Created: latency-svc-wg8fp
Sep 23 08:33:35.709: INFO: Got endpoints: latency-svc-n4bcx [754.362514ms]
Sep 23 08:33:35.718: INFO: Created: latency-svc-8jbgj
Sep 23 08:33:35.755: INFO: Got endpoints: latency-svc-tz2b9 [738.198166ms]
Sep 23 08:33:35.763: INFO: Created: latency-svc-4wrp7
Sep 23 08:33:35.805: INFO: Got endpoints: latency-svc-r4rl8 [750.230706ms]
Sep 23 08:33:35.810: INFO: Created: latency-svc-n8lfm
Sep 23 08:33:35.859: INFO: Got endpoints: latency-svc-dg66w [739.796043ms]
Sep 23 08:33:35.868: INFO: Created: latency-svc-tplb8
Sep 23 08:33:35.905: INFO: Got endpoints: latency-svc-7z5ng [742.584117ms]
Sep 23 08:33:35.911: INFO: Created: latency-svc-d8n2l
Sep 23 08:33:35.955: INFO: Got endpoints: latency-svc-lgrl9 [749.9196ms]
Sep 23 08:33:35.962: INFO: Created: latency-svc-tthdg
Sep 23 08:33:36.004: INFO: Got endpoints: latency-svc-7hfrh [749.744344ms]
Sep 23 08:33:36.010: INFO: Created: latency-svc-2w7l4
Sep 23 08:33:36.056: INFO: Got endpoints: latency-svc-hx9s7 [750.433991ms]
Sep 23 08:33:36.062: INFO: Created: latency-svc-47ktk
Sep 23 08:33:36.104: INFO: Got endpoints: latency-svc-dfwn2 [749.203662ms]
Sep 23 08:33:36.112: INFO: Created: latency-svc-j8wrb
Sep 23 08:33:36.155: INFO: Got endpoints: latency-svc-6dwdp [747.576985ms]
Sep 23 08:33:36.164: INFO: Created: latency-svc-tl4xk
Sep 23 08:33:36.204: INFO: Got endpoints: latency-svc-rmmmf [739.195573ms]
Sep 23 08:33:36.210: INFO: Created: latency-svc-zbs5t
Sep 23 08:33:36.260: INFO: Got endpoints: latency-svc-zdlpd [755.418943ms]
Sep 23 08:33:36.278: INFO: Created: latency-svc-kf8mn
Sep 23 08:33:36.309: INFO: Got endpoints: latency-svc-42zxm [754.021421ms]
Sep 23 08:33:36.317: INFO: Created: latency-svc-5rq5x
Sep 23 08:33:36.360: INFO: Got endpoints: latency-svc-rqhp9 [741.517865ms]
Sep 23 08:33:36.371: INFO: Created: latency-svc-mk695
Sep 23 08:33:36.406: INFO: Got endpoints: latency-svc-wg8fp [751.394811ms]
Sep 23 08:33:36.420: INFO: Created: latency-svc-xkxwf
Sep 23 08:33:36.455: INFO: Got endpoints: latency-svc-8jbgj [745.714746ms]
Sep 23 08:33:36.460: INFO: Created: latency-svc-gcc5f
Sep 23 08:33:36.505: INFO: Got endpoints: latency-svc-4wrp7 [750.346651ms]
Sep 23 08:33:36.511: INFO: Created: latency-svc-8dngp
Sep 23 08:33:36.555: INFO: Got endpoints: latency-svc-n8lfm [749.666552ms]
Sep 23 08:33:36.560: INFO: Created: latency-svc-496tm
Sep 23 08:33:36.604: INFO: Got endpoints: latency-svc-tplb8 [745.33958ms]
Sep 23 08:33:36.610: INFO: Created: latency-svc-zbht7
Sep 23 08:33:36.658: INFO: Got endpoints: latency-svc-d8n2l [752.66989ms]
Sep 23 08:33:36.663: INFO: Created: latency-svc-74jgv
Sep 23 08:33:36.707: INFO: Got endpoints: latency-svc-tthdg [751.928817ms]
Sep 23 08:33:36.712: INFO: Created: latency-svc-sc9tz
Sep 23 08:33:36.756: INFO: Got endpoints: latency-svc-2w7l4 [751.099582ms]
Sep 23 08:33:36.762: INFO: Created: latency-svc-k75bn
Sep 23 08:33:36.805: INFO: Got endpoints: latency-svc-47ktk [748.821264ms]
Sep 23 08:33:36.810: INFO: Created: latency-svc-82cjq
Sep 23 08:33:36.855: INFO: Got endpoints: latency-svc-j8wrb [750.655992ms]
Sep 23 08:33:36.862: INFO: Created: latency-svc-6hq5x
Sep 23 08:33:36.905: INFO: Got endpoints: latency-svc-tl4xk [749.35614ms]
Sep 23 08:33:36.911: INFO: Created: latency-svc-b74dh
Sep 23 08:33:36.955: INFO: Got endpoints: latency-svc-zbs5t [750.465558ms]
Sep 23 08:33:36.961: INFO: Created: latency-svc-fw76t
Sep 23 08:33:37.006: INFO: Got endpoints: latency-svc-kf8mn [745.651449ms]
Sep 23 08:33:37.013: INFO: Created: latency-svc-wttpj
Sep 23 08:33:37.057: INFO: Got endpoints: latency-svc-5rq5x [747.46207ms]
Sep 23 08:33:37.062: INFO: Created: latency-svc-ms9ld
Sep 23 08:33:37.105: INFO: Got endpoints: latency-svc-mk695 [745.002256ms]
Sep 23 08:33:37.116: INFO: Created: latency-svc-v2rj5
Sep 23 08:33:37.155: INFO: Got endpoints: latency-svc-xkxwf [748.123738ms]
Sep 23 08:33:37.161: INFO: Created: latency-svc-vs9hd
Sep 23 08:33:37.204: INFO: Got endpoints: latency-svc-gcc5f [749.404877ms]
Sep 23 08:33:37.212: INFO: Created: latency-svc-cjzzf
Sep 23 08:33:37.254: INFO: Got endpoints: latency-svc-8dngp [749.141493ms]
Sep 23 08:33:37.260: INFO: Created: latency-svc-hq5tr
Sep 23 08:33:37.306: INFO: Got endpoints: latency-svc-496tm [751.263063ms]
Sep 23 08:33:37.313: INFO: Created: latency-svc-6sbj6
Sep 23 08:33:37.355: INFO: Got endpoints: latency-svc-zbht7 [750.790541ms]
Sep 23 08:33:37.367: INFO: Created: latency-svc-vpfw8
Sep 23 08:33:37.405: INFO: Got endpoints: latency-svc-74jgv [747.313393ms]
Sep 23 08:33:37.410: INFO: Created: latency-svc-vltch
Sep 23 08:33:37.460: INFO: Got endpoints: latency-svc-sc9tz [753.133158ms]
Sep 23 08:33:37.467: INFO: Created: latency-svc-shmsf
Sep 23 08:33:37.506: INFO: Got endpoints: latency-svc-k75bn [750.033395ms]
Sep 23 08:33:37.514: INFO: Created: latency-svc-j2rpt
Sep 23 08:33:37.555: INFO: Got endpoints: latency-svc-82cjq [750.481084ms]
Sep 23 08:33:37.562: INFO: Created: latency-svc-kqd6l
Sep 23 08:33:37.616: INFO: Got endpoints: latency-svc-6hq5x [760.588696ms]
Sep 23 08:33:37.622: INFO: Created: latency-svc-mnzlb
Sep 23 08:33:37.655: INFO: Got endpoints: latency-svc-b74dh [749.939532ms]
Sep 23 08:33:37.666: INFO: Created: latency-svc-bcjm8
Sep 23 08:33:37.706: INFO: Got endpoints: latency-svc-fw76t [751.669742ms]
Sep 23 08:33:37.711: INFO: Created: latency-svc-56kxz
Sep 23 08:33:37.755: INFO: Got endpoints: latency-svc-wttpj [748.737726ms]
Sep 23 08:33:37.764: INFO: Created: latency-svc-6bzm7
Sep 23 08:33:37.811: INFO: Got endpoints: latency-svc-ms9ld [754.506846ms]
Sep 23 08:33:37.820: INFO: Created: latency-svc-2v89t
Sep 23 08:33:37.854: INFO: Got endpoints: latency-svc-v2rj5 [749.056584ms]
Sep 23 08:33:37.860: INFO: Created: latency-svc-5lbg7
Sep 23 08:33:37.905: INFO: Got endpoints: latency-svc-vs9hd [749.872563ms]
Sep 23 08:33:37.910: INFO: Created: latency-svc-dwb9d
Sep 23 08:33:37.955: INFO: Got endpoints: latency-svc-cjzzf [751.276189ms]
Sep 23 08:33:37.962: INFO: Created: latency-svc-gk2dg
Sep 23 08:33:38.004: INFO: Got endpoints: latency-svc-hq5tr [750.207294ms]
Sep 23 08:33:38.014: INFO: Created: latency-svc-ccf2h
Sep 23 08:33:38.058: INFO: Got endpoints: latency-svc-6sbj6 [751.445186ms]
Sep 23 08:33:38.063: INFO: Created: latency-svc-sspf5
Sep 23 08:33:38.105: INFO: Got endpoints: latency-svc-vpfw8 [749.406257ms]
Sep 23 08:33:38.110: INFO: Created: latency-svc-nb4rz
Sep 23 08:33:38.155: INFO: Got endpoints: latency-svc-vltch [749.861628ms]
Sep 23 08:33:38.164: INFO: Created: latency-svc-r6djz
Sep 23 08:33:38.205: INFO: Got endpoints: latency-svc-shmsf [744.875215ms]
Sep 23 08:33:38.211: INFO: Created: latency-svc-c66kt
Sep 23 08:33:38.256: INFO: Got endpoints: latency-svc-j2rpt [750.172486ms]
Sep 23 08:33:38.262: INFO: Created: latency-svc-r5n8r
Sep 23 08:33:38.304: INFO: Got endpoints: latency-svc-kqd6l [749.359708ms]
Sep 23 08:33:38.310: INFO: Created: latency-svc-q2g6f
Sep 23 08:33:38.360: INFO: Got endpoints: latency-svc-mnzlb [743.837344ms]
Sep 23 08:33:38.366: INFO: Created: latency-svc-q5s4l
Sep 23 08:33:38.405: INFO: Got endpoints: latency-svc-bcjm8 [750.081786ms]
Sep 23 08:33:38.410: INFO: Created: latency-svc-s9vvw
Sep 23 08:33:38.455: INFO: Got endpoints: latency-svc-56kxz [748.270266ms]
Sep 23 08:33:38.460: INFO: Created: latency-svc-k4pc2
Sep 23 08:33:38.506: INFO: Got endpoints: latency-svc-6bzm7 [751.417078ms]
Sep 23 08:33:38.512: INFO: Created: latency-svc-wllpw
Sep 23 08:33:38.555: INFO: Got endpoints: latency-svc-2v89t [743.245542ms]
Sep 23 08:33:38.560: INFO: Created: latency-svc-qfn8c
Sep 23 08:33:38.607: INFO: Got endpoints: latency-svc-5lbg7 [752.877593ms]
Sep 23 08:33:38.616: INFO: Created: latency-svc-zwgtz
Sep 23 08:33:38.656: INFO: Got endpoints: latency-svc-dwb9d [751.028742ms]
Sep 23 08:33:38.661: INFO: Created: latency-svc-rxv4p
Sep 23 08:33:38.704: INFO: Got endpoints: latency-svc-gk2dg [748.86166ms]
Sep 23 08:33:38.711: INFO: Created: latency-svc-fnblm
Sep 23 08:33:38.756: INFO: Got endpoints: latency-svc-ccf2h [751.200044ms]
Sep 23 08:33:38.762: INFO: Created: latency-svc-k6w2z
Sep 23 08:33:38.806: INFO: Got endpoints: latency-svc-sspf5 [747.894017ms]
Sep 23 08:33:38.811: INFO: Created: latency-svc-s4rfn
Sep 23 08:33:38.855: INFO: Got endpoints: latency-svc-nb4rz [749.985713ms]
Sep 23 08:33:38.864: INFO: Created: latency-svc-flfss
Sep 23 08:33:38.904: INFO: Got endpoints: latency-svc-r6djz [749.563816ms]
Sep 23 08:33:38.910: INFO: Created: latency-svc-glk94
Sep 23 08:33:38.955: INFO: Got endpoints: latency-svc-c66kt [750.542814ms]
Sep 23 08:33:38.964: INFO: Created: latency-svc-l9f5d
Sep 23 08:33:39.005: INFO: Got endpoints: latency-svc-r5n8r [749.43584ms]
Sep 23 08:33:39.011: INFO: Created: latency-svc-hfcnx
Sep 23 08:33:39.055: INFO: Got endpoints: latency-svc-q2g6f [750.3047ms]
Sep 23 08:33:39.062: INFO: Created: latency-svc-6kdg6
Sep 23 08:33:39.106: INFO: Got endpoints: latency-svc-q5s4l [745.944802ms]
Sep 23 08:33:39.112: INFO: Created: latency-svc-b99l5
Sep 23 08:33:39.155: INFO: Got endpoints: latency-svc-s9vvw [749.958525ms]
Sep 23 08:33:39.160: INFO: Created: latency-svc-tsfb5
Sep 23 08:33:39.204: INFO: Got endpoints: latency-svc-k4pc2 [749.223528ms]
Sep 23 08:33:39.219: INFO: Created: latency-svc-t5rvs
Sep 23 08:33:39.255: INFO: Got endpoints: latency-svc-wllpw [748.324157ms]
Sep 23 08:33:39.261: INFO: Created: latency-svc-gdv8q
Sep 23 08:33:39.305: INFO: Got endpoints: latency-svc-qfn8c [750.281323ms]
Sep 23 08:33:39.310: INFO: Created: latency-svc-gjlwx
Sep 23 08:33:39.355: INFO: Got endpoints: latency-svc-zwgtz [747.980535ms]
Sep 23 08:33:39.361: INFO: Created: latency-svc-kq9sg
Sep 23 08:33:39.414: INFO: Got endpoints: latency-svc-rxv4p [758.580601ms]
Sep 23 08:33:39.420: INFO: Created: latency-svc-drkww
Sep 23 08:33:39.455: INFO: Got endpoints: latency-svc-fnblm [750.784138ms]
Sep 23 08:33:39.460: INFO: Created: latency-svc-tcrml
Sep 23 08:33:39.510: INFO: Got endpoints: latency-svc-k6w2z [753.750856ms]
Sep 23 08:33:39.514: INFO: Created: latency-svc-h5xm7
Sep 23 08:33:39.555: INFO: Got endpoints: latency-svc-s4rfn [749.418456ms]
Sep 23 08:33:39.561: INFO: Created: latency-svc-b5v86
Sep 23 08:33:39.604: INFO: Got endpoints: latency-svc-flfss [748.930469ms]
Sep 23 08:33:39.609: INFO: Created: latency-svc-tkwbm
Sep 23 08:33:39.654: INFO: Got endpoints: latency-svc-glk94 [749.881015ms]
Sep 23 08:33:39.660: INFO: Created: latency-svc-8qhdd
Sep 23 08:33:39.705: INFO: Got endpoints: latency-svc-l9f5d [749.0908ms]
Sep 23 08:33:39.710: INFO: Created: latency-svc-cd7bq
Sep 23 08:33:39.755: INFO: Got endpoints: latency-svc-hfcnx [749.209412ms]
Sep 23 08:33:39.764: INFO: Created: latency-svc-hnqmx
Sep 23 08:33:39.805: INFO: Got endpoints: latency-svc-6kdg6 [750.152739ms]
Sep 23 08:33:39.811: INFO: Created: latency-svc-vz54z
Sep 23 08:33:39.857: INFO: Got endpoints: latency-svc-b99l5 [751.207281ms]
Sep 23 08:33:39.862: INFO: Created: latency-svc-7plmh
Sep 23 08:33:39.906: INFO: Got endpoints: latency-svc-tsfb5 [751.685017ms]
Sep 23 08:33:39.911: INFO: Created: latency-svc-vmw5x
Sep 23 08:33:39.955: INFO: Got endpoints: latency-svc-t5rvs [751.315612ms]
Sep 23 08:33:39.960: INFO: Created: latency-svc-stqw2
Sep 23 08:33:40.005: INFO: Got endpoints: latency-svc-gdv8q [750.334079ms]
Sep 23 08:33:40.011: INFO: Created: latency-svc-7nzvp
Sep 23 08:33:40.055: INFO: Got endpoints: latency-svc-gjlwx [750.356879ms]
Sep 23 08:33:40.061: INFO: Created: latency-svc-rhsd5
Sep 23 08:33:40.105: INFO: Got endpoints: latency-svc-kq9sg [749.260021ms]
Sep 23 08:33:40.110: INFO: Created: latency-svc-pgsx8
Sep 23 08:33:40.155: INFO: Got endpoints: latency-svc-drkww [740.916936ms]
Sep 23 08:33:40.162: INFO: Created: latency-svc-mmvk8
Sep 23 08:33:40.207: INFO: Got endpoints: latency-svc-tcrml [752.083706ms]
Sep 23 08:33:40.212: INFO: Created: latency-svc-gfh5k
Sep 23 08:33:40.255: INFO: Got endpoints: latency-svc-h5xm7 [745.239365ms]
Sep 23 08:33:40.263: INFO: Created: latency-svc-hc7sp
Sep 23 08:33:40.304: INFO: Got endpoints: latency-svc-b5v86 [749.402322ms]
Sep 23 08:33:40.309: INFO: Created: latency-svc-b654r
Sep 23 08:33:40.357: INFO: Got endpoints: latency-svc-tkwbm [753.475907ms]
Sep 23 08:33:40.365: INFO: Created: latency-svc-dtsgg
Sep 23 08:33:40.405: INFO: Got endpoints: latency-svc-8qhdd [750.336093ms]
Sep 23 08:33:40.412: INFO: Created: latency-svc-8hbqz
Sep 23 08:33:40.455: INFO: Got endpoints: latency-svc-cd7bq [750.256004ms]
Sep 23 08:33:40.481: INFO: Created: latency-svc-mw7qt
Sep 23 08:33:40.508: INFO: Got endpoints: latency-svc-hnqmx [752.977969ms]
Sep 23 08:33:40.514: INFO: Created: latency-svc-rznsc
Sep 23 08:33:40.555: INFO: Got endpoints: latency-svc-vz54z [749.723733ms]
Sep 23 08:33:40.560: INFO: Created: latency-svc-5jld6
Sep 23 08:33:40.605: INFO: Got endpoints: latency-svc-7plmh [748.081756ms]
Sep 23 08:33:40.611: INFO: Created: latency-svc-87dr9
Sep 23 08:33:40.654: INFO: Got endpoints: latency-svc-vmw5x [747.843524ms]
Sep 23 08:33:40.661: INFO: Created: latency-svc-66zh8
Sep 23 08:33:40.712: INFO: Got endpoints: latency-svc-stqw2 [756.675799ms]
Sep 23 08:33:40.720: INFO: Created: latency-svc-qq7xv
Sep 23 08:33:40.772: INFO: Got endpoints: latency-svc-7nzvp [766.69007ms]
Sep 23 08:33:40.778: INFO: Created: latency-svc-64glj
Sep 23 08:33:40.806: INFO: Got endpoints: latency-svc-rhsd5 [750.289807ms]
Sep 23 08:33:40.813: INFO: Created: latency-svc-m4k96
Sep 23 08:33:40.854: INFO: Got endpoints: latency-svc-pgsx8 [749.558074ms]
Sep 23 08:33:40.863: INFO: Created: latency-svc-kwgdq
Sep 23 08:33:40.905: INFO: Got endpoints: latency-svc-mmvk8 [749.817206ms]
Sep 23 08:33:40.911: INFO: Created: latency-svc-2r9nw
Sep 23 08:33:40.956: INFO: Got endpoints: latency-svc-gfh5k [748.296415ms]
Sep 23 08:33:40.961: INFO: Created: latency-svc-8ppsl
Sep 23 08:33:41.004: INFO: Got endpoints: latency-svc-hc7sp [749.536539ms]
Sep 23 08:33:41.011: INFO: Created: latency-svc-zv9sw
Sep 23 08:33:41.055: INFO: Got endpoints: latency-svc-b654r [750.827187ms]
Sep 23 08:33:41.063: INFO: Created: latency-svc-h8948
Sep 23 08:33:41.105: INFO: Got endpoints: latency-svc-dtsgg [747.317338ms]
Sep 23 08:33:41.111: INFO: Created: latency-svc-4582s
Sep 23 08:33:41.155: INFO: Got endpoints: latency-svc-8hbqz [750.061544ms]
Sep 23 08:33:41.161: INFO: Created: latency-svc-h2zm8
Sep 23 08:33:41.204: INFO: Got endpoints: latency-svc-mw7qt [749.274774ms]
Sep 23 08:33:41.209: INFO: Created: latency-svc-6b9ht
Sep 23 08:33:41.254: INFO: Got endpoints: latency-svc-rznsc [746.795377ms]
Sep 23 08:33:41.263: INFO: Created: latency-svc-f4b48
Sep 23 08:33:41.305: INFO: Got endpoints: latency-svc-5jld6 [750.364908ms]
Sep 23 08:33:41.310: INFO: Created: latency-svc-56vck
Sep 23 08:33:41.355: INFO: Got endpoints: latency-svc-87dr9 [749.570442ms]
Sep 23 08:33:41.364: INFO: Created: latency-svc-n8k6q
Sep 23 08:33:41.404: INFO: Got endpoints: latency-svc-66zh8 [750.094745ms]
Sep 23 08:33:41.410: INFO: Created: latency-svc-27vlq
Sep 23 08:33:41.455: INFO: Got endpoints: latency-svc-qq7xv [742.587813ms]
Sep 23 08:33:41.473: INFO: Created: latency-svc-j5r6b
Sep 23 08:33:41.512: INFO: Got endpoints: latency-svc-64glj [739.942052ms]
Sep 23 08:33:41.520: INFO: Created: latency-svc-2k69t
Sep 23 08:33:41.554: INFO: Got endpoints: latency-svc-m4k96 [748.476541ms]
Sep 23 08:33:41.569: INFO: Created: latency-svc-vs62r
Sep 23 08:33:41.610: INFO: Got endpoints: latency-svc-kwgdq [756.001029ms]
Sep 23 08:33:41.623: INFO: Created: latency-svc-z5dpk
Sep 23 08:33:41.655: INFO: Got endpoints: latency-svc-2r9nw [749.948666ms]
Sep 23 08:33:41.705: INFO: Got endpoints: latency-svc-8ppsl [749.295739ms]
Sep 23 08:33:41.755: INFO: Got endpoints: latency-svc-zv9sw [750.888883ms]
Sep 23 08:33:41.805: INFO: Got endpoints: latency-svc-h8948 [749.943246ms]
Sep 23 08:33:41.855: INFO: Got endpoints: latency-svc-4582s [750.708314ms]
Sep 23 08:33:41.906: INFO: Got endpoints: latency-svc-h2zm8 [751.276129ms]
Sep 23 08:33:41.955: INFO: Got endpoints: latency-svc-6b9ht [750.964951ms]
Sep 23 08:33:42.004: INFO: Got endpoints: latency-svc-f4b48 [749.663106ms]
Sep 23 08:33:42.056: INFO: Got endpoints: latency-svc-56vck [750.521305ms]
Sep 23 08:33:42.110: INFO: Got endpoints: latency-svc-n8k6q [755.752299ms]
Sep 23 08:33:42.154: INFO: Got endpoints: latency-svc-27vlq [749.366073ms]
Sep 23 08:33:42.204: INFO: Got endpoints: latency-svc-j5r6b [749.408566ms]
Sep 23 08:33:42.254: INFO: Got endpoints: latency-svc-2k69t [742.374824ms]
Sep 23 08:33:42.304: INFO: Got endpoints: latency-svc-vs62r [750.115771ms]
Sep 23 08:33:42.354: INFO: Got endpoints: latency-svc-z5dpk [744.207734ms]
Sep 23 08:33:42.354: INFO: Latencies: [15.416117ms 15.474697ms 19.214359ms 21.43291ms 26.797064ms 42.220522ms 44.585221ms 50.981315ms 54.182337ms 58.955658ms 64.497891ms 68.405933ms 70.404583ms 71.292016ms 72.177571ms 74.042266ms 74.120072ms 74.451417ms 74.618738ms 74.771705ms 74.969094ms 75.205166ms 75.411173ms 75.748955ms 76.80832ms 77.252409ms 77.78338ms 79.800463ms 82.436796ms 83.327157ms 84.393262ms 87.219651ms 95.156142ms 148.067904ms 187.85415ms 225.879257ms 275.088977ms 324.96778ms 368.410472ms 414.297403ms 457.909493ms 502.519168ms 546.406842ms 596.905206ms 638.954468ms 684.818993ms 730.686803ms 738.198166ms 739.195573ms 739.796043ms 739.942052ms 740.916936ms 741.517865ms 742.374824ms 742.584117ms 742.587813ms 743.245542ms 743.837344ms 744.207734ms 744.875215ms 745.002256ms 745.0325ms 745.222734ms 745.239365ms 745.33958ms 745.651449ms 745.714746ms 745.944802ms 746.795377ms 747.313393ms 747.317338ms 747.46207ms 747.576985ms 747.704524ms 747.779423ms 747.843524ms 747.894017ms 747.917012ms 747.980535ms 748.081756ms 748.123738ms 748.270266ms 748.296415ms 748.324157ms 748.476541ms 748.712616ms 748.737726ms 748.821264ms 748.86166ms 748.930469ms 749.056584ms 749.0908ms 749.141493ms 749.146321ms 749.156281ms 749.203662ms 749.209412ms 749.223528ms 749.260021ms 749.274774ms 749.295739ms 749.35614ms 749.359708ms 749.366073ms 749.402322ms 749.404877ms 749.406257ms 749.408566ms 749.418456ms 749.43584ms 749.536539ms 749.558074ms 749.563816ms 749.570442ms 749.642321ms 749.663106ms 749.666552ms 749.723733ms 749.744344ms 749.817206ms 749.861628ms 749.872563ms 749.881015ms 749.9196ms 749.939532ms 749.943246ms 749.948666ms 749.958525ms 749.985713ms 750.033395ms 750.061544ms 750.069444ms 750.081786ms 750.094745ms 750.115771ms 750.139295ms 750.152739ms 750.172486ms 750.207294ms 750.230706ms 750.256004ms 750.281323ms 750.289807ms 750.3047ms 750.334079ms 750.336093ms 750.346651ms 750.356879ms 750.364908ms 750.433991ms 750.465558ms 750.481084ms 750.521305ms 750.542814ms 750.655992ms 750.658561ms 750.708314ms 750.784138ms 750.790541ms 750.827187ms 750.888883ms 750.964951ms 751.028742ms 751.099582ms 751.200044ms 751.207281ms 751.263063ms 751.276129ms 751.276189ms 751.315612ms 751.394811ms 751.417078ms 751.445186ms 751.58118ms 751.669742ms 751.685017ms 751.928817ms 752.083706ms 752.292147ms 752.66989ms 752.877593ms 752.977969ms 753.089055ms 753.133158ms 753.475907ms 753.750856ms 754.021421ms 754.362514ms 754.506846ms 754.882197ms 755.418943ms 755.752299ms 756.001029ms 756.675799ms 758.580601ms 759.318612ms 760.588696ms 761.575821ms 763.052175ms 766.69007ms]
Sep 23 08:33:42.355: INFO: 50 %ile: 749.295739ms
Sep 23 08:33:42.355: INFO: 90 %ile: 752.877593ms
Sep 23 08:33:42.355: INFO: 99 %ile: 763.052175ms
Sep 23 08:33:42.355: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:42.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6428" for this suite.

• [SLOW TEST:11.761 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":33,"skipped":488,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:42.361: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:33:44.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-773" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:33:44.411: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-ae1b4c91-bd81-407a-bf9a-8efb65d5bf07 in namespace container-probe-7347
Sep 23 08:33:46.443: INFO: Started pod busybox-ae1b4c91-bd81-407a-bf9a-8efb65d5bf07 in namespace container-probe-7347
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 08:33:46.445: INFO: Initial restart count of pod busybox-ae1b4c91-bd81-407a-bf9a-8efb65d5bf07 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:37:46.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7347" for this suite.

• [SLOW TEST:242.505 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":547,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:37:46.916: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:37:46.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6042" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":36,"skipped":568,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:37:46.947: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep 23 08:37:46.980: INFO: Waiting up to 5m0s for pod "downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3" in namespace "downward-api-4050" to be "Succeeded or Failed"
Sep 23 08:37:46.982: INFO: Pod "downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619564ms
Sep 23 08:37:48.987: INFO: Pod "downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006742451s
Sep 23 08:37:50.991: INFO: Pod "downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010515218s
STEP: Saw pod success
Sep 23 08:37:50.991: INFO: Pod "downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3" satisfied condition "Succeeded or Failed"
Sep 23 08:37:50.998: INFO: Trying to get logs from node worker-s002 pod downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3 container dapi-container: <nil>
STEP: delete the pod
Sep 23 08:37:51.154: INFO: Waiting for pod downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3 to disappear
Sep 23 08:37:51.157: INFO: Pod downward-api-641673fd-0b72-4145-a0bb-b526c16e35b3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:37:51.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4050" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":574,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:37:51.161: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:37:51.471: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 08:37:53.477: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983071, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983071, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983071, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983071, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:37:56.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:37:56.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7656" for this suite.
STEP: Destroying namespace "webhook-7656-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.434 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":38,"skipped":574,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:37:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:37:57.300: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 08:37:59.305: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983077, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983077, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:38:02.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:38:02.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5965" for this suite.
STEP: Destroying namespace "webhook-5965-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.898 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":39,"skipped":580,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:38:02.493: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5966.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5966.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:39:20.549: INFO: DNS probes using dns-5966/dns-test-2a8900b6-f5b1-46ad-8a01-e8425558243e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:20.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5966" for this suite.

• [SLOW TEST:78.073 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":40,"skipped":591,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:20.566: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:39:20.593: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 23 08:39:22.618: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:23.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8771" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":41,"skipped":609,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 08:39:23.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3" in namespace "projected-4977" to be "Succeeded or Failed"
Sep 23 08:39:23.670: INFO: Pod "downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.336272ms
Sep 23 08:39:25.672: INFO: Pod "downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003228323s
Sep 23 08:39:27.675: INFO: Pod "downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006740656s
STEP: Saw pod success
Sep 23 08:39:27.675: INFO: Pod "downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3" satisfied condition "Succeeded or Failed"
Sep 23 08:39:27.677: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3 container client-container: <nil>
STEP: delete the pod
Sep 23 08:39:27.695: INFO: Waiting for pod downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3 to disappear
Sep 23 08:39:27.700: INFO: Pod downwardapi-volume-4e76dce1-5aa7-4270-9fcc-ca4ff810f7b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:27.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4977" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:27.706: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:27.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4962" for this suite.
STEP: Destroying namespace "nspatchtest-341eb0e6-bff8-467f-892f-38fe1f92967b-5202" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":43,"skipped":644,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:27.780: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 08:39:27.817: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:27.819: INFO: Number of nodes with available pods: 0
Sep 23 08:39:27.819: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:28.822: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:28.824: INFO: Number of nodes with available pods: 0
Sep 23 08:39:28.824: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:29.822: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:29.824: INFO: Number of nodes with available pods: 1
Sep 23 08:39:29.824: INFO: Node worker-s002 is running more than one daemon pod
Sep 23 08:39:30.824: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:30.826: INFO: Number of nodes with available pods: 2
Sep 23 08:39:30.826: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 23 08:39:30.840: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:30.841: INFO: Number of nodes with available pods: 1
Sep 23 08:39:30.842: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:31.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:31.847: INFO: Number of nodes with available pods: 1
Sep 23 08:39:31.847: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:32.844: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:32.847: INFO: Number of nodes with available pods: 1
Sep 23 08:39:32.847: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:33.844: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:33.846: INFO: Number of nodes with available pods: 1
Sep 23 08:39:33.846: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:34.846: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:34.850: INFO: Number of nodes with available pods: 1
Sep 23 08:39:34.850: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:35.852: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:35.853: INFO: Number of nodes with available pods: 1
Sep 23 08:39:35.853: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:36.847: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:36.848: INFO: Number of nodes with available pods: 1
Sep 23 08:39:36.848: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:37.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:37.850: INFO: Number of nodes with available pods: 1
Sep 23 08:39:37.850: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:38.846: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:38.848: INFO: Number of nodes with available pods: 1
Sep 23 08:39:38.848: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:39.849: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:39.852: INFO: Number of nodes with available pods: 1
Sep 23 08:39:39.852: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:40.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:40.847: INFO: Number of nodes with available pods: 1
Sep 23 08:39:40.847: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:41.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:41.848: INFO: Number of nodes with available pods: 1
Sep 23 08:39:41.848: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:42.844: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:42.846: INFO: Number of nodes with available pods: 1
Sep 23 08:39:42.846: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:43.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:43.850: INFO: Number of nodes with available pods: 1
Sep 23 08:39:43.850: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:39:44.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:39:44.847: INFO: Number of nodes with available pods: 2
Sep 23 08:39:44.847: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6183, will wait for the garbage collector to delete the pods
Sep 23 08:39:44.906: INFO: Deleting DaemonSet.extensions daemon-set took: 6.265256ms
Sep 23 08:39:46.407: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.500136288s
Sep 23 08:39:48.509: INFO: Number of nodes with available pods: 0
Sep 23 08:39:48.509: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 08:39:48.511: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72155"},"items":null}

Sep 23 08:39:48.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72155"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:48.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6183" for this suite.

• [SLOW TEST:20.746 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":44,"skipped":649,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-64f363e6-bcb3-4165-948d-b7ff5a7e6d86
STEP: Creating a pod to test consume configMaps
Sep 23 08:39:48.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf" in namespace "projected-7452" to be "Succeeded or Failed"
Sep 23 08:39:48.564: INFO: Pod "pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.704014ms
Sep 23 08:39:50.568: INFO: Pod "pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005779289s
Sep 23 08:39:52.570: INFO: Pod "pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007862962s
STEP: Saw pod success
Sep 23 08:39:52.570: INFO: Pod "pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf" satisfied condition "Succeeded or Failed"
Sep 23 08:39:52.571: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:39:52.588: INFO: Waiting for pod pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf to disappear
Sep 23 08:39:52.591: INFO: Pod pod-projected-configmaps-4c761010-81d2-4097-9279-f49897178caf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:52.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7452" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":658,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:52.597: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 23 08:39:52.638: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 23 08:39:57.640: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:39:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8477" for this suite.

• [SLOW TEST:6.068 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":46,"skipped":672,"failed":0}
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:39:58.666: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5989 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5989;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5989 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5989;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5989.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5989.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5989.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5989.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5989.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5989.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5989.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5989.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5989.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 112.95.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.95.112_udp@PTR;check="$$(dig +tcp +noall +answer +search 112.95.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.95.112_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5989 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5989;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5989 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5989;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5989.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5989.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5989.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5989.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5989.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5989.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5989.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5989.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5989.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5989.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 112.95.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.95.112_udp@PTR;check="$$(dig +tcp +noall +answer +search 112.95.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.95.112_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:40:02.736: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.738: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.739: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.753: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.758: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.773: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.775: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.782: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.784: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.785: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.787: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:02.799: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc wheezy_udp@_http._tcp.dns-test-service.dns-5989.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:07.801: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.803: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.812: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.814: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.822: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.823: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.835: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.836: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.838: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.842: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.844: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:07.855: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc wheezy_udp@_http._tcp.dns-test-service.dns-5989.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:12.801: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.802: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.804: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.807: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.808: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.828: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.829: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.831: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.834: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:12.849: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:17.801: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.804: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.805: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.808: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.810: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.811: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.824: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.826: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.827: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.828: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.830: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.831: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:17.843: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:22.801: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.803: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.815: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.943: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.945: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.969: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.973: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.974: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.975: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.977: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.978: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:22.992: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:27.801: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.803: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.805: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.806: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.828: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.829: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.831: INFO: Unable to read jessie_udp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989 from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.833: INFO: Unable to read jessie_udp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.834: INFO: Unable to read jessie_tcp@dns-test-service.dns-5989.svc from pod dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729: the server could not find the requested resource (get pods dns-test-5bee64db-dbd8-42b6-ab58-395f43145729)
Sep 23 08:40:27.844: INFO: Lookups using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5989 wheezy_tcp@dns-test-service.dns-5989 wheezy_udp@dns-test-service.dns-5989.svc wheezy_tcp@dns-test-service.dns-5989.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5989 jessie_tcp@dns-test-service.dns-5989 jessie_udp@dns-test-service.dns-5989.svc jessie_tcp@dns-test-service.dns-5989.svc]

Sep 23 08:40:32.842: INFO: DNS probes using dns-5989/dns-test-5bee64db-dbd8-42b6-ab58-395f43145729 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:40:32.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5989" for this suite.

• [SLOW TEST:34.223 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":47,"skipped":672,"failed":0}
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:40:32.889: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Sep 23 08:40:32.919: INFO: Waiting up to 5m0s for pod "var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b" in namespace "var-expansion-334" to be "Succeeded or Failed"
Sep 23 08:40:32.921: INFO: Pod "var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.410772ms
Sep 23 08:40:34.925: INFO: Pod "var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006414237s
Sep 23 08:40:36.928: INFO: Pod "var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581387s
STEP: Saw pod success
Sep 23 08:40:36.928: INFO: Pod "var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b" satisfied condition "Succeeded or Failed"
Sep 23 08:40:36.930: INFO: Trying to get logs from node worker-s002 pod var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b container dapi-container: <nil>
STEP: delete the pod
Sep 23 08:40:36.945: INFO: Waiting for pod var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b to disappear
Sep 23 08:40:36.946: INFO: Pod var-expansion-bd3698b2-1364-438f-a4f4-26c2e61b051b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:40:36.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-334" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":672,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:40:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 23 08:40:42.008: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:40:43.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9820" for this suite.

• [SLOW TEST:6.081 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":49,"skipped":676,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:40:43.032: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0923 08:40:53.115115      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 08:41:55.131: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 23 08:41:55.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-29d9t" in namespace "gc-6376"
Sep 23 08:41:55.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-2h72f" in namespace "gc-6376"
Sep 23 08:41:55.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-5tsjf" in namespace "gc-6376"
Sep 23 08:41:55.171: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8ktk" in namespace "gc-6376"
Sep 23 08:41:55.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-lr6sl" in namespace "gc-6376"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:41:55.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6376" for this suite.

• [SLOW TEST:72.162 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":50,"skipped":678,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:41:55.194: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 23 08:41:55.229: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 08:42:55.260: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:42:55.261: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:42:55.298: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Sep 23 08:42:55.299: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:42:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-362" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:42:55.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-805" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.156 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":51,"skipped":695,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:42:55.350: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4030
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 08:42:55.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 23 08:42:55.399: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 08:42:57.401: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 08:42:59.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:43:01.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:43:03.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:43:05.402: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:43:07.401: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:43:09.402: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 23 08:43:09.405: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 23 08:43:11.480: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 23 08:43:15.495: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 23 08:43:15.495: INFO: Breadth first check of 10.10.28.28 on host 172.16.0.6...
Sep 23 08:43:15.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.131.161:9080/dial?request=hostname&protocol=http&host=10.10.28.28&port=8080&tries=1'] Namespace:pod-network-test-4030 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:43:15.497: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:43:15.584: INFO: Waiting for responses: map[]
Sep 23 08:43:15.584: INFO: reached 10.10.28.28 after 0/1 tries
Sep 23 08:43:15.584: INFO: Breadth first check of 10.10.131.159 on host 172.16.0.7...
Sep 23 08:43:15.586: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.131.161:9080/dial?request=hostname&protocol=http&host=10.10.131.159&port=8080&tries=1'] Namespace:pod-network-test-4030 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:43:15.586: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:43:15.683: INFO: Waiting for responses: map[]
Sep 23 08:43:15.684: INFO: reached 10.10.131.159 after 0/1 tries
Sep 23 08:43:15.684: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:43:15.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4030" for this suite.

• [SLOW TEST:20.458 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":713,"failed":0}
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:43:15.808: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 08:43:15.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71" in namespace "downward-api-2649" to be "Succeeded or Failed"
Sep 23 08:43:15.841: INFO: Pod "downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455156ms
Sep 23 08:43:17.844: INFO: Pod "downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005351492s
STEP: Saw pod success
Sep 23 08:43:17.844: INFO: Pod "downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71" satisfied condition "Succeeded or Failed"
Sep 23 08:43:17.846: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71 container client-container: <nil>
STEP: delete the pod
Sep 23 08:43:17.868: INFO: Waiting for pod downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71 to disappear
Sep 23 08:43:17.872: INFO: Pod downwardapi-volume-997a29cc-72ee-44ff-9754-bd54abdbba71 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:43:17.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2649" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":713,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:43:17.877: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7979
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Sep 23 08:43:17.912: INFO: Found 0 stateful pods, waiting for 3
Sep 23 08:43:27.915: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:43:27.915: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:43:27.915: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:43:27.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7979 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 08:43:28.210: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 08:43:28.210: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 08:43:28.210: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 23 08:43:38.234: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 23 08:43:48.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7979 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 08:43:48.393: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 08:43:48.393: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 08:43:48.393: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 08:44:48.412: INFO: Waiting for StatefulSet statefulset-7979/ss2 to complete update
Sep 23 08:44:48.412: INFO: Waiting for Pod statefulset-7979/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Sep 23 08:44:58.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7979 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 08:44:58.579: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 08:44:58.579: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 08:44:58.579: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 08:45:08.623: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 23 08:45:18.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7979 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 08:45:18.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 08:45:18.793: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 08:45:18.793: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 08:45:38.805: INFO: Waiting for StatefulSet statefulset-7979/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 08:45:48.813: INFO: Deleting all statefulset in ns statefulset-7979
Sep 23 08:45:48.815: INFO: Scaling statefulset ss2 to 0
Sep 23 08:46:18.825: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 08:46:18.827: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:46:18.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7979" for this suite.

• [SLOW TEST:180.968 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":54,"skipped":722,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:46:18.845: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Sep 23 08:46:18.868: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2098 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:46:18.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2098" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":55,"skipped":725,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:46:18.921: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 23 08:46:18.965: INFO: starting watch
STEP: patching
STEP: updating
Sep 23 08:46:18.971: INFO: waiting for watch events with expected annotations
Sep 23 08:46:18.971: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:46:18.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6708" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":56,"skipped":736,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:46:18.988: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:46:35.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9531" for this suite.

• [SLOW TEST:16.118 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":57,"skipped":751,"failed":0}
S
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:46:35.106: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 23 08:46:35.135: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 08:47:35.175: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:47:35.180: INFO: Starting informer...
STEP: Starting pods...
Sep 23 08:47:35.395: INFO: Pod1 is running on worker-s002. Tainting Node
Sep 23 08:47:39.616: INFO: Pod2 is running on worker-s002. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 23 08:47:51.054: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 23 08:48:06.316: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:06.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8003" for this suite.

• [SLOW TEST:91.271 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":58,"skipped":752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1215" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":59,"skipped":780,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:06.429: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 23 08:48:06.457: INFO: Waiting up to 5m0s for pod "pod-7b485b20-de65-44fc-b83f-9cd1856089b6" in namespace "emptydir-3554" to be "Succeeded or Failed"
Sep 23 08:48:06.459: INFO: Pod "pod-7b485b20-de65-44fc-b83f-9cd1856089b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.943863ms
Sep 23 08:48:08.462: INFO: Pod "pod-7b485b20-de65-44fc-b83f-9cd1856089b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005152439s
Sep 23 08:48:10.470: INFO: Pod "pod-7b485b20-de65-44fc-b83f-9cd1856089b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013191424s
STEP: Saw pod success
Sep 23 08:48:10.471: INFO: Pod "pod-7b485b20-de65-44fc-b83f-9cd1856089b6" satisfied condition "Succeeded or Failed"
Sep 23 08:48:10.472: INFO: Trying to get logs from node worker-s002 pod pod-7b485b20-de65-44fc-b83f-9cd1856089b6 container test-container: <nil>
STEP: delete the pod
Sep 23 08:48:10.494: INFO: Waiting for pod pod-7b485b20-de65-44fc-b83f-9cd1856089b6 to disappear
Sep 23 08:48:10.496: INFO: Pod pod-7b485b20-de65-44fc-b83f-9cd1856089b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:10.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3554" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:10.501: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep 23 08:48:15.057: INFO: Successfully updated pod "annotationupdate7d717c9d-d47f-4c22-b056-2a464d23c7c1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:17.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9190" for this suite.

• [SLOW TEST:6.579 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":61,"skipped":866,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:17.080: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:17.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7326" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":62,"skipped":874,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:17.141: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 23 08:48:19.176: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:19.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2584" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":895,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:19.200: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-ee417489-cc2a-4edf-8809-336ce9e541b8
STEP: Creating a pod to test consume configMaps
Sep 23 08:48:19.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a" in namespace "projected-7125" to be "Succeeded or Failed"
Sep 23 08:48:19.236: INFO: Pod "pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632084ms
Sep 23 08:48:21.241: INFO: Pod "pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007386233s
STEP: Saw pod success
Sep 23 08:48:21.241: INFO: Pod "pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a" satisfied condition "Succeeded or Failed"
Sep 23 08:48:21.243: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:48:21.254: INFO: Waiting for pod pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a to disappear
Sep 23 08:48:21.256: INFO: Pod pod-projected-configmaps-f1ddbd12-b598-4a27-b28e-a3786977018a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:21.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7125" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":64,"skipped":902,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:21.273: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep 23 08:48:21.304: INFO: Waiting up to 5m0s for pod "downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae" in namespace "downward-api-1699" to be "Succeeded or Failed"
Sep 23 08:48:21.307: INFO: Pod "downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.910022ms
Sep 23 08:48:23.310: INFO: Pod "downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006355223s
STEP: Saw pod success
Sep 23 08:48:23.310: INFO: Pod "downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae" satisfied condition "Succeeded or Failed"
Sep 23 08:48:23.313: INFO: Trying to get logs from node worker-s002 pod downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae container dapi-container: <nil>
STEP: delete the pod
Sep 23 08:48:23.327: INFO: Waiting for pod downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae to disappear
Sep 23 08:48:23.329: INFO: Pod downward-api-9dcf213d-d574-4fd0-a63f-8d84abc347ae no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:23.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1699" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:23.334: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:48:23.603: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 23 08:48:25.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983703, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983703, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983703, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983703, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:48:28.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:48:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5377-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:29.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5729" for this suite.
STEP: Destroying namespace "webhook-5729-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.401 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":66,"skipped":935,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 23 08:48:29.763: INFO: Waiting up to 5m0s for pod "pod-54370270-3292-40a2-b98c-c5b8fb96b951" in namespace "emptydir-6940" to be "Succeeded or Failed"
Sep 23 08:48:29.764: INFO: Pod "pod-54370270-3292-40a2-b98c-c5b8fb96b951": Phase="Pending", Reason="", readiness=false. Elapsed: 1.466844ms
Sep 23 08:48:31.767: INFO: Pod "pod-54370270-3292-40a2-b98c-c5b8fb96b951": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004459555s
STEP: Saw pod success
Sep 23 08:48:31.767: INFO: Pod "pod-54370270-3292-40a2-b98c-c5b8fb96b951" satisfied condition "Succeeded or Failed"
Sep 23 08:48:31.770: INFO: Trying to get logs from node worker-s002 pod pod-54370270-3292-40a2-b98c-c5b8fb96b951 container test-container: <nil>
STEP: delete the pod
Sep 23 08:48:31.779: INFO: Waiting for pod pod-54370270-3292-40a2-b98c-c5b8fb96b951 to disappear
Sep 23 08:48:31.781: INFO: Pod pod-54370270-3292-40a2-b98c-c5b8fb96b951 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:31.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6940" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:31.786: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 23 08:48:33.831: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7939" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":1013,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 23 08:48:33.893: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76367 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 08:48:33.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76368 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 08:48:33.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76369 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 23 08:48:43.925: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76446 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 08:48:43.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76447 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 08:48:43.925: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1304  9e4d74e6-b40b-4428-a223-132b24d38e0b 76448 0 2021-09-23 08:48:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-23 08:48:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:43.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1304" for this suite.

• [SLOW TEST:10.081 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":69,"skipped":1027,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:43.929: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 23 08:48:43.962: INFO: Waiting up to 5m0s for pod "pod-7b0be2cf-c285-4aee-bbf2-3749c433b353" in namespace "emptydir-9155" to be "Succeeded or Failed"
Sep 23 08:48:43.966: INFO: Pod "pod-7b0be2cf-c285-4aee-bbf2-3749c433b353": Phase="Pending", Reason="", readiness=false. Elapsed: 4.460768ms
Sep 23 08:48:45.969: INFO: Pod "pod-7b0be2cf-c285-4aee-bbf2-3749c433b353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007650074s
Sep 23 08:48:47.972: INFO: Pod "pod-7b0be2cf-c285-4aee-bbf2-3749c433b353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010142165s
STEP: Saw pod success
Sep 23 08:48:47.972: INFO: Pod "pod-7b0be2cf-c285-4aee-bbf2-3749c433b353" satisfied condition "Succeeded or Failed"
Sep 23 08:48:47.973: INFO: Trying to get logs from node worker-s002 pod pod-7b0be2cf-c285-4aee-bbf2-3749c433b353 container test-container: <nil>
STEP: delete the pod
Sep 23 08:48:47.990: INFO: Waiting for pod pod-7b0be2cf-c285-4aee-bbf2-3749c433b353 to disappear
Sep 23 08:48:47.992: INFO: Pod pod-7b0be2cf-c285-4aee-bbf2-3749c433b353 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:47.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9155" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1041,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:47.996: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Sep 23 08:48:48.023: INFO: created test-podtemplate-1
Sep 23 08:48:48.025: INFO: created test-podtemplate-2
Sep 23 08:48:48.027: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 23 08:48:48.030: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 23 08:48:48.036: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7893" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":71,"skipped":1045,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Sep 23 08:48:48.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-5971 create -f -'
Sep 23 08:48:48.360: INFO: stderr: ""
Sep 23 08:48:48.360: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 23 08:48:48.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-5971 diff -f -'
Sep 23 08:48:48.712: INFO: rc: 1
Sep 23 08:48:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-5971 delete -f -'
Sep 23 08:48:48.769: INFO: stderr: ""
Sep 23 08:48:48.769: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:48.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5971" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":72,"skipped":1054,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:48.776: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:48:49.205: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 23 08:48:51.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983729, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983729, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983729, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767983729, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:48:54.222: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:48:54.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5192" for this suite.
STEP: Destroying namespace "webhook-5192-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.507 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":73,"skipped":1066,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:48:54.283: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-83b1e3c5-5cb1-48f0-acfb-aeeefecc720c
STEP: Creating secret with name s-test-opt-upd-68bcf495-5a47-40f0-b881-d80603760ee0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-83b1e3c5-5cb1-48f0-acfb-aeeefecc720c
STEP: Updating secret s-test-opt-upd-68bcf495-5a47-40f0-b881-d80603760ee0
STEP: Creating secret with name s-test-opt-create-d5b862a8-08eb-4a86-b3da-cdb030cfc59b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:50:18.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5068" for this suite.

• [SLOW TEST:84.398 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1115,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:50:18.682: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-2388
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2388 to expose endpoints map[]
Sep 23 08:50:18.729: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep 23 08:50:19.734: INFO: successfully validated that service endpoint-test2 in namespace services-2388 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2388
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2388 to expose endpoints map[pod1:[80]]
Sep 23 08:50:22.748: INFO: successfully validated that service endpoint-test2 in namespace services-2388 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2388
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2388 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 23 08:50:24.773: INFO: successfully validated that service endpoint-test2 in namespace services-2388 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2388
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2388 to expose endpoints map[pod2:[80]]
Sep 23 08:50:24.793: INFO: successfully validated that service endpoint-test2 in namespace services-2388 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2388
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2388 to expose endpoints map[]
Sep 23 08:50:24.815: INFO: successfully validated that service endpoint-test2 in namespace services-2388 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:50:24.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2388" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.162 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":75,"skipped":1132,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:50:24.844: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-ef5469f6-f40b-473f-a220-7388ec2b90a2
STEP: Creating a pod to test consume secrets
Sep 23 08:50:24.878: INFO: Waiting up to 5m0s for pod "pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2" in namespace "secrets-6055" to be "Succeeded or Failed"
Sep 23 08:50:24.881: INFO: Pod "pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455542ms
Sep 23 08:50:26.885: INFO: Pod "pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006941348s
Sep 23 08:50:28.889: INFO: Pod "pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010752984s
STEP: Saw pod success
Sep 23 08:50:28.889: INFO: Pod "pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2" satisfied condition "Succeeded or Failed"
Sep 23 08:50:28.890: INFO: Trying to get logs from node worker-s002 pod pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 08:50:28.905: INFO: Waiting for pod pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2 to disappear
Sep 23 08:50:28.906: INFO: Pod pod-secrets-62465e4a-fc3e-4770-8db9-22afc4b0c8d2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:50:28.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6055" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:50:28.912: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-c23e3a95-46a6-4ebf-9e99-f25769ee807e
STEP: Creating a pod to test consume secrets
Sep 23 08:50:28.947: INFO: Waiting up to 5m0s for pod "pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2" in namespace "secrets-1680" to be "Succeeded or Failed"
Sep 23 08:50:28.949: INFO: Pod "pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.536675ms
Sep 23 08:50:30.952: INFO: Pod "pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004577963s
Sep 23 08:50:32.958: INFO: Pod "pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010539567s
STEP: Saw pod success
Sep 23 08:50:32.958: INFO: Pod "pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2" satisfied condition "Succeeded or Failed"
Sep 23 08:50:32.959: INFO: Trying to get logs from node worker-s002 pod pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 08:50:32.974: INFO: Waiting for pod pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2 to disappear
Sep 23 08:50:32.976: INFO: Pod pod-secrets-9037ca18-ee17-4423-b2b9-8ad7eecd47c2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:50:32.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1680" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:50:32.980: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1231.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:50:37.029: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.034: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.036: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.037: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.042: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.044: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.049: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.051: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:37.054: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:50:42.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.058: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.060: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.061: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.079: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.081: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.082: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.083: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:42.092: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:50:47.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.058: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.064: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.066: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.070: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.072: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.073: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.075: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:47.082: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:50:52.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.058: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.061: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.062: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.069: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.071: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.072: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.073: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:52.076: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:50:57.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.058: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.060: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.061: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.068: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.069: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.071: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.073: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:50:57.076: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:51:02.056: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.058: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.071: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.080: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.101: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.103: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.104: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.112: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local from pod dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6: the server could not find the requested resource (get pods dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6)
Sep 23 08:51:02.115: INFO: Lookups using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1231.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1231.svc.cluster.local jessie_udp@dns-test-service-2.dns-1231.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1231.svc.cluster.local]

Sep 23 08:51:07.073: INFO: DNS probes using dns-1231/dns-test-3cb18dd3-9d19-4d2e-b200-c5f8625238a6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:07.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1231" for this suite.

• [SLOW TEST:34.129 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":78,"skipped":1228,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:07.109: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep 23 08:51:11.671: INFO: Successfully updated pod "labelsupdatea8a2d830-64f6-4ef3-b9ae-685357e75b67"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-612" for this suite.

• [SLOW TEST:6.585 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1232,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:13.694: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:51:15.738: INFO: DNS probes using dns-test-a5f73a02-a48d-4100-9ea4-28c825de9cf8 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:51:19.771: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:19.773: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:19.773: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:24.776: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:24.779: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:24.779: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:29.775: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:29.777: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:29.777: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:34.776: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:34.777: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:34.778: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:39.775: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:39.777: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:39.777: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:44.776: INFO: File wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:44.778: INFO: File jessie_udp@dns-test-service-3.dns-935.svc.cluster.local from pod  dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 23 08:51:44.778: INFO: Lookups using dns-935/dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 failed for: [wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local jessie_udp@dns-test-service-3.dns-935.svc.cluster.local]

Sep 23 08:51:49.777: INFO: DNS probes using dns-test-28a15065-4429-4b3e-b345-1f9a31c21528 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-935.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-935.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 08:51:53.818: INFO: DNS probes using dns-test-141ec8c4-29de-47c5-a88d-11856d59248d succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:53.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-935" for this suite.

• [SLOW TEST:40.156 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":80,"skipped":1272,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:53.851: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:51:53.896: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-c5cb2988-2936-4ced-90d0-cc392b5ece3e" in namespace "security-context-test-7892" to be "Succeeded or Failed"
Sep 23 08:51:53.900: INFO: Pod "busybox-privileged-false-c5cb2988-2936-4ced-90d0-cc392b5ece3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18481ms
Sep 23 08:51:55.903: INFO: Pod "busybox-privileged-false-c5cb2988-2936-4ced-90d0-cc392b5ece3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007326468s
Sep 23 08:51:55.903: INFO: Pod "busybox-privileged-false-c5cb2988-2936-4ced-90d0-cc392b5ece3e" satisfied condition "Succeeded or Failed"
Sep 23 08:51:55.913: INFO: Got logs for pod "busybox-privileged-false-c5cb2988-2936-4ced-90d0-cc392b5ece3e": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7892" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1277,"failed":0}

------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:55.920: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Sep 23 08:51:55.954: INFO: created test-event-1
Sep 23 08:51:55.956: INFO: created test-event-2
Sep 23 08:51:55.957: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 23 08:51:55.959: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 23 08:51:55.965: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:55.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9604" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":82,"skipped":1277,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:55.976: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 23 08:51:56.004: INFO: Waiting up to 5m0s for pod "pod-b52c23da-f803-44f2-8146-085da42ea52b" in namespace "emptydir-8509" to be "Succeeded or Failed"
Sep 23 08:51:56.005: INFO: Pod "pod-b52c23da-f803-44f2-8146-085da42ea52b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.629716ms
Sep 23 08:51:58.008: INFO: Pod "pod-b52c23da-f803-44f2-8146-085da42ea52b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003813983s
STEP: Saw pod success
Sep 23 08:51:58.008: INFO: Pod "pod-b52c23da-f803-44f2-8146-085da42ea52b" satisfied condition "Succeeded or Failed"
Sep 23 08:51:58.011: INFO: Trying to get logs from node worker-s002 pod pod-b52c23da-f803-44f2-8146-085da42ea52b container test-container: <nil>
STEP: delete the pod
Sep 23 08:51:58.026: INFO: Waiting for pod pod-b52c23da-f803-44f2-8146-085da42ea52b to disappear
Sep 23 08:51:58.030: INFO: Pod pod-b52c23da-f803-44f2-8146-085da42ea52b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:51:58.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8509" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1281,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:51:58.035: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:51:58.063: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:52:00.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8398" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1295,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:52:00.096: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4236
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 08:52:00.121: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 23 08:52:00.152: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 08:52:02.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:52:04.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:52:06.157: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:52:08.154: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:52:10.156: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 08:52:12.156: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 23 08:52:12.159: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 23 08:52:14.164: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 23 08:52:16.164: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 23 08:52:18.161: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep 23 08:52:20.166: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 23 08:52:24.188: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 23 08:52:24.188: INFO: Going to poll 10.10.28.42 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Sep 23 08:52:24.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.28.42:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:52:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:52:24.266: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 23 08:52:24.266: INFO: Going to poll 10.10.131.143 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Sep 23 08:52:24.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.131.143:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4236 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:52:24.268: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:52:24.339: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:52:24.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4236" for this suite.

• [SLOW TEST:24.249 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1309,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:52:24.345: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 23 08:52:24.379: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:52:28.669: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:52:45.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-994" for this suite.

• [SLOW TEST:21.183 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":86,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:52:45.528: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0923 08:52:55.595992      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 08:53:57.606: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:53:57.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1353" for this suite.

• [SLOW TEST:72.089 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":87,"skipped":1344,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:53:57.617: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-zvjh
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 08:53:57.659: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zvjh" in namespace "subpath-7852" to be "Succeeded or Failed"
Sep 23 08:53:57.662: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.317873ms
Sep 23 08:53:59.665: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.005491986s
Sep 23 08:54:01.669: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.009324891s
Sep 23 08:54:03.672: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.012327088s
Sep 23 08:54:05.689: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.029317447s
Sep 23 08:54:07.692: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.032588673s
Sep 23 08:54:09.696: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.036111316s
Sep 23 08:54:11.699: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.03911575s
Sep 23 08:54:13.702: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.042055835s
Sep 23 08:54:15.704: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.044814846s
Sep 23 08:54:17.707: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.047883511s
Sep 23 08:54:19.710: INFO: Pod "pod-subpath-test-projected-zvjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.050406348s
STEP: Saw pod success
Sep 23 08:54:19.710: INFO: Pod "pod-subpath-test-projected-zvjh" satisfied condition "Succeeded or Failed"
Sep 23 08:54:19.711: INFO: Trying to get logs from node worker-s002 pod pod-subpath-test-projected-zvjh container test-container-subpath-projected-zvjh: <nil>
STEP: delete the pod
Sep 23 08:54:19.738: INFO: Waiting for pod pod-subpath-test-projected-zvjh to disappear
Sep 23 08:54:19.740: INFO: Pod pod-subpath-test-projected-zvjh no longer exists
STEP: Deleting pod pod-subpath-test-projected-zvjh
Sep 23 08:54:19.740: INFO: Deleting pod "pod-subpath-test-projected-zvjh" in namespace "subpath-7852"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:54:19.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7852" for this suite.

• [SLOW TEST:22.128 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":88,"skipped":1353,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:54:19.746: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 23 08:54:19.782: INFO: Waiting up to 5m0s for pod "pod-924cb86d-d9a7-4c30-8b19-9c3b85876826" in namespace "emptydir-2164" to be "Succeeded or Failed"
Sep 23 08:54:19.784: INFO: Pod "pod-924cb86d-d9a7-4c30-8b19-9c3b85876826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.72617ms
Sep 23 08:54:21.787: INFO: Pod "pod-924cb86d-d9a7-4c30-8b19-9c3b85876826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005807888s
Sep 23 08:54:23.790: INFO: Pod "pod-924cb86d-d9a7-4c30-8b19-9c3b85876826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008848224s
STEP: Saw pod success
Sep 23 08:54:23.790: INFO: Pod "pod-924cb86d-d9a7-4c30-8b19-9c3b85876826" satisfied condition "Succeeded or Failed"
Sep 23 08:54:23.792: INFO: Trying to get logs from node worker-s002 pod pod-924cb86d-d9a7-4c30-8b19-9c3b85876826 container test-container: <nil>
STEP: delete the pod
Sep 23 08:54:23.802: INFO: Waiting for pod pod-924cb86d-d9a7-4c30-8b19-9c3b85876826 to disappear
Sep 23 08:54:23.804: INFO: Pod pod-924cb86d-d9a7-4c30-8b19-9c3b85876826 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:54:23.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2164" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:54:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Sep 23 08:54:23.840: INFO: Waiting up to 5m0s for pod "client-containers-0b461248-0803-4f4f-be00-8fdd377dd843" in namespace "containers-326" to be "Succeeded or Failed"
Sep 23 08:54:23.843: INFO: Pod "client-containers-0b461248-0803-4f4f-be00-8fdd377dd843": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773472ms
Sep 23 08:54:25.847: INFO: Pod "client-containers-0b461248-0803-4f4f-be00-8fdd377dd843": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006889828s
Sep 23 08:54:27.850: INFO: Pod "client-containers-0b461248-0803-4f4f-be00-8fdd377dd843": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010138057s
STEP: Saw pod success
Sep 23 08:54:27.850: INFO: Pod "client-containers-0b461248-0803-4f4f-be00-8fdd377dd843" satisfied condition "Succeeded or Failed"
Sep 23 08:54:27.851: INFO: Trying to get logs from node worker-s002 pod client-containers-0b461248-0803-4f4f-be00-8fdd377dd843 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:54:27.864: INFO: Waiting for pod client-containers-0b461248-0803-4f4f-be00-8fdd377dd843 to disappear
Sep 23 08:54:27.865: INFO: Pod client-containers-0b461248-0803-4f4f-be00-8fdd377dd843 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:54:27.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-326" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":90,"skipped":1379,"failed":0}
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:54:27.870: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3699
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Sep 23 08:54:27.909: INFO: Found 0 stateful pods, waiting for 3
Sep 23 08:54:37.913: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:54:37.913: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:54:37.913: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Sep 23 08:54:37.932: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 23 08:54:47.960: INFO: Updating stateful set ss2
Sep 23 08:54:47.963: INFO: Waiting for Pod statefulset-3699/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Sep 23 08:54:58.009: INFO: Found 2 stateful pods, waiting for 3
Sep 23 08:55:08.012: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:55:08.012: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 08:55:08.012: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 23 08:55:08.032: INFO: Updating stateful set ss2
Sep 23 08:55:08.041: INFO: Waiting for Pod statefulset-3699/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Sep 23 08:55:18.066: INFO: Updating stateful set ss2
Sep 23 08:55:18.079: INFO: Waiting for StatefulSet statefulset-3699/ss2 to complete update
Sep 23 08:55:18.079: INFO: Waiting for Pod statefulset-3699/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 08:55:28.089: INFO: Deleting all statefulset in ns statefulset-3699
Sep 23 08:55:28.091: INFO: Scaling statefulset ss2 to 0
Sep 23 08:55:48.103: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 08:55:48.104: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:55:48.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3699" for this suite.

• [SLOW TEST:80.248 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":91,"skipped":1385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:55:48.118: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d462fbc4-ca72-4b6c-9368-f21ad3d43aaa
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d462fbc4-ca72-4b6c-9368-f21ad3d43aaa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:57:20.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3236" for this suite.

• [SLOW TEST:92.428 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1425,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:57:20.546: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:57:35.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-608" for this suite.
STEP: Destroying namespace "nsdeletetest-299" for this suite.
Sep 23 08:57:35.658: INFO: Namespace nsdeletetest-299 was already deleted
STEP: Destroying namespace "nsdeletetest-9827" for this suite.

• [SLOW TEST:15.115 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":93,"skipped":1432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:57:35.661: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 23 08:57:35.687: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 08:58:35.722: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Sep 23 08:58:35.744: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 23 08:58:35.759: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:58:55.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5997" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:80.147 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":94,"skipped":1466,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:58:55.809: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:58:55.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2652" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":95,"skipped":1477,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:58:55.851: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-138f8131-d209-4e19-9c63-3da13fac473a
STEP: Creating a pod to test consume secrets
Sep 23 08:58:55.877: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc" in namespace "projected-9345" to be "Succeeded or Failed"
Sep 23 08:58:55.879: INFO: Pod "pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.549801ms
Sep 23 08:58:57.884: INFO: Pod "pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006746731s
STEP: Saw pod success
Sep 23 08:58:57.884: INFO: Pod "pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc" satisfied condition "Succeeded or Failed"
Sep 23 08:58:57.885: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 08:58:57.908: INFO: Waiting for pod pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc to disappear
Sep 23 08:58:57.910: INFO: Pod pod-projected-secrets-aab8e43d-5dcd-4472-82ba-0ad118f360fc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:58:57.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9345" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:58:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:58:57.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 create -f -'
Sep 23 08:58:58.352: INFO: stderr: ""
Sep 23 08:58:58.352: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 23 08:58:58.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 create -f -'
Sep 23 08:58:58.570: INFO: stderr: ""
Sep 23 08:58:58.570: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 23 08:58:59.575: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:58:59.575: INFO: Found 0 / 1
Sep 23 08:59:00.578: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:00.578: INFO: Found 1 / 1
Sep 23 08:59:00.578: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 23 08:59:00.580: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:00.580: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 08:59:00.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 describe pod agnhost-primary-lxbk2'
Sep 23 08:59:00.649: INFO: stderr: ""
Sep 23 08:59:00.649: INFO: stdout: "Name:         agnhost-primary-lxbk2\nNamespace:    kubectl-2500\nPriority:     0\nNode:         worker-s002/172.16.0.7\nStart Time:   Thu, 23 Sep 2021 08:58:58 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.10.131.158/32\n              cni.projectcalico.org/podIPs: 10.10.131.158/32\nStatus:       Running\nIP:           10.10.131.158\nIPs:\n  IP:           10.10.131.158\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://eb7e6870edf89e83527b7fad8fdc46f734526a65c57fda9b1c73fdaeb996b01a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 23 Sep 2021 08:58:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wwvm4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wwvm4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wwvm4\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2500/agnhost-primary-lxbk2 to worker-s002\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep 23 08:59:00.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 describe rc agnhost-primary'
Sep 23 08:59:00.730: INFO: stderr: ""
Sep 23 08:59:00.730: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2500\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lxbk2\n"
Sep 23 08:59:00.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 describe service agnhost-primary'
Sep 23 08:59:00.792: INFO: stderr: ""
Sep 23 08:59:00.792: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2500\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.96.0.219\nIPs:               10.96.0.219\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.131.158:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 23 08:59:00.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 describe node master1'
Sep 23 08:59:00.896: INFO: stderr: ""
Sep 23 08:59:00.896: INFO: stdout: "Name:               master1\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    topology.csi-qingcloud/instance-type=Enterprise2\n                    topology.csi-qingcloud/zone=ap3a\n                    topology.kubernetes.io/zone=ap3a\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi-qingcloud\":\"i-p3g98sdc\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.beta.kubernetes.io/instance-id: i-p3g98sdc\n                    projectcalico.org/IPv4Address: 172.16.0.8/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.10.137.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Sep 2021 03:56:46 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 23 Sep 2021 08:58:53 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 23 Sep 2021 03:57:18 +0000   Thu, 23 Sep 2021 03:57:18 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 23 Sep 2021 08:56:23 +0000   Thu, 23 Sep 2021 03:56:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 23 Sep 2021 08:56:23 +0000   Thu, 23 Sep 2021 03:56:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 23 Sep 2021 08:56:23 +0000   Thu, 23 Sep 2021 03:56:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 23 Sep 2021 08:56:23 +0000   Thu, 23 Sep 2021 03:57:24 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.0.8\n  Hostname:    master1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  82045336Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8167484Ki\n  pods:               120\nAllocatable:\n  cpu:                3600m\n  ephemeral-storage:  82045336Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6921328429\n  pods:               120\nSystem Info:\n  Machine ID:                   519fa94a2f53476e0db45e26614bfa62\n  System UUID:                  B419324C-51E1-35A2-9540-51A552A2368A\n  Boot ID:                      ab94c579-91b9-4397-b635-37a58ef34849\n  Kernel Version:               4.15.0-158-generic\n  OS Image:                     Ubuntu 18.04.6 LTS\n  Operating System:             linux\n  Architecture:                 amd64\n  Container Runtime Version:    docker://20.10.6\n  Kubelet Version:              v1.20.6\n  Kube-Proxy Version:           v1.20.6\nPodCIDR:                        10.10.0.0/24\nPodCIDRs:                       10.10.0.0/24\nNon-terminated Pods:            (15 in total)\n  Namespace                     Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                     ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                   calico-kube-controllers-8f59968d4-n5nl4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h1m\n  kube-system                   calico-node-n859k                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         5h1m\n  kube-system                   cloud-controller-manager-cf79d654-zc8ct                    100m (2%)     200m (5%)   50Mi (0%)        100Mi (1%)     5h\n  kube-system                   coredns-84c9b7569d-4fgl9                                   100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     5h1m\n  kube-system                   coredns-84c9b7569d-84dlp                                   100m (2%)     0 (0%)      70Mi (1%)        170Mi (2%)     5h1m\n  kube-system                   csi-qingcloud-node-mfm2g                                   60m (1%)      60m (1%)    70Mi (1%)        70Mi (1%)      5h\n  kube-system                   kube-apiserver-master1                                     250m (6%)     0 (0%)      0 (0%)           0 (0%)         5h2m\n  kube-system                   kube-controller-manager-master1                            200m (5%)     0 (0%)      0 (0%)           0 (0%)         5h2m\n  kube-system                   kube-proxy-mwb8w                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h1m\n  kube-system                   kube-scheduler-master1                                     100m (2%)     0 (0%)      0 (0%)           0 (0%)         5h2m\n  kubesphere-monitoring-system  node-exporter-wzc26                                        112m (3%)     2 (55%)     200Mi (3%)       600Mi (9%)     4h57m\n  kubesphere-system             ks-apiserver-65948874f4-j9v8c                              20m (0%)      1 (27%)     100Mi (1%)       1Gi (15%)      29m\n  kubesphere-system             ks-console-6d6d765964-8n9vk                                20m (0%)      1 (27%)     100Mi (1%)       512Mi (7%)     4h59m\n  kubesphere-system             ks-controller-manager-868df6b544-p8jkt                     30m (0%)      1 (27%)     50Mi (0%)        1000Mi (15%)   29m\n  sonobuoy                      sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-flgs7    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1342m (37%)  5260m (146%)\n  memory             710Mi (10%)  3646Mi (55%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Sep 23 08:59:00.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-2500 describe namespace kubectl-2500'
Sep 23 08:59:00.962: INFO: stderr: ""
Sep 23 08:59:00.962: INFO: stdout: "Name:         kubectl-2500\nLabels:       e2e-framework=kubectl\n              e2e-run=c4ecebf6-fdbc-4981-a198-1ded3cf4d60d\n              kubesphere.io/namespace=kubectl-2500\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:00.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2500" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":97,"skipped":1547,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:00.974: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-715a7acc-a84f-419f-929c-eb1d1ed3efd2
STEP: Creating a pod to test consume configMaps
Sep 23 08:59:01.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f" in namespace "configmap-4876" to be "Succeeded or Failed"
Sep 23 08:59:01.016: INFO: Pod "pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.553718ms
Sep 23 08:59:03.019: INFO: Pod "pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005893339s
Sep 23 08:59:05.022: INFO: Pod "pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009273328s
STEP: Saw pod success
Sep 23 08:59:05.022: INFO: Pod "pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f" satisfied condition "Succeeded or Failed"
Sep 23 08:59:05.024: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f container agnhost-container: <nil>
STEP: delete the pod
Sep 23 08:59:05.036: INFO: Waiting for pod pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f to disappear
Sep 23 08:59:05.039: INFO: Pod pod-configmaps-9b4e8f95-ed17-4453-9134-3b1acc86af6f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:05.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4876" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1556,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:05.044: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:59:05.085: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 23 08:59:05.092: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:05.094: INFO: Number of nodes with available pods: 0
Sep 23 08:59:05.094: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:59:06.097: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:06.099: INFO: Number of nodes with available pods: 0
Sep 23 08:59:06.099: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:59:07.103: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:07.111: INFO: Number of nodes with available pods: 1
Sep 23 08:59:07.111: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:59:08.098: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:08.100: INFO: Number of nodes with available pods: 2
Sep 23 08:59:08.100: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 23 08:59:08.117: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:08.117: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:08.119: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:09.122: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:09.122: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:09.124: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:10.124: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:10.124: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:10.126: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:11.122: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:11.122: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:11.122: INFO: Pod daemon-set-9z5ft is not available
Sep 23 08:59:11.124: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:12.123: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:12.123: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:12.123: INFO: Pod daemon-set-9z5ft is not available
Sep 23 08:59:12.125: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:13.123: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:13.123: INFO: Wrong image for pod: daemon-set-9z5ft. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:13.123: INFO: Pod daemon-set-9z5ft is not available
Sep 23 08:59:13.125: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:14.122: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:14.122: INFO: Pod daemon-set-hjn7f is not available
Sep 23 08:59:14.127: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:15.122: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:15.122: INFO: Pod daemon-set-hjn7f is not available
Sep 23 08:59:15.124: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:16.125: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:16.127: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:17.123: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:17.125: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:18.123: INFO: Wrong image for pod: daemon-set-2z5sz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Sep 23 08:59:18.123: INFO: Pod daemon-set-2z5sz is not available
Sep 23 08:59:18.125: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:19.122: INFO: Pod daemon-set-xzvqt is not available
Sep 23 08:59:19.124: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 23 08:59:19.126: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:19.128: INFO: Number of nodes with available pods: 1
Sep 23 08:59:19.128: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:59:20.132: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:20.134: INFO: Number of nodes with available pods: 1
Sep 23 08:59:20.134: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 08:59:21.133: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 08:59:21.134: INFO: Number of nodes with available pods: 2
Sep 23 08:59:21.134: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9670, will wait for the garbage collector to delete the pods
Sep 23 08:59:21.211: INFO: Deleting DaemonSet.extensions daemon-set took: 3.418291ms
Sep 23 08:59:21.311: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165164ms
Sep 23 08:59:34.113: INFO: Number of nodes with available pods: 0
Sep 23 08:59:34.113: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 08:59:34.115: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80975"},"items":null}

Sep 23 08:59:34.116: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80975"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:34.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9670" for this suite.

• [SLOW TEST:29.082 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":99,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:34.126: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 23 08:59:34.499: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 23 08:59:36.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984374, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984374, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984374, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984374, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 08:59:39.513: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 08:59:39.515: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:40.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5840" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.561 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":100,"skipped":1585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:40.687: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 23 08:59:44.741: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:44.741: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:44.834: INFO: Exec stderr: ""
Sep 23 08:59:44.834: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:44.834: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:44.902: INFO: Exec stderr: ""
Sep 23 08:59:44.902: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:44.902: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:44.981: INFO: Exec stderr: ""
Sep 23 08:59:44.981: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:44.981: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.068: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 23 08:59:45.069: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.069: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.163: INFO: Exec stderr: ""
Sep 23 08:59:45.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.163: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.237: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 23 08:59:45.237: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.237: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.317: INFO: Exec stderr: ""
Sep 23 08:59:45.317: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.317: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.413: INFO: Exec stderr: ""
Sep 23 08:59:45.413: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.413: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.493: INFO: Exec stderr: ""
Sep 23 08:59:45.493: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6783 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 08:59:45.493: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 08:59:45.567: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:45.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6783" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":101,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:45.574: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:45.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6133" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":102,"skipped":1662,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:45.621: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-e2290386-e4e4-4d8d-8f04-bf07d069e361
STEP: Creating a pod to test consume configMaps
Sep 23 08:59:45.657: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a" in namespace "projected-476" to be "Succeeded or Failed"
Sep 23 08:59:45.659: INFO: Pod "pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.961806ms
Sep 23 08:59:47.663: INFO: Pod "pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005881943s
Sep 23 08:59:49.669: INFO: Pod "pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012338273s
STEP: Saw pod success
Sep 23 08:59:49.669: INFO: Pod "pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a" satisfied condition "Succeeded or Failed"
Sep 23 08:59:49.671: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 08:59:49.687: INFO: Waiting for pod pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a to disappear
Sep 23 08:59:49.689: INFO: Pod pod-projected-configmaps-fd7f5e61-886f-4011-92c2-32d5eaa6ce7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:49.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-476" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1681,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:49.693: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Sep 23 08:59:49.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-6081 create -f -'
Sep 23 08:59:49.935: INFO: stderr: ""
Sep 23 08:59:49.935: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 23 08:59:50.937: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:50.937: INFO: Found 0 / 1
Sep 23 08:59:51.938: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:51.938: INFO: Found 0 / 1
Sep 23 08:59:52.938: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:52.938: INFO: Found 0 / 1
Sep 23 08:59:53.938: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:53.938: INFO: Found 1 / 1
Sep 23 08:59:53.938: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 23 08:59:53.942: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:53.942: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 08:59:53.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-6081 patch pod agnhost-primary-jmtdd -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 23 08:59:54.004: INFO: stderr: ""
Sep 23 08:59:54.004: INFO: stdout: "pod/agnhost-primary-jmtdd patched\n"
STEP: checking annotations
Sep 23 08:59:54.011: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 08:59:54.011: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 08:59:54.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6081" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":104,"skipped":1693,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 08:59:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-7349
STEP: creating replication controller nodeport-test in namespace services-7349
I0923 08:59:54.063513      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7349, replica count: 2
I0923 08:59:57.114119      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 08:59:57.114: INFO: Creating new exec pod
Sep 23 09:00:00.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7349 exec execpodtz4m9 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Sep 23 09:00:00.288: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 23 09:00:00.288: INFO: stdout: ""
Sep 23 09:00:00.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7349 exec execpodtz4m9 -- /bin/sh -x -c nc -zv -t -w 2 10.96.147.115 80'
Sep 23 09:00:00.434: INFO: stderr: "+ nc -zv -t -w 2 10.96.147.115 80\nConnection to 10.96.147.115 80 port [tcp/http] succeeded!\n"
Sep 23 09:00:00.434: INFO: stdout: ""
Sep 23 09:00:00.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7349 exec execpodtz4m9 -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.6 32355'
Sep 23 09:00:00.643: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.6 32355\nConnection to 172.16.0.6 32355 port [tcp/32355] succeeded!\n"
Sep 23 09:00:00.643: INFO: stdout: ""
Sep 23 09:00:00.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7349 exec execpodtz4m9 -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.7 32355'
Sep 23 09:00:00.817: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.7 32355\nConnection to 172.16.0.7 32355 port [tcp/32355] succeeded!\n"
Sep 23 09:00:00.817: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:00.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7349" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.806 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":105,"skipped":1702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:00.823: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Sep 23 09:00:00.847: INFO: Waiting up to 5m0s for pod "client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b" in namespace "containers-3630" to be "Succeeded or Failed"
Sep 23 09:00:00.851: INFO: Pod "client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829112ms
Sep 23 09:00:02.854: INFO: Pod "client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006609656s
Sep 23 09:00:04.857: INFO: Pod "client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009683305s
STEP: Saw pod success
Sep 23 09:00:04.857: INFO: Pod "client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b" satisfied condition "Succeeded or Failed"
Sep 23 09:00:04.858: INFO: Trying to get logs from node worker-s002 pod client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:00:04.871: INFO: Waiting for pod client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b to disappear
Sep 23 09:00:04.873: INFO: Pod client-containers-77191a5c-e78f-4852-8832-bcd1b4dfe24b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:04.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3630" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1724,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:04.878: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-a2d41e36-025e-4251-a25c-bc25fb02c7a3
STEP: Creating a pod to test consume secrets
Sep 23 09:00:04.913: INFO: Waiting up to 5m0s for pod "pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c" in namespace "secrets-3704" to be "Succeeded or Failed"
Sep 23 09:00:04.916: INFO: Pod "pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084591ms
Sep 23 09:00:06.918: INFO: Pod "pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005233347s
Sep 23 09:00:08.920: INFO: Pod "pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007311695s
STEP: Saw pod success
Sep 23 09:00:08.920: INFO: Pod "pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c" satisfied condition "Succeeded or Failed"
Sep 23 09:00:08.921: INFO: Trying to get logs from node worker-s002 pod pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:00:08.935: INFO: Waiting for pod pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c to disappear
Sep 23 09:00:08.937: INFO: Pod pod-secrets-0f5f0a68-554f-4991-87b8-6c23421c849c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3704" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":107,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:08.942: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-5947/secret-test-62d13628-f33d-4b71-bae0-9b10f71ebe44
STEP: Creating a pod to test consume secrets
Sep 23 09:00:08.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff" in namespace "secrets-5947" to be "Succeeded or Failed"
Sep 23 09:00:08.981: INFO: Pod "pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.387124ms
Sep 23 09:00:10.983: INFO: Pod "pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008963015s
STEP: Saw pod success
Sep 23 09:00:10.983: INFO: Pod "pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff" satisfied condition "Succeeded or Failed"
Sep 23 09:00:10.985: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff container env-test: <nil>
STEP: delete the pod
Sep 23 09:00:10.999: INFO: Waiting for pod pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff to disappear
Sep 23 09:00:11.000: INFO: Pod pod-configmaps-7af2f374-95f3-4066-a6ba-20f2a4da9eff no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:11.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5947" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1756,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:11.004: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:00:11.033: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:12.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2750" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":109,"skipped":1771,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:12.055: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 23 09:00:18.138: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 09:00:18.140: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 09:00:20.141: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 09:00:20.146: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 23 09:00:22.141: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 23 09:00:22.145: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:00:22.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3283" for this suite.

• [SLOW TEST:10.095 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":110,"skipped":1772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:00:22.150: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0923 09:00:28.202873      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 09:01:30.219: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:01:30.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1565" for this suite.

• [SLOW TEST:68.077 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":111,"skipped":1838,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:01:30.227: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:01:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-546" for this suite.

• [SLOW TEST:13.095 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":112,"skipped":1855,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:01:43.322: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1573.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1573.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1573.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1573.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1573.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 09:01:45.425: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-1573/dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c: the server could not find the requested resource (get pods dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c)
Sep 23 09:01:45.426: INFO: Unable to read jessie_udp@PodARecord from pod dns-1573/dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c: the server could not find the requested resource (get pods dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c)
Sep 23 09:01:45.428: INFO: Unable to read jessie_tcp@PodARecord from pod dns-1573/dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c: the server could not find the requested resource (get pods dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c)
Sep 23 09:01:45.428: INFO: Lookups using dns-1573/dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c failed for: [jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Sep 23 09:01:50.445: INFO: DNS probes using dns-1573/dns-test-dbeffb20-3780-47dd-85f8-d17b2be0630c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:01:50.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1573" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":113,"skipped":1860,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:01:50.498: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0923 09:01:52.062529      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 09:02:54.076: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:02:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9602" for this suite.

• [SLOW TEST:63.606 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":114,"skipped":1874,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:02:54.104: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:02.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5256" for this suite.

• [SLOW TEST:8.049 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":115,"skipped":1885,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:02.152: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:03:02.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3" in namespace "projected-8990" to be "Succeeded or Failed"
Sep 23 09:03:02.207: INFO: Pod "downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280893ms
Sep 23 09:03:04.211: INFO: Pod "downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006400896s
Sep 23 09:03:06.215: INFO: Pod "downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010059887s
STEP: Saw pod success
Sep 23 09:03:06.215: INFO: Pod "downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3" satisfied condition "Succeeded or Failed"
Sep 23 09:03:06.216: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3 container client-container: <nil>
STEP: delete the pod
Sep 23 09:03:06.237: INFO: Waiting for pod downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3 to disappear
Sep 23 09:03:06.249: INFO: Pod downwardapi-volume-d39f295f-fa33-4a95-9f02-3be54e0737b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:06.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8990" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":1888,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:06.255: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep 23 09:03:06.304: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 09:03:06.309: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 09:03:06.311: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Sep 23 09:03:06.335: INFO: calico-node-gfgx9 from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:03:06.335: INFO: csi-qingcloud-controller-6b58955cdd-l56st from kube-system started at 2021-09-23 03:58:05 +0000 UTC (5 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 23 09:03:06.335: INFO: csi-qingcloud-node-xqq5p from kube-system started at 2021-09-23 03:58:05 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:03:06.335: INFO: kube-proxy-8nnxr from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:03:06.335: INFO: metrics-server-57bcd9bccd-hbn8w from kube-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container metrics-server ready: true, restart count 0
Sep 23 09:03:06.335: INFO: snapshot-controller-0 from kube-system started at 2021-09-23 08:47:46 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 23 09:03:06.335: INFO: default-http-backend-76d9fb4bb7-56kpj from kubesphere-controls-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container default-http-backend ready: true, restart count 0
Sep 23 09:03:06.335: INFO: kubectl-admin-776b98f44f-hftkv from kubesphere-controls-system started at 2021-09-23 04:02:15 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kubectl ready: true, restart count 0
Sep 23 09:03:06.335: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-09-23 08:47:47 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:03:06.335: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-09-23 04:01:40 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:03:06.335: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-09-23 08:47:48 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:03:06.335: INFO: kube-state-metrics-67588479db-tvwbm from kubesphere-monitoring-system started at 2021-09-23 04:01:32 +0000 UTC (3 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 23 09:03:06.335: INFO: node-exporter-t9zb4 from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:03:06.335: INFO: notification-manager-deployment-7bd887ffb4-n9rvc from kubesphere-monitoring-system started at 2021-09-23 04:02:07 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:03:06.335: INFO: notification-manager-deployment-7bd887ffb4-nmd9g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:03:06.335: INFO: notification-manager-operator-78595d8666-49k2z from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container notification-manager-operator ready: true, restart count 0
Sep 23 09:03:06.335: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-09-23 04:02:27 +0000 UTC (3 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:03:06.335: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:03:06.335: INFO: prometheus-operator-d7fdfccbf-nhp4g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 23 09:03:06.335: INFO: ks-installer-f58dbc4cc-q6zkt from kubesphere-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container installer ready: true, restart count 1
Sep 23 09:03:06.335: INFO: sonobuoy from sonobuoy started at 2021-09-23 08:25:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 09:03:06.335: INFO: sonobuoy-e2e-job-57e5d9d7846a4ccc from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container e2e ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:03:06.335: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 09:03:06.335: INFO: ss2-0 from statefulset-7832 started at 2021-09-23 08:23:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:03:06.335: INFO: ss2-1 from statefulset-7832 started at 2021-09-23 08:47:47 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:03:06.335: INFO: ss2-2 from statefulset-7832 started at 2021-09-23 08:47:50 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.335: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:03:06.335: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Sep 23 09:03:06.347: INFO: fail-once-local-cx55k from job-5256 started at 2021-09-23 09:02:54 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container c ready: false, restart count 1
Sep 23 09:03:06.347: INFO: fail-once-local-dmgcn from job-5256 started at 2021-09-23 09:02:54 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container c ready: false, restart count 1
Sep 23 09:03:06.347: INFO: fail-once-local-lkq9h from job-5256 started at 2021-09-23 09:02:57 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container c ready: false, restart count 1
Sep 23 09:03:06.347: INFO: fail-once-local-xwtbg from job-5256 started at 2021-09-23 09:02:57 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container c ready: false, restart count 1
Sep 23 09:03:06.347: INFO: calico-node-f5cnk from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:03:06.347: INFO: csi-qingcloud-node-ll5nv from kube-system started at 2021-09-23 08:48:06 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:03:06.347: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:03:06.347: INFO: kube-proxy-fn894 from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:03:06.347: INFO: node-exporter-6n95k from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:03:06.347: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:03:06.347: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-09-23 08:48:23 +0000 UTC (3 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:03:06.347: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:03:06.347: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:03:06.347: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:03:06.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:03:06.347: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a7680affbfb96b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a7680b003f7809], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:07.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8703" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":117,"skipped":1892,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:07.390: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:03:07.718: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 09:03:09.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984587, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984587, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984587, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984587, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:03:12.733: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:12.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9323" for this suite.
STEP: Destroying namespace "webhook-9323-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.410 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":118,"skipped":1899,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1557
STEP: creating service affinity-clusterip-transition in namespace services-1557
STEP: creating replication controller affinity-clusterip-transition in namespace services-1557
I0923 09:03:12.838666      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1557, replica count: 3
I0923 09:03:15.888951      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:03:15.893: INFO: Creating new exec pod
Sep 23 09:03:20.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1557 exec execpod-affinityj62vx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep 23 09:03:21.054: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 23 09:03:21.054: INFO: stdout: ""
Sep 23 09:03:21.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1557 exec execpod-affinityj62vx -- /bin/sh -x -c nc -zv -t -w 2 10.96.138.89 80'
Sep 23 09:03:21.200: INFO: stderr: "+ nc -zv -t -w 2 10.96.138.89 80\nConnection to 10.96.138.89 80 port [tcp/http] succeeded!\n"
Sep 23 09:03:21.200: INFO: stdout: ""
Sep 23 09:03:21.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1557 exec execpod-affinityj62vx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.138.89:80/ ; done'
Sep 23 09:03:21.410: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n"
Sep 23 09:03:21.410: INFO: stdout: "\naffinity-clusterip-transition-bdwdr\naffinity-clusterip-transition-gptls\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-bdwdr\naffinity-clusterip-transition-gptls\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-bdwdr\naffinity-clusterip-transition-gptls\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-bdwdr\naffinity-clusterip-transition-gptls\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-bdwdr\naffinity-clusterip-transition-gptls\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-bdwdr"
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-gptls
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-gptls
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-gptls
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-gptls
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-gptls
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.410: INFO: Received response from host: affinity-clusterip-transition-bdwdr
Sep 23 09:03:21.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1557 exec execpod-affinityj62vx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.138.89:80/ ; done'
Sep 23 09:03:21.645: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.138.89:80/\n"
Sep 23 09:03:21.645: INFO: stdout: "\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v\naffinity-clusterip-transition-vpv5v"
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Received response from host: affinity-clusterip-transition-vpv5v
Sep 23 09:03:21.645: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1557, will wait for the garbage collector to delete the pods
Sep 23 09:03:21.710: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.724764ms
Sep 23 09:03:21.810: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.15326ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:34.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1557" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.334 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":119,"skipped":1902,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:34.135: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 23 09:03:38.687: INFO: Successfully updated pod "adopt-release-qd4nb"
STEP: Checking that the Job readopts the Pod
Sep 23 09:03:38.688: INFO: Waiting up to 15m0s for pod "adopt-release-qd4nb" in namespace "job-2977" to be "adopted"
Sep 23 09:03:38.691: INFO: Pod "adopt-release-qd4nb": Phase="Running", Reason="", readiness=true. Elapsed: 3.466948ms
Sep 23 09:03:40.695: INFO: Pod "adopt-release-qd4nb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00727665s
Sep 23 09:03:40.695: INFO: Pod "adopt-release-qd4nb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 23 09:03:41.202: INFO: Successfully updated pod "adopt-release-qd4nb"
STEP: Checking that the Job releases the Pod
Sep 23 09:03:41.203: INFO: Waiting up to 15m0s for pod "adopt-release-qd4nb" in namespace "job-2977" to be "released"
Sep 23 09:03:41.216: INFO: Pod "adopt-release-qd4nb": Phase="Running", Reason="", readiness=true. Elapsed: 13.038417ms
Sep 23 09:03:43.219: INFO: Pod "adopt-release-qd4nb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016620682s
Sep 23 09:03:43.219: INFO: Pod "adopt-release-qd4nb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:43.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2977" for this suite.

• [SLOW TEST:9.090 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":120,"skipped":1909,"failed":0}
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:43.225: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 23 09:03:43.261: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:03:43.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-360" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":121,"skipped":1909,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:03:43.272: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-16b6b82e-cf3d-4317-9e6f-992cee5ca36c in namespace container-probe-9989
Sep 23 09:03:47.302: INFO: Started pod busybox-16b6b82e-cf3d-4317-9e6f-992cee5ca36c in namespace container-probe-9989
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:03:47.303: INFO: Initial restart count of pod busybox-16b6b82e-cf3d-4317-9e6f-992cee5ca36c is 0
Sep 23 09:04:35.419: INFO: Restart count of pod container-probe-9989/busybox-16b6b82e-cf3d-4317-9e6f-992cee5ca36c is now 1 (48.116343299s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:04:35.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9989" for this suite.

• [SLOW TEST:52.164 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":1920,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:04:35.436: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep 23 09:04:35.464: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 09:04:35.470: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 09:04:35.472: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Sep 23 09:04:35.480: INFO: calico-node-gfgx9 from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:04:35.480: INFO: csi-qingcloud-controller-6b58955cdd-l56st from kube-system started at 2021-09-23 03:58:05 +0000 UTC (5 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 23 09:04:35.480: INFO: csi-qingcloud-node-xqq5p from kube-system started at 2021-09-23 03:58:05 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:04:35.480: INFO: kube-proxy-8nnxr from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:04:35.480: INFO: metrics-server-57bcd9bccd-hbn8w from kube-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container metrics-server ready: true, restart count 0
Sep 23 09:04:35.480: INFO: snapshot-controller-0 from kube-system started at 2021-09-23 08:47:46 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 23 09:04:35.480: INFO: default-http-backend-76d9fb4bb7-56kpj from kubesphere-controls-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container default-http-backend ready: true, restart count 0
Sep 23 09:04:35.480: INFO: kubectl-admin-776b98f44f-hftkv from kubesphere-controls-system started at 2021-09-23 04:02:15 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kubectl ready: true, restart count 0
Sep 23 09:04:35.480: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-09-23 08:47:47 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:04:35.480: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-09-23 04:01:40 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:04:35.480: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-09-23 08:47:48 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:04:35.480: INFO: kube-state-metrics-67588479db-tvwbm from kubesphere-monitoring-system started at 2021-09-23 04:01:32 +0000 UTC (3 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 23 09:04:35.480: INFO: node-exporter-t9zb4 from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:04:35.480: INFO: notification-manager-deployment-7bd887ffb4-n9rvc from kubesphere-monitoring-system started at 2021-09-23 04:02:07 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:04:35.480: INFO: notification-manager-deployment-7bd887ffb4-nmd9g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:04:35.480: INFO: notification-manager-operator-78595d8666-49k2z from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container notification-manager-operator ready: true, restart count 0
Sep 23 09:04:35.480: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-09-23 04:02:27 +0000 UTC (3 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:04:35.480: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:04:35.480: INFO: prometheus-operator-d7fdfccbf-nhp4g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 23 09:04:35.480: INFO: ks-installer-f58dbc4cc-q6zkt from kubesphere-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container installer ready: true, restart count 1
Sep 23 09:04:35.480: INFO: sonobuoy from sonobuoy started at 2021-09-23 08:25:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 09:04:35.480: INFO: sonobuoy-e2e-job-57e5d9d7846a4ccc from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container e2e ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:04:35.480: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 09:04:35.480: INFO: ss2-0 from statefulset-7832 started at 2021-09-23 08:23:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:04:35.480: INFO: ss2-1 from statefulset-7832 started at 2021-09-23 08:47:47 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:04:35.480: INFO: ss2-2 from statefulset-7832 started at 2021-09-23 08:47:50 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.480: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:04:35.480: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Sep 23 09:04:35.485: INFO: calico-node-f5cnk from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:04:35.485: INFO: csi-qingcloud-node-ll5nv from kube-system started at 2021-09-23 08:48:06 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:04:35.485: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:04:35.485: INFO: kube-proxy-fn894 from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:04:35.485: INFO: node-exporter-6n95k from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:04:35.485: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:04:35.485: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-09-23 08:48:23 +0000 UTC (3 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:04:35.485: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:04:35.485: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:04:35.485: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:04:35.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:04:35.485: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2cd4d81a-168e-4101-889f-413f074a2f92 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 172.16.0.7 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 172.16.0.7 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Sep 23 09:04:49.581: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.7 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:49.581: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321
Sep 23 09:04:49.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.7:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:49.667: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321 UDP
Sep 23 09:04:49.759: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.16.0.7 54321] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:49.759: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Sep 23 09:04:54.833: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.7 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:54.833: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321
Sep 23 09:04:54.915: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.7:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:54.915: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321 UDP
Sep 23 09:04:55.007: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.16.0.7 54321] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:04:55.007: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Sep 23 09:05:00.075: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.7 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:00.075: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321
Sep 23 09:05:00.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.7:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:00.167: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321 UDP
Sep 23 09:05:00.262: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.16.0.7 54321] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:00.262: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Sep 23 09:05:05.358: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.7 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:05.359: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321
Sep 23 09:05:05.468: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.7:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:05.468: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321 UDP
Sep 23 09:05:05.558: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.16.0.7 54321] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:05.558: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Sep 23 09:05:10.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.0.7 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:10.638: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321
Sep 23 09:05:10.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.0.7:54321/hostname] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:10.731: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.0.7, port: 54321 UDP
Sep 23 09:05:10.821: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.16.0.7 54321] Namespace:sched-pred-6363 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:05:10.821: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: removing the label kubernetes.io/e2e-2cd4d81a-168e-4101-889f-413f074a2f92 off the node worker-s002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2cd4d81a-168e-4101-889f-413f074a2f92
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:05:15.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6363" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:40.515 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":123,"skipped":1964,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:05:15.951: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 23 09:05:17.983: INFO: &Pod{ObjectMeta:{send-events-954a7392-af6c-4878-8704-0b5fe58bd723  events-2832  24e017d4-34d8-4bc6-a1a5-903319533b6e 84106 0 2021-09-23 09:05:15 +0000 UTC <nil> <nil> map[name:foo time:973217760] map[cni.projectcalico.org/podIP:10.10.131.190/32 cni.projectcalico.org/podIPs:10.10.131.190/32] [] []  [{e2e.test Update v1 2021-09-23 09:05:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:05:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:05:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9t2kl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9t2kl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9t2kl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:05:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:05:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:05:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:05:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.190,StartTime:2021-09-23 09:05:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:05:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://d98846775e74220085a25721ba3388c2ffd6960f25fdfb5faf3300441a1f186e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 23 09:05:19.987: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 23 09:05:21.989: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:05:21.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2832" for this suite.

• [SLOW TEST:6.059 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":124,"skipped":1966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:05:22.010: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-6j89
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:05:22.047: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6j89" in namespace "subpath-5355" to be "Succeeded or Failed"
Sep 23 09:05:22.059: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Pending", Reason="", readiness=false. Elapsed: 11.880418ms
Sep 23 09:05:24.061: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014233162s
Sep 23 09:05:26.069: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 4.02197332s
Sep 23 09:05:28.071: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 6.024449718s
Sep 23 09:05:30.074: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 8.026924766s
Sep 23 09:05:32.076: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 10.029049764s
Sep 23 09:05:34.082: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 12.035595048s
Sep 23 09:05:36.085: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 14.037754805s
Sep 23 09:05:38.087: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 16.039979501s
Sep 23 09:05:40.088: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 18.041710261s
Sep 23 09:05:42.092: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 20.045656584s
Sep 23 09:05:44.096: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Running", Reason="", readiness=true. Elapsed: 22.049381784s
Sep 23 09:05:46.100: INFO: Pod "pod-subpath-test-secret-6j89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.052822137s
STEP: Saw pod success
Sep 23 09:05:46.100: INFO: Pod "pod-subpath-test-secret-6j89" satisfied condition "Succeeded or Failed"
Sep 23 09:05:46.101: INFO: Trying to get logs from node worker-s002 pod pod-subpath-test-secret-6j89 container test-container-subpath-secret-6j89: <nil>
STEP: delete the pod
Sep 23 09:05:46.122: INFO: Waiting for pod pod-subpath-test-secret-6j89 to disappear
Sep 23 09:05:46.125: INFO: Pod pod-subpath-test-secret-6j89 no longer exists
STEP: Deleting pod pod-subpath-test-secret-6j89
Sep 23 09:05:46.125: INFO: Deleting pod "pod-subpath-test-secret-6j89" in namespace "subpath-5355"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:05:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5355" for this suite.

• [SLOW TEST:24.120 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":125,"skipped":2000,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:05:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep 23 09:05:46.162: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:05:50.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5422" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":126,"skipped":2019,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:05:50.662: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6218.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6218.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6218.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6218.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6218.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6218.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 09:05:54.747: INFO: DNS probes using dns-6218/dns-test-17a73113-0218-436c-9b88-18c3574b2b2a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:05:54.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6218" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":127,"skipped":2032,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:05:54.761: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:05:55.441: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep 23 09:05:57.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984755, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984755, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984755, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984755, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:06:00.455: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:06:00.459: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:06:01.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7422" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.807 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":128,"skipped":2045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:06:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:06:01.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd" in namespace "projected-9775" to be "Succeeded or Failed"
Sep 23 09:06:01.619: INFO: Pod "downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.767522ms
Sep 23 09:06:03.624: INFO: Pod "downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019626237s
STEP: Saw pod success
Sep 23 09:06:03.624: INFO: Pod "downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd" satisfied condition "Succeeded or Failed"
Sep 23 09:06:03.626: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd container client-container: <nil>
STEP: delete the pod
Sep 23 09:06:03.637: INFO: Waiting for pod downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd to disappear
Sep 23 09:06:03.639: INFO: Pod downwardapi-volume-648be9af-71a7-4482-9555-429d300da3fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:06:03.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9775" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2086,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:06:03.644: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:06:04.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:06:07.179: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:06:07.182: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:06:08.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5657" for this suite.
STEP: Destroying namespace "webhook-5657-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":130,"skipped":2089,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:06:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 23 09:06:08.627: INFO: Pod name wrapped-volume-race-fa0d33d0-530c-457e-99b9-a79a5cc24b89: Found 3 pods out of 5
Sep 23 09:06:13.633: INFO: Pod name wrapped-volume-race-fa0d33d0-530c-457e-99b9-a79a5cc24b89: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fa0d33d0-530c-457e-99b9-a79a5cc24b89 in namespace emptydir-wrapper-5609, will wait for the garbage collector to delete the pods
Sep 23 09:06:23.703: INFO: Deleting ReplicationController wrapped-volume-race-fa0d33d0-530c-457e-99b9-a79a5cc24b89 took: 4.224279ms
Sep 23 09:06:23.803: INFO: Terminating ReplicationController wrapped-volume-race-fa0d33d0-530c-457e-99b9-a79a5cc24b89 pods took: 100.167258ms
STEP: Creating RC which spawns configmap-volume pods
Sep 23 09:06:32.213: INFO: Pod name wrapped-volume-race-ccc24e4e-24e5-4a62-a5cf-3d4280cb7ba5: Found 0 pods out of 5
Sep 23 09:06:37.219: INFO: Pod name wrapped-volume-race-ccc24e4e-24e5-4a62-a5cf-3d4280cb7ba5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ccc24e4e-24e5-4a62-a5cf-3d4280cb7ba5 in namespace emptydir-wrapper-5609, will wait for the garbage collector to delete the pods
Sep 23 09:06:47.290: INFO: Deleting ReplicationController wrapped-volume-race-ccc24e4e-24e5-4a62-a5cf-3d4280cb7ba5 took: 7.250419ms
Sep 23 09:06:48.790: INFO: Terminating ReplicationController wrapped-volume-race-ccc24e4e-24e5-4a62-a5cf-3d4280cb7ba5 pods took: 1.500222719s
STEP: Creating RC which spawns configmap-volume pods
Sep 23 09:07:04.212: INFO: Pod name wrapped-volume-race-bff3f68b-dd9a-4a19-b025-dc7e64cb1f1f: Found 0 pods out of 5
Sep 23 09:07:09.218: INFO: Pod name wrapped-volume-race-bff3f68b-dd9a-4a19-b025-dc7e64cb1f1f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bff3f68b-dd9a-4a19-b025-dc7e64cb1f1f in namespace emptydir-wrapper-5609, will wait for the garbage collector to delete the pods
Sep 23 09:07:19.286: INFO: Deleting ReplicationController wrapped-volume-race-bff3f68b-dd9a-4a19-b025-dc7e64cb1f1f took: 3.617229ms
Sep 23 09:07:20.786: INFO: Terminating ReplicationController wrapped-volume-race-bff3f68b-dd9a-4a19-b025-dc7e64cb1f1f pods took: 1.500149176s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:07:32.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5609" for this suite.

• [SLOW TEST:84.001 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":131,"skipped":2090,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:07:32.310: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Sep 23 09:07:32.353: INFO: observed Pod pod-test in namespace pods-9896 in phase Pending conditions []
Sep 23 09:07:32.354: INFO: observed Pod pod-test in namespace pods-9896 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC  }]
Sep 23 09:07:32.362: INFO: observed Pod pod-test in namespace pods-9896 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC  }]
Sep 23 09:07:33.463: INFO: observed Pod pod-test in namespace pods-9896 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:07:32 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Sep 23 09:07:34.584: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Sep 23 09:07:34.601: INFO: observed event type ADDED
Sep 23 09:07:34.602: INFO: observed event type MODIFIED
Sep 23 09:07:34.602: INFO: observed event type MODIFIED
Sep 23 09:07:34.602: INFO: observed event type MODIFIED
Sep 23 09:07:34.602: INFO: observed event type MODIFIED
Sep 23 09:07:34.602: INFO: observed event type MODIFIED
Sep 23 09:07:34.610: INFO: observed event type MODIFIED
Sep 23 09:07:34.610: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:07:34.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9896" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":132,"skipped":2109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:07:34.616: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:07:34.648: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 23 09:07:39.652: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 09:07:39.652: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 09:07:39.662: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7394  022c15d1-cfec-4ca7-b8c3-3f28e5f70c5a 86072 1 2021-09-23 09:07:39 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-23 09:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028f6f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 23 09:07:39.668: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-7394  4311de8f-6800-4a61-a791-dd50f85fb9f1 86075 1 2021-09-23 09:07:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 022c15d1-cfec-4ca7-b8c3-3f28e5f70c5a 0xc0028f7357 0xc0028f7358}] []  [{kube-controller-manager Update apps/v1 2021-09-23 09:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"022c15d1-cfec-4ca7-b8c3-3f28e5f70c5a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0028f73e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:07:39.668: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 23 09:07:39.668: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7394  83ab43a8-dbf1-4365-b3d9-c1483ea59f5a 86074 1 2021-09-23 09:07:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 022c15d1-cfec-4ca7-b8c3-3f28e5f70c5a 0xc0028f7247 0xc0028f7248}] []  [{e2e.test Update apps/v1 2021-09-23 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 09:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"022c15d1-cfec-4ca7-b8c3-3f28e5f70c5a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028f72e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:07:39.674: INFO: Pod "test-cleanup-controller-2grkv" is available:
&Pod{ObjectMeta:{test-cleanup-controller-2grkv test-cleanup-controller- deployment-7394  b3c6d062-484b-455c-8a27-a9cf26711f4b 85875 0 2021-09-23 09:07:34 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.10.131.145/32 cni.projectcalico.org/podIPs:10.10.131.145/32] [{apps/v1 ReplicaSet test-cleanup-controller 83ab43a8-dbf1-4365-b3d9-c1483ea59f5a 0xc0028f7927 0xc0028f7928}] []  [{kube-controller-manager Update v1 2021-09-23 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ab43a8-dbf1-4365-b3d9-c1483ea59f5a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:07:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:07:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j2lx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j2lx2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j2lx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:07:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:07:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:07:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.145,StartTime:2021-09-23 09:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:07:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://41529cc4fcac43db1eb70ccf6559ce409e5d942328cee45b68941cd0b2809f89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:07:39.675: INFO: Pod "test-cleanup-deployment-685c4f8568-h4qmp" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-h4qmp test-cleanup-deployment-685c4f8568- deployment-7394  e6ab2677-e868-4344-8972-b2d768da8c7e 86078 0 2021-09-23 09:07:39 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 4311de8f-6800-4a61-a791-dd50f85fb9f1 0xc0028f7ae7 0xc0028f7ae8}] []  [{kube-controller-manager Update v1 2021-09-23 09:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4311de8f-6800-4a61-a791-dd50f85fb9f1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j2lx2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j2lx2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j2lx2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:07:39.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7394" for this suite.

• [SLOW TEST:5.070 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":133,"skipped":2185,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:07:39.687: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 23 09:07:39.719: INFO: Waiting up to 5m0s for pod "pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba" in namespace "emptydir-7115" to be "Succeeded or Failed"
Sep 23 09:07:39.721: INFO: Pod "pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.694032ms
Sep 23 09:07:41.723: INFO: Pod "pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004153935s
Sep 23 09:07:43.729: INFO: Pod "pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009330688s
STEP: Saw pod success
Sep 23 09:07:43.729: INFO: Pod "pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba" satisfied condition "Succeeded or Failed"
Sep 23 09:07:43.732: INFO: Trying to get logs from node worker-s002 pod pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba container test-container: <nil>
STEP: delete the pod
Sep 23 09:07:43.761: INFO: Waiting for pod pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba to disappear
Sep 23 09:07:43.763: INFO: Pod pod-c6563ebf-c1c8-4841-8cdb-5c527f4a82ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:07:43.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7115" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2185,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:07:43.769: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Sep 23 09:07:43.802: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1177 proxy --unix-socket=/tmp/kubectl-proxy-unix705107223/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:07:43.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1177" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":135,"skipped":2205,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:07:43.854: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 23 09:07:43.881: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86175 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:07:43.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86175 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 23 09:07:53.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86250 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:07:53.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86250 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 23 09:08:03.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86288 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:08:03.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86288 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 23 09:08:13.904: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86324 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:08:13.904: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5148  f68942eb-f45b-4c92-8115-1651635a4d32 86324 0 2021-09-23 09:07:43 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-23 09:07:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 23 09:08:23.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5148  68e6104f-b83b-4c2e-b2cb-dfb0d4c3c593 86362 0 2021-09-23 09:08:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-23 09:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:08:23.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5148  68e6104f-b83b-4c2e-b2cb-dfb0d4c3c593 86362 0 2021-09-23 09:08:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-23 09:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 23 09:08:33.919: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5148  68e6104f-b83b-4c2e-b2cb-dfb0d4c3c593 86400 0 2021-09-23 09:08:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-23 09:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:08:33.919: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5148  68e6104f-b83b-4c2e-b2cb-dfb0d4c3c593 86400 0 2021-09-23 09:08:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-23 09:08:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:08:43.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5148" for this suite.

• [SLOW TEST:60.073 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":136,"skipped":2207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:08:43.928: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep 23 09:08:43.963: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:08:46.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9804" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":137,"skipped":2246,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:08:46.971: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4954.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4954.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4954.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4954.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.76.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.76.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.76.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.76.239_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4954.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4954.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4954.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4954.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4954.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4954.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.76.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.76.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.76.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.76.239_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 23 09:08:51.050: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.055: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.066: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.067: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.069: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.070: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:51.078: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:08:56.083: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.085: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.087: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.089: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.100: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.102: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.105: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:08:56.114: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:09:01.083: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.085: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.086: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.088: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.103: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.105: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.106: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:01.119: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:09:06.081: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.102: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.104: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.115: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.117: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.119: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.121: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:06.133: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:09:11.081: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.082: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.084: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.086: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.097: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.098: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.099: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:11.111: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:09:16.086: INFO: Unable to read wheezy_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.089: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.091: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.093: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.108: INFO: Unable to read jessie_udp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.109: INFO: Unable to read jessie_tcp@dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.112: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local from pod dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6: the server could not find the requested resource (get pods dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6)
Sep 23 09:09:16.120: INFO: Lookups using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 failed for: [wheezy_udp@dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@dns-test-service.dns-4954.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_udp@dns-test-service.dns-4954.svc.cluster.local jessie_tcp@dns-test-service.dns-4954.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4954.svc.cluster.local]

Sep 23 09:09:21.128: INFO: DNS probes using dns-4954/dns-test-2ea227bf-b091-4349-bdfc-62560081c0a6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:21.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4954" for this suite.

• [SLOW TEST:34.216 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":138,"skipped":2257,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:21.187: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:25.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8385" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":139,"skipped":2272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:25.992: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:09:26.414: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 23 09:09:28.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984966, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984966, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984966, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767984966, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:09:31.436: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:09:31.527: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6988-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:32.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2835" for this suite.
STEP: Destroying namespace "webhook-2835-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":140,"skipped":2294,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:32.715: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:32.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4136" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":141,"skipped":2306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:32.797: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[]
Sep 23 09:09:32.857: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep 23 09:09:33.867: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod1:[100]]
Sep 23 09:09:35.892: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 23 09:09:37.914: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[pod2:[101]]
Sep 23 09:09:37.930: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9118
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9118 to expose endpoints map[]
Sep 23 09:09:37.941: INFO: successfully validated that service multi-endpoint-test in namespace services-9118 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:37.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9118" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.168 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":142,"skipped":2362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:37.965: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-b7d52d94-7677-4c2e-8a9a-a16891c57191
STEP: Creating secret with name s-test-opt-upd-5af428ff-7eb4-452e-bb69-f4e001122360
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b7d52d94-7677-4c2e-8a9a-a16891c57191
STEP: Updating secret s-test-opt-upd-5af428ff-7eb4-452e-bb69-f4e001122360
STEP: Creating secret with name s-test-opt-create-257c6dbb-ce56-4340-9ca6-a145dbcbcd7f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:09:42.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6103" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2385,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:09:42.071: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Sep 23 09:09:42.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 create -f -'
Sep 23 09:09:42.472: INFO: stderr: ""
Sep 23 09:09:42.472: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:09:42.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:09:42.533: INFO: stderr: ""
Sep 23 09:09:42.533: INFO: stdout: "update-demo-nautilus-cj2km update-demo-nautilus-qwzc9 "
Sep 23 09:09:42.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-cj2km -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:09:42.589: INFO: stderr: ""
Sep 23 09:09:42.589: INFO: stdout: ""
Sep 23 09:09:42.589: INFO: update-demo-nautilus-cj2km is created but not running
Sep 23 09:09:47.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:09:47.659: INFO: stderr: ""
Sep 23 09:09:47.659: INFO: stdout: "update-demo-nautilus-cj2km update-demo-nautilus-qwzc9 "
Sep 23 09:09:47.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-cj2km -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:09:47.727: INFO: stderr: ""
Sep 23 09:09:47.727: INFO: stdout: ""
Sep 23 09:09:47.727: INFO: update-demo-nautilus-cj2km is created but not running
Sep 23 09:09:52.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:09:52.792: INFO: stderr: ""
Sep 23 09:09:52.792: INFO: stdout: "update-demo-nautilus-cj2km update-demo-nautilus-qwzc9 "
Sep 23 09:09:52.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-cj2km -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:09:52.848: INFO: stderr: ""
Sep 23 09:09:52.848: INFO: stdout: "true"
Sep 23 09:09:52.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-cj2km -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:09:52.903: INFO: stderr: ""
Sep 23 09:09:52.903: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:09:52.903: INFO: validating pod update-demo-nautilus-cj2km
Sep 23 09:09:52.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:09:52.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:09:52.905: INFO: update-demo-nautilus-cj2km is verified up and running
Sep 23 09:09:52.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:09:52.961: INFO: stderr: ""
Sep 23 09:09:52.961: INFO: stdout: "true"
Sep 23 09:09:52.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:09:53.014: INFO: stderr: ""
Sep 23 09:09:53.014: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:09:53.014: INFO: validating pod update-demo-nautilus-qwzc9
Sep 23 09:09:53.020: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:09:53.020: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:09:53.020: INFO: update-demo-nautilus-qwzc9 is verified up and running
STEP: scaling down the replication controller
Sep 23 09:09:53.022: INFO: scanned /root for discovery docs: <nil>
Sep 23 09:09:53.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep 23 09:09:54.093: INFO: stderr: ""
Sep 23 09:09:54.093: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:09:54.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:09:54.157: INFO: stderr: ""
Sep 23 09:09:54.157: INFO: stdout: "update-demo-nautilus-cj2km update-demo-nautilus-qwzc9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 23 09:09:59.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:09:59.221: INFO: stderr: ""
Sep 23 09:09:59.221: INFO: stdout: "update-demo-nautilus-qwzc9 "
Sep 23 09:09:59.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:09:59.277: INFO: stderr: ""
Sep 23 09:09:59.277: INFO: stdout: "true"
Sep 23 09:09:59.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:09:59.332: INFO: stderr: ""
Sep 23 09:09:59.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:09:59.332: INFO: validating pod update-demo-nautilus-qwzc9
Sep 23 09:09:59.334: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:09:59.334: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:09:59.334: INFO: update-demo-nautilus-qwzc9 is verified up and running
STEP: scaling up the replication controller
Sep 23 09:09:59.336: INFO: scanned /root for discovery docs: <nil>
Sep 23 09:09:59.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep 23 09:10:00.406: INFO: stderr: ""
Sep 23 09:10:00.406: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:10:00.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:10:00.469: INFO: stderr: ""
Sep 23 09:10:00.469: INFO: stdout: "update-demo-nautilus-4wwvh update-demo-nautilus-qwzc9 "
Sep 23 09:10:00.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-4wwvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:10:00.537: INFO: stderr: ""
Sep 23 09:10:00.537: INFO: stdout: ""
Sep 23 09:10:00.537: INFO: update-demo-nautilus-4wwvh is created but not running
Sep 23 09:10:05.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:10:05.595: INFO: stderr: ""
Sep 23 09:10:05.595: INFO: stdout: "update-demo-nautilus-4wwvh update-demo-nautilus-qwzc9 "
Sep 23 09:10:05.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-4wwvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:10:05.651: INFO: stderr: ""
Sep 23 09:10:05.651: INFO: stdout: "true"
Sep 23 09:10:05.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-4wwvh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:10:05.707: INFO: stderr: ""
Sep 23 09:10:05.707: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:10:05.707: INFO: validating pod update-demo-nautilus-4wwvh
Sep 23 09:10:05.709: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:10:05.709: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:10:05.709: INFO: update-demo-nautilus-4wwvh is verified up and running
Sep 23 09:10:05.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:10:05.768: INFO: stderr: ""
Sep 23 09:10:05.768: INFO: stdout: "true"
Sep 23 09:10:05.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods update-demo-nautilus-qwzc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:10:05.826: INFO: stderr: ""
Sep 23 09:10:05.826: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:10:05.826: INFO: validating pod update-demo-nautilus-qwzc9
Sep 23 09:10:05.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:10:05.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:10:05.828: INFO: update-demo-nautilus-qwzc9 is verified up and running
STEP: using delete to clean up resources
Sep 23 09:10:05.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 delete --grace-period=0 --force -f -'
Sep 23 09:10:05.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:10:05.887: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 23 09:10:05.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get rc,svc -l name=update-demo --no-headers'
Sep 23 09:10:05.953: INFO: stderr: "No resources found in kubectl-7913 namespace.\n"
Sep 23 09:10:05.953: INFO: stdout: ""
Sep 23 09:10:05.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:10:06.010: INFO: stderr: ""
Sep 23 09:10:06.010: INFO: stdout: "update-demo-nautilus-4wwvh\nupdate-demo-nautilus-qwzc9\n"
Sep 23 09:10:06.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get rc,svc -l name=update-demo --no-headers'
Sep 23 09:10:06.570: INFO: stderr: "No resources found in kubectl-7913 namespace.\n"
Sep 23 09:10:06.570: INFO: stdout: ""
Sep 23 09:10:06.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:10:06.627: INFO: stderr: ""
Sep 23 09:10:06.627: INFO: stdout: "update-demo-nautilus-4wwvh\nupdate-demo-nautilus-qwzc9\n"
Sep 23 09:10:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get rc,svc -l name=update-demo --no-headers'
Sep 23 09:10:07.072: INFO: stderr: "No resources found in kubectl-7913 namespace.\n"
Sep 23 09:10:07.072: INFO: stdout: ""
Sep 23 09:10:07.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:10:07.134: INFO: stderr: ""
Sep 23 09:10:07.134: INFO: stdout: "update-demo-nautilus-4wwvh\nupdate-demo-nautilus-qwzc9\n"
Sep 23 09:10:07.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get rc,svc -l name=update-demo --no-headers'
Sep 23 09:10:07.580: INFO: stderr: "No resources found in kubectl-7913 namespace.\n"
Sep 23 09:10:07.580: INFO: stdout: ""
Sep 23 09:10:07.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-7913 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:10:07.641: INFO: stderr: ""
Sep 23 09:10:07.641: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:07.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7913" for this suite.

• [SLOW TEST:25.575 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":144,"skipped":2395,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:07.646: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 23 09:10:07.678: INFO: Waiting up to 5m0s for pod "pod-d482e5d6-c052-4898-8dad-2b1615d86367" in namespace "emptydir-7984" to be "Succeeded or Failed"
Sep 23 09:10:07.682: INFO: Pod "pod-d482e5d6-c052-4898-8dad-2b1615d86367": Phase="Pending", Reason="", readiness=false. Elapsed: 4.144278ms
Sep 23 09:10:09.684: INFO: Pod "pod-d482e5d6-c052-4898-8dad-2b1615d86367": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006713295s
Sep 23 09:10:11.687: INFO: Pod "pod-d482e5d6-c052-4898-8dad-2b1615d86367": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009192315s
STEP: Saw pod success
Sep 23 09:10:11.687: INFO: Pod "pod-d482e5d6-c052-4898-8dad-2b1615d86367" satisfied condition "Succeeded or Failed"
Sep 23 09:10:11.688: INFO: Trying to get logs from node worker-s002 pod pod-d482e5d6-c052-4898-8dad-2b1615d86367 container test-container: <nil>
STEP: delete the pod
Sep 23 09:10:11.704: INFO: Waiting for pod pod-d482e5d6-c052-4898-8dad-2b1615d86367 to disappear
Sep 23 09:10:11.706: INFO: Pod pod-d482e5d6-c052-4898-8dad-2b1615d86367 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:11.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7984" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2402,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:11.709: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:15.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4576" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2408,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:15.767: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-474b46a6-fe62-463b-8675-92d9e9527cb4
STEP: Creating a pod to test consume secrets
Sep 23 09:10:15.802: INFO: Waiting up to 5m0s for pod "pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55" in namespace "secrets-5815" to be "Succeeded or Failed"
Sep 23 09:10:15.807: INFO: Pod "pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458382ms
Sep 23 09:10:17.810: INFO: Pod "pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007157183s
Sep 23 09:10:19.812: INFO: Pod "pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009985845s
STEP: Saw pod success
Sep 23 09:10:19.812: INFO: Pod "pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55" satisfied condition "Succeeded or Failed"
Sep 23 09:10:19.814: INFO: Trying to get logs from node worker-s002 pod pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55 container secret-env-test: <nil>
STEP: delete the pod
Sep 23 09:10:19.829: INFO: Waiting for pod pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55 to disappear
Sep 23 09:10:19.832: INFO: Pod pod-secrets-d688f364-65d8-42a9-b112-079f6fa7bb55 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:19.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5815" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2417,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:19.836: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 23 09:10:19.869: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3343  4fd2d14d-a267-49c4-80cf-3932cd1173f7 87607 0 2021-09-23 09:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-23 09:10:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:10:19.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3343  4fd2d14d-a267-49c4-80cf-3932cd1173f7 87608 0 2021-09-23 09:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-23 09:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 23 09:10:19.875: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3343  4fd2d14d-a267-49c4-80cf-3932cd1173f7 87609 0 2021-09-23 09:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-23 09:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 09:10:19.875: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3343  4fd2d14d-a267-49c4-80cf-3932cd1173f7 87610 0 2021-09-23 09:10:19 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-23 09:10:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:19.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3343" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":148,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:19.894: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434
Sep 23 09:10:19.918: INFO: Pod name my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434: Found 0 pods out of 1
Sep 23 09:10:24.923: INFO: Pod name my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434: Found 1 pods out of 1
Sep 23 09:10:24.923: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434" are running
Sep 23 09:10:24.924: INFO: Pod "my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434-qtlxs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:10:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:10:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:10:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:10:19 +0000 UTC Reason: Message:}])
Sep 23 09:10:24.925: INFO: Trying to dial the pod
Sep 23 09:10:29.931: INFO: Controller my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434: Got expected result from replica 1 [my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434-qtlxs]: "my-hostname-basic-fb4d2d17-fd9f-4f9b-8bee-dbe8b3c07434-qtlxs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:10:29.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2336" for this suite.

• [SLOW TEST:10.041 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":149,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:10:29.935: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-d1bcdecf-9740-4ae5-b2ae-9b167f20398a in namespace container-probe-32
Sep 23 09:10:33.983: INFO: Started pod test-webserver-d1bcdecf-9740-4ae5-b2ae-9b167f20398a in namespace container-probe-32
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:10:33.994: INFO: Initial restart count of pod test-webserver-d1bcdecf-9740-4ae5-b2ae-9b167f20398a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:14:34.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-32" for this suite.

• [SLOW TEST:244.586 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:14:34.523: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:14:34.895: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep 23 09:14:36.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985274, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985274, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985274, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985274, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:14:39.912: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:14:39.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9004" for this suite.
STEP: Destroying namespace "webhook-9004-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":151,"skipped":2524,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:14:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-d1b98326-70df-4e8f-ae5c-950406cec3c8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d1b98326-70df-4e8f-ae5c-950406cec3c8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:14:44.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7574" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:14:44.086: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d556b908-e525-4694-9793-cd77db372b4a
STEP: Creating a pod to test consume secrets
Sep 23 09:14:44.168: INFO: Waiting up to 5m0s for pod "pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b" in namespace "secrets-771" to be "Succeeded or Failed"
Sep 23 09:14:44.171: INFO: Pod "pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.540561ms
Sep 23 09:14:46.173: INFO: Pod "pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004980513s
STEP: Saw pod success
Sep 23 09:14:46.173: INFO: Pod "pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b" satisfied condition "Succeeded or Failed"
Sep 23 09:14:46.174: INFO: Trying to get logs from node worker-s002 pod pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:14:46.197: INFO: Waiting for pod pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b to disappear
Sep 23 09:14:46.199: INFO: Pod pod-secrets-0e3e9a0d-46bc-43fe-bced-f3851d33155b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:14:46.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-771" for this suite.
STEP: Destroying namespace "secret-namespace-6063" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":153,"skipped":2583,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:14:46.207: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:14:46.245: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482" in namespace "projected-3987" to be "Succeeded or Failed"
Sep 23 09:14:46.247: INFO: Pod "downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378048ms
Sep 23 09:14:48.251: INFO: Pod "downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005298378s
STEP: Saw pod success
Sep 23 09:14:48.251: INFO: Pod "downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482" satisfied condition "Succeeded or Failed"
Sep 23 09:14:48.252: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482 container client-container: <nil>
STEP: delete the pod
Sep 23 09:14:48.266: INFO: Waiting for pod downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482 to disappear
Sep 23 09:14:48.268: INFO: Pod downwardapi-volume-e3f7e3d7-1d64-4085-8692-1773b879e482 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:14:48.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3987" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2585,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:14:48.273: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:14:48.304: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Pending, waiting for it to be Running (with Ready = true)
Sep 23 09:14:50.308: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:14:52.307: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:14:54.307: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:14:56.307: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:14:58.309: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:00.308: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:02.308: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:04.308: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:06.307: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:08.307: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:10.310: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:12.309: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = false)
Sep 23 09:15:14.310: INFO: The status of Pod test-webserver-8d100ff3-635d-43bc-af71-9e8127cd574a is Running (Ready = true)
Sep 23 09:15:14.316: INFO: Container started at 2021-09-23 09:14:49 +0000 UTC, pod became ready at 2021-09-23 09:15:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:14.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9120" for this suite.

• [SLOW TEST:26.049 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":155,"skipped":2645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:14.322: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:15:14.358: INFO: Creating ReplicaSet my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76
Sep 23 09:15:14.363: INFO: Pod name my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76: Found 0 pods out of 1
Sep 23 09:15:19.366: INFO: Pod name my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76: Found 1 pods out of 1
Sep 23 09:15:19.366: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76" is running
Sep 23 09:15:19.367: INFO: Pod "my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76-h6v5d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:15:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:15:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:15:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-23 09:15:14 +0000 UTC Reason: Message:}])
Sep 23 09:15:19.368: INFO: Trying to dial the pod
Sep 23 09:15:24.389: INFO: Controller my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76: Got expected result from replica 1 [my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76-h6v5d]: "my-hostname-basic-4250d6ba-a7f3-4e1a-bbed-3670d79dce76-h6v5d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5687" for this suite.

• [SLOW TEST:10.072 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":156,"skipped":2682,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:24.394: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9638" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":157,"skipped":2693,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:28.478: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:15:28.510: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:34.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-897" for this suite.

• [SLOW TEST:6.478 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":158,"skipped":2700,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:34.956: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:15:35.189: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 23 09:15:35.190: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 23 09:15:35.190: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.190: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 23 09:15:35.190: INFO: Checking APIGroup: apps
Sep 23 09:15:35.191: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 23 09:15:35.191: INFO: Versions found [{apps/v1 v1}]
Sep 23 09:15:35.191: INFO: apps/v1 matches apps/v1
Sep 23 09:15:35.191: INFO: Checking APIGroup: events.k8s.io
Sep 23 09:15:35.191: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 23 09:15:35.191: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.191: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 23 09:15:35.191: INFO: Checking APIGroup: authentication.k8s.io
Sep 23 09:15:35.192: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 23 09:15:35.192: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.192: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 23 09:15:35.192: INFO: Checking APIGroup: authorization.k8s.io
Sep 23 09:15:35.203: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 23 09:15:35.203: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.203: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 23 09:15:35.203: INFO: Checking APIGroup: autoscaling
Sep 23 09:15:35.204: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep 23 09:15:35.204: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 23 09:15:35.204: INFO: autoscaling/v1 matches autoscaling/v1
Sep 23 09:15:35.204: INFO: Checking APIGroup: batch
Sep 23 09:15:35.204: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 23 09:15:35.204: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 23 09:15:35.204: INFO: batch/v1 matches batch/v1
Sep 23 09:15:35.204: INFO: Checking APIGroup: certificates.k8s.io
Sep 23 09:15:35.205: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 23 09:15:35.205: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.205: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 23 09:15:35.205: INFO: Checking APIGroup: networking.k8s.io
Sep 23 09:15:35.206: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 23 09:15:35.206: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.206: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 23 09:15:35.206: INFO: Checking APIGroup: extensions
Sep 23 09:15:35.211: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep 23 09:15:35.211: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep 23 09:15:35.211: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep 23 09:15:35.211: INFO: Checking APIGroup: policy
Sep 23 09:15:35.211: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep 23 09:15:35.211: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep 23 09:15:35.211: INFO: policy/v1beta1 matches policy/v1beta1
Sep 23 09:15:35.211: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 23 09:15:35.212: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 23 09:15:35.212: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.212: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 23 09:15:35.212: INFO: Checking APIGroup: storage.k8s.io
Sep 23 09:15:35.213: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 23 09:15:35.213: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.213: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 23 09:15:35.213: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 23 09:15:35.213: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 23 09:15:35.213: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.213: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 23 09:15:35.213: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 23 09:15:35.214: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 23 09:15:35.214: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.214: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 23 09:15:35.214: INFO: Checking APIGroup: scheduling.k8s.io
Sep 23 09:15:35.215: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 23 09:15:35.215: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.215: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 23 09:15:35.215: INFO: Checking APIGroup: coordination.k8s.io
Sep 23 09:15:35.215: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 23 09:15:35.215: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.215: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 23 09:15:35.215: INFO: Checking APIGroup: node.k8s.io
Sep 23 09:15:35.216: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep 23 09:15:35.216: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.216: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep 23 09:15:35.216: INFO: Checking APIGroup: discovery.k8s.io
Sep 23 09:15:35.217: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep 23 09:15:35.217: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.217: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep 23 09:15:35.217: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 23 09:15:35.217: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Sep 23 09:15:35.217: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.217: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Sep 23 09:15:35.217: INFO: Checking APIGroup: crd.projectcalico.org
Sep 23 09:15:35.218: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep 23 09:15:35.218: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep 23 09:15:35.218: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep 23 09:15:35.218: INFO: Checking APIGroup: monitoring.coreos.com
Sep 23 09:15:35.218: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Sep 23 09:15:35.218: INFO: Versions found [{monitoring.coreos.com/v1 v1}]
Sep 23 09:15:35.218: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Sep 23 09:15:35.218: INFO: Checking APIGroup: application.kubesphere.io
Sep 23 09:15:35.219: INFO: PreferredVersion.GroupVersion: application.kubesphere.io/v1alpha1
Sep 23 09:15:35.219: INFO: Versions found [{application.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.219: INFO: application.kubesphere.io/v1alpha1 matches application.kubesphere.io/v1alpha1
Sep 23 09:15:35.219: INFO: Checking APIGroup: cluster.kubesphere.io
Sep 23 09:15:35.220: INFO: PreferredVersion.GroupVersion: cluster.kubesphere.io/v1alpha1
Sep 23 09:15:35.220: INFO: Versions found [{cluster.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.220: INFO: cluster.kubesphere.io/v1alpha1 matches cluster.kubesphere.io/v1alpha1
Sep 23 09:15:35.220: INFO: Checking APIGroup: devops.kubesphere.io
Sep 23 09:15:35.220: INFO: PreferredVersion.GroupVersion: devops.kubesphere.io/v1alpha3
Sep 23 09:15:35.220: INFO: Versions found [{devops.kubesphere.io/v1alpha3 v1alpha3} {devops.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.220: INFO: devops.kubesphere.io/v1alpha3 matches devops.kubesphere.io/v1alpha3
Sep 23 09:15:35.220: INFO: Checking APIGroup: installer.kubesphere.io
Sep 23 09:15:35.221: INFO: PreferredVersion.GroupVersion: installer.kubesphere.io/v1alpha1
Sep 23 09:15:35.221: INFO: Versions found [{installer.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.221: INFO: installer.kubesphere.io/v1alpha1 matches installer.kubesphere.io/v1alpha1
Sep 23 09:15:35.221: INFO: Checking APIGroup: monitoring.kubesphere.io
Sep 23 09:15:35.222: INFO: PreferredVersion.GroupVersion: monitoring.kubesphere.io/v1alpha1
Sep 23 09:15:35.222: INFO: Versions found [{monitoring.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.222: INFO: monitoring.kubesphere.io/v1alpha1 matches monitoring.kubesphere.io/v1alpha1
Sep 23 09:15:35.222: INFO: Checking APIGroup: network.kubesphere.io
Sep 23 09:15:35.222: INFO: PreferredVersion.GroupVersion: network.kubesphere.io/v1alpha1
Sep 23 09:15:35.222: INFO: Versions found [{network.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.222: INFO: network.kubesphere.io/v1alpha1 matches network.kubesphere.io/v1alpha1
Sep 23 09:15:35.222: INFO: Checking APIGroup: storage.kubesphere.io
Sep 23 09:15:35.223: INFO: PreferredVersion.GroupVersion: storage.kubesphere.io/v1alpha1
Sep 23 09:15:35.223: INFO: Versions found [{storage.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.223: INFO: storage.kubesphere.io/v1alpha1 matches storage.kubesphere.io/v1alpha1
Sep 23 09:15:35.223: INFO: Checking APIGroup: tenant.kubesphere.io
Sep 23 09:15:35.224: INFO: PreferredVersion.GroupVersion: tenant.kubesphere.io/v1alpha2
Sep 23 09:15:35.224: INFO: Versions found [{tenant.kubesphere.io/v1alpha2 v1alpha2} {tenant.kubesphere.io/v1alpha1 v1alpha1}]
Sep 23 09:15:35.224: INFO: tenant.kubesphere.io/v1alpha2 matches tenant.kubesphere.io/v1alpha2
Sep 23 09:15:35.224: INFO: Checking APIGroup: iam.kubesphere.io
Sep 23 09:15:35.224: INFO: PreferredVersion.GroupVersion: iam.kubesphere.io/v1alpha2
Sep 23 09:15:35.224: INFO: Versions found [{iam.kubesphere.io/v1alpha2 v1alpha2}]
Sep 23 09:15:35.224: INFO: iam.kubesphere.io/v1alpha2 matches iam.kubesphere.io/v1alpha2
Sep 23 09:15:35.224: INFO: Checking APIGroup: quota.kubesphere.io
Sep 23 09:15:35.225: INFO: PreferredVersion.GroupVersion: quota.kubesphere.io/v1alpha2
Sep 23 09:15:35.225: INFO: Versions found [{quota.kubesphere.io/v1alpha2 v1alpha2}]
Sep 23 09:15:35.225: INFO: quota.kubesphere.io/v1alpha2 matches quota.kubesphere.io/v1alpha2
Sep 23 09:15:35.225: INFO: Checking APIGroup: servicemesh.kubesphere.io
Sep 23 09:15:35.225: INFO: PreferredVersion.GroupVersion: servicemesh.kubesphere.io/v1alpha2
Sep 23 09:15:35.225: INFO: Versions found [{servicemesh.kubesphere.io/v1alpha2 v1alpha2}]
Sep 23 09:15:35.225: INFO: servicemesh.kubesphere.io/v1alpha2 matches servicemesh.kubesphere.io/v1alpha2
Sep 23 09:15:35.225: INFO: Checking APIGroup: app.k8s.io
Sep 23 09:15:35.226: INFO: PreferredVersion.GroupVersion: app.k8s.io/v1beta1
Sep 23 09:15:35.226: INFO: Versions found [{app.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.226: INFO: app.k8s.io/v1beta1 matches app.k8s.io/v1beta1
Sep 23 09:15:35.226: INFO: Checking APIGroup: snapshot.storage.k8s.io
Sep 23 09:15:35.226: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Sep 23 09:15:35.226: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.226: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Sep 23 09:15:35.226: INFO: Checking APIGroup: notification.kubesphere.io
Sep 23 09:15:35.227: INFO: PreferredVersion.GroupVersion: notification.kubesphere.io/v2beta1
Sep 23 09:15:35.227: INFO: Versions found [{notification.kubesphere.io/v2beta1 v2beta1}]
Sep 23 09:15:35.227: INFO: notification.kubesphere.io/v2beta1 matches notification.kubesphere.io/v2beta1
Sep 23 09:15:35.227: INFO: Checking APIGroup: metrics.k8s.io
Sep 23 09:15:35.228: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep 23 09:15:35.228: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep 23 09:15:35.228: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:35.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5799" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":159,"skipped":2712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:35.232: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:15:35.693: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 09:15:37.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985335, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985335, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985335, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985335, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:15:40.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-445" for this suite.
STEP: Destroying namespace "webhook-445-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.628 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":160,"skipped":2758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:50.860: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Sep 23 09:15:50.890: INFO: Waiting up to 5m0s for pod "client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5" in namespace "containers-3876" to be "Succeeded or Failed"
Sep 23 09:15:50.892: INFO: Pod "client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.42132ms
Sep 23 09:15:52.895: INFO: Pod "client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004506887s
Sep 23 09:15:54.898: INFO: Pod "client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007369755s
STEP: Saw pod success
Sep 23 09:15:54.898: INFO: Pod "client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5" satisfied condition "Succeeded or Failed"
Sep 23 09:15:54.899: INFO: Trying to get logs from node worker-s002 pod client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:15:54.913: INFO: Waiting for pod client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5 to disappear
Sep 23 09:15:54.915: INFO: Pod client-containers-52646f3e-4d4e-45c4-a9dd-d641b7bd78b5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:54.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3876" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2781,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 23 09:15:54.955: INFO: Waiting up to 5m0s for pod "pod-4c60eb25-071b-4db7-95f8-c4d277a033fa" in namespace "emptydir-3470" to be "Succeeded or Failed"
Sep 23 09:15:54.960: INFO: Pod "pod-4c60eb25-071b-4db7-95f8-c4d277a033fa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019309ms
Sep 23 09:15:56.961: INFO: Pod "pod-4c60eb25-071b-4db7-95f8-c4d277a033fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00681121s
STEP: Saw pod success
Sep 23 09:15:56.961: INFO: Pod "pod-4c60eb25-071b-4db7-95f8-c4d277a033fa" satisfied condition "Succeeded or Failed"
Sep 23 09:15:56.963: INFO: Trying to get logs from node worker-s002 pod pod-4c60eb25-071b-4db7-95f8-c4d277a033fa container test-container: <nil>
STEP: delete the pod
Sep 23 09:15:56.973: INFO: Waiting for pod pod-4c60eb25-071b-4db7-95f8-c4d277a033fa to disappear
Sep 23 09:15:56.974: INFO: Pod pod-4c60eb25-071b-4db7-95f8-c4d277a033fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:15:56.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3470" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:15:56.979: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5236, will wait for the garbage collector to delete the pods
Sep 23 09:16:01.077: INFO: Deleting Job.batch foo took: 3.049852ms
Sep 23 09:16:01.277: INFO: Terminating Job.batch foo pods took: 200.175126ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:16:34.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5236" for this suite.

• [SLOW TEST:37.705 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":163,"skipped":2855,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:16:34.684: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:16:34.723: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f" in namespace "downward-api-7032" to be "Succeeded or Failed"
Sep 23 09:16:34.728: INFO: Pod "downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.079982ms
Sep 23 09:16:36.732: INFO: Pod "downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008654872s
STEP: Saw pod success
Sep 23 09:16:36.732: INFO: Pod "downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f" satisfied condition "Succeeded or Failed"
Sep 23 09:16:36.742: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f container client-container: <nil>
STEP: delete the pod
Sep 23 09:16:36.752: INFO: Waiting for pod downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f to disappear
Sep 23 09:16:36.756: INFO: Pod downwardapi-volume-54a83bf0-1b29-4997-8069-b9cb73c55d2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:16:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7032" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":164,"skipped":2874,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:16:36.760: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:16:40.878: INFO: Waiting up to 5m0s for pod "client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef" in namespace "pods-1807" to be "Succeeded or Failed"
Sep 23 09:16:40.881: INFO: Pod "client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302556ms
Sep 23 09:16:42.884: INFO: Pod "client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006058605s
STEP: Saw pod success
Sep 23 09:16:42.884: INFO: Pod "client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef" satisfied condition "Succeeded or Failed"
Sep 23 09:16:42.885: INFO: Trying to get logs from node worker-s002 pod client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef container env3cont: <nil>
STEP: delete the pod
Sep 23 09:16:42.897: INFO: Waiting for pod client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef to disappear
Sep 23 09:16:42.902: INFO: Pod client-envvars-5a6cc047-d552-4545-b917-a7bf43de8fef no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:16:42.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1807" for this suite.

• [SLOW TEST:6.146 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:16:42.907: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:16:59.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7102" for this suite.

• [SLOW TEST:16.118 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":166,"skipped":2915,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:16:59.026: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 23 09:16:59.065: INFO: Waiting up to 5m0s for pod "pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8" in namespace "emptydir-1744" to be "Succeeded or Failed"
Sep 23 09:16:59.070: INFO: Pod "pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73203ms
Sep 23 09:17:01.170: INFO: Pod "pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105434249s
Sep 23 09:17:03.174: INFO: Pod "pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109498924s
STEP: Saw pod success
Sep 23 09:17:03.174: INFO: Pod "pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8" satisfied condition "Succeeded or Failed"
Sep 23 09:17:03.176: INFO: Trying to get logs from node worker-s002 pod pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8 container test-container: <nil>
STEP: delete the pod
Sep 23 09:17:03.195: INFO: Waiting for pod pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8 to disappear
Sep 23 09:17:03.196: INFO: Pod pod-f18726f4-0ee8-4014-bfbf-1a2065ef64e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:03.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1744" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2917,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:03.200: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:17:03.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91" in namespace "projected-3302" to be "Succeeded or Failed"
Sep 23 09:17:03.232: INFO: Pod "downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889647ms
Sep 23 09:17:05.234: INFO: Pod "downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004007489s
Sep 23 09:17:07.237: INFO: Pod "downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006764955s
STEP: Saw pod success
Sep 23 09:17:07.237: INFO: Pod "downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91" satisfied condition "Succeeded or Failed"
Sep 23 09:17:07.239: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91 container client-container: <nil>
STEP: delete the pod
Sep 23 09:17:07.252: INFO: Waiting for pod downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91 to disappear
Sep 23 09:17:07.254: INFO: Pod downwardapi-volume-879ef928-051e-473b-a060-4eb848d99b91 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:07.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3302" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":2917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:07.259: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:14.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5151" for this suite.

• [SLOW TEST:7.066 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":169,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:14.325: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 23 09:17:14.392: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 23 09:17:14.394: INFO: starting watch
STEP: patching
STEP: updating
Sep 23 09:17:14.402: INFO: waiting for watch events with expected annotations
Sep 23 09:17:14.402: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7243" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":170,"skipped":3001,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:14.433: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-28ec5e60-0478-4ab3-93bc-cc9cb061fcca
STEP: Creating secret with name secret-projected-all-test-volume-4f855c2b-3ff9-4519-b16f-c094e057690c
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 23 09:17:14.465: INFO: Waiting up to 5m0s for pod "projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd" in namespace "projected-6140" to be "Succeeded or Failed"
Sep 23 09:17:14.467: INFO: Pod "projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173196ms
Sep 23 09:17:16.471: INFO: Pod "projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005684755s
STEP: Saw pod success
Sep 23 09:17:16.471: INFO: Pod "projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd" satisfied condition "Succeeded or Failed"
Sep 23 09:17:16.473: INFO: Trying to get logs from node worker-s002 pod projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 23 09:17:16.492: INFO: Waiting for pod projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd to disappear
Sep 23 09:17:16.494: INFO: Pod projected-volume-f268ae09-a96c-4310-b7ff-60a943cd30fd no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:16.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6140" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":171,"skipped":3019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:16.499: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-674fae97-4f26-474a-b92f-d85d5f6b3746
STEP: Creating configMap with name cm-test-opt-upd-a234fa83-225f-4d5b-8e1d-66952d3ec637
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-674fae97-4f26-474a-b92f-d85d5f6b3746
STEP: Updating configmap cm-test-opt-upd-a234fa83-225f-4d5b-8e1d-66952d3ec637
STEP: Creating configMap with name cm-test-opt-create-84429786-5fac-4a01-9a5d-e657a395ca49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:20.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5602" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3105,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:20.630: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-513c3636-90ad-4e4d-b840-50682bd50e64
STEP: Creating a pod to test consume configMaps
Sep 23 09:17:20.665: INFO: Waiting up to 5m0s for pod "pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495" in namespace "configmap-4390" to be "Succeeded or Failed"
Sep 23 09:17:20.668: INFO: Pod "pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.761586ms
Sep 23 09:17:22.670: INFO: Pod "pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004867336s
STEP: Saw pod success
Sep 23 09:17:22.670: INFO: Pod "pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495" satisfied condition "Succeeded or Failed"
Sep 23 09:17:22.671: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:17:22.687: INFO: Waiting for pod pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495 to disappear
Sep 23 09:17:22.689: INFO: Pod pod-configmaps-99b5b0ef-461a-4aff-94a6-5e3c78214495 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:17:22.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4390" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":3116,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:17:22.694: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-b3026012-9396-4d01-ba5d-3a9e104f0631
STEP: Creating configMap with name cm-test-opt-upd-d5b8f90f-c566-432f-a96f-326e1932ba3d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b3026012-9396-4d01-ba5d-3a9e104f0631
STEP: Updating configmap cm-test-opt-upd-d5b8f90f-c566-432f-a96f-326e1932ba3d
STEP: Creating configMap with name cm-test-opt-create-e40dd7e5-744a-4a09-91cd-efa2189b297c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:18:51.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8656" for this suite.

• [SLOW TEST:88.406 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":174,"skipped":3134,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:18:51.100: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:18:51.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2400" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":175,"skipped":3147,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:18:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 23 09:18:53.690: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5360 pod-service-account-2ba2e046-2edf-4c59-8140-ecbcb8066400 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 23 09:18:53.843: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5360 pod-service-account-2ba2e046-2edf-4c59-8140-ecbcb8066400 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 23 09:18:53.987: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5360 pod-service-account-2ba2e046-2edf-4c59-8140-ecbcb8066400 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:18:54.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5360" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":176,"skipped":3156,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:18:54.164: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1089
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1089
STEP: creating replication controller externalsvc in namespace services-1089
I0923 09:18:54.210752      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1089, replica count: 2
I0923 09:18:57.261046      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 23 09:18:57.283: INFO: Creating new exec pod
Sep 23 09:18:59.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1089 exec execpodnh5np -- /bin/sh -x -c nslookup nodeport-service.services-1089.svc.cluster.local'
Sep 23 09:18:59.443: INFO: stderr: "+ nslookup nodeport-service.services-1089.svc.cluster.local\n"
Sep 23 09:18:59.443: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1089.svc.cluster.local\tcanonical name = externalsvc.services-1089.svc.cluster.local.\nName:\texternalsvc.services-1089.svc.cluster.local\nAddress: 10.96.5.5\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1089, will wait for the garbage collector to delete the pods
Sep 23 09:18:59.498: INFO: Deleting ReplicationController externalsvc took: 3.22006ms
Sep 23 09:19:00.998: INFO: Terminating ReplicationController externalsvc pods took: 1.500172199s
Sep 23 09:19:14.217: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:14.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1089" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:20.079 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":177,"skipped":3164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:18.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1792" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":178,"skipped":3190,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:18.301: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7425
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7425
I0923 09:19:18.350555      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-7425, replica count: 2
I0923 09:19:21.400845      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:19:21.400: INFO: Creating new exec pod
Sep 23 09:19:26.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7425 exec execpodd8ncd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 23 09:19:26.581: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 23 09:19:26.581: INFO: stdout: ""
Sep 23 09:19:26.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7425 exec execpodd8ncd -- /bin/sh -x -c nc -zv -t -w 2 10.96.218.133 80'
Sep 23 09:19:26.729: INFO: stderr: "+ nc -zv -t -w 2 10.96.218.133 80\nConnection to 10.96.218.133 80 port [tcp/http] succeeded!\n"
Sep 23 09:19:26.729: INFO: stdout: ""
Sep 23 09:19:26.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7425 exec execpodd8ncd -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.6 30668'
Sep 23 09:19:26.880: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.6 30668\nConnection to 172.16.0.6 30668 port [tcp/30668] succeeded!\n"
Sep 23 09:19:26.880: INFO: stdout: ""
Sep 23 09:19:26.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-7425 exec execpodd8ncd -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.7 30668'
Sep 23 09:19:27.022: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.7 30668\nConnection to 172.16.0.7 30668 port [tcp/30668] succeeded!\n"
Sep 23 09:19:27.022: INFO: stdout: ""
Sep 23 09:19:27.022: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:27.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7425" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.752 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":179,"skipped":3195,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-ead24355-08f7-4c0e-b4e6-bcbb801d9e12
STEP: Creating a pod to test consume secrets
Sep 23 09:19:27.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718" in namespace "projected-1033" to be "Succeeded or Failed"
Sep 23 09:19:27.089: INFO: Pod "pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718": Phase="Pending", Reason="", readiness=false. Elapsed: 2.715416ms
Sep 23 09:19:29.093: INFO: Pod "pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006712717s
STEP: Saw pod success
Sep 23 09:19:29.093: INFO: Pod "pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718" satisfied condition "Succeeded or Failed"
Sep 23 09:19:29.095: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:19:29.110: INFO: Waiting for pod pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718 to disappear
Sep 23 09:19:29.112: INFO: Pod pod-projected-secrets-9ecfc7f0-94a4-4372-aecd-8e0367248718 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:29.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1033" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":3196,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep 23 09:19:29.148: INFO: Waiting up to 5m0s for pod "downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b" in namespace "downward-api-9929" to be "Succeeded or Failed"
Sep 23 09:19:29.151: INFO: Pod "downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688338ms
Sep 23 09:19:31.156: INFO: Pod "downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008332842s
STEP: Saw pod success
Sep 23 09:19:31.156: INFO: Pod "downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b" satisfied condition "Succeeded or Failed"
Sep 23 09:19:31.157: INFO: Trying to get logs from node worker-s002 pod downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:19:31.174: INFO: Waiting for pod downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b to disappear
Sep 23 09:19:31.181: INFO: Pod downward-api-0610727c-9893-469a-bc72-2db1b6d1d38b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:31.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9929" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":181,"skipped":3215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:31.186: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:31.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5759" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":182,"skipped":3243,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-plkd
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:19:31.259: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-plkd" in namespace "subpath-4675" to be "Succeeded or Failed"
Sep 23 09:19:31.264: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.679708ms
Sep 23 09:19:33.267: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007674261s
Sep 23 09:19:35.270: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 4.010684332s
Sep 23 09:19:37.273: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 6.013434602s
Sep 23 09:19:39.276: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 8.017129718s
Sep 23 09:19:41.280: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 10.02040309s
Sep 23 09:19:43.283: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 12.023432635s
Sep 23 09:19:45.287: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 14.027292031s
Sep 23 09:19:47.289: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 16.029926288s
Sep 23 09:19:49.294: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 18.034783324s
Sep 23 09:19:51.299: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 20.03993191s
Sep 23 09:19:53.303: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Running", Reason="", readiness=true. Elapsed: 22.043577934s
Sep 23 09:19:55.307: INFO: Pod "pod-subpath-test-configmap-plkd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.047630559s
STEP: Saw pod success
Sep 23 09:19:55.307: INFO: Pod "pod-subpath-test-configmap-plkd" satisfied condition "Succeeded or Failed"
Sep 23 09:19:55.309: INFO: Trying to get logs from node worker-s002 pod pod-subpath-test-configmap-plkd container test-container-subpath-configmap-plkd: <nil>
STEP: delete the pod
Sep 23 09:19:55.337: INFO: Waiting for pod pod-subpath-test-configmap-plkd to disappear
Sep 23 09:19:55.340: INFO: Pod pod-subpath-test-configmap-plkd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-plkd
Sep 23 09:19:55.340: INFO: Deleting pod "pod-subpath-test-configmap-plkd" in namespace "subpath-4675"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:19:55.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4675" for this suite.

• [SLOW TEST:24.121 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":183,"skipped":3254,"failed":0}
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:19:55.346: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep 23 09:19:55.376: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Sep 23 09:19:55.759: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Sep 23 09:19:57.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:19:59.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:01.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:03.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:05.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:07.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:09.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:11.806: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:13.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:15.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:17.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:19.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:21.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:23.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:25.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:27.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:29.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:31.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:33.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:35.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:37.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:39.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:41.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:43.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:45.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:47.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:49.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:51.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:53.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:55.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:57.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:20:59.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:01.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:03.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:05.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:07.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:09.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:11.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:13.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:15.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:17.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:19.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:21.800: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:23.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:25.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:27.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:29.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:31.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:33.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:35.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:37.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:39.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767985595, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:21:43.016: INFO: Waited 1.213912774s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:21:44.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5075" for this suite.

• [SLOW TEST:109.368 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":184,"skipped":3257,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:21:44.714: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Sep 23 09:21:48.760: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3096 PodName:pod-sharedvolume-f8e3bfa0-f3ba-42b4-bb02-fd8b98b15e8e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:21:48.760: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:21:48.824: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:21:48.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3096" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":185,"skipped":3263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:21:48.830: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep 23 09:21:48.869: INFO: Waiting up to 5m0s for pod "downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad" in namespace "downward-api-4974" to be "Succeeded or Failed"
Sep 23 09:21:48.872: INFO: Pod "downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.235691ms
Sep 23 09:21:50.874: INFO: Pod "downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004442055s
STEP: Saw pod success
Sep 23 09:21:50.874: INFO: Pod "downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad" satisfied condition "Succeeded or Failed"
Sep 23 09:21:50.875: INFO: Trying to get logs from node worker-s002 pod downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:21:50.905: INFO: Waiting for pod downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad to disappear
Sep 23 09:21:50.906: INFO: Pod downward-api-204d0a3d-27b3-4589-adc4-8f1efff1b6ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:21:50.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4974" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:21:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 23 09:21:50.942: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 23 09:22:07.210: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:22:12.015: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:22:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1577" for this suite.

• [SLOW TEST:37.129 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":187,"skipped":3333,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:22:28.040: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:22:28.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9466" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":188,"skipped":3338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:22:28.161: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 23 09:22:28.196: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:22:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6756" for this suite.

• [SLOW TEST:6.030 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:22:34.191: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep 23 09:22:34.225: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 09:22:34.229: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 09:22:34.231: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Sep 23 09:22:34.239: INFO: calico-node-gfgx9 from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:22:34.239: INFO: csi-qingcloud-controller-6b58955cdd-l56st from kube-system started at 2021-09-23 03:58:05 +0000 UTC (5 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 23 09:22:34.239: INFO: csi-qingcloud-node-xqq5p from kube-system started at 2021-09-23 03:58:05 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:22:34.239: INFO: kube-proxy-8nnxr from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:22:34.239: INFO: metrics-server-57bcd9bccd-hbn8w from kube-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container metrics-server ready: true, restart count 0
Sep 23 09:22:34.239: INFO: snapshot-controller-0 from kube-system started at 2021-09-23 08:47:46 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 23 09:22:34.239: INFO: default-http-backend-76d9fb4bb7-56kpj from kubesphere-controls-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container default-http-backend ready: true, restart count 0
Sep 23 09:22:34.239: INFO: kubectl-admin-776b98f44f-hftkv from kubesphere-controls-system started at 2021-09-23 04:02:15 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kubectl ready: true, restart count 0
Sep 23 09:22:34.239: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-09-23 08:47:47 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:22:34.239: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-09-23 04:01:40 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:22:34.239: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-09-23 08:47:48 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:22:34.239: INFO: kube-state-metrics-67588479db-tvwbm from kubesphere-monitoring-system started at 2021-09-23 04:01:32 +0000 UTC (3 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 23 09:22:34.239: INFO: node-exporter-t9zb4 from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:22:34.239: INFO: notification-manager-deployment-7bd887ffb4-n9rvc from kubesphere-monitoring-system started at 2021-09-23 04:02:07 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:22:34.239: INFO: notification-manager-deployment-7bd887ffb4-nmd9g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:22:34.239: INFO: notification-manager-operator-78595d8666-49k2z from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container notification-manager-operator ready: true, restart count 0
Sep 23 09:22:34.239: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-09-23 04:02:27 +0000 UTC (3 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:22:34.239: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:22:34.239: INFO: prometheus-operator-d7fdfccbf-nhp4g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 23 09:22:34.239: INFO: ks-installer-f58dbc4cc-q6zkt from kubesphere-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container installer ready: true, restart count 1
Sep 23 09:22:34.239: INFO: sonobuoy from sonobuoy started at 2021-09-23 08:25:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 09:22:34.239: INFO: sonobuoy-e2e-job-57e5d9d7846a4ccc from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container e2e ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:22:34.239: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:22:34.239: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 09:22:34.239: INFO: ss2-0 from statefulset-7832 started at 2021-09-23 08:23:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:22:34.239: INFO: ss2-1 from statefulset-7832 started at 2021-09-23 08:47:47 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.239: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:22:34.240: INFO: ss2-2 from statefulset-7832 started at 2021-09-23 08:47:50 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.240: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:22:34.240: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Sep 23 09:22:34.246: INFO: calico-node-f5cnk from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:22:34.246: INFO: csi-qingcloud-node-ll5nv from kube-system started at 2021-09-23 08:48:06 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:22:34.246: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:22:34.246: INFO: kube-proxy-fn894 from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:22:34.246: INFO: node-exporter-6n95k from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:22:34.246: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:22:34.246: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-09-23 08:48:23 +0000 UTC (3 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:22:34.246: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:22:34.246: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:22:34.246: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:22:34.246: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:22:34.246: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-22155c77-c7bd-4630-b5e1-7027f9f40174 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.0.7 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-22155c77-c7bd-4630-b5e1-7027f9f40174 off the node worker-s002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-22155c77-c7bd-4630-b5e1-7027f9f40174
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:27:40.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7037" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.163 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":190,"skipped":3438,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:27:40.355: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3737
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3737
I0923 09:27:40.403164      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3737, replica count: 2
Sep 23 09:27:43.453: INFO: Creating new exec pod
I0923 09:27:43.453480      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:27:46.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3737 exec execpodjr4pf -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep 23 09:27:46.742: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 23 09:27:46.742: INFO: stdout: ""
Sep 23 09:27:46.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3737 exec execpodjr4pf -- /bin/sh -x -c nc -zv -t -w 2 10.96.83.219 80'
Sep 23 09:27:46.871: INFO: stderr: "+ nc -zv -t -w 2 10.96.83.219 80\nConnection to 10.96.83.219 80 port [tcp/http] succeeded!\n"
Sep 23 09:27:46.871: INFO: stdout: ""
Sep 23 09:27:46.871: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:27:46.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3737" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.547 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":191,"skipped":3439,"failed":0}
SSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:27:46.902: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Sep 23 09:27:46.933: INFO: Major version: 1
STEP: Confirm minor version
Sep 23 09:27:46.933: INFO: cleanMinorVersion: 20
Sep 23 09:27:46.933: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:27:46.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6113" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":192,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:27:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-92ls
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:27:46.970: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-92ls" in namespace "subpath-6696" to be "Succeeded or Failed"
Sep 23 09:27:46.972: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053458ms
Sep 23 09:27:48.976: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 2.005267767s
Sep 23 09:27:50.979: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 4.00821184s
Sep 23 09:27:52.981: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 6.010478393s
Sep 23 09:27:54.985: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 8.014729385s
Sep 23 09:27:56.989: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 10.018850892s
Sep 23 09:27:58.993: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 12.022996507s
Sep 23 09:28:00.998: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 14.027140864s
Sep 23 09:28:03.000: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 16.030024228s
Sep 23 09:28:05.004: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 18.033567453s
Sep 23 09:28:07.007: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 20.036859355s
Sep 23 09:28:09.010: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Running", Reason="", readiness=true. Elapsed: 22.040109919s
Sep 23 09:28:11.013: INFO: Pod "pod-subpath-test-downwardapi-92ls": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042136201s
STEP: Saw pod success
Sep 23 09:28:11.013: INFO: Pod "pod-subpath-test-downwardapi-92ls" satisfied condition "Succeeded or Failed"
Sep 23 09:28:11.016: INFO: Trying to get logs from node worker-s002 pod pod-subpath-test-downwardapi-92ls container test-container-subpath-downwardapi-92ls: <nil>
STEP: delete the pod
Sep 23 09:28:11.035: INFO: Waiting for pod pod-subpath-test-downwardapi-92ls to disappear
Sep 23 09:28:11.037: INFO: Pod pod-subpath-test-downwardapi-92ls no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-92ls
Sep 23 09:28:11.037: INFO: Deleting pod "pod-subpath-test-downwardapi-92ls" in namespace "subpath-6696"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:28:11.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6696" for this suite.

• [SLOW TEST:24.101 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":193,"skipped":3511,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:28:11.043: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:28:11.080: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f" in namespace "downward-api-5163" to be "Succeeded or Failed"
Sep 23 09:28:11.082: INFO: Pod "downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536883ms
Sep 23 09:28:13.086: INFO: Pod "downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005850881s
Sep 23 09:28:15.089: INFO: Pod "downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008905039s
STEP: Saw pod success
Sep 23 09:28:15.089: INFO: Pod "downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f" satisfied condition "Succeeded or Failed"
Sep 23 09:28:15.090: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f container client-container: <nil>
STEP: delete the pod
Sep 23 09:28:15.104: INFO: Waiting for pod downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f to disappear
Sep 23 09:28:15.106: INFO: Pod downwardapi-volume-9472aa57-d4ae-4615-b89b-febd9207211f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:28:15.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5163" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3514,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:28:15.110: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Sep 23 09:28:15.139: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 23 09:28:15.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:15.422: INFO: stderr: ""
Sep 23 09:28:15.422: INFO: stdout: "service/agnhost-replica created\n"
Sep 23 09:28:15.422: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 23 09:28:15.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:15.633: INFO: stderr: ""
Sep 23 09:28:15.633: INFO: stdout: "service/agnhost-primary created\n"
Sep 23 09:28:15.633: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 23 09:28:15.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:15.835: INFO: stderr: ""
Sep 23 09:28:15.835: INFO: stdout: "service/frontend created\n"
Sep 23 09:28:15.835: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 23 09:28:15.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:16.043: INFO: stderr: ""
Sep 23 09:28:16.043: INFO: stdout: "deployment.apps/frontend created\n"
Sep 23 09:28:16.043: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 23 09:28:16.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:16.251: INFO: stderr: ""
Sep 23 09:28:16.252: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 23 09:28:16.252: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 23 09:28:16.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 create -f -'
Sep 23 09:28:16.454: INFO: stderr: ""
Sep 23 09:28:16.454: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 23 09:28:16.454: INFO: Waiting for all frontend pods to be Running.
Sep 23 09:28:21.504: INFO: Waiting for frontend to serve content.
Sep 23 09:28:21.510: INFO: Trying to add a new entry to the guestbook.
Sep 23 09:28:21.518: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 23 09:28:21.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.599: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.599: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 09:28:21.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.661: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.661: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 09:28:21.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.745: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 09:28:21.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.802: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.802: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 09:28:21.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.858: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.858: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 23 09:28:21.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1388 delete --grace-period=0 --force -f -'
Sep 23 09:28:21.912: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:28:21.912: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:28:21.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1388" for this suite.

• [SLOW TEST:6.808 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":195,"skipped":3532,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:28:21.918: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:28:22.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 09:28:24.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986102, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986102, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986102, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986102, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:28:27.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:28:27.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6804" for this suite.
STEP: Destroying namespace "webhook-6804-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":196,"skipped":3540,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:28:27.501: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 23 09:28:27.553: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 09:29:27.615: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Sep 23 09:29:27.635: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 23 09:29:27.658: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:29:45.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8260" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:78.232 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":197,"skipped":3554,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:29:45.733: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 23 09:29:49.774: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5336 PodName:var-expansion-1650ceb7-902b-47c3-b60f-27363e588def ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:29:49.774: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: test for file in mounted path
Sep 23 09:29:49.850: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5336 PodName:var-expansion-1650ceb7-902b-47c3-b60f-27363e588def ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:29:49.850: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: updating the annotation value
Sep 23 09:29:50.441: INFO: Successfully updated pod "var-expansion-1650ceb7-902b-47c3-b60f-27363e588def"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 23 09:29:50.445: INFO: Deleting pod "var-expansion-1650ceb7-902b-47c3-b60f-27363e588def" in namespace "var-expansion-5336"
Sep 23 09:29:50.451: INFO: Wait up to 5m0s for pod "var-expansion-1650ceb7-902b-47c3-b60f-27363e588def" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:30:34.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5336" for this suite.

• [SLOW TEST:48.731 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":198,"skipped":3564,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:30:34.464: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Sep 23 09:30:34.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 23 09:30:34.574: INFO: stderr: ""
Sep 23 09:30:34.574: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Sep 23 09:30:34.574: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 23 09:30:34.574: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-965" to be "running and ready, or succeeded"
Sep 23 09:30:34.579: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903503ms
Sep 23 09:30:36.583: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009060644s
Sep 23 09:30:36.583: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 23 09:30:36.583: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 23 09:30:36.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator'
Sep 23 09:30:36.656: INFO: stderr: ""
Sep 23 09:30:36.656: INFO: stdout: "I0923 09:30:35.909031       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/t96z 493\nI0923 09:30:36.109165       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nwng 485\nI0923 09:30:36.309213       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mg5 349\nI0923 09:30:36.509096       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/k57d 283\n"
STEP: limiting log lines
Sep 23 09:30:36.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator --tail=1'
Sep 23 09:30:36.729: INFO: stderr: ""
Sep 23 09:30:36.729: INFO: stdout: "I0923 09:30:36.509096       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/k57d 283\n"
Sep 23 09:30:36.729: INFO: got output "I0923 09:30:36.509096       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/k57d 283\n"
STEP: limiting log bytes
Sep 23 09:30:36.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator --limit-bytes=1'
Sep 23 09:30:36.795: INFO: stderr: ""
Sep 23 09:30:36.795: INFO: stdout: "I"
Sep 23 09:30:36.795: INFO: got output "I"
STEP: exposing timestamps
Sep 23 09:30:36.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator --tail=1 --timestamps'
Sep 23 09:30:36.858: INFO: stderr: ""
Sep 23 09:30:36.858: INFO: stdout: "2021-09-23T09:30:36.709257788Z I0923 09:30:36.709150       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/9jgx 420\n"
Sep 23 09:30:36.858: INFO: got output "2021-09-23T09:30:36.709257788Z I0923 09:30:36.709150       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/9jgx 420\n"
STEP: restricting to a time range
Sep 23 09:30:39.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator --since=1s'
Sep 23 09:30:39.423: INFO: stderr: ""
Sep 23 09:30:39.423: INFO: stdout: "I0923 09:30:38.509146       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wnr 520\nI0923 09:30:38.709176       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/pkt 447\nI0923 09:30:38.909171       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/zb9 528\nI0923 09:30:39.109157       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/gc8c 558\nI0923 09:30:39.309152       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/gl2 261\n"
Sep 23 09:30:39.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 logs logs-generator logs-generator --since=24h'
Sep 23 09:30:39.496: INFO: stderr: ""
Sep 23 09:30:39.496: INFO: stdout: "I0923 09:30:35.909031       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/t96z 493\nI0923 09:30:36.109165       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/nwng 485\nI0923 09:30:36.309213       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mg5 349\nI0923 09:30:36.509096       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/k57d 283\nI0923 09:30:36.709150       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/9jgx 420\nI0923 09:30:36.909156       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/hk5 599\nI0923 09:30:37.109156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/h2lx 232\nI0923 09:30:37.309158       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/c8m 569\nI0923 09:30:37.509141       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/xg6x 213\nI0923 09:30:37.709155       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/5crh 382\nI0923 09:30:37.909151       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/dcnd 223\nI0923 09:30:38.109168       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/vgqd 439\nI0923 09:30:38.309148       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/kkhn 487\nI0923 09:30:38.509146       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wnr 520\nI0923 09:30:38.709176       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/pkt 447\nI0923 09:30:38.909171       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/zb9 528\nI0923 09:30:39.109157       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/gc8c 558\nI0923 09:30:39.309152       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/gl2 261\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Sep 23 09:30:39.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-965 delete pod logs-generator'
Sep 23 09:30:44.148: INFO: stderr: ""
Sep 23 09:30:44.148: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:30:44.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-965" for this suite.

• [SLOW TEST:9.690 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":199,"skipped":3575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:30:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:30:49.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-420" for this suite.

• [SLOW TEST:5.440 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":200,"skipped":3624,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:30:49.595: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1909
STEP: creating service affinity-nodeport-transition in namespace services-1909
STEP: creating replication controller affinity-nodeport-transition in namespace services-1909
I0923 09:30:49.630679      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1909, replica count: 3
I0923 09:30:52.681004      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:30:52.686: INFO: Creating new exec pod
Sep 23 09:30:57.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Sep 23 09:30:57.834: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 23 09:30:57.834: INFO: stdout: ""
Sep 23 09:30:57.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c nc -zv -t -w 2 10.96.110.164 80'
Sep 23 09:30:57.981: INFO: stderr: "+ nc -zv -t -w 2 10.96.110.164 80\nConnection to 10.96.110.164 80 port [tcp/http] succeeded!\n"
Sep 23 09:30:57.981: INFO: stdout: ""
Sep 23 09:30:57.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.6 30795'
Sep 23 09:30:58.126: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.6 30795\nConnection to 172.16.0.6 30795 port [tcp/30795] succeeded!\n"
Sep 23 09:30:58.126: INFO: stdout: ""
Sep 23 09:30:58.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.7 30795'
Sep 23 09:30:58.266: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.7 30795\nConnection to 172.16.0.7 30795 port [tcp/30795] succeeded!\n"
Sep 23 09:30:58.266: INFO: stdout: ""
Sep 23 09:30:58.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.6:30795/ ; done'
Sep 23 09:30:58.497: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n"
Sep 23 09:30:58.497: INFO: stdout: "\naffinity-nodeport-transition-kkfhs\naffinity-nodeport-transition-vgk8v\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-kkfhs\naffinity-nodeport-transition-vgk8v\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-kkfhs\naffinity-nodeport-transition-vgk8v\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-kkfhs\naffinity-nodeport-transition-vgk8v\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-kkfhs\naffinity-nodeport-transition-vgk8v\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-kkfhs"
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-vgk8v
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-vgk8v
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-vgk8v
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-vgk8v
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-vgk8v
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.498: INFO: Received response from host: affinity-nodeport-transition-kkfhs
Sep 23 09:30:58.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-1909 exec execpod-affinitymclpg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.6:30795/ ; done'
Sep 23 09:30:58.721: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30795/\n"
Sep 23 09:30:58.721: INFO: stdout: "\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m\naffinity-nodeport-transition-hgk6m"
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Received response from host: affinity-nodeport-transition-hgk6m
Sep 23 09:30:58.721: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1909, will wait for the garbage collector to delete the pods
Sep 23 09:30:58.794: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.033768ms
Sep 23 09:30:58.894: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.173979ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:14.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1909" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:24.636 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":201,"skipped":3637,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:14.230: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1825" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":202,"skipped":3639,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:14.265: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Sep 23 09:31:14.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 create -f -'
Sep 23 09:31:14.486: INFO: stderr: ""
Sep 23 09:31:14.486: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 23 09:31:14.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:31:14.556: INFO: stderr: ""
Sep 23 09:31:14.556: INFO: stdout: "update-demo-nautilus-jqc8g update-demo-nautilus-l5r5d "
Sep 23 09:31:14.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods update-demo-nautilus-jqc8g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:31:14.611: INFO: stderr: ""
Sep 23 09:31:14.611: INFO: stdout: ""
Sep 23 09:31:14.611: INFO: update-demo-nautilus-jqc8g is created but not running
Sep 23 09:31:19.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 23 09:31:19.667: INFO: stderr: ""
Sep 23 09:31:19.667: INFO: stdout: "update-demo-nautilus-jqc8g update-demo-nautilus-l5r5d "
Sep 23 09:31:19.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods update-demo-nautilus-jqc8g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:31:19.723: INFO: stderr: ""
Sep 23 09:31:19.723: INFO: stdout: "true"
Sep 23 09:31:19.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods update-demo-nautilus-jqc8g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:31:19.776: INFO: stderr: ""
Sep 23 09:31:19.776: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:31:19.776: INFO: validating pod update-demo-nautilus-jqc8g
Sep 23 09:31:19.778: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:31:19.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:31:19.778: INFO: update-demo-nautilus-jqc8g is verified up and running
Sep 23 09:31:19.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods update-demo-nautilus-l5r5d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 23 09:31:19.833: INFO: stderr: ""
Sep 23 09:31:19.833: INFO: stdout: "true"
Sep 23 09:31:19.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods update-demo-nautilus-l5r5d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 23 09:31:19.893: INFO: stderr: ""
Sep 23 09:31:19.893: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 23 09:31:19.893: INFO: validating pod update-demo-nautilus-l5r5d
Sep 23 09:31:19.896: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 23 09:31:19.896: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 23 09:31:19.896: INFO: update-demo-nautilus-l5r5d is verified up and running
STEP: using delete to clean up resources
Sep 23 09:31:19.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 delete --grace-period=0 --force -f -'
Sep 23 09:31:19.956: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 09:31:19.956: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 23 09:31:19.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get rc,svc -l name=update-demo --no-headers'
Sep 23 09:31:20.027: INFO: stderr: "No resources found in kubectl-626 namespace.\n"
Sep 23 09:31:20.027: INFO: stdout: ""
Sep 23 09:31:20.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-626 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 09:31:20.101: INFO: stderr: ""
Sep 23 09:31:20.101: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:20.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-626" for this suite.

• [SLOW TEST:5.847 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":203,"skipped":3642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:20.112: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Sep 23 09:31:20.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-6296 cluster-info'
Sep 23 09:31:20.225: INFO: stderr: ""
Sep 23 09:31:20.225: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:20.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6296" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":204,"skipped":3675,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:20.230: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep 23 09:31:24.786: INFO: Successfully updated pod "annotationupdatec7bdab08-d246-45e3-ad27-e10e85729af6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:26.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3757" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":205,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:26.819: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:31:27.163: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 09:31:29.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986287, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986287, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986287, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986287, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:31:32.184: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:31:32.185: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9806-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:33.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8498" for this suite.
STEP: Destroying namespace "webhook-8498-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.466 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":206,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:33.285: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 23 09:31:37.350: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 09:31:37.356: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 09:31:39.356: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 09:31:39.358: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 09:31:41.356: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 09:31:41.358: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 09:31:43.356: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 09:31:43.364: INFO: Pod pod-with-prestop-http-hook still exists
Sep 23 09:31:45.356: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 23 09:31:45.362: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:31:45.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1803" for this suite.

• [SLOW TEST:12.095 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:31:45.381: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:10.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3940" for this suite.

• [SLOW TEST:25.295 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3776,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:10.676: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-008ae6cf-e94d-49cd-902a-cfb518a17131
STEP: Creating a pod to test consume configMaps
Sep 23 09:32:10.713: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3" in namespace "projected-5715" to be "Succeeded or Failed"
Sep 23 09:32:10.715: INFO: Pod "pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140253ms
Sep 23 09:32:12.720: INFO: Pod "pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007269301s
Sep 23 09:32:14.734: INFO: Pod "pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020881869s
STEP: Saw pod success
Sep 23 09:32:14.734: INFO: Pod "pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3" satisfied condition "Succeeded or Failed"
Sep 23 09:32:14.736: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:32:14.752: INFO: Waiting for pod pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3 to disappear
Sep 23 09:32:14.753: INFO: Pod pod-projected-configmaps-180883bc-39eb-47b8-bbb9-a87ad3f472c3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:14.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5715" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3781,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Sep 23 09:32:14.796: INFO: Waiting up to 5m0s for pod "var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a" in namespace "var-expansion-6331" to be "Succeeded or Failed"
Sep 23 09:32:14.800: INFO: Pod "var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74625ms
Sep 23 09:32:16.804: INFO: Pod "var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007487624s
Sep 23 09:32:18.807: INFO: Pod "var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010888206s
STEP: Saw pod success
Sep 23 09:32:18.807: INFO: Pod "var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a" satisfied condition "Succeeded or Failed"
Sep 23 09:32:18.809: INFO: Trying to get logs from node worker-s002 pod var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:32:18.825: INFO: Waiting for pod var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a to disappear
Sep 23 09:32:18.828: INFO: Pod var-expansion-fcae4037-533e-4e16-bcfa-d819b752ae1a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:18.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6331" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":210,"skipped":3796,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:18.832: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Sep 23 09:32:18.856: INFO: namespace kubectl-1280
Sep 23 09:32:18.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1280 create -f -'
Sep 23 09:32:19.052: INFO: stderr: ""
Sep 23 09:32:19.052: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 23 09:32:20.054: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 09:32:20.054: INFO: Found 0 / 1
Sep 23 09:32:21.057: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 09:32:21.057: INFO: Found 1 / 1
Sep 23 09:32:21.057: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 23 09:32:21.058: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 23 09:32:21.058: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 23 09:32:21.058: INFO: wait on agnhost-primary startup in kubectl-1280 
Sep 23 09:32:21.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1280 logs agnhost-primary-7wthp agnhost-primary'
Sep 23 09:32:21.121: INFO: stderr: ""
Sep 23 09:32:21.121: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 23 09:32:21.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1280 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep 23 09:32:21.204: INFO: stderr: ""
Sep 23 09:32:21.204: INFO: stdout: "service/rm2 exposed\n"
Sep 23 09:32:21.209: INFO: Service rm2 in namespace kubectl-1280 found.
STEP: exposing service
Sep 23 09:32:23.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1280 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep 23 09:32:23.282: INFO: stderr: ""
Sep 23 09:32:23.282: INFO: stdout: "service/rm3 exposed\n"
Sep 23 09:32:23.293: INFO: Service rm3 in namespace kubectl-1280 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:25.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1280" for this suite.

• [SLOW TEST:6.472 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":211,"skipped":3796,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:25.304: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:53.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4095" for this suite.

• [SLOW TEST:28.086 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":212,"skipped":3801,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:53.390: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 23 09:32:53.427: INFO: Waiting up to 5m0s for pod "pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c" in namespace "emptydir-3658" to be "Succeeded or Failed"
Sep 23 09:32:53.430: INFO: Pod "pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.517822ms
Sep 23 09:32:55.435: INFO: Pod "pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007333593s
STEP: Saw pod success
Sep 23 09:32:55.435: INFO: Pod "pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c" satisfied condition "Succeeded or Failed"
Sep 23 09:32:55.437: INFO: Trying to get logs from node worker-s002 pod pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c container test-container: <nil>
STEP: delete the pod
Sep 23 09:32:55.450: INFO: Waiting for pod pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c to disappear
Sep 23 09:32:55.452: INFO: Pod pod-94cd97f5-31f0-4ee8-9c97-3982cf6e6b8c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:55.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3658" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":213,"skipped":3807,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:55.456: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-b8991d68-e3ce-4ebd-b08b-62db803fb381
STEP: Creating a pod to test consume configMaps
Sep 23 09:32:55.487: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915" in namespace "configmap-1957" to be "Succeeded or Failed"
Sep 23 09:32:55.490: INFO: Pod "pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577294ms
Sep 23 09:32:57.495: INFO: Pod "pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007182824s
Sep 23 09:32:59.498: INFO: Pod "pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010693502s
STEP: Saw pod success
Sep 23 09:32:59.498: INFO: Pod "pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915" satisfied condition "Succeeded or Failed"
Sep 23 09:32:59.500: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:32:59.513: INFO: Waiting for pod pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915 to disappear
Sep 23 09:32:59.514: INFO: Pod pod-configmaps-0d63016a-8998-4890-9968-5a9eab824915 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:32:59.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1957" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3807,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:32:59.521: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:10.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1851" for this suite.

• [SLOW TEST:11.063 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":215,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:10.585: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Sep 23 09:33:10.616: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:32.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1504" for this suite.

• [SLOW TEST:21.823 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":216,"skipped":3874,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:32.408: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6944
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6944
STEP: creating replication controller externalsvc in namespace services-6944
I0923 09:33:32.452767      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6944, replica count: 2
I0923 09:33:35.503043      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 23 09:33:35.516: INFO: Creating new exec pod
Sep 23 09:33:39.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-6944 exec execpod8nzbm -- /bin/sh -x -c nslookup clusterip-service.services-6944.svc.cluster.local'
Sep 23 09:33:39.684: INFO: stderr: "+ nslookup clusterip-service.services-6944.svc.cluster.local\n"
Sep 23 09:33:39.684: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-6944.svc.cluster.local\tcanonical name = externalsvc.services-6944.svc.cluster.local.\nName:\texternalsvc.services-6944.svc.cluster.local\nAddress: 10.96.104.20\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6944, will wait for the garbage collector to delete the pods
Sep 23 09:33:39.740: INFO: Deleting ReplicationController externalsvc took: 3.497203ms
Sep 23 09:33:41.240: INFO: Terminating ReplicationController externalsvc pods took: 1.500187237s
Sep 23 09:33:44.955: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:44.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6944" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:12.562 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":217,"skipped":3881,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:44.970: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 23 09:33:45.013: INFO: Waiting up to 5m0s for pod "pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b" in namespace "emptydir-5855" to be "Succeeded or Failed"
Sep 23 09:33:45.015: INFO: Pod "pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572114ms
Sep 23 09:33:47.018: INFO: Pod "pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004912616s
STEP: Saw pod success
Sep 23 09:33:47.018: INFO: Pod "pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b" satisfied condition "Succeeded or Failed"
Sep 23 09:33:47.019: INFO: Trying to get logs from node worker-s002 pod pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b container test-container: <nil>
STEP: delete the pod
Sep 23 09:33:47.042: INFO: Waiting for pod pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b to disappear
Sep 23 09:33:47.044: INFO: Pod pod-b55042ce-0886-46dc-8f89-6fc4f2bee91b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:47.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5855" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3896,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:33:47.511: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 09:33:49.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986427, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986427, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986427, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986427, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:33:52.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:52.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6107" for this suite.
STEP: Destroying namespace "webhook-6107-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.567 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":219,"skipped":3899,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:52.617: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-814f1351-37d5-4f38-aa39-30c17cf861c4
STEP: Creating a pod to test consume configMaps
Sep 23 09:33:52.669: INFO: Waiting up to 5m0s for pod "pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6" in namespace "configmap-6041" to be "Succeeded or Failed"
Sep 23 09:33:52.671: INFO: Pod "pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075221ms
Sep 23 09:33:54.675: INFO: Pod "pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005793679s
Sep 23 09:33:56.684: INFO: Pod "pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014813115s
STEP: Saw pod success
Sep 23 09:33:56.684: INFO: Pod "pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6" satisfied condition "Succeeded or Failed"
Sep 23 09:33:56.686: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 23 09:33:56.699: INFO: Waiting for pod pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6 to disappear
Sep 23 09:33:56.700: INFO: Pod pod-configmaps-ace892a3-a66d-499c-8c8a-c062ba6fd5d6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:33:56.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6041" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":220,"skipped":3900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:33:56.708: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:33:56.734: INFO: Creating deployment "webserver-deployment"
Sep 23 09:33:56.738: INFO: Waiting for observed generation 1
Sep 23 09:33:58.744: INFO: Waiting for all required pods to come up
Sep 23 09:33:58.748: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 23 09:34:02.754: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 23 09:34:02.760: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 23 09:34:02.766: INFO: Updating deployment webserver-deployment
Sep 23 09:34:02.766: INFO: Waiting for observed generation 2
Sep 23 09:34:04.778: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 23 09:34:04.780: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 23 09:34:04.781: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 23 09:34:04.786: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 23 09:34:04.786: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 23 09:34:04.787: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 23 09:34:04.789: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 23 09:34:04.789: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 23 09:34:04.794: INFO: Updating deployment webserver-deployment
Sep 23 09:34:04.794: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 23 09:34:04.800: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 23 09:34:04.804: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 09:34:04.815: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8048  3a9e148c-08e3-4f34-94ac-7f03dc0c92f6 97354 3 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009181a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-23 09:34:02 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-23 09:34:04 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 23 09:34:04.818: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8048  a08a3863-b167-4cca-8e24-85e31ba4eccb 97352 3 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3a9e148c-08e3-4f34-94ac-7f03dc0c92f6 0xc009181f77 0xc009181f78}] []  [{kube-controller-manager Update apps/v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9e148c-08e3-4f34-94ac-7f03dc0c92f6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ce0018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:34:04.818: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 23 09:34:04.819: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-8048  f86d0e06-9e88-47cd-bc41-5bae9818f729 97351 3 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3a9e148c-08e3-4f34-94ac-7f03dc0c92f6 0xc000ce0087 0xc000ce0088}] []  [{kube-controller-manager Update apps/v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a9e148c-08e3-4f34-94ac-7f03dc0c92f6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000ce00f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:34:04.836: INFO: Pod "webserver-deployment-795d758f88-6k8vm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6k8vm webserver-deployment-795d758f88- deployment-8048  2232ce6b-4293-4f17-80f5-faec610ec5c5 97342 0 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.131.170/32 cni.projectcalico.org/podIPs:10.10.131.170/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce0ea7 0xc000ce0ea8}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 09:34:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.836: INFO: Pod "webserver-deployment-795d758f88-9zt6r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9zt6r webserver-deployment-795d758f88- deployment-8048  5fe6aa92-7466-4d4e-8b22-91e955ce98a1 97365 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1060 0xc000ce1061}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.836: INFO: Pod "webserver-deployment-795d758f88-bz2nf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bz2nf webserver-deployment-795d758f88- deployment-8048  3ae153d5-ed53-4a90-82b3-f1179a47825f 97368 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1277 0xc000ce1278}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 09:34:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.836: INFO: Pod "webserver-deployment-795d758f88-gq6wg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gq6wg webserver-deployment-795d758f88- deployment-8048  3d8b41d1-6be1-4f17-a995-fb8d620d313e 97346 0 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.131.171/32 cni.projectcalico.org/podIPs:10.10.131.171/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1540 0xc000ce1541}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 09:34:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-795d758f88-gqg9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gqg9b webserver-deployment-795d758f88- deployment-8048  412434bf-f852-4b6a-8cf0-9f9161a6afc6 97337 0 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.28.42/32 cni.projectcalico.org/podIPs:10.10.28.42/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1840 0xc000ce1841}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:,StartTime:2021-09-23 09:34:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-795d758f88-mdhct" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mdhct webserver-deployment-795d758f88- deployment-8048  1931b3d0-ba3a-4faa-bcf0-c650b7221744 97331 0 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.131.148/32 cni.projectcalico.org/podIPs:10.10.131.148/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1a80 0xc000ce1a81}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 09:34:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-795d758f88-s99gx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s99gx webserver-deployment-795d758f88- deployment-8048  e9d4b934-e743-45c1-9984-7f4fa5135da7 97369 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1dc0 0xc000ce1dc1}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-795d758f88-st4lx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-st4lx webserver-deployment-795d758f88- deployment-8048  90769183-818c-4d27-aa65-1b281df6a8bf 97326 0 2021-09-23 09:34:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.10.28.41/32 cni.projectcalico.org/podIPs:10.10.28.41/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 a08a3863-b167-4cca-8e24-85e31ba4eccb 0xc000ce1f17 0xc000ce1f18}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a08a3863-b167-4cca-8e24-85e31ba4eccb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-09-23 09:34:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:,StartTime:2021-09-23 09:34:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-dd94f59b7-4n9nl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4n9nl webserver-deployment-dd94f59b7- deployment-8048  60c56e2f-74f8-4d3b-b27d-8d9831b37ee3 97219 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.28.30/32 cni.projectcalico.org/podIPs:10.10.28.30/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002464500 0xc002464501}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:10.10.28.30,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a6928d264340263c72957b096cd8c28fa722233eb4ab62c9644d191afef26c56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.837: INFO: Pod "webserver-deployment-dd94f59b7-6n89x" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6n89x webserver-deployment-dd94f59b7- deployment-8048  e5ada7a2-30c7-42d1-b250-8ce507951a77 97370 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002464b40 0xc002464b41}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-97nts" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-97nts webserver-deployment-dd94f59b7- deployment-8048  39429bfe-176d-4da1-ac9c-b8ea1173e2c4 97223 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.28.29/32 cni.projectcalico.org/podIPs:10.10.28.29/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465130 0xc002465131}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:10.10.28.29,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://866a2889648e42de864518e89e9ebcafb5ba6091410c2b50e2a38a8230e69797,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-9p5xl" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9p5xl webserver-deployment-dd94f59b7- deployment-8048  a7e60d9d-d39f-4e03-baa1-0d468c1e3c42 97358 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc0024654c0 0xc0024654c1}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-f5mbx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f5mbx webserver-deployment-dd94f59b7- deployment-8048  6e92f35f-e683-4357-ad14-5fd807ee5b03 97229 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.131.149/32 cni.projectcalico.org/podIPs:10.10.131.149/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465610 0xc002465611}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.149,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://66899597dd79f0f561a8a91df7fef512a14f41db6b03bd5eb77e81ba4134b9aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-jbzqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jbzqz webserver-deployment-dd94f59b7- deployment-8048  5d371c5f-6067-4708-92d4-6df52105a1a0 97367 0 2021-09-23 09:34:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465830 0xc002465831}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-mghbb" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mghbb webserver-deployment-dd94f59b7- deployment-8048  a89ee9ed-849a-4c0d-a842-f31def127a90 97201 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.131.151/32 cni.projectcalico.org/podIPs:10.10.131.151/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465a00 0xc002465a01}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.151,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://40e2cad538eeba137dcab38db0a08ba2e04df41c8f50e7ebe10594cc954ffe3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-prp8m" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-prp8m webserver-deployment-dd94f59b7- deployment-8048  9fbb20c4-bd7d-4328-965d-3f70f7b3a367 97179 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.28.28/32 cni.projectcalico.org/podIPs:10.10.28.28/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465c50 0xc002465c51}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:10.10.28.28,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d46c0bfddfe34cb8c920ca44641cddb284bbd40a3176d1a7d070c520c2cfd72e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.838: INFO: Pod "webserver-deployment-dd94f59b7-rbfc7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rbfc7 webserver-deployment-dd94f59b7- deployment-8048  a8d77fce-b137-4b8e-a3cb-4116fa294184 97217 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.28.31/32 cni.projectcalico.org/podIPs:10.10.28.31/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc002465e60 0xc002465e61}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:10.10.28.31,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a7ccf1bb3c924d5a5d5573253ba4964172be5dd6b27dd2732578340f8de030fa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.839: INFO: Pod "webserver-deployment-dd94f59b7-tf6wc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tf6wc webserver-deployment-dd94f59b7- deployment-8048  ccf679f9-4e0e-4cde-9403-46275b764bbc 97211 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.131.153/32 cni.projectcalico.org/podIPs:10.10.131.153/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc0050de0b0 0xc0050de0b1}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.153,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0b0948bf15eca54d57ac7c1f125434b86cd3518107c2a2f2f0c3e8922241cf4a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 09:34:04.839: INFO: Pod "webserver-deployment-dd94f59b7-z8g84" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z8g84 webserver-deployment-dd94f59b7- deployment-8048  ea014a49-741b-47ff-add1-6f6ff7ff12b9 97198 0 2021-09-23 09:33:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.10.131.152/32 cni.projectcalico.org/podIPs:10.10.131.152/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f86d0e06-9e88-47cd-bc41-5bae9818f729 0xc0050de2e0 0xc0050de2e1}] []  [{kube-controller-manager Update v1 2021-09-23 09:33:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f86d0e06-9e88-47cd-bc41-5bae9818f729\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 09:33:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sppqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sppqg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sppqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:33:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.152,StartTime:2021-09-23 09:33:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 09:33:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9d49b342e5c2db7fa1fd5ee498b5a8ee461099458ac18087b2126fdb286a64ea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:34:04.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8048" for this suite.

• [SLOW TEST:8.157 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":221,"skipped":3936,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:34:04.865: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8469
STEP: creating service affinity-nodeport in namespace services-8469
STEP: creating replication controller affinity-nodeport in namespace services-8469
I0923 09:34:04.941124      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-8469, replica count: 3
I0923 09:34:07.991482      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 09:34:10.991648      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:34:10.998: INFO: Creating new exec pod
Sep 23 09:34:16.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-8469 exec execpod-affinityvxkds -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Sep 23 09:34:16.184: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 23 09:34:16.184: INFO: stdout: ""
Sep 23 09:34:16.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-8469 exec execpod-affinityvxkds -- /bin/sh -x -c nc -zv -t -w 2 10.96.126.209 80'
Sep 23 09:34:16.349: INFO: stderr: "+ nc -zv -t -w 2 10.96.126.209 80\nConnection to 10.96.126.209 80 port [tcp/http] succeeded!\n"
Sep 23 09:34:16.349: INFO: stdout: ""
Sep 23 09:34:16.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-8469 exec execpod-affinityvxkds -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.6 30687'
Sep 23 09:34:16.505: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.6 30687\nConnection to 172.16.0.6 30687 port [tcp/30687] succeeded!\n"
Sep 23 09:34:16.505: INFO: stdout: ""
Sep 23 09:34:16.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-8469 exec execpod-affinityvxkds -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.7 30687'
Sep 23 09:34:16.649: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.7 30687\nConnection to 172.16.0.7 30687 port [tcp/30687] succeeded!\n"
Sep 23 09:34:16.649: INFO: stdout: ""
Sep 23 09:34:16.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-8469 exec execpod-affinityvxkds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.6:30687/ ; done'
Sep 23 09:34:16.865: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:30687/\n"
Sep 23 09:34:16.865: INFO: stdout: "\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7\naffinity-nodeport-2xvj7"
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Received response from host: affinity-nodeport-2xvj7
Sep 23 09:34:16.865: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8469, will wait for the garbage collector to delete the pods
Sep 23 09:34:16.930: INFO: Deleting ReplicationController affinity-nodeport took: 2.961682ms
Sep 23 09:34:17.030: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.178019ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:34:24.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8469" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.383 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":222,"skipped":3943,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:34:24.248: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2477
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-2477
Sep 23 09:34:24.288: INFO: Found 0 stateful pods, waiting for 1
Sep 23 09:34:34.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 09:34:34.302: INFO: Deleting all statefulset in ns statefulset-2477
Sep 23 09:34:34.306: INFO: Scaling statefulset ss to 0
Sep 23 09:34:54.331: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:34:54.333: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:34:54.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2477" for this suite.

• [SLOW TEST:30.105 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":223,"skipped":3945,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:34:54.354: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:34:54.386: INFO: Creating deployment "test-recreate-deployment"
Sep 23 09:34:54.390: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 23 09:34:54.405: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 23 09:34:56.409: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 23 09:34:56.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986494, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986494, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986494, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767986494, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 09:34:58.430: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 23 09:34:58.446: INFO: Updating deployment test-recreate-deployment
Sep 23 09:34:58.446: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 09:34:58.503: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5950  1f76e846-c56f-43b8-b613-daea00c6bdd0 98169 2 2021-09-23 09:34:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053e81c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-23 09:34:58 +0000 UTC,LastTransitionTime:2021-09-23 09:34:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-09-23 09:34:58 +0000 UTC,LastTransitionTime:2021-09-23 09:34:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 23 09:34:58.504: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5950  5c355507-b71f-4481-beab-7f95005c299a 98167 1 2021-09-23 09:34:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1f76e846-c56f-43b8-b613-daea00c6bdd0 0xc0053e86c0 0xc0053e86c1}] []  [{kube-controller-manager Update apps/v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f76e846-c56f-43b8-b613-daea00c6bdd0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053e8748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:34:58.504: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 23 09:34:58.505: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-5950  a4bad11f-2621-44c0-91fc-e8d584a84b52 98156 2 2021-09-23 09:34:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1f76e846-c56f-43b8-b613-daea00c6bdd0 0xc0053e8587 0xc0053e8588}] []  [{kube-controller-manager Update apps/v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f76e846-c56f-43b8-b613-daea00c6bdd0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053e8618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 09:34:58.510: INFO: Pod "test-recreate-deployment-f79dd4667-dnpw9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-dnpw9 test-recreate-deployment-f79dd4667- deployment-5950  ffb8e83c-f614-4431-9143-193fc00a63ba 98168 0 2021-09-23 09:34:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 5c355507-b71f-4481-beab-7f95005c299a 0xc0053e8c40 0xc0053e8c41}] []  [{kube-controller-manager Update v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c355507-b71f-4481-beab-7f95005c299a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 09:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-476fb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-476fb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-476fb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 09:34:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 09:34:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:34:58.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5950" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":224,"skipped":3949,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:34:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:34:58.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-8138 version'
Sep 23 09:34:58.611: INFO: stderr: ""
Sep 23 09:34:58.611: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:19:55Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:34:58.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8138" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":225,"skipped":3961,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:34:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-3a566644-b533-46de-9dd4-6eee8053fa4a
STEP: Creating a pod to test consume secrets
Sep 23 09:34:58.647: INFO: Waiting up to 5m0s for pod "pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807" in namespace "secrets-5828" to be "Succeeded or Failed"
Sep 23 09:34:58.651: INFO: Pod "pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371653ms
Sep 23 09:35:00.654: INFO: Pod "pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006511115s
Sep 23 09:35:02.656: INFO: Pod "pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008986309s
STEP: Saw pod success
Sep 23 09:35:02.656: INFO: Pod "pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807" satisfied condition "Succeeded or Failed"
Sep 23 09:35:02.658: INFO: Trying to get logs from node worker-s002 pod pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:35:02.676: INFO: Waiting for pod pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807 to disappear
Sep 23 09:35:02.681: INFO: Pod pod-secrets-b204a79c-961a-4f62-9ac3-7fb7103f7807 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:02.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5828" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":226,"skipped":3969,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 23 09:35:05.735: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:05.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4169" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":227,"skipped":3983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:05.753: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:35:05.789: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1d1b5a77-9f50-44ab-b644-58029965bf5c" in namespace "security-context-test-583" to be "Succeeded or Failed"
Sep 23 09:35:05.790: INFO: Pod "busybox-readonly-false-1d1b5a77-9f50-44ab-b644-58029965bf5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.232047ms
Sep 23 09:35:07.792: INFO: Pod "busybox-readonly-false-1d1b5a77-9f50-44ab-b644-58029965bf5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003183627s
Sep 23 09:35:07.792: INFO: Pod "busybox-readonly-false-1d1b5a77-9f50-44ab-b644-58029965bf5c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:07.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-583" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":4035,"failed":0}
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:07.796: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 23 09:35:10.848: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:10.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":4037,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:10.864: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep 23 09:35:10.890: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:15.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8306" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":230,"skipped":4053,"failed":0}
S
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:15.043: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-b2646dec-a5ab-4f31-811e-61f69e2b14b6
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:15.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9969" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":231,"skipped":4054,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:15.079: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:35:15.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f" in namespace "projected-1119" to be "Succeeded or Failed"
Sep 23 09:35:15.127: INFO: Pod "downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262684ms
Sep 23 09:35:17.130: INFO: Pod "downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005434321s
STEP: Saw pod success
Sep 23 09:35:17.130: INFO: Pod "downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f" satisfied condition "Succeeded or Failed"
Sep 23 09:35:17.131: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f container client-container: <nil>
STEP: delete the pod
Sep 23 09:35:17.144: INFO: Waiting for pod downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f to disappear
Sep 23 09:35:17.147: INFO: Pod downwardapi-volume-01a1fbc9-1df0-4f82-afa5-9440a91f544f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:17.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1119" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4055,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-f2324116-0886-49b2-bdcb-06a024882a84 in namespace container-probe-2345
Sep 23 09:35:21.188: INFO: Started pod liveness-f2324116-0886-49b2-bdcb-06a024882a84 in namespace container-probe-2345
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:35:21.190: INFO: Initial restart count of pod liveness-f2324116-0886-49b2-bdcb-06a024882a84 is 0
Sep 23 09:35:43.233: INFO: Restart count of pod container-probe-2345/liveness-f2324116-0886-49b2-bdcb-06a024882a84 is now 1 (22.043256142s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:35:43.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2345" for this suite.

• [SLOW TEST:26.106 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":233,"skipped":4065,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:35:43.257: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3974
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 09:35:43.288: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 23 09:35:43.306: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 09:35:45.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:47.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:49.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:51.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:53.308: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:55.310: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:57.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:35:59.309: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:36:01.309: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 23 09:36:01.312: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 23 09:36:05.349: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 23 09:36:05.349: INFO: Going to poll 10.10.28.48 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep 23 09:36:05.350: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.28.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3974 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:36:05.350: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:36:06.441: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 23 09:36:06.441: INFO: Going to poll 10.10.131.139 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep 23 09:36:06.443: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.131.139 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3974 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:36:06.443: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:36:07.524: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:07.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3974" for this suite.

• [SLOW TEST:24.284 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":234,"skipped":4080,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:07.542: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:36:07.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d" in namespace "projected-9324" to be "Succeeded or Failed"
Sep 23 09:36:07.579: INFO: Pod "downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350946ms
Sep 23 09:36:09.583: INFO: Pod "downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007241806s
STEP: Saw pod success
Sep 23 09:36:09.583: INFO: Pod "downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d" satisfied condition "Succeeded or Failed"
Sep 23 09:36:09.586: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d container client-container: <nil>
STEP: delete the pod
Sep 23 09:36:09.600: INFO: Waiting for pod downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d to disappear
Sep 23 09:36:09.601: INFO: Pod downwardapi-volume-b447bf96-d803-41d7-ac16-e3765946a98d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:09.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9324" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":235,"skipped":4091,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:09.606: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep 23 09:36:09.631: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 23 09:36:09.635: INFO: Waiting for terminating namespaces to be deleted...
Sep 23 09:36:09.636: INFO: 
Logging pods the apiserver thinks is on node worker-s001 before test
Sep 23 09:36:09.645: INFO: calico-node-gfgx9 from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:36:09.645: INFO: csi-qingcloud-controller-6b58955cdd-l56st from kube-system started at 2021-09-23 03:58:05 +0000 UTC (5 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container csi-attacher ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container csi-provisioner ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container csi-resizer ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container csi-snapshotter ready: true, restart count 0
Sep 23 09:36:09.645: INFO: csi-qingcloud-node-xqq5p from kube-system started at 2021-09-23 03:58:05 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:36:09.645: INFO: kube-proxy-8nnxr from kube-system started at 2021-09-23 03:57:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:36:09.645: INFO: metrics-server-57bcd9bccd-hbn8w from kube-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container metrics-server ready: true, restart count 0
Sep 23 09:36:09.645: INFO: snapshot-controller-0 from kube-system started at 2021-09-23 08:47:46 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container snapshot-controller ready: true, restart count 0
Sep 23 09:36:09.645: INFO: default-http-backend-76d9fb4bb7-56kpj from kubesphere-controls-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container default-http-backend ready: true, restart count 0
Sep 23 09:36:09.645: INFO: kubectl-admin-776b98f44f-hftkv from kubesphere-controls-system started at 2021-09-23 04:02:15 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kubectl ready: true, restart count 0
Sep 23 09:36:09.645: INFO: alertmanager-main-0 from kubesphere-monitoring-system started at 2021-09-23 08:47:47 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:36:09.645: INFO: alertmanager-main-1 from kubesphere-monitoring-system started at 2021-09-23 04:01:40 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:36:09.645: INFO: alertmanager-main-2 from kubesphere-monitoring-system started at 2021-09-23 08:47:48 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container alertmanager ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container config-reloader ready: true, restart count 0
Sep 23 09:36:09.645: INFO: kube-state-metrics-67588479db-tvwbm from kubesphere-monitoring-system started at 2021-09-23 04:01:32 +0000 UTC (3 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep 23 09:36:09.645: INFO: node-exporter-t9zb4 from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:36:09.645: INFO: notification-manager-deployment-7bd887ffb4-n9rvc from kubesphere-monitoring-system started at 2021-09-23 04:02:07 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:36:09.645: INFO: notification-manager-deployment-7bd887ffb4-nmd9g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container notification-manager ready: true, restart count 0
Sep 23 09:36:09.645: INFO: notification-manager-operator-78595d8666-49k2z from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container notification-manager-operator ready: true, restart count 0
Sep 23 09:36:09.645: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2021-09-23 04:02:27 +0000 UTC (3 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:36:09.645: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:36:09.645: INFO: prometheus-operator-d7fdfccbf-nhp4g from kubesphere-monitoring-system started at 2021-09-23 08:47:39 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container prometheus-operator ready: true, restart count 0
Sep 23 09:36:09.645: INFO: ks-installer-f58dbc4cc-q6zkt from kubesphere-system started at 2021-09-23 08:26:37 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container installer ready: true, restart count 1
Sep 23 09:36:09.645: INFO: netserver-0 from pod-network-test-3974 started at 2021-09-23 09:35:43 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.645: INFO: sonobuoy from sonobuoy started at 2021-09-23 08:25:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 23 09:36:09.645: INFO: sonobuoy-e2e-job-57e5d9d7846a4ccc from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container e2e ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:36:09.645: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-ccvkd from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 23 09:36:09.645: INFO: ss2-0 from statefulset-7832 started at 2021-09-23 08:23:22 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.645: INFO: ss2-1 from statefulset-7832 started at 2021-09-23 08:47:47 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.645: INFO: ss2-2 from statefulset-7832 started at 2021-09-23 08:47:50 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.645: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.645: INFO: 
Logging pods the apiserver thinks is on node worker-s002 before test
Sep 23 09:36:09.650: INFO: calico-node-f5cnk from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container calico-node ready: true, restart count 0
Sep 23 09:36:09.650: INFO: csi-qingcloud-node-ll5nv from kube-system started at 2021-09-23 08:48:06 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container csi-qingcloud ready: true, restart count 0
Sep 23 09:36:09.650: INFO: 	Container node-registrar ready: true, restart count 0
Sep 23 09:36:09.650: INFO: kube-proxy-fn894 from kube-system started at 2021-09-23 03:57:23 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 23 09:36:09.650: INFO: node-exporter-6n95k from kubesphere-monitoring-system started at 2021-09-23 04:01:33 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep 23 09:36:09.650: INFO: 	Container node-exporter ready: true, restart count 0
Sep 23 09:36:09.650: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2021-09-23 08:48:23 +0000 UTC (3 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container prometheus ready: true, restart count 1
Sep 23 09:36:09.650: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Sep 23 09:36:09.650: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Sep 23 09:36:09.650: INFO: host-test-container-pod from pod-network-test-3974 started at 2021-09-23 09:36:01 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container agnhost-container ready: true, restart count 0
Sep 23 09:36:09.650: INFO: netserver-1 from pod-network-test-3974 started at 2021-09-23 09:35:43 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.650: INFO: test-container-pod from pod-network-test-3974 started at 2021-09-23 09:36:01 +0000 UTC (1 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container webserver ready: true, restart count 0
Sep 23 09:36:09.650: INFO: sonobuoy-systemd-logs-daemon-set-eab337e0387548f3-q9ll8 from sonobuoy started at 2021-09-23 08:25:24 +0000 UTC (2 container statuses recorded)
Sep 23 09:36:09.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 23 09:36:09.650: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c9d8ae2a-b4b7-4c54-8566-143bcac8f075 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c9d8ae2a-b4b7-4c54-8566-143bcac8f075 off the node worker-s002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c9d8ae2a-b4b7-4c54-8566-143bcac8f075
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:15.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-283" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.100 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":236,"skipped":4100,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:15.706: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:36:15.736: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 23 09:36:19.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-3000 --namespace=crd-publish-openapi-3000 create -f -'
Sep 23 09:36:20.428: INFO: stderr: ""
Sep 23 09:36:20.428: INFO: stdout: "e2e-test-crd-publish-openapi-5944-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 23 09:36:20.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-3000 --namespace=crd-publish-openapi-3000 delete e2e-test-crd-publish-openapi-5944-crds test-cr'
Sep 23 09:36:20.493: INFO: stderr: ""
Sep 23 09:36:20.493: INFO: stdout: "e2e-test-crd-publish-openapi-5944-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 23 09:36:20.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-3000 --namespace=crd-publish-openapi-3000 apply -f -'
Sep 23 09:36:20.706: INFO: stderr: ""
Sep 23 09:36:20.706: INFO: stdout: "e2e-test-crd-publish-openapi-5944-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 23 09:36:20.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-3000 --namespace=crd-publish-openapi-3000 delete e2e-test-crd-publish-openapi-5944-crds test-cr'
Sep 23 09:36:20.774: INFO: stderr: ""
Sep 23 09:36:20.774: INFO: stdout: "e2e-test-crd-publish-openapi-5944-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 23 09:36:20.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-3000 explain e2e-test-crd-publish-openapi-5944-crds'
Sep 23 09:36:20.964: INFO: stderr: ""
Sep 23 09:36:20.964: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5944-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:25.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3000" for this suite.

• [SLOW TEST:10.031 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":237,"skipped":4110,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-512e8257-2f37-4d24-86ba-2dac7b718150
STEP: Creating a pod to test consume secrets
Sep 23 09:36:25.775: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272" in namespace "projected-9575" to be "Succeeded or Failed"
Sep 23 09:36:25.777: INFO: Pod "pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272": Phase="Pending", Reason="", readiness=false. Elapsed: 1.411421ms
Sep 23 09:36:27.779: INFO: Pod "pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003988174s
STEP: Saw pod success
Sep 23 09:36:27.779: INFO: Pod "pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272" satisfied condition "Succeeded or Failed"
Sep 23 09:36:27.781: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:36:27.793: INFO: Waiting for pod pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272 to disappear
Sep 23 09:36:27.795: INFO: Pod pod-projected-secrets-294b6679-ade8-4b11-9727-9b9153df5272 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:27.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9575" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4114,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:27.799: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-2b3efa30-9a23-4247-a5ae-f2039f91dbab
STEP: Creating a pod to test consume secrets
Sep 23 09:36:27.830: INFO: Waiting up to 5m0s for pod "pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b" in namespace "secrets-7508" to be "Succeeded or Failed"
Sep 23 09:36:27.832: INFO: Pod "pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470794ms
Sep 23 09:36:29.836: INFO: Pod "pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006152351s
Sep 23 09:36:31.839: INFO: Pod "pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009410403s
STEP: Saw pod success
Sep 23 09:36:31.839: INFO: Pod "pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b" satisfied condition "Succeeded or Failed"
Sep 23 09:36:31.841: INFO: Trying to get logs from node worker-s002 pod pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:36:31.852: INFO: Waiting for pod pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b to disappear
Sep 23 09:36:31.854: INFO: Pod pod-secrets-107a6f7e-6aea-46d6-971f-a9ff10ad844b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:31.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7508" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":4121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:31.859: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:36:31.898: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 23 09:36:31.901: INFO: Number of nodes with available pods: 0
Sep 23 09:36:31.901: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 23 09:36:31.916: INFO: Number of nodes with available pods: 0
Sep 23 09:36:31.916: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:32.918: INFO: Number of nodes with available pods: 0
Sep 23 09:36:32.918: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:33.920: INFO: Number of nodes with available pods: 0
Sep 23 09:36:33.920: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:34.919: INFO: Number of nodes with available pods: 1
Sep 23 09:36:34.919: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 23 09:36:34.942: INFO: Number of nodes with available pods: 1
Sep 23 09:36:34.942: INFO: Number of running nodes: 0, number of available pods: 1
Sep 23 09:36:35.950: INFO: Number of nodes with available pods: 0
Sep 23 09:36:35.950: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 23 09:36:35.954: INFO: Number of nodes with available pods: 0
Sep 23 09:36:35.954: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:36.956: INFO: Number of nodes with available pods: 0
Sep 23 09:36:36.956: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:37.957: INFO: Number of nodes with available pods: 0
Sep 23 09:36:37.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:38.958: INFO: Number of nodes with available pods: 0
Sep 23 09:36:38.958: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:39.958: INFO: Number of nodes with available pods: 0
Sep 23 09:36:39.958: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:40.957: INFO: Number of nodes with available pods: 0
Sep 23 09:36:40.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:41.957: INFO: Number of nodes with available pods: 0
Sep 23 09:36:41.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:42.957: INFO: Number of nodes with available pods: 0
Sep 23 09:36:42.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:43.957: INFO: Number of nodes with available pods: 0
Sep 23 09:36:43.957: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 09:36:44.957: INFO: Number of nodes with available pods: 1
Sep 23 09:36:44.957: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4301, will wait for the garbage collector to delete the pods
Sep 23 09:36:45.015: INFO: Deleting DaemonSet.extensions daemon-set took: 3.709727ms
Sep 23 09:36:46.516: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.500158132s
Sep 23 09:36:48.221: INFO: Number of nodes with available pods: 0
Sep 23 09:36:48.221: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 09:36:48.224: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"99322"},"items":null}

Sep 23 09:36:48.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"99322"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:48.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4301" for this suite.

• [SLOW TEST:16.388 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":240,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:48.247: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:48.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6099" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":241,"skipped":4168,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:48.300: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Sep 23 09:36:48.327: INFO: Waiting up to 5m0s for pod "var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c" in namespace "var-expansion-4707" to be "Succeeded or Failed"
Sep 23 09:36:48.328: INFO: Pod "var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.502836ms
Sep 23 09:36:50.334: INFO: Pod "var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006965923s
STEP: Saw pod success
Sep 23 09:36:50.334: INFO: Pod "var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c" satisfied condition "Succeeded or Failed"
Sep 23 09:36:50.335: INFO: Trying to get logs from node worker-s002 pod var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c container dapi-container: <nil>
STEP: delete the pod
Sep 23 09:36:50.348: INFO: Waiting for pod var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c to disappear
Sep 23 09:36:50.351: INFO: Pod var-expansion-6bb41766-5786-410c-ab8e-c93e04bc076c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:36:50.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4707" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4170,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:36:50.358: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8942
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-8942
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8942
Sep 23 09:36:50.400: INFO: Found 0 stateful pods, waiting for 1
Sep 23 09:37:00.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 23 09:37:00.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 09:37:00.555: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 09:37:00.555: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 09:37:00.555: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 09:37:00.558: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 23 09:37:10.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:37:10.562: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:37:10.582: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:10.582: INFO: ss-0  worker-s002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:10.582: INFO: 
Sep 23 09:37:10.582: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 23 09:37:11.585: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996398202s
Sep 23 09:37:12.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992761609s
Sep 23 09:37:13.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990370372s
Sep 23 09:37:14.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986746506s
Sep 23 09:37:15.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982107151s
Sep 23 09:37:16.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978472548s
Sep 23 09:37:17.610: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973356722s
Sep 23 09:37:18.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96831669s
Sep 23 09:37:19.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.663105ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8942
Sep 23 09:37:20.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:37:20.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 09:37:20.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 09:37:20.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 09:37:20.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:37:20.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 23 09:37:20.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 09:37:20.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 09:37:20.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:37:21.072: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 23 09:37:21.072: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 09:37:21.072: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 09:37:21.075: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 09:37:21.075: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 09:37:21.075: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 23 09:37:21.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 09:37:21.212: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 09:37:21.212: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 09:37:21.212: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 09:37:21.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 09:37:21.354: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 09:37:21.354: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 09:37:21.354: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 09:37:21.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 09:37:21.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 09:37:21.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 09:37:21.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 09:37:21.533: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:37:21.535: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 23 09:37:31.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:37:31.540: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:37:31.540: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 09:37:31.554: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:31.554: INFO: ss-0  worker-s002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:31.554: INFO: ss-1  worker-s001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:31.554: INFO: ss-2  worker-s002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:31.554: INFO: 
Sep 23 09:37:31.554: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:32.557: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:32.557: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:32.557: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:32.557: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:32.557: INFO: 
Sep 23 09:37:32.557: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:33.562: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:33.562: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:33.562: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:33.562: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:33.562: INFO: 
Sep 23 09:37:33.562: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:34.567: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:34.567: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:34.567: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:34.567: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:34.567: INFO: 
Sep 23 09:37:34.567: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:35.570: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:35.570: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:35.570: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:35.570: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:35.570: INFO: 
Sep 23 09:37:35.570: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:36.580: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:36.580: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:36.580: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:36.580: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:36.580: INFO: 
Sep 23 09:37:36.580: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:37.584: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:37.584: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:37.584: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:37.584: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:37.584: INFO: 
Sep 23 09:37:37.584: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:38.588: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:38.588: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:38.588: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:38.588: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:38.588: INFO: 
Sep 23 09:37:38.588: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:39.592: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:39.592: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:39.592: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:39.592: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:39.592: INFO: 
Sep 23 09:37:39.592: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 23 09:37:40.596: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Sep 23 09:37:40.596: INFO: ss-0  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:36:50 +0000 UTC  }]
Sep 23 09:37:40.596: INFO: ss-1  worker-s001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:40.596: INFO: ss-2  worker-s002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-23 09:37:10 +0000 UTC  }]
Sep 23 09:37:40.596: INFO: 
Sep 23 09:37:40.596: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8942
Sep 23 09:37:41.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:37:41.690: INFO: rc: 1
Sep 23 09:37:41.690: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 23 09:37:51.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:37:51.748: INFO: rc: 1
Sep 23 09:37:51.748: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:01.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:01.805: INFO: rc: 1
Sep 23 09:38:01.805: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:11.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:11.863: INFO: rc: 1
Sep 23 09:38:11.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:21.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:21.922: INFO: rc: 1
Sep 23 09:38:21.922: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:31.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:31.983: INFO: rc: 1
Sep 23 09:38:31.983: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:41.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:42.039: INFO: rc: 1
Sep 23 09:38:42.039: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:38:52.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:38:52.099: INFO: rc: 1
Sep 23 09:38:52.099: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:02.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:02.157: INFO: rc: 1
Sep 23 09:39:02.157: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:12.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:12.215: INFO: rc: 1
Sep 23 09:39:12.215: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:22.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:22.271: INFO: rc: 1
Sep 23 09:39:22.271: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:32.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:32.330: INFO: rc: 1
Sep 23 09:39:32.330: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:42.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:42.387: INFO: rc: 1
Sep 23 09:39:42.387: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:39:52.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:39:52.445: INFO: rc: 1
Sep 23 09:39:52.445: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:02.504: INFO: rc: 1
Sep 23 09:40:02.504: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:12.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:12.560: INFO: rc: 1
Sep 23 09:40:12.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:22.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:22.615: INFO: rc: 1
Sep 23 09:40:22.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:32.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:32.671: INFO: rc: 1
Sep 23 09:40:32.671: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:42.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:42.727: INFO: rc: 1
Sep 23 09:40:42.727: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:40:52.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:40:52.783: INFO: rc: 1
Sep 23 09:40:52.783: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:02.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:02.845: INFO: rc: 1
Sep 23 09:41:02.845: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:12.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:12.902: INFO: rc: 1
Sep 23 09:41:12.902: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:22.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:22.958: INFO: rc: 1
Sep 23 09:41:22.958: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:32.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:33.017: INFO: rc: 1
Sep 23 09:41:33.017: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:43.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:43.072: INFO: rc: 1
Sep 23 09:41:43.072: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:41:53.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:41:53.131: INFO: rc: 1
Sep 23 09:41:53.131: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:42:03.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:42:03.191: INFO: rc: 1
Sep 23 09:42:03.191: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:42:13.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:42:13.259: INFO: rc: 1
Sep 23 09:42:13.259: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:42:23.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:42:23.319: INFO: rc: 1
Sep 23 09:42:23.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:42:33.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:42:33.379: INFO: rc: 1
Sep 23 09:42:33.379: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep 23 09:42:43.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-8942 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 09:42:43.444: INFO: rc: 1
Sep 23 09:42:43.444: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep 23 09:42:43.444: INFO: Scaling statefulset ss to 0
Sep 23 09:42:43.451: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 09:42:43.452: INFO: Deleting all statefulset in ns statefulset-8942
Sep 23 09:42:43.453: INFO: Scaling statefulset ss to 0
Sep 23 09:42:43.458: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:42:43.459: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:42:43.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8942" for this suite.

• [SLOW TEST:353.117 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":243,"skipped":4172,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:42:43.475: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:42:54.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6522" for this suite.

• [SLOW TEST:11.057 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":244,"skipped":4183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:42:54.533: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 23 09:42:54.570: INFO: Waiting up to 5m0s for pod "pod-9b5d8685-344b-4063-b1c4-90075cf75a93" in namespace "emptydir-7024" to be "Succeeded or Failed"
Sep 23 09:42:54.572: INFO: Pod "pod-9b5d8685-344b-4063-b1c4-90075cf75a93": Phase="Pending", Reason="", readiness=false. Elapsed: 1.543361ms
Sep 23 09:42:56.574: INFO: Pod "pod-9b5d8685-344b-4063-b1c4-90075cf75a93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004207093s
STEP: Saw pod success
Sep 23 09:42:56.574: INFO: Pod "pod-9b5d8685-344b-4063-b1c4-90075cf75a93" satisfied condition "Succeeded or Failed"
Sep 23 09:42:56.576: INFO: Trying to get logs from node worker-s002 pod pod-9b5d8685-344b-4063-b1c4-90075cf75a93 container test-container: <nil>
STEP: delete the pod
Sep 23 09:42:56.606: INFO: Waiting for pod pod-9b5d8685-344b-4063-b1c4-90075cf75a93 to disappear
Sep 23 09:42:56.608: INFO: Pod pod-9b5d8685-344b-4063-b1c4-90075cf75a93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:42:56.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7024" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4212,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:42:56.613: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4940
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4940
STEP: Creating statefulset with conflicting port in namespace statefulset-4940
STEP: Waiting until pod test-pod will start running in namespace statefulset-4940
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4940
Sep 23 09:43:00.660: INFO: Observed stateful pod in namespace: statefulset-4940, name: ss-0, uid: 34f50341-3057-41fd-bf5d-3b848bb4de4f, status phase: Pending. Waiting for statefulset controller to delete.
Sep 23 09:43:00.961: INFO: Observed stateful pod in namespace: statefulset-4940, name: ss-0, uid: 34f50341-3057-41fd-bf5d-3b848bb4de4f, status phase: Failed. Waiting for statefulset controller to delete.
Sep 23 09:43:00.966: INFO: Observed stateful pod in namespace: statefulset-4940, name: ss-0, uid: 34f50341-3057-41fd-bf5d-3b848bb4de4f, status phase: Failed. Waiting for statefulset controller to delete.
Sep 23 09:43:00.970: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4940
STEP: Removing pod with conflicting port in namespace statefulset-4940
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4940 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 09:43:05.025: INFO: Deleting all statefulset in ns statefulset-4940
Sep 23 09:43:05.026: INFO: Scaling statefulset ss to 0
Sep 23 09:43:15.037: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 09:43:15.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:43:15.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4940" for this suite.

• [SLOW TEST:18.453 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":246,"skipped":4213,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:43:15.066: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3399
Sep 23 09:43:17.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 23 09:43:17.252: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 23 09:43:17.252: INFO: stdout: "ipvs"
Sep 23 09:43:17.252: INFO: proxyMode: ipvs
Sep 23 09:43:17.259: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 23 09:43:17.261: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3399
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3399
I0923 09:43:17.275562      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3399, replica count: 3
I0923 09:43:20.326217      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:43:20.332: INFO: Creating new exec pod
Sep 23 09:43:25.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Sep 23 09:43:25.507: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 23 09:43:25.507: INFO: stdout: ""
Sep 23 09:43:25.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c nc -zv -t -w 2 10.96.35.245 80'
Sep 23 09:43:25.655: INFO: stderr: "+ nc -zv -t -w 2 10.96.35.245 80\nConnection to 10.96.35.245 80 port [tcp/http] succeeded!\n"
Sep 23 09:43:25.655: INFO: stdout: ""
Sep 23 09:43:25.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.6 31324'
Sep 23 09:43:25.795: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.6 31324\nConnection to 172.16.0.6 31324 port [tcp/31324] succeeded!\n"
Sep 23 09:43:25.795: INFO: stdout: ""
Sep 23 09:43:25.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c nc -zv -t -w 2 172.16.0.7 31324'
Sep 23 09:43:25.927: INFO: stderr: "+ nc -zv -t -w 2 172.16.0.7 31324\nConnection to 172.16.0.7 31324 port [tcp/31324] succeeded!\n"
Sep 23 09:43:25.927: INFO: stdout: ""
Sep 23 09:43:25.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.0.6:31324/ ; done'
Sep 23 09:43:26.155: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n"
Sep 23 09:43:26.155: INFO: stdout: "\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h\naffinity-nodeport-timeout-b9l6h"
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Received response from host: affinity-nodeport-timeout-b9l6h
Sep 23 09:43:26.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.6:31324/'
Sep 23 09:43:26.293: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n"
Sep 23 09:43:26.293: INFO: stdout: "affinity-nodeport-timeout-b9l6h"
Sep 23 09:45:36.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3399 exec execpod-affinityd62sn -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.16.0.6:31324/'
Sep 23 09:45:36.454: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.16.0.6:31324/\n"
Sep 23 09:45:36.454: INFO: stdout: "affinity-nodeport-timeout-h7nsf"
Sep 23 09:45:36.454: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3399, will wait for the garbage collector to delete the pods
Sep 23 09:45:36.529: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 3.313931ms
Sep 23 09:45:38.030: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.500179432s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:45:54.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3399" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:159.189 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":247,"skipped":4221,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:45:54.255: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-2g2b
STEP: Creating a pod to test atomic-volume-subpath
Sep 23 09:45:54.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2g2b" in namespace "subpath-9946" to be "Succeeded or Failed"
Sep 23 09:45:54.304: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.857087ms
Sep 23 09:45:56.307: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00483312s
Sep 23 09:45:58.311: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 4.008262867s
Sep 23 09:46:00.314: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 6.011258114s
Sep 23 09:46:02.319: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 8.01603904s
Sep 23 09:46:04.322: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 10.019865176s
Sep 23 09:46:06.327: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 12.024488864s
Sep 23 09:46:08.330: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 14.027714648s
Sep 23 09:46:10.333: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 16.03055421s
Sep 23 09:46:12.338: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 18.035683809s
Sep 23 09:46:14.341: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 20.038569963s
Sep 23 09:46:16.346: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Running", Reason="", readiness=true. Elapsed: 22.043461157s
Sep 23 09:46:18.351: INFO: Pod "pod-subpath-test-configmap-2g2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048641287s
STEP: Saw pod success
Sep 23 09:46:18.351: INFO: Pod "pod-subpath-test-configmap-2g2b" satisfied condition "Succeeded or Failed"
Sep 23 09:46:18.353: INFO: Trying to get logs from node worker-s002 pod pod-subpath-test-configmap-2g2b container test-container-subpath-configmap-2g2b: <nil>
STEP: delete the pod
Sep 23 09:46:18.375: INFO: Waiting for pod pod-subpath-test-configmap-2g2b to disappear
Sep 23 09:46:18.378: INFO: Pod pod-subpath-test-configmap-2g2b no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2g2b
Sep 23 09:46:18.378: INFO: Deleting pod "pod-subpath-test-configmap-2g2b" in namespace "subpath-9946"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:46:18.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9946" for this suite.

• [SLOW TEST:24.131 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":248,"skipped":4221,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:46:18.386: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-3417/configmap-test-076d30bd-e2a3-4063-819c-d7dec8ba67dc
STEP: Creating a pod to test consume configMaps
Sep 23 09:46:18.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea" in namespace "configmap-3417" to be "Succeeded or Failed"
Sep 23 09:46:18.436: INFO: Pod "pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772017ms
Sep 23 09:46:20.440: INFO: Pod "pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00630822s
Sep 23 09:46:22.443: INFO: Pod "pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009903979s
STEP: Saw pod success
Sep 23 09:46:22.443: INFO: Pod "pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea" satisfied condition "Succeeded or Failed"
Sep 23 09:46:22.445: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea container env-test: <nil>
STEP: delete the pod
Sep 23 09:46:22.466: INFO: Waiting for pod pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea to disappear
Sep 23 09:46:22.467: INFO: Pod pod-configmaps-43903d8f-071f-4372-8744-2bd9347ad7ea no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:46:22.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3417" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:46:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-m2ctz in namespace proxy-1578
I0923 09:46:22.516326      25 runners.go:190] Created replication controller with name: proxy-service-m2ctz, namespace: proxy-1578, replica count: 1
I0923 09:46:23.566609      25 runners.go:190] proxy-service-m2ctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 09:46:24.566769      25 runners.go:190] proxy-service-m2ctz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0923 09:46:25.566961      25 runners.go:190] proxy-service-m2ctz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:46:26.567117      25 runners.go:190] proxy-service-m2ctz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0923 09:46:27.567295      25 runners.go:190] proxy-service-m2ctz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 09:46:27.570: INFO: setup took 5.067732794s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 23 09:46:27.578: INFO: (0) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 8.041705ms)
Sep 23 09:46:27.578: INFO: (0) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 7.988255ms)
Sep 23 09:46:27.578: INFO: (0) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 8.033384ms)
Sep 23 09:46:27.578: INFO: (0) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 8.083671ms)
Sep 23 09:46:27.578: INFO: (0) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 8.082001ms)
Sep 23 09:46:27.580: INFO: (0) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 9.943786ms)
Sep 23 09:46:27.580: INFO: (0) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 9.922235ms)
Sep 23 09:46:27.581: INFO: (0) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 10.891124ms)
Sep 23 09:46:27.581: INFO: (0) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 11.037036ms)
Sep 23 09:46:27.581: INFO: (0) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 10.965128ms)
Sep 23 09:46:27.584: INFO: (0) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 14.455791ms)
Sep 23 09:46:27.585: INFO: (0) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 15.320486ms)
Sep 23 09:46:27.585: INFO: (0) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 15.276026ms)
Sep 23 09:46:27.585: INFO: (0) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 15.301796ms)
Sep 23 09:46:27.589: INFO: (0) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 18.915759ms)
Sep 23 09:46:27.589: INFO: (0) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 18.923054ms)
Sep 23 09:46:27.593: INFO: (1) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.558675ms)
Sep 23 09:46:27.593: INFO: (1) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.077457ms)
Sep 23 09:46:27.593: INFO: (1) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 4.097274ms)
Sep 23 09:46:27.593: INFO: (1) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.325198ms)
Sep 23 09:46:27.594: INFO: (1) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 4.414788ms)
Sep 23 09:46:27.594: INFO: (1) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 4.63529ms)
Sep 23 09:46:27.594: INFO: (1) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 4.557695ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.675842ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 5.813967ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.987554ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 6.071624ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 5.992351ms)
Sep 23 09:46:27.595: INFO: (1) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 6.055235ms)
Sep 23 09:46:27.596: INFO: (1) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 6.560464ms)
Sep 23 09:46:27.596: INFO: (1) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 6.540393ms)
Sep 23 09:46:27.596: INFO: (1) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 6.804478ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 6.53619ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 6.609094ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 6.545031ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 7.007991ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 7.142264ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 7.117273ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 7.152223ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 7.236361ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 7.386823ms)
Sep 23 09:46:27.603: INFO: (2) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 7.399307ms)
Sep 23 09:46:27.604: INFO: (2) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 8.14624ms)
Sep 23 09:46:27.604: INFO: (2) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 8.349278ms)
Sep 23 09:46:27.604: INFO: (2) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.388125ms)
Sep 23 09:46:27.608: INFO: (2) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 11.634287ms)
Sep 23 09:46:27.608: INFO: (2) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 11.874939ms)
Sep 23 09:46:27.608: INFO: (2) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 12.183629ms)
Sep 23 09:46:27.611: INFO: (3) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 2.977026ms)
Sep 23 09:46:27.612: INFO: (3) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.183001ms)
Sep 23 09:46:27.612: INFO: (3) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.111343ms)
Sep 23 09:46:27.615: INFO: (3) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 6.463332ms)
Sep 23 09:46:27.617: INFO: (3) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 8.329581ms)
Sep 23 09:46:27.619: INFO: (3) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 10.401658ms)
Sep 23 09:46:27.621: INFO: (3) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 13.000941ms)
Sep 23 09:46:27.623: INFO: (3) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 14.371385ms)
Sep 23 09:46:27.626: INFO: (3) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 17.740833ms)
Sep 23 09:46:27.626: INFO: (3) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 17.929003ms)
Sep 23 09:46:27.626: INFO: (3) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 18.061318ms)
Sep 23 09:46:27.628: INFO: (3) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 19.479824ms)
Sep 23 09:46:27.628: INFO: (3) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 19.634172ms)
Sep 23 09:46:27.628: INFO: (3) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 19.898292ms)
Sep 23 09:46:27.628: INFO: (3) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 19.856631ms)
Sep 23 09:46:27.630: INFO: (3) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 21.898912ms)
Sep 23 09:46:27.634: INFO: (4) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.769515ms)
Sep 23 09:46:27.634: INFO: (4) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 3.833513ms)
Sep 23 09:46:27.634: INFO: (4) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 4.107697ms)
Sep 23 09:46:27.635: INFO: (4) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.712888ms)
Sep 23 09:46:27.635: INFO: (4) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.117461ms)
Sep 23 09:46:27.635: INFO: (4) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.147295ms)
Sep 23 09:46:27.636: INFO: (4) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 5.18321ms)
Sep 23 09:46:27.636: INFO: (4) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.239948ms)
Sep 23 09:46:27.636: INFO: (4) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 5.220392ms)
Sep 23 09:46:27.636: INFO: (4) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 5.236244ms)
Sep 23 09:46:27.638: INFO: (4) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.060039ms)
Sep 23 09:46:27.639: INFO: (4) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 8.243424ms)
Sep 23 09:46:27.639: INFO: (4) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 8.468176ms)
Sep 23 09:46:27.639: INFO: (4) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 8.587834ms)
Sep 23 09:46:27.643: INFO: (4) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 12.902989ms)
Sep 23 09:46:27.643: INFO: (4) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 13.052896ms)
Sep 23 09:46:27.649: INFO: (5) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.728365ms)
Sep 23 09:46:27.650: INFO: (5) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 6.022349ms)
Sep 23 09:46:27.650: INFO: (5) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 6.05175ms)
Sep 23 09:46:27.650: INFO: (5) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 6.574706ms)
Sep 23 09:46:27.650: INFO: (5) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 6.596027ms)
Sep 23 09:46:27.650: INFO: (5) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 6.668088ms)
Sep 23 09:46:27.651: INFO: (5) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 7.838113ms)
Sep 23 09:46:27.651: INFO: (5) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 7.799686ms)
Sep 23 09:46:27.652: INFO: (5) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 8.226948ms)
Sep 23 09:46:27.652: INFO: (5) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 8.700783ms)
Sep 23 09:46:27.652: INFO: (5) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 8.880824ms)
Sep 23 09:46:27.652: INFO: (5) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 8.861331ms)
Sep 23 09:46:27.652: INFO: (5) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 8.973173ms)
Sep 23 09:46:27.653: INFO: (5) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 9.424646ms)
Sep 23 09:46:27.653: INFO: (5) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 9.415121ms)
Sep 23 09:46:27.653: INFO: (5) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 9.899177ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.069335ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 4.31754ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.44407ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.589954ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 4.660275ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 4.78568ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.812309ms)
Sep 23 09:46:27.658: INFO: (6) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 4.851264ms)
Sep 23 09:46:27.659: INFO: (6) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.984077ms)
Sep 23 09:46:27.659: INFO: (6) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.971441ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 8.439134ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 8.383974ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 8.459998ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 8.296754ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 8.447667ms)
Sep 23 09:46:27.662: INFO: (6) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.507942ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 10.451292ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 10.838814ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 10.845575ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 10.885978ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 11.048497ms)
Sep 23 09:46:27.673: INFO: (7) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 11.292786ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 11.317418ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 11.464806ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 11.505593ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 11.936016ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 12.01776ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 12.128792ms)
Sep 23 09:46:27.674: INFO: (7) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 12.289664ms)
Sep 23 09:46:27.675: INFO: (7) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 12.350582ms)
Sep 23 09:46:27.675: INFO: (7) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 12.486204ms)
Sep 23 09:46:27.675: INFO: (7) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 12.660351ms)
Sep 23 09:46:27.676: INFO: (8) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 1.364334ms)
Sep 23 09:46:27.678: INFO: (8) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.53632ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.63567ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 4.958307ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 4.899325ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.948934ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.173543ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.15824ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 5.205228ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 5.287008ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.528738ms)
Sep 23 09:46:27.680: INFO: (8) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.471276ms)
Sep 23 09:46:27.681: INFO: (8) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 5.840173ms)
Sep 23 09:46:27.681: INFO: (8) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.795378ms)
Sep 23 09:46:27.681: INFO: (8) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 6.541442ms)
Sep 23 09:46:27.682: INFO: (8) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 6.814097ms)
Sep 23 09:46:27.684: INFO: (9) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 2.536713ms)
Sep 23 09:46:27.685: INFO: (9) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 3.021642ms)
Sep 23 09:46:27.686: INFO: (9) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.156504ms)
Sep 23 09:46:27.686: INFO: (9) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 4.169578ms)
Sep 23 09:46:27.687: INFO: (9) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.665598ms)
Sep 23 09:46:27.687: INFO: (9) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 4.667357ms)
Sep 23 09:46:27.687: INFO: (9) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.783595ms)
Sep 23 09:46:27.687: INFO: (9) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 5.299746ms)
Sep 23 09:46:27.687: INFO: (9) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.441675ms)
Sep 23 09:46:27.688: INFO: (9) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 6.113086ms)
Sep 23 09:46:27.688: INFO: (9) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 6.167256ms)
Sep 23 09:46:27.688: INFO: (9) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 6.336609ms)
Sep 23 09:46:27.688: INFO: (9) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 6.485663ms)
Sep 23 09:46:27.689: INFO: (9) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 6.710694ms)
Sep 23 09:46:27.689: INFO: (9) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 6.62661ms)
Sep 23 09:46:27.689: INFO: (9) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 6.698602ms)
Sep 23 09:46:27.692: INFO: (10) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 3.501799ms)
Sep 23 09:46:27.692: INFO: (10) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 3.712384ms)
Sep 23 09:46:27.692: INFO: (10) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 3.645909ms)
Sep 23 09:46:27.692: INFO: (10) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 3.768147ms)
Sep 23 09:46:27.693: INFO: (10) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.790652ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 5.189859ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 5.254173ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.400868ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.404266ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.428372ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 5.540771ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.721275ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 5.785059ms)
Sep 23 09:46:27.694: INFO: (10) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.790791ms)
Sep 23 09:46:27.695: INFO: (10) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.795699ms)
Sep 23 09:46:27.697: INFO: (10) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.359544ms)
Sep 23 09:46:27.700: INFO: (11) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 2.509229ms)
Sep 23 09:46:27.700: INFO: (11) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 2.812149ms)
Sep 23 09:46:27.700: INFO: (11) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.12828ms)
Sep 23 09:46:27.700: INFO: (11) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 3.109416ms)
Sep 23 09:46:27.701: INFO: (11) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 3.63782ms)
Sep 23 09:46:27.701: INFO: (11) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.78356ms)
Sep 23 09:46:27.701: INFO: (11) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.852194ms)
Sep 23 09:46:27.701: INFO: (11) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.923778ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.370616ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 5.526716ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.842637ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 5.831061ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 5.968879ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.889028ms)
Sep 23 09:46:27.703: INFO: (11) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 5.945447ms)
Sep 23 09:46:27.706: INFO: (11) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.474468ms)
Sep 23 09:46:27.709: INFO: (12) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 3.082148ms)
Sep 23 09:46:27.709: INFO: (12) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.154365ms)
Sep 23 09:46:27.711: INFO: (12) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 5.609726ms)
Sep 23 09:46:27.711: INFO: (12) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 5.661246ms)
Sep 23 09:46:27.711: INFO: (12) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.641606ms)
Sep 23 09:46:27.711: INFO: (12) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 5.72398ms)
Sep 23 09:46:27.712: INFO: (12) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 5.79983ms)
Sep 23 09:46:27.712: INFO: (12) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.803092ms)
Sep 23 09:46:27.712: INFO: (12) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.900696ms)
Sep 23 09:46:27.712: INFO: (12) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.921804ms)
Sep 23 09:46:27.713: INFO: (12) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 7.346331ms)
Sep 23 09:46:27.713: INFO: (12) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 7.523749ms)
Sep 23 09:46:27.713: INFO: (12) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 7.619221ms)
Sep 23 09:46:27.713: INFO: (12) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 7.707005ms)
Sep 23 09:46:27.714: INFO: (12) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 7.909179ms)
Sep 23 09:46:27.714: INFO: (12) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 7.926165ms)
Sep 23 09:46:27.716: INFO: (13) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 2.541354ms)
Sep 23 09:46:27.718: INFO: (13) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.729862ms)
Sep 23 09:46:27.718: INFO: (13) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 4.376037ms)
Sep 23 09:46:27.718: INFO: (13) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 4.274171ms)
Sep 23 09:46:27.718: INFO: (13) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.238622ms)
Sep 23 09:46:27.718: INFO: (13) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 4.614475ms)
Sep 23 09:46:27.719: INFO: (13) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 5.348707ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 7.263962ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 7.4238ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 7.486711ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 7.330702ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 7.359897ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 7.357415ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 7.391983ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 7.618213ms)
Sep 23 09:46:27.721: INFO: (13) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 7.493174ms)
Sep 23 09:46:27.723: INFO: (14) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 1.901616ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 3.682845ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.643717ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.620647ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.841887ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 3.851091ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 3.823017ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 3.793547ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 3.879153ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.814885ms)
Sep 23 09:46:27.725: INFO: (14) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.796752ms)
Sep 23 09:46:27.726: INFO: (14) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 4.718592ms)
Sep 23 09:46:27.726: INFO: (14) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 4.678975ms)
Sep 23 09:46:27.726: INFO: (14) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 4.789877ms)
Sep 23 09:46:27.726: INFO: (14) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 4.777455ms)
Sep 23 09:46:27.726: INFO: (14) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 4.826305ms)
Sep 23 09:46:27.729: INFO: (15) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 2.347314ms)
Sep 23 09:46:27.729: INFO: (15) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 2.481154ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.598291ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.484152ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 3.510253ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.589414ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 3.629104ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 3.913436ms)
Sep 23 09:46:27.730: INFO: (15) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.003035ms)
Sep 23 09:46:27.731: INFO: (15) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.322142ms)
Sep 23 09:46:27.731: INFO: (15) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.391264ms)
Sep 23 09:46:27.731: INFO: (15) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 4.525179ms)
Sep 23 09:46:27.732: INFO: (15) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.164183ms)
Sep 23 09:46:27.732: INFO: (15) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 5.171349ms)
Sep 23 09:46:27.732: INFO: (15) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 5.178033ms)
Sep 23 09:46:27.732: INFO: (15) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.122579ms)
Sep 23 09:46:27.736: INFO: (16) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.326699ms)
Sep 23 09:46:27.736: INFO: (16) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.233607ms)
Sep 23 09:46:27.736: INFO: (16) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.477877ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 4.826255ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 4.947343ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 5.163201ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.310353ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 5.445856ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 5.462084ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 5.466186ms)
Sep 23 09:46:27.737: INFO: (16) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 5.766403ms)
Sep 23 09:46:27.738: INFO: (16) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 5.989686ms)
Sep 23 09:46:27.738: INFO: (16) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 5.927597ms)
Sep 23 09:46:27.738: INFO: (16) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 6.085794ms)
Sep 23 09:46:27.738: INFO: (16) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 6.153246ms)
Sep 23 09:46:27.738: INFO: (16) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 6.208802ms)
Sep 23 09:46:27.742: INFO: (17) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.570026ms)
Sep 23 09:46:27.742: INFO: (17) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.554058ms)
Sep 23 09:46:27.742: INFO: (17) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.450907ms)
Sep 23 09:46:27.743: INFO: (17) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 4.572843ms)
Sep 23 09:46:27.743: INFO: (17) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 4.492792ms)
Sep 23 09:46:27.744: INFO: (17) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 6.146034ms)
Sep 23 09:46:27.744: INFO: (17) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 6.238013ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 6.722547ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 6.638716ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 6.893265ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 6.981876ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 6.927852ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 6.91925ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 7.0442ms)
Sep 23 09:46:27.745: INFO: (17) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 6.945928ms)
Sep 23 09:46:27.748: INFO: (17) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 10.448189ms)
Sep 23 09:46:27.752: INFO: (18) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.939195ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 4.053247ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 4.068951ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 4.216167ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 4.484687ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.532331ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 4.709641ms)
Sep 23 09:46:27.753: INFO: (18) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.736141ms)
Sep 23 09:46:27.754: INFO: (18) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 5.040239ms)
Sep 23 09:46:27.754: INFO: (18) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 5.079703ms)
Sep 23 09:46:27.754: INFO: (18) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 5.1035ms)
Sep 23 09:46:27.754: INFO: (18) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 5.036503ms)
Sep 23 09:46:27.757: INFO: (18) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 8.510888ms)
Sep 23 09:46:27.757: INFO: (18) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 8.510138ms)
Sep 23 09:46:27.757: INFO: (18) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 8.694002ms)
Sep 23 09:46:27.760: INFO: (18) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 10.990545ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:460/proxy/: tls baz (200; 3.011932ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">test<... (200; 3.197692ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:1080/proxy/rewriteme">... (200; 3.548837ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.476517ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:443/proxy/tlsrewritem... (200; 3.51086ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/pods/http:proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 3.717723ms)
Sep 23 09:46:27.763: INFO: (19) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname1/proxy/: foo (200; 3.787602ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:160/proxy/: foo (200; 3.885702ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/pods/https:proxy-service-m2ctz-ncvjd:462/proxy/: tls qux (200; 3.902425ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/services/proxy-service-m2ctz:portname2/proxy/: bar (200; 3.927608ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname2/proxy/: tls qux (200; 3.970645ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/: <a href="/api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd/proxy/rewriteme">test</a> (200; 4.183638ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/pods/proxy-service-m2ctz-ncvjd:162/proxy/: bar (200; 4.089649ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname1/proxy/: foo (200; 4.7929ms)
Sep 23 09:46:27.764: INFO: (19) /api/v1/namespaces/proxy-1578/services/https:proxy-service-m2ctz:tlsportname1/proxy/: tls baz (200; 4.754356ms)
Sep 23 09:46:27.765: INFO: (19) /api/v1/namespaces/proxy-1578/services/http:proxy-service-m2ctz:portname2/proxy/: bar (200; 4.990455ms)
STEP: deleting ReplicationController proxy-service-m2ctz in namespace proxy-1578, will wait for the garbage collector to delete the pods
Sep 23 09:46:27.819: INFO: Deleting ReplicationController proxy-service-m2ctz took: 2.715084ms
Sep 23 09:46:29.319: INFO: Terminating ReplicationController proxy-service-m2ctz pods took: 1.500153464s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:46:34.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1578" for this suite.

• [SLOW TEST:11.754 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":250,"skipped":4246,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:46:34.225: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-397b2998-df01-41b2-a503-70345c115028 in namespace container-probe-870
Sep 23 09:46:36.294: INFO: Started pod liveness-397b2998-df01-41b2-a503-70345c115028 in namespace container-probe-870
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:46:36.296: INFO: Initial restart count of pod liveness-397b2998-df01-41b2-a503-70345c115028 is 0
Sep 23 09:46:52.326: INFO: Restart count of pod container-probe-870/liveness-397b2998-df01-41b2-a503-70345c115028 is now 1 (16.030377168s elapsed)
Sep 23 09:47:12.368: INFO: Restart count of pod container-probe-870/liveness-397b2998-df01-41b2-a503-70345c115028 is now 2 (36.071892706s elapsed)
Sep 23 09:47:32.416: INFO: Restart count of pod container-probe-870/liveness-397b2998-df01-41b2-a503-70345c115028 is now 3 (56.120097315s elapsed)
Sep 23 09:47:52.457: INFO: Restart count of pod container-probe-870/liveness-397b2998-df01-41b2-a503-70345c115028 is now 4 (1m16.160753383s elapsed)
Sep 23 09:49:00.574: INFO: Restart count of pod container-probe-870/liveness-397b2998-df01-41b2-a503-70345c115028 is now 5 (2m24.277906898s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:49:00.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-870" for this suite.

• [SLOW TEST:146.365 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":251,"skipped":4251,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:49:00.590: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 09:49:00.794: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 09:49:03.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:49:03.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9979" for this suite.
STEP: Destroying namespace "webhook-9979-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":252,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:49:03.954: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:49:03.985: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7d78d208-9748-46fd-93b1-9d7096e58021" in namespace "security-context-test-2331" to be "Succeeded or Failed"
Sep 23 09:49:03.988: INFO: Pod "busybox-user-65534-7d78d208-9748-46fd-93b1-9d7096e58021": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930875ms
Sep 23 09:49:05.990: INFO: Pod "busybox-user-65534-7d78d208-9748-46fd-93b1-9d7096e58021": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005900033s
Sep 23 09:49:07.993: INFO: Pod "busybox-user-65534-7d78d208-9748-46fd-93b1-9d7096e58021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008344295s
Sep 23 09:49:07.993: INFO: Pod "busybox-user-65534-7d78d208-9748-46fd-93b1-9d7096e58021" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:49:07.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2331" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4281,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:49:07.998: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4288168d-c0c0-4212-87e2-924d417a39df in namespace container-probe-9950
Sep 23 09:49:10.035: INFO: Started pod liveness-4288168d-c0c0-4212-87e2-924d417a39df in namespace container-probe-9950
STEP: checking the pod's current state and verifying that restartCount is present
Sep 23 09:49:10.040: INFO: Initial restart count of pod liveness-4288168d-c0c0-4212-87e2-924d417a39df is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:10.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9950" for this suite.

• [SLOW TEST:242.565 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":254,"skipped":4285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:10.563: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-1b7a0a5c-e9db-49c7-9b2f-20e7b9c35374
STEP: Creating a pod to test consume secrets
Sep 23 09:53:10.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761" in namespace "projected-9205" to be "Succeeded or Failed"
Sep 23 09:53:10.631: INFO: Pod "pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904303ms
Sep 23 09:53:12.633: INFO: Pod "pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003731634s
Sep 23 09:53:14.646: INFO: Pod "pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016857103s
STEP: Saw pod success
Sep 23 09:53:14.646: INFO: Pod "pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761" satisfied condition "Succeeded or Failed"
Sep 23 09:53:14.647: INFO: Trying to get logs from node worker-s002 pod pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:53:14.671: INFO: Waiting for pod pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761 to disappear
Sep 23 09:53:14.673: INFO: Pod pod-projected-secrets-f5aa21d2-087e-427a-b1a7-ae969935b761 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:14.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9205" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4315,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:14.680: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 23 09:53:18.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 09:53:18.733: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 09:53:20.733: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 09:53:20.735: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 23 09:53:22.733: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 23 09:53:22.735: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:22.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9259" for this suite.

• [SLOW TEST:8.064 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:22.744: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 23 09:53:22.777: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 23 09:53:22.781: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 23 09:53:22.781: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 23 09:53:22.788: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 23 09:53:22.788: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 23 09:53:22.800: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 23 09:53:22.800: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 23 09:53:29.839: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:29.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8328" for this suite.

• [SLOW TEST:7.117 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":257,"skipped":4354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:29.861: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-2ef7320d-4239-4676-b376-d3eb79f376f1
STEP: Creating a pod to test consume configMaps
Sep 23 09:53:29.895: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe" in namespace "projected-9547" to be "Succeeded or Failed"
Sep 23 09:53:29.902: INFO: Pod "pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.978581ms
Sep 23 09:53:31.905: INFO: Pod "pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009806885s
STEP: Saw pod success
Sep 23 09:53:31.905: INFO: Pod "pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe" satisfied condition "Succeeded or Failed"
Sep 23 09:53:31.906: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:53:31.916: INFO: Waiting for pod pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe to disappear
Sep 23 09:53:31.918: INFO: Pod pod-projected-configmaps-b2665fa5-4865-400c-a77e-6df212ff7abe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:31.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9547" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":258,"skipped":4394,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:31.922: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1507
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 23 09:53:31.951: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 23 09:53:31.989: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 09:53:33.993: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 23 09:53:35.991: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:37.994: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:39.993: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:41.993: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:43.994: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:45.991: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 23 09:53:47.993: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 23 09:53:47.996: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Sep 23 09:53:52.017: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep 23 09:53:52.017: INFO: Breadth first check of 10.10.28.52 on host 172.16.0.6...
Sep 23 09:53:52.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.131.167:9080/dial?request=hostname&protocol=udp&host=10.10.28.52&port=8081&tries=1'] Namespace:pod-network-test-1507 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:53:52.020: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:53:52.108: INFO: Waiting for responses: map[]
Sep 23 09:53:52.108: INFO: reached 10.10.28.52 after 0/1 tries
Sep 23 09:53:52.108: INFO: Breadth first check of 10.10.131.164 on host 172.16.0.7...
Sep 23 09:53:52.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.131.167:9080/dial?request=hostname&protocol=udp&host=10.10.131.164&port=8081&tries=1'] Namespace:pod-network-test-1507 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 09:53:52.110: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 09:53:52.198: INFO: Waiting for responses: map[]
Sep 23 09:53:52.198: INFO: reached 10.10.131.164 after 0/1 tries
Sep 23 09:53:52.198: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:53:52.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1507" for this suite.

• [SLOW TEST:20.280 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:53:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:53:52.237: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08" in namespace "security-context-test-1285" to be "Succeeded or Failed"
Sep 23 09:53:52.238: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08": Phase="Pending", Reason="", readiness=false. Elapsed: 1.283704ms
Sep 23 09:53:54.241: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004073496s
Sep 23 09:53:56.243: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006252009s
Sep 23 09:53:58.251: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014307581s
Sep 23 09:54:00.255: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018364777s
Sep 23 09:54:00.255: INFO: Pod "alpine-nnp-false-0de2c39b-edf0-41eb-a539-652cc6482b08" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:54:00.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1285" for this suite.

• [SLOW TEST:8.068 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4439,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:54:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 23 09:54:04.343: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:04.347: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 09:54:06.347: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:06.351: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 09:54:08.347: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:08.350: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 09:54:10.347: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:10.351: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 09:54:12.347: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:12.351: INFO: Pod pod-with-poststart-http-hook still exists
Sep 23 09:54:14.347: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 23 09:54:14.353: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:54:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6117" for this suite.

• [SLOW TEST:14.086 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4444,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:54:14.358: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:54:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 23 09:54:19.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-9696 --namespace=crd-publish-openapi-9696 create -f -'
Sep 23 09:54:19.566: INFO: stderr: ""
Sep 23 09:54:19.566: INFO: stdout: "e2e-test-crd-publish-openapi-2275-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 23 09:54:19.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-9696 --namespace=crd-publish-openapi-9696 delete e2e-test-crd-publish-openapi-2275-crds test-cr'
Sep 23 09:54:19.626: INFO: stderr: ""
Sep 23 09:54:19.626: INFO: stdout: "e2e-test-crd-publish-openapi-2275-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 23 09:54:19.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-9696 --namespace=crd-publish-openapi-9696 apply -f -'
Sep 23 09:54:19.827: INFO: stderr: ""
Sep 23 09:54:19.827: INFO: stdout: "e2e-test-crd-publish-openapi-2275-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 23 09:54:19.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-9696 --namespace=crd-publish-openapi-9696 delete e2e-test-crd-publish-openapi-2275-crds test-cr'
Sep 23 09:54:19.884: INFO: stderr: ""
Sep 23 09:54:19.884: INFO: stdout: "e2e-test-crd-publish-openapi-2275-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 23 09:54:19.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-9696 explain e2e-test-crd-publish-openapi-2275-crds'
Sep 23 09:54:20.084: INFO: stderr: ""
Sep 23 09:54:20.084: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2275-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:54:24.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9696" for this suite.

• [SLOW TEST:10.529 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":262,"skipped":4465,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:54:24.887: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:54:24.919: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Creating first CR 
Sep 23 09:54:25.467: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:25Z]] name:name1 resourceVersion:104953 uid:b05fff95-1e25-4a2f-80bd-62e98eb5ec3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 23 09:54:35.471: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:35Z]] name:name2 resourceVersion:104997 uid:62ecd7ad-9309-490b-a976-c01d858c2b4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 23 09:54:45.476: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:45Z]] name:name1 resourceVersion:105035 uid:b05fff95-1e25-4a2f-80bd-62e98eb5ec3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 23 09:54:55.497: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:55Z]] name:name2 resourceVersion:105072 uid:62ecd7ad-9309-490b-a976-c01d858c2b4e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 23 09:55:05.502: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:45Z]] name:name1 resourceVersion:105113 uid:b05fff95-1e25-4a2f-80bd-62e98eb5ec3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 23 09:55:15.507: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-23T09:54:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-23T09:54:55Z]] name:name2 resourceVersion:105149 uid:62ecd7ad-9309-490b-a976-c01d858c2b4e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-8301" for this suite.

• [SLOW TEST:61.133 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":263,"skipped":4471,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:26.020: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:30.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4808" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":264,"skipped":4482,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:30.083: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 23 09:55:30.111: INFO: Waiting up to 5m0s for pod "pod-35970aba-d5a5-4246-bdff-1abfec324f84" in namespace "emptydir-9365" to be "Succeeded or Failed"
Sep 23 09:55:30.115: INFO: Pod "pod-35970aba-d5a5-4246-bdff-1abfec324f84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664083ms
Sep 23 09:55:32.119: INFO: Pod "pod-35970aba-d5a5-4246-bdff-1abfec324f84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007057153s
Sep 23 09:55:34.122: INFO: Pod "pod-35970aba-d5a5-4246-bdff-1abfec324f84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010149759s
STEP: Saw pod success
Sep 23 09:55:34.122: INFO: Pod "pod-35970aba-d5a5-4246-bdff-1abfec324f84" satisfied condition "Succeeded or Failed"
Sep 23 09:55:34.124: INFO: Trying to get logs from node worker-s002 pod pod-35970aba-d5a5-4246-bdff-1abfec324f84 container test-container: <nil>
STEP: delete the pod
Sep 23 09:55:34.153: INFO: Waiting for pod pod-35970aba-d5a5-4246-bdff-1abfec324f84 to disappear
Sep 23 09:55:34.154: INFO: Pod pod-35970aba-d5a5-4246-bdff-1abfec324f84 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:34.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9365" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:34.159: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-6104/configmap-test-f8946124-c1fb-4759-a1b3-55bde72ce244
STEP: Creating a pod to test consume configMaps
Sep 23 09:55:34.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f" in namespace "configmap-6104" to be "Succeeded or Failed"
Sep 23 09:55:34.190: INFO: Pod "pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31635ms
Sep 23 09:55:36.192: INFO: Pod "pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00304531s
Sep 23 09:55:38.196: INFO: Pod "pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006918293s
STEP: Saw pod success
Sep 23 09:55:38.196: INFO: Pod "pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f" satisfied condition "Succeeded or Failed"
Sep 23 09:55:38.197: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f container env-test: <nil>
STEP: delete the pod
Sep 23 09:55:38.216: INFO: Waiting for pod pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f to disappear
Sep 23 09:55:38.219: INFO: Pod pod-configmaps-2ce8b7d6-e567-4e5d-9fb3-af3acb5edd2f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:38.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6104" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4586,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:38.225: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 23 09:55:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-806 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Sep 23 09:55:38.317: INFO: stderr: ""
Sep 23 09:55:38.318: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Sep 23 09:55:38.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-806 delete pods e2e-test-httpd-pod'
Sep 23 09:55:44.185: INFO: stderr: ""
Sep 23 09:55:44.185: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-806" for this suite.

• [SLOW TEST:5.969 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":267,"skipped":4600,"failed":0}
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:44.194: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Sep 23 09:55:44.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-4123 api-versions'
Sep 23 09:55:44.280: INFO: stderr: ""
Sep 23 09:55:44.280: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.k8s.io/v1beta1\napplication.kubesphere.io/v1alpha1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.kubesphere.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndevops.kubesphere.io/v1alpha1\ndevops.kubesphere.io/v1alpha3\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\niam.kubesphere.io/v1alpha2\ninstaller.kubesphere.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.kubesphere.io/v1alpha1\nnetwork.kubesphere.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnotification.kubesphere.io/v2beta1\npolicy/v1beta1\nquota.kubesphere.io/v1alpha2\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nservicemesh.kubesphere.io/v1alpha2\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nstorage.kubesphere.io/v1alpha1\ntenant.kubesphere.io/v1alpha1\ntenant.kubesphere.io/v1alpha2\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:44.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4123" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":268,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:44.285: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:55:44.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa" in namespace "downward-api-4784" to be "Succeeded or Failed"
Sep 23 09:55:44.316: INFO: Pod "downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.408824ms
Sep 23 09:55:46.320: INFO: Pod "downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005402769s
Sep 23 09:55:48.323: INFO: Pod "downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008920161s
STEP: Saw pod success
Sep 23 09:55:48.323: INFO: Pod "downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa" satisfied condition "Succeeded or Failed"
Sep 23 09:55:48.325: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa container client-container: <nil>
STEP: delete the pod
Sep 23 09:55:48.337: INFO: Waiting for pod downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa to disappear
Sep 23 09:55:48.342: INFO: Pod downwardapi-volume-048474a4-fc4c-4b84-9984-5521e3a11ffa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:48.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4784" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:48.347: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Sep 23 09:55:50.391: INFO: Pod pod-hostip-29322d1e-4179-4d00-9d22-aae5b1e6918a has hostIP: 172.16.0.7
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:55:50.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8328" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4653,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:55:50.402: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:07.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2419" for this suite.

• [SLOW TEST:17.068 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":271,"skipped":4668,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:07.470: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-fbe082f1-1a9c-48f1-9abd-d2add9cdcf73
STEP: Creating a pod to test consume secrets
Sep 23 09:56:07.507: INFO: Waiting up to 5m0s for pod "pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9" in namespace "secrets-706" to be "Succeeded or Failed"
Sep 23 09:56:07.509: INFO: Pod "pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.497718ms
Sep 23 09:56:09.511: INFO: Pod "pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003732513s
STEP: Saw pod success
Sep 23 09:56:09.511: INFO: Pod "pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9" satisfied condition "Succeeded or Failed"
Sep 23 09:56:09.512: INFO: Trying to get logs from node worker-s002 pod pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9 container secret-volume-test: <nil>
STEP: delete the pod
Sep 23 09:56:09.523: INFO: Waiting for pod pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9 to disappear
Sep 23 09:56:09.524: INFO: Pod pod-secrets-5f5d3faf-b397-446a-a7c0-e3832eb746b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:09.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-706" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4692,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:09.529: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Sep 23 09:56:09.560: INFO: Waiting up to 5m0s for pod "test-pod-97338a39-1520-4d12-a984-1bb58ed1d997" in namespace "svcaccounts-9364" to be "Succeeded or Failed"
Sep 23 09:56:09.565: INFO: Pod "test-pod-97338a39-1520-4d12-a984-1bb58ed1d997": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238001ms
Sep 23 09:56:11.567: INFO: Pod "test-pod-97338a39-1520-4d12-a984-1bb58ed1d997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006417486s
STEP: Saw pod success
Sep 23 09:56:11.567: INFO: Pod "test-pod-97338a39-1520-4d12-a984-1bb58ed1d997" satisfied condition "Succeeded or Failed"
Sep 23 09:56:11.568: INFO: Trying to get logs from node worker-s002 pod test-pod-97338a39-1520-4d12-a984-1bb58ed1d997 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:56:11.594: INFO: Waiting for pod test-pod-97338a39-1520-4d12-a984-1bb58ed1d997 to disappear
Sep 23 09:56:11.597: INFO: Pod test-pod-97338a39-1520-4d12-a984-1bb58ed1d997 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:11.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9364" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":273,"skipped":4698,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:11.601: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-4885fb24-174d-4308-a04f-a5270d461c5b
STEP: Creating a pod to test consume configMaps
Sep 23 09:56:11.636: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b" in namespace "configmap-1017" to be "Succeeded or Failed"
Sep 23 09:56:11.646: INFO: Pod "pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.240764ms
Sep 23 09:56:13.649: INFO: Pod "pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012772393s
Sep 23 09:56:15.655: INFO: Pod "pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019246778s
STEP: Saw pod success
Sep 23 09:56:15.655: INFO: Pod "pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b" satisfied condition "Succeeded or Failed"
Sep 23 09:56:15.657: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b container agnhost-container: <nil>
STEP: delete the pod
Sep 23 09:56:15.670: INFO: Waiting for pod pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b to disappear
Sep 23 09:56:15.671: INFO: Pod pod-configmaps-6a41c45b-5a11-4b0d-a7be-bcd320af874b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:15.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1017" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4707,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:15.675: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Sep 23 09:56:15.703: INFO: created test-pod-1
Sep 23 09:56:15.720: INFO: created test-pod-2
Sep 23 09:56:15.727: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:15.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6166" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":275,"skipped":4711,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:15.769: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:56:15.818: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 23 09:56:20.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-4658 --namespace=crd-publish-openapi-4658 create -f -'
Sep 23 09:56:20.530: INFO: stderr: ""
Sep 23 09:56:20.530: INFO: stdout: "e2e-test-crd-publish-openapi-563-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 23 09:56:20.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-4658 --namespace=crd-publish-openapi-4658 delete e2e-test-crd-publish-openapi-563-crds test-cr'
Sep 23 09:56:20.592: INFO: stderr: ""
Sep 23 09:56:20.592: INFO: stdout: "e2e-test-crd-publish-openapi-563-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 23 09:56:20.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-4658 --namespace=crd-publish-openapi-4658 apply -f -'
Sep 23 09:56:20.790: INFO: stderr: ""
Sep 23 09:56:20.790: INFO: stdout: "e2e-test-crd-publish-openapi-563-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 23 09:56:20.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-4658 --namespace=crd-publish-openapi-4658 delete e2e-test-crd-publish-openapi-563-crds test-cr'
Sep 23 09:56:20.861: INFO: stderr: ""
Sep 23 09:56:20.861: INFO: stdout: "e2e-test-crd-publish-openapi-563-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 23 09:56:20.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=crd-publish-openapi-4658 explain e2e-test-crd-publish-openapi-563-crds'
Sep 23 09:56:21.065: INFO: stderr: ""
Sep 23 09:56:21.065: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-563-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:25.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4658" for this suite.

• [SLOW TEST:10.058 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":276,"skipped":4731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:25.827: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Sep 23 09:56:25.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-55 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep 23 09:56:25.942: INFO: stderr: ""
Sep 23 09:56:25.942: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 23 09:56:30.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-55 get pod e2e-test-httpd-pod -o json'
Sep 23 09:56:31.047: INFO: stderr: ""
Sep 23 09:56:31.047: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.10.131.177/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.10.131.177/32\"\n        },\n        \"creationTimestamp\": \"2021-09-23T09:56:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-23T09:56:25Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-23T09:56:26Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.10.131.177\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-09-23T09:56:28Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-55\",\n        \"resourceVersion\": \"105883\",\n        \"uid\": \"8a2b842f-63d4-40ef-a4a0-a2ca6c07f936\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-f67rq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-s002\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-f67rq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-f67rq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-23T09:56:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-23T09:56:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-23T09:56:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-23T09:56:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6bcc97308b59dffdecc7c2e2b2530de7dc3934e763912d59dde95d59adec2192\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-23T09:56:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.0.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.131.177\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.131.177\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-23T09:56:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 23 09:56:31.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-55 replace -f -'
Sep 23 09:56:31.320: INFO: stderr: ""
Sep 23 09:56:31.320: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Sep 23 09:56:31.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-55 delete pods e2e-test-httpd-pod'
Sep 23 09:56:44.180: INFO: stderr: ""
Sep 23 09:56:44.180: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:44.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-55" for this suite.

• [SLOW TEST:18.358 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":277,"skipped":4758,"failed":0}
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:44.185: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-2046
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2046
STEP: Deleting pre-stop pod
Sep 23 09:56:53.235: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:53.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2046" for this suite.

• [SLOW TEST:9.072 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":278,"skipped":4759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:53.258: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 09:56:53.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b" in namespace "projected-315" to be "Succeeded or Failed"
Sep 23 09:56:53.306: INFO: Pod "downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041769ms
Sep 23 09:56:55.313: INFO: Pod "downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009234342s
STEP: Saw pod success
Sep 23 09:56:55.313: INFO: Pod "downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b" satisfied condition "Succeeded or Failed"
Sep 23 09:56:55.315: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b container client-container: <nil>
STEP: delete the pod
Sep 23 09:56:55.331: INFO: Waiting for pod downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b to disappear
Sep 23 09:56:55.333: INFO: Pod downwardapi-volume-d02578ff-da31-4d01-8de1-0d34356ee93b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:56:55.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-315" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":279,"skipped":4843,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:56:55.337: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 23 09:56:55.396: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 23 09:57:55.439: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:57:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 23 09:57:59.503: INFO: found a healthy node: worker-s002
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 09:58:17.556: INFO: pods created so far: [1 1 1]
Sep 23 09:58:17.556: INFO: length of pods created so far: 3
Sep 23 09:58:29.565: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:58:36.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2149" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:58:36.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9999" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:101.298 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":280,"skipped":4844,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:58:36.635: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 09:59:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1567" for this suite.

• [SLOW TEST:60.038 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4854,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 09:59:36.674: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep 23 09:59:36.702: INFO: PodSpec: initContainers in spec.initContainers
Sep 23 10:00:24.084: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-56b1cad4-d343-45a0-bde9-b59cc76d98ff", GenerateName:"", Namespace:"init-container-402", SelfLink:"", UID:"5dd13eed-366e-4d6b-824c-749301d66c09", ResourceVersion:"107275", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63767987976, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"702013147"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.10.131.182/32", "cni.projectcalico.org/podIPs":"10.10.131.182/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004385d80), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004385da0)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004385dc0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004385de0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc004385e00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004385e20)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-drgh6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0031de280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-drgh6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-drgh6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-drgh6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0042589f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-s002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0011242a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004258a80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004258aa0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004258aa8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004258aac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0040ad4b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767987976, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767987976, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767987976, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767987976, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.0.7", PodIP:"10.10.131.182", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.131.182"}}, StartTime:(*v1.Time)(0xc004385e40), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0011243f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001124460)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://972d7d13e006c6d367d037c745ad0c887d03adf6ba44cc4d1f312f032dbdf1d6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004385e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004385e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004258b2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:00:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-402" for this suite.

• [SLOW TEST:47.426 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":282,"skipped":4856,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:00:24.099: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3715
Sep 23 10:00:26.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 23 10:00:26.279: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 23 10:00:26.279: INFO: stdout: "ipvs"
Sep 23 10:00:26.279: INFO: proxyMode: ipvs
Sep 23 10:00:26.285: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 23 10:00:26.287: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3715
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3715
I0923 10:00:26.300657      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3715, replica count: 3
I0923 10:00:29.350958      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 23 10:00:29.355: INFO: Creating new exec pod
Sep 23 10:00:34.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec execpod-affinityg9mb8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Sep 23 10:00:34.526: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 23 10:00:34.526: INFO: stdout: ""
Sep 23 10:00:34.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec execpod-affinityg9mb8 -- /bin/sh -x -c nc -zv -t -w 2 10.96.170.86 80'
Sep 23 10:00:34.671: INFO: stderr: "+ nc -zv -t -w 2 10.96.170.86 80\nConnection to 10.96.170.86 80 port [tcp/http] succeeded!\n"
Sep 23 10:00:34.671: INFO: stdout: ""
Sep 23 10:00:34.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec execpod-affinityg9mb8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.170.86:80/ ; done'
Sep 23 10:00:34.883: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n"
Sep 23 10:00:34.883: INFO: stdout: "\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk\naffinity-clusterip-timeout-8hxmk"
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Received response from host: affinity-clusterip-timeout-8hxmk
Sep 23 10:00:34.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec execpod-affinityg9mb8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.170.86:80/'
Sep 23 10:00:35.042: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n"
Sep 23 10:00:35.042: INFO: stdout: "affinity-clusterip-timeout-8hxmk"
Sep 23 10:02:45.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=services-3715 exec execpod-affinityg9mb8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.170.86:80/'
Sep 23 10:02:45.202: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.170.86:80/\n"
Sep 23 10:02:45.202: INFO: stdout: "affinity-clusterip-timeout-p8cvx"
Sep 23 10:02:45.202: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3715, will wait for the garbage collector to delete the pods
Sep 23 10:02:45.275: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 3.080398ms
Sep 23 10:02:46.775: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.500170101s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:03:02.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3715" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:158.107 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":283,"skipped":4860,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:03:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:03:02.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2" in namespace "downward-api-7030" to be "Succeeded or Failed"
Sep 23 10:03:02.245: INFO: Pod "downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.704738ms
Sep 23 10:03:04.249: INFO: Pod "downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010237806s
STEP: Saw pod success
Sep 23 10:03:04.249: INFO: Pod "downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2" satisfied condition "Succeeded or Failed"
Sep 23 10:03:04.253: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2 container client-container: <nil>
STEP: delete the pod
Sep 23 10:03:04.271: INFO: Waiting for pod downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2 to disappear
Sep 23 10:03:04.277: INFO: Pod downwardapi-volume-676d6508-5268-4919-8a3a-6bc05cfb9eb2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:03:04.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7030" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4874,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:03:04.282: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-0c180151-848e-4dc8-9a4b-6636f460ab56
STEP: Creating a pod to test consume configMaps
Sep 23 10:03:04.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21" in namespace "projected-4242" to be "Succeeded or Failed"
Sep 23 10:03:04.318: INFO: Pod "pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421675ms
Sep 23 10:03:06.337: INFO: Pod "pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02199501s
STEP: Saw pod success
Sep 23 10:03:06.338: INFO: Pod "pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21" satisfied condition "Succeeded or Failed"
Sep 23 10:03:06.339: INFO: Trying to get logs from node worker-s002 pod pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21 container agnhost-container: <nil>
STEP: delete the pod
Sep 23 10:03:06.367: INFO: Waiting for pod pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21 to disappear
Sep 23 10:03:06.369: INFO: Pod pod-projected-configmaps-90ece5aa-34de-4155-ad90-71a138d5fd21 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:03:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4242" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":285,"skipped":4877,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:03:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0923 10:03:46.439293      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 10:04:48.454: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 23 10:04:48.454: INFO: Deleting pod "simpletest.rc-294jc" in namespace "gc-6924"
Sep 23 10:04:48.462: INFO: Deleting pod "simpletest.rc-6mqgs" in namespace "gc-6924"
Sep 23 10:04:48.490: INFO: Deleting pod "simpletest.rc-bmrdk" in namespace "gc-6924"
Sep 23 10:04:48.497: INFO: Deleting pod "simpletest.rc-crn2j" in namespace "gc-6924"
Sep 23 10:04:48.503: INFO: Deleting pod "simpletest.rc-gs5vx" in namespace "gc-6924"
Sep 23 10:04:48.513: INFO: Deleting pod "simpletest.rc-k2dtl" in namespace "gc-6924"
Sep 23 10:04:48.522: INFO: Deleting pod "simpletest.rc-ngf2s" in namespace "gc-6924"
Sep 23 10:04:48.531: INFO: Deleting pod "simpletest.rc-xbwt5" in namespace "gc-6924"
Sep 23 10:04:48.546: INFO: Deleting pod "simpletest.rc-zh2jf" in namespace "gc-6924"
Sep 23 10:04:48.575: INFO: Deleting pod "simpletest.rc-zvgsq" in namespace "gc-6924"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:04:48.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6924" for this suite.

• [SLOW TEST:102.221 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":286,"skipped":4880,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:04:48.599: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep 23 10:04:53.193: INFO: Successfully updated pod "labelsupdate1a740fae-c3ec-48b5-82ec-d388ca63bfc8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:04:55.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1094" for this suite.

• [SLOW TEST:6.615 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":4888,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:04:55.214: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0923 10:04:57.286255      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 23 10:05:59.295: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:05:59.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6906" for this suite.

• [SLOW TEST:64.087 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":288,"skipped":4895,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:05:59.302: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:10.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4815" for this suite.

• [SLOW TEST:11.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":289,"skipped":4913,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:10.391: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 10:06:10.777: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 10:06:12.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988370, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988370, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988370, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988370, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 10:06:15.793: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:27.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2195" for this suite.
STEP: Destroying namespace "webhook-2195-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.542 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":290,"skipped":4927,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:27.932: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 10:06:27.990: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:29.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4069" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":291,"skipped":4939,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:29.322: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:06:29.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c" in namespace "downward-api-7819" to be "Succeeded or Failed"
Sep 23 10:06:29.353: INFO: Pod "downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005231ms
Sep 23 10:06:31.357: INFO: Pod "downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005107315s
STEP: Saw pod success
Sep 23 10:06:31.357: INFO: Pod "downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c" satisfied condition "Succeeded or Failed"
Sep 23 10:06:31.371: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c container client-container: <nil>
STEP: delete the pod
Sep 23 10:06:31.387: INFO: Waiting for pod downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c to disappear
Sep 23 10:06:31.388: INFO: Pod downwardapi-volume-3be14bc2-bf2c-48ca-933a-7ec243f4ba6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7819" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4940,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:31.393: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 23 10:06:31.418: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 10:06:35.709: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:51.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5027" for this suite.

• [SLOW TEST:20.200 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":293,"skipped":4956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 23 10:06:51.627: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-747  5e6e030a-4f75-43e1-9189-6a474f025e8a 109606 0 2021-09-23 10:06:51 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-09-23 10:06:51 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7kjhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7kjhj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7kjhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 23 10:06:51.630: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 23 10:06:53.635: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 23 10:06:55.632: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 23 10:06:55.632: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-747 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 10:06:55.632: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Verifying customized DNS server is configured on pod...
Sep 23 10:06:55.722: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-747 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 23 10:06:55.722: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
Sep 23 10:06:55.799: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:06:55.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-747" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":294,"skipped":4984,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:06:55.829: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 23 10:06:56.093: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 23 10:06:58.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988416, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988416, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988416, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988416, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 23 10:07:01.130: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:01.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3325" for this suite.
STEP: Destroying namespace "webhook-3325-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.379 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":295,"skipped":4989,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:01.208: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Sep 23 10:07:01.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 create -f -'
Sep 23 10:07:01.662: INFO: stderr: ""
Sep 23 10:07:01.662: INFO: stdout: "pod/pause created\n"
Sep 23 10:07:01.662: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 23 10:07:01.662: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1992" to be "running and ready"
Sep 23 10:07:01.665: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.88839ms
Sep 23 10:07:03.669: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006801501s
Sep 23 10:07:03.669: INFO: Pod "pause" satisfied condition "running and ready"
Sep 23 10:07:03.669: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 23 10:07:03.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 label pods pause testing-label=testing-label-value'
Sep 23 10:07:03.732: INFO: stderr: ""
Sep 23 10:07:03.732: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 23 10:07:03.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 get pod pause -L testing-label'
Sep 23 10:07:03.789: INFO: stderr: ""
Sep 23 10:07:03.789: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 23 10:07:03.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 label pods pause testing-label-'
Sep 23 10:07:03.857: INFO: stderr: ""
Sep 23 10:07:03.857: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 23 10:07:03.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 get pod pause -L testing-label'
Sep 23 10:07:03.913: INFO: stderr: ""
Sep 23 10:07:03.913: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Sep 23 10:07:03.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 delete --grace-period=0 --force -f -'
Sep 23 10:07:03.973: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 23 10:07:03.973: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 23 10:07:03.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 get rc,svc -l name=pause --no-headers'
Sep 23 10:07:04.035: INFO: stderr: "No resources found in kubectl-1992 namespace.\n"
Sep 23 10:07:04.035: INFO: stdout: ""
Sep 23 10:07:04.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=kubectl-1992 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 23 10:07:04.100: INFO: stderr: ""
Sep 23 10:07:04.100: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:04.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1992" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":296,"skipped":4991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:04.105: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 23 10:07:04.154: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1154  9e2ef5c3-8115-4181-80e1-81ddfe3a5e81 109802 0 2021-09-23 10:07:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-23 10:07:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 23 10:07:04.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1154  9e2ef5c3-8115-4181-80e1-81ddfe3a5e81 109803 0 2021-09-23 10:07:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-23 10:07:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:04.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1154" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":297,"skipped":5082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:04.159: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 10:07:08.187: INFO: Deleting pod "var-expansion-f860a682-e469-4f95-ac28-ca107496c485" in namespace "var-expansion-4735"
Sep 23 10:07:08.191: INFO: Wait up to 5m0s for pod "var-expansion-f860a682-e469-4f95-ac28-ca107496c485" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:16.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4735" for this suite.

• [SLOW TEST:12.044 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":298,"skipped":5117,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:16.203: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:21.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9672" for this suite.

• [SLOW TEST:5.080 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":299,"skipped":5136,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:21.284: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-387de6ab-8320-491c-80c5-89d35b9d44f3
STEP: Creating a pod to test consume configMaps
Sep 23 10:07:21.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e" in namespace "configmap-5416" to be "Succeeded or Failed"
Sep 23 10:07:21.321: INFO: Pod "pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.975158ms
Sep 23 10:07:23.325: INFO: Pod "pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006846616s
Sep 23 10:07:25.328: INFO: Pod "pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009959585s
STEP: Saw pod success
Sep 23 10:07:25.328: INFO: Pod "pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e" satisfied condition "Succeeded or Failed"
Sep 23 10:07:25.330: INFO: Trying to get logs from node worker-s002 pod pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e container agnhost-container: <nil>
STEP: delete the pod
Sep 23 10:07:25.345: INFO: Waiting for pod pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e to disappear
Sep 23 10:07:25.349: INFO: Pod pod-configmaps-1b110993-0a10-4433-8932-0c4dd0d0638e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:07:25.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5416" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":300,"skipped":5139,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:07:25.355: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7226
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7226
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7226
Sep 23 10:07:25.430: INFO: Found 0 stateful pods, waiting for 1
Sep 23 10:07:35.434: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 23 10:07:35.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 10:07:35.587: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 10:07:35.587: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 10:07:35.587: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 10:07:35.589: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 23 10:07:45.593: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:07:45.593: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:07:45.604: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999744s
Sep 23 10:07:46.609: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994623369s
Sep 23 10:07:47.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989955867s
Sep 23 10:07:48.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985667811s
Sep 23 10:07:49.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.980926883s
Sep 23 10:07:50.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969624524s
Sep 23 10:07:51.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.965632187s
Sep 23 10:07:52.640: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960784794s
Sep 23 10:07:53.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.957986645s
Sep 23 10:07:54.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.078167ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7226
Sep 23 10:07:55.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 10:07:55.794: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 10:07:55.794: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 10:07:55.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 10:07:55.797: INFO: Found 1 stateful pods, waiting for 3
Sep 23 10:08:05.801: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:08:05.801: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 23 10:08:05.801: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 23 10:08:05.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 10:08:05.957: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 10:08:05.957: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 10:08:05.957: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 10:08:05.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 10:08:06.124: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 10:08:06.124: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 10:08:06.124: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 10:08:06.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 23 10:08:06.269: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 23 10:08:06.269: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 23 10:08:06.269: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 23 10:08:06.269: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:08:06.273: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 23 10:08:16.277: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:08:16.277: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:08:16.277: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 23 10:08:16.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999794s
Sep 23 10:08:17.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997392198s
Sep 23 10:08:18.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994116867s
Sep 23 10:08:19.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991258352s
Sep 23 10:08:20.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988399435s
Sep 23 10:08:21.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983584934s
Sep 23 10:08:22.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980520039s
Sep 23 10:08:23.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978036069s
Sep 23 10:08:24.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974308078s
Sep 23 10:08:25.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.542198ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7226
Sep 23 10:08:26.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 10:08:26.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 10:08:26.478: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 10:08:26.478: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 10:08:26.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 10:08:26.631: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 10:08:26.631: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 10:08:26.631: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 10:08:26.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-977129552 --namespace=statefulset-7226 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 23 10:08:26.768: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 23 10:08:26.768: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 23 10:08:26.768: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 23 10:08:26.768: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep 23 10:08:46.794: INFO: Deleting all statefulset in ns statefulset-7226
Sep 23 10:08:46.796: INFO: Scaling statefulset ss to 0
Sep 23 10:08:46.800: INFO: Waiting for statefulset status.replicas updated to 0
Sep 23 10:08:46.802: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:08:46.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7226" for this suite.

• [SLOW TEST:81.469 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":301,"skipped":5160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:08:46.824: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Sep 23 10:08:46.846: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:09:10.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4321" for this suite.

• [SLOW TEST:24.146 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":302,"skipped":5183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:09:10.971: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:09:11.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d" in namespace "projected-4768" to be "Succeeded or Failed"
Sep 23 10:09:11.013: INFO: Pod "downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174363ms
Sep 23 10:09:13.015: INFO: Pod "downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004500107s
Sep 23 10:09:15.020: INFO: Pod "downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009754864s
STEP: Saw pod success
Sep 23 10:09:15.020: INFO: Pod "downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d" satisfied condition "Succeeded or Failed"
Sep 23 10:09:15.022: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d container client-container: <nil>
STEP: delete the pod
Sep 23 10:09:15.042: INFO: Waiting for pod downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d to disappear
Sep 23 10:09:15.044: INFO: Pod downwardapi-volume-128f4a8c-9eb7-4f8b-a397-179c4a59514d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:09:15.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4768" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":303,"skipped":5230,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:09:15.048: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 10:09:15.093: INFO: Create a RollingUpdate DaemonSet
Sep 23 10:09:15.096: INFO: Check that daemon pods launch on every node of the cluster
Sep 23 10:09:15.102: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:15.113: INFO: Number of nodes with available pods: 0
Sep 23 10:09:15.113: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 10:09:16.117: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:16.119: INFO: Number of nodes with available pods: 0
Sep 23 10:09:16.119: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 10:09:17.121: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:17.123: INFO: Number of nodes with available pods: 0
Sep 23 10:09:17.123: INFO: Node worker-s001 is running more than one daemon pod
Sep 23 10:09:18.117: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:18.119: INFO: Number of nodes with available pods: 2
Sep 23 10:09:18.119: INFO: Number of running nodes: 2, number of available pods: 2
Sep 23 10:09:18.119: INFO: Update the DaemonSet to trigger a rollout
Sep 23 10:09:18.128: INFO: Updating DaemonSet daemon-set
Sep 23 10:09:32.139: INFO: Roll back the DaemonSet before rollout is complete
Sep 23 10:09:32.145: INFO: Updating DaemonSet daemon-set
Sep 23 10:09:32.145: INFO: Make sure DaemonSet rollback is complete
Sep 23 10:09:32.148: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:32.148: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:32.151: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:33.154: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:33.154: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:33.157: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:34.156: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:34.156: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:34.158: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:35.155: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:35.155: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:35.162: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:36.155: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:36.155: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:36.157: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:37.156: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:37.156: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:37.163: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:38.156: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:38.156: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:38.164: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:39.156: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:39.156: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:39.158: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:40.155: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:40.155: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:40.157: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:41.155: INFO: Wrong image for pod: daemon-set-7zjcm. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep 23 10:09:41.155: INFO: Pod daemon-set-7zjcm is not available
Sep 23 10:09:41.157: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 23 10:09:42.153: INFO: Pod daemon-set-msd2k is not available
Sep 23 10:09:42.156: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-349, will wait for the garbage collector to delete the pods
Sep 23 10:09:42.213: INFO: Deleting DaemonSet.extensions daemon-set took: 2.511604ms
Sep 23 10:09:43.713: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.500178846s
Sep 23 10:09:54.317: INFO: Number of nodes with available pods: 0
Sep 23 10:09:54.317: INFO: Number of running nodes: 0, number of available pods: 0
Sep 23 10:09:54.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111127"},"items":null}

Sep 23 10:09:54.325: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111127"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:09:54.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-349" for this suite.

• [SLOW TEST:39.286 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":304,"skipped":5231,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:09:54.335: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep 23 10:09:54.369: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe" in namespace "downward-api-5954" to be "Succeeded or Failed"
Sep 23 10:09:54.380: INFO: Pod "downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.525961ms
Sep 23 10:09:56.384: INFO: Pod "downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe": Phase="Running", Reason="", readiness=true. Elapsed: 2.014310619s
Sep 23 10:09:58.387: INFO: Pod "downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01736261s
STEP: Saw pod success
Sep 23 10:09:58.387: INFO: Pod "downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe" satisfied condition "Succeeded or Failed"
Sep 23 10:09:58.388: INFO: Trying to get logs from node worker-s002 pod downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe container client-container: <nil>
STEP: delete the pod
Sep 23 10:09:58.423: INFO: Waiting for pod downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe to disappear
Sep 23 10:09:58.424: INFO: Pod downwardapi-volume-c74eb674-de33-41d7-96d5-b9f2b8e7bcfe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:09:58.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5954" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:09:58.430: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-32b8d33a-c2d1-4708-a827-b0377f1a7af7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:02.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6206" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5271,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:10:02.511: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:02.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2004" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":307,"skipped":5282,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:10:02.559: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep 23 10:10:02.578: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 23 10:10:02.582: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 23 10:10:07.585: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 23 10:10:07.585: INFO: Creating deployment "test-rolling-update-deployment"
Sep 23 10:10:07.587: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 23 10:10:07.591: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 23 10:10:09.597: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 23 10:10:09.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988607, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988607, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988607, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63767988607, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 23 10:10:11.601: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 10:10:11.606: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1189  e1e6d72c-2f44-44da-a920-93660cf2e77a 111374 1 2021-09-23 10:10:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-23 10:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00656e378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-23 10:10:07 +0000 UTC,LastTransitionTime:2021-09-23 10:10:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-09-23 10:10:10 +0000 UTC,LastTransitionTime:2021-09-23 10:10:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 23 10:10:11.607: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-1189  921c5c80-8b3e-4a0b-a521-aaa68d5d82b5 111364 1 2021-09-23 10:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e1e6d72c-2f44-44da-a920-93660cf2e77a 0xc00656e877 0xc00656e878}] []  [{kube-controller-manager Update apps/v1 2021-09-23 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1e6d72c-2f44-44da-a920-93660cf2e77a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00656e918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 23 10:10:11.607: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 23 10:10:11.608: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1189  515fc9d1-5de4-4959-9002-3c4246663f1e 111372 2 2021-09-23 10:10:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e1e6d72c-2f44-44da-a920-93660cf2e77a 0xc00656e717 0xc00656e718}] []  [{e2e.test Update apps/v1 2021-09-23 10:10:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-23 10:10:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1e6d72c-2f44-44da-a920-93660cf2e77a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00656e7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 23 10:10:11.609: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-49256" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-49256 test-rolling-update-deployment-6b6bf9df46- deployment-1189  b5614239-baa0-4667-b20a-525ad249d8b9 111363 0 2021-09-23 10:10:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:10.10.131.135/32 cni.projectcalico.org/podIPs:10.10.131.135/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 921c5c80-8b3e-4a0b-a521-aaa68d5d82b5 0xc00656ee27 0xc00656ee28}] []  [{kube-controller-manager Update v1 2021-09-23 10:10:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"921c5c80-8b3e-4a0b-a521-aaa68d5d82b5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 10:10:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 10:10:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-kzb4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-kzb4f,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-kzb4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.135,StartTime:2021-09-23 10:10:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 10:10:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://a5c785660abe32f5bd409b294ae26808d39c55cf3c78b69ab6d5d9d15d70dd9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:11.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1189" for this suite.

• [SLOW TEST:9.060 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":308,"skipped":5302,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:10:11.619: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:11.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5595" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5312,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:10:11.673: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 23 10:10:16.220: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d929f25d-92db-4df0-bc21-ae763bc67484"
Sep 23 10:10:16.220: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d929f25d-92db-4df0-bc21-ae763bc67484" in namespace "pods-6326" to be "terminated due to deadline exceeded"
Sep 23 10:10:16.227: INFO: Pod "pod-update-activedeadlineseconds-d929f25d-92db-4df0-bc21-ae763bc67484": Phase="Running", Reason="", readiness=true. Elapsed: 7.594291ms
Sep 23 10:10:18.230: INFO: Pod "pod-update-activedeadlineseconds-d929f25d-92db-4df0-bc21-ae763bc67484": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.01005935s
Sep 23 10:10:18.230: INFO: Pod "pod-update-activedeadlineseconds-d929f25d-92db-4df0-bc21-ae763bc67484" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:18.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6326" for this suite.

• [SLOW TEST:6.562 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":310,"skipped":5315,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep 23 10:10:18.235: INFO: >>> kubeConfig: /tmp/kubeconfig-977129552
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Sep 23 10:10:18.276: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.276: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.281: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.281: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.298: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.298: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.341: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:18.341: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 23 10:10:20.245: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 23 10:10:20.245: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 23 10:10:20.278: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Sep 23 10:10:20.284: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 0
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.285: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.286: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.286: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.292: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.292: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.297: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.297: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.313: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.313: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 2
Sep 23 10:10:20.323: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
STEP: listing Deployments
Sep 23 10:10:20.326: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Sep 23 10:10:20.334: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Sep 23 10:10:20.338: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 23 10:10:20.344: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 23 10:10:20.357: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 23 10:10:20.374: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 23 10:10:20.395: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 23 10:10:20.429: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
Sep 23 10:10:23.214: INFO: observed Deployment test-deployment in namespace deployment-8536 with ReadyReplicas 1
STEP: deleting the Deployment
Sep 23 10:10:23.231: INFO: observed event type MODIFIED
Sep 23 10:10:23.231: INFO: observed event type MODIFIED
Sep 23 10:10:23.231: INFO: observed event type MODIFIED
Sep 23 10:10:23.231: INFO: observed event type MODIFIED
Sep 23 10:10:23.231: INFO: observed event type MODIFIED
Sep 23 10:10:23.232: INFO: observed event type MODIFIED
Sep 23 10:10:23.232: INFO: observed event type MODIFIED
Sep 23 10:10:23.232: INFO: observed event type MODIFIED
Sep 23 10:10:23.232: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep 23 10:10:23.237: INFO: Log out all the ReplicaSets if there is no deployment created
Sep 23 10:10:23.242: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-8536  bcb24255-a82d-47b8-9e5d-e5d54276e077 111658 3 2021-09-23 10:10:20 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 8d2bf0b4-e325-402b-ac86-9f89af79d660 0xc0066ec427 0xc0066ec428}] []  [{kube-controller-manager Update apps/v1 2021-09-23 10:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2bf0b4-e325-402b-ac86-9f89af79d660\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0066ec4c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Sep 23 10:10:23.245: INFO: pod: "test-deployment-768947d6f5-hq9tv":
&Pod{ObjectMeta:{test-deployment-768947d6f5-hq9tv test-deployment-768947d6f5- deployment-8536  3179c40d-83ff-4351-b84f-9aa80254fab3 111637 0 2021-09-23 10:10:20 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:10.10.131.162/32 cni.projectcalico.org/podIPs:10.10.131.162/32] [{apps/v1 ReplicaSet test-deployment-768947d6f5 bcb24255-a82d-47b8-9e5d-e5d54276e077 0xc0066ec8a7 0xc0066ec8a8}] []  [{kube-controller-manager Update v1 2021-09-23 10:10:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bcb24255-a82d-47b8-9e5d-e5d54276e077\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 10:10:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 10:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.131.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sj544,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sj544,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sj544,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:10.10.131.162,StartTime:2021-09-23 10:10:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 10:10:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://dd12a65526d2867bf0fa7c73e25287c9d1c7d0b56f01626df79f49111a0616d3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.131.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 23 10:10:23.245: INFO: pod: "test-deployment-768947d6f5-xzbvw":
&Pod{ObjectMeta:{test-deployment-768947d6f5-xzbvw test-deployment-768947d6f5- deployment-8536  6583f01f-2f15-4c52-b465-aa826c174525 111657 0 2021-09-23 10:10:23 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 bcb24255-a82d-47b8-9e5d-e5d54276e077 0xc0066ecaa7 0xc0066ecaa8}] []  [{kube-controller-manager Update v1 2021-09-23 10:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bcb24255-a82d-47b8-9e5d-e5d54276e077\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-23 10:10:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sj544,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sj544,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sj544,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.7,PodIP:,StartTime:2021-09-23 10:10:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 23 10:10:23.245: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-8536  9fddd807-e77a-4fc4-9fda-b5dbdd63c52e 111569 2 2021-09-23 10:10:18 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 8d2bf0b4-e325-402b-ac86-9f89af79d660 0xc0066ec547 0xc0066ec548}] []  [{kube-controller-manager Update apps/v1 2021-09-23 10:10:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2bf0b4-e325-402b-ac86-9f89af79d660\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0066ec5c0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Sep 23 10:10:23.254: INFO: pod: "test-deployment-8b6954bfb-x2cwb":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-x2cwb test-deployment-8b6954bfb- deployment-8536  53666c47-c7d3-451d-84c8-521a1bac9313 111541 0 2021-09-23 10:10:18 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:10.10.28.7/32 cni.projectcalico.org/podIPs:10.10.28.7/32] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 9fddd807-e77a-4fc4-9fda-b5dbdd63c52e 0xc0066edd37 0xc0066edd38}] []  [{kube-controller-manager Update v1 2021-09-23 10:10:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fddd807-e77a-4fc4-9fda-b5dbdd63c52e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-09-23 10:10:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-09-23 10:10:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.28.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sj544,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sj544,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sj544,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-s001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-23 10:10:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.0.6,PodIP:10.10.28.7,StartTime:2021-09-23 10:10:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-23 10:10:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://fe4aee2443eb1d5b98f2476005fa194c4d317e4ca1b5be629023d35198461d81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.28.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep 23 10:10:23.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8536" for this suite.

• [SLOW TEST:5.034 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":311,"skipped":5338,"failed":0}
SSSSSSSSSSSSSSSSSSSep 23 10:10:23.269: INFO: Running AfterSuite actions on all nodes
Sep 23 10:10:23.269: INFO: Running AfterSuite actions on node 1
Sep 23 10:10:23.269: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6296.593 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h44m57.807785194s
Test Suite Passed
