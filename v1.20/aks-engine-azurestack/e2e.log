I0429 21:53:12.501383      21 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-127715093
I0429 21:53:12.501423      21 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0429 21:53:12.501519      21 e2e.go:129] Starting e2e run "821a7fd5-0184-4739-98d8-647759f1e36d" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1619733190 - Will randomize all specs
Will run 311 of 5667 specs

Apr 29 21:53:12.518: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 21:53:12.523: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 29 21:53:12.566: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 29 21:53:12.621: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 29 21:53:12.621: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Apr 29 21:53:12.621: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 29 21:53:12.637: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
Apr 29 21:53:12.637: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-secrets-store' (0 seconds elapsed)
Apr 29 21:53:12.637: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-secrets-store-provider-azure' (0 seconds elapsed)
Apr 29 21:53:12.637: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 29 21:53:12.637: INFO: e2e test version: v1.20.6
Apr 29 21:53:12.638: INFO: kube-apiserver version: v1.20.6
Apr 29 21:53:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 21:53:12.645: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:53:12.646: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename svcaccounts
Apr 29 21:53:12.734: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Apr 29 21:53:12.742: INFO: PSP annotation exists on dry run pod: "privileged"; assuming PodSecurityPolicy is enabled
Apr 29 21:53:12.758: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:53:12.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8507" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":1,"skipped":33,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:53:13.003: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 21:53:18.289: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:53:18.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2328" for this suite.

• [SLOW TEST:5.345 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":34,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:53:18.349: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4840
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-18e121da-9437-4a64-805c-3286014652df
STEP: Creating configMap with name cm-test-opt-upd-c7e5bd07-7ad4-413b-b592-ae04b10046f2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-18e121da-9437-4a64-805c-3286014652df
STEP: Updating configmap cm-test-opt-upd-c7e5bd07-7ad4-413b-b592-ae04b10046f2
STEP: Creating configMap with name cm-test-opt-create-ebce78b6-a8be-4409-9416-506506acd8a0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:55:03.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4840" for this suite.

• [SLOW TEST:105.006 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:55:03.355: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7487
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Apr 29 21:55:09.586: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7487 PodName:pod-sharedvolume-d83046b0-a195-4dfc-bd56-a970b8ef7701 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 21:55:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 21:55:09.751: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:55:09.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7487" for this suite.

• [SLOW TEST:6.422 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":4,"skipped":95,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:55:09.777: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-5135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 21:55:09.987: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 21:56:10.072: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 29 21:56:10.114: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 29 21:56:10.149: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 29 21:56:10.423: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:56:46.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5135" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:96.931 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":5,"skipped":95,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:56:46.709: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 21:56:46.925: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:56:51.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1940" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":6,"skipped":113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:56:51.132: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 21:57:17.403: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.408: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.421: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.430: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.443: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.448: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.451: INFO: Unable to read jessie_udp@dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.455: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-252.svc.cluster.local from pod dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7: the server could not find the requested resource (get pods dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7)
Apr 29 21:57:17.463: INFO: Lookups using dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local wheezy_udp@dns-test-service-2.dns-252.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-252.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-252.svc.cluster.local jessie_udp@dns-test-service-2.dns-252.svc.cluster.local jessie_tcp@dns-test-service-2.dns-252.svc.cluster.local]

Apr 29 21:57:22.528: INFO: DNS probes using dns-252/dns-test-8fd74951-b654-41f6-8e93-cbbf447da6b7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:57:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-252" for this suite.

• [SLOW TEST:31.506 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":7,"skipped":140,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:57:22.639: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-0a90261c-7fcb-46e8-a2fb-5dccf54412fa
STEP: Creating a pod to test consume configMaps
Apr 29 21:57:22.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d" in namespace "configmap-7658" to be "Succeeded or Failed"
Apr 29 21:57:22.835: INFO: Pod "pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.12787ms
Apr 29 21:57:24.847: INFO: Pod "pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016628482s
Apr 29 21:57:26.859: INFO: Pod "pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028351606s
STEP: Saw pod success
Apr 29 21:57:26.859: INFO: Pod "pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d" satisfied condition "Succeeded or Failed"
Apr 29 21:57:26.863: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 21:57:26.926: INFO: Waiting for pod pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d to disappear
Apr 29 21:57:26.930: INFO: Pod pod-configmaps-51ec8f8d-29c2-49ac-b089-3b04e7f5521d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:57:26.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7658" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":8,"skipped":153,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:57:26.948: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9427
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 29 21:57:27.145: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:57:31.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9427" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":9,"skipped":166,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:57:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:57:31.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5736" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":10,"skipped":186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:57:31.648: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 21:57:31.862: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 21:58:31.941: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:58:31.945: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 21:58:32.148: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Apr 29 21:58:32.158: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:58:32.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1508" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:58:32.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6123" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.726 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":11,"skipped":219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:58:32.375: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 21:58:32.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e" in namespace "projected-3975" to be "Succeeded or Failed"
Apr 29 21:58:32.600: INFO: Pod "downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.863297ms
Apr 29 21:58:34.610: INFO: Pod "downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029678916s
Apr 29 21:58:36.623: INFO: Pod "downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041821392s
STEP: Saw pod success
Apr 29 21:58:36.623: INFO: Pod "downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e" satisfied condition "Succeeded or Failed"
Apr 29 21:58:36.626: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e container client-container: <nil>
STEP: delete the pod
Apr 29 21:58:36.689: INFO: Waiting for pod downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e to disappear
Apr 29 21:58:36.696: INFO: Pod downwardapi-volume-eb00a062-e169-483c-9c44-0907acd21d3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 21:58:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3975" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 21:58:36.712: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 29 21:58:37.383: INFO: Pod name wrapped-volume-race-3ac11632-1e37-473d-95f7-c89adf350ecf: Found 0 pods out of 5
Apr 29 21:58:42.396: INFO: Pod name wrapped-volume-race-3ac11632-1e37-473d-95f7-c89adf350ecf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3ac11632-1e37-473d-95f7-c89adf350ecf in namespace emptydir-wrapper-4008, will wait for the garbage collector to delete the pods
Apr 29 21:58:56.538: INFO: Deleting ReplicationController wrapped-volume-race-3ac11632-1e37-473d-95f7-c89adf350ecf took: 17.634771ms
Apr 29 21:58:57.238: INFO: Terminating ReplicationController wrapped-volume-race-3ac11632-1e37-473d-95f7-c89adf350ecf pods took: 700.193959ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 21:59:02.371: INFO: Pod name wrapped-volume-race-b37ffb2c-787a-420b-8db1-8578c6e8d3ff: Found 0 pods out of 5
Apr 29 21:59:07.402: INFO: Pod name wrapped-volume-race-b37ffb2c-787a-420b-8db1-8578c6e8d3ff: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b37ffb2c-787a-420b-8db1-8578c6e8d3ff in namespace emptydir-wrapper-4008, will wait for the garbage collector to delete the pods
Apr 29 21:59:19.515: INFO: Deleting ReplicationController wrapped-volume-race-b37ffb2c-787a-420b-8db1-8578c6e8d3ff took: 14.810423ms
Apr 29 21:59:20.215: INFO: Terminating ReplicationController wrapped-volume-race-b37ffb2c-787a-420b-8db1-8578c6e8d3ff pods took: 700.180423ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 21:59:31.800: INFO: Pod name wrapped-volume-race-cb6382ff-8785-425b-85df-9a1196b35a0f: Found 0 pods out of 5
Apr 29 21:59:36.825: INFO: Pod name wrapped-volume-race-cb6382ff-8785-425b-85df-9a1196b35a0f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cb6382ff-8785-425b-85df-9a1196b35a0f in namespace emptydir-wrapper-4008, will wait for the garbage collector to delete the pods
Apr 29 21:59:48.978: INFO: Deleting ReplicationController wrapped-volume-race-cb6382ff-8785-425b-85df-9a1196b35a0f took: 47.572096ms
Apr 29 21:59:49.678: INFO: Terminating ReplicationController wrapped-volume-race-cb6382ff-8785-425b-85df-9a1196b35a0f pods took: 700.193333ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:00:02.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4008" for this suite.

• [SLOW TEST:85.397 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":13,"skipped":316,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:00:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Apr 29 22:00:06.477: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7876 PodName:var-expansion-13f47c43-8e75-4dd0-bbbe-c120f9e508cb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:00:06.477: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: test for file in mounted path
Apr 29 22:00:06.730: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7876 PodName:var-expansion-13f47c43-8e75-4dd0-bbbe-c120f9e508cb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:00:06.730: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: updating the annotation value
Apr 29 22:00:07.423: INFO: Successfully updated pod "var-expansion-13f47c43-8e75-4dd0-bbbe-c120f9e508cb"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Apr 29 22:00:07.429: INFO: Deleting pod "var-expansion-13f47c43-8e75-4dd0-bbbe-c120f9e508cb" in namespace "var-expansion-7876"
Apr 29 22:00:07.445: INFO: Wait up to 5m0s for pod "var-expansion-13f47c43-8e75-4dd0-bbbe-c120f9e508cb" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:00:41.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7876" for this suite.

• [SLOW TEST:39.369 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":14,"skipped":321,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:00:41.478: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1365
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 29 22:00:41.722: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 29 22:00:46.736: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:00:47.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1365" for this suite.

• [SLOW TEST:6.317 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":15,"skipped":330,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:00:47.796: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:00:48.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e" in namespace "downward-api-5983" to be "Succeeded or Failed"
Apr 29 22:00:48.017: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.162358ms
Apr 29 22:00:50.030: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017067743s
Apr 29 22:00:52.041: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028207455s
Apr 29 22:00:54.051: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03727899s
Apr 29 22:00:56.064: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.050481934s
STEP: Saw pod success
Apr 29 22:00:56.064: INFO: Pod "downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e" satisfied condition "Succeeded or Failed"
Apr 29 22:00:56.069: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e container client-container: <nil>
STEP: delete the pod
Apr 29 22:00:56.140: INFO: Waiting for pod downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e to disappear
Apr 29 22:00:56.144: INFO: Pod downwardapi-volume-3a3c9023-f2ee-40aa-b173-206ad2279d7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:00:56.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5983" for this suite.

• [SLOW TEST:8.367 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":332,"failed":0}
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:00:56.163: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5547
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:00:56.408: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:00:58.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5547" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":17,"skipped":332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:00:58.624: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-64
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-19eaaf42-1ee3-47b2-9cc5-8d36e76565f5
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-19eaaf42-1ee3-47b2-9cc5-8d36e76565f5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:02:13.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-64" for this suite.

• [SLOW TEST:74.999 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":355,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:02:13.623: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 29 22:02:13.817: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:02:31.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-623" for this suite.

• [SLOW TEST:17.888 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":357,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:02:31.513: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-2fch
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 22:02:31.799: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2fch" in namespace "subpath-5921" to be "Succeeded or Failed"
Apr 29 22:02:31.805: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874475ms
Apr 29 22:02:33.810: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011202982s
Apr 29 22:02:35.823: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 4.023834442s
Apr 29 22:02:37.834: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 6.035313148s
Apr 29 22:02:39.844: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 8.045102393s
Apr 29 22:02:41.857: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 10.057882336s
Apr 29 22:02:43.862: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 12.062595139s
Apr 29 22:02:45.872: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 14.073029375s
Apr 29 22:02:47.882: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 16.082908865s
Apr 29 22:02:49.893: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 18.093440325s
Apr 29 22:02:51.911: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 20.11231895s
Apr 29 22:02:53.916: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Running", Reason="", readiness=true. Elapsed: 22.117144162s
Apr 29 22:02:55.936: INFO: Pod "pod-subpath-test-projected-2fch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.137072824s
STEP: Saw pod success
Apr 29 22:02:55.936: INFO: Pod "pod-subpath-test-projected-2fch" satisfied condition "Succeeded or Failed"
Apr 29 22:02:55.941: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-subpath-test-projected-2fch container test-container-subpath-projected-2fch: <nil>
STEP: delete the pod
Apr 29 22:02:56.001: INFO: Waiting for pod pod-subpath-test-projected-2fch to disappear
Apr 29 22:02:56.006: INFO: Pod pod-subpath-test-projected-2fch no longer exists
STEP: Deleting pod pod-subpath-test-projected-2fch
Apr 29 22:02:56.006: INFO: Deleting pod "pod-subpath-test-projected-2fch" in namespace "subpath-5921"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:02:56.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5921" for this suite.

• [SLOW TEST:24.542 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":20,"skipped":359,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:02:56.060: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2lj2p in namespace proxy-8399
I0429 22:02:56.440410      21 runners.go:190] Created replication controller with name: proxy-service-2lj2p, namespace: proxy-8399, replica count: 1
I0429 22:02:57.491102      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 22:02:58.491273      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 22:02:59.491463      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 22:03:00.491592      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 22:03:01.491802      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 22:03:02.492051      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 22:03:03.492225      21 runners.go:190] proxy-service-2lj2p Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:03:03.519: INFO: setup took 7.210195036s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 29 22:03:03.541: INFO: (0) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 20.85526ms)
Apr 29 22:03:03.541: INFO: (0) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 21.142163ms)
Apr 29 22:03:03.541: INFO: (0) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 21.954773ms)
Apr 29 22:03:03.544: INFO: (0) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 23.986299ms)
Apr 29 22:03:03.544: INFO: (0) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 24.303202ms)
Apr 29 22:03:03.544: INFO: (0) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 24.97871ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 24.97701ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 25.073511ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 25.395116ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 25.888622ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 25.511517ms)
Apr 29 22:03:03.545: INFO: (0) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 25.580318ms)
Apr 29 22:03:03.546: INFO: (0) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 26.424829ms)
Apr 29 22:03:03.546: INFO: (0) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 26.679331ms)
Apr 29 22:03:03.547: INFO: (0) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 27.305639ms)
Apr 29 22:03:03.547: INFO: (0) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 27.37774ms)
Apr 29 22:03:03.558: INFO: (1) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 11.22994ms)
Apr 29 22:03:03.559: INFO: (1) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 11.491642ms)
Apr 29 22:03:03.559: INFO: (1) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 11.28844ms)
Apr 29 22:03:03.563: INFO: (1) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 15.27569ms)
Apr 29 22:03:03.564: INFO: (1) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 17.112213ms)
Apr 29 22:03:03.564: INFO: (1) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 16.971811ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 16.849109ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 17.039911ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.098713ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 17.69292ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 17.404316ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 17.612019ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 17.744521ms)
Apr 29 22:03:03.565: INFO: (1) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 17.977923ms)
Apr 29 22:03:03.568: INFO: (1) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 21.318665ms)
Apr 29 22:03:03.569: INFO: (1) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 21.70897ms)
Apr 29 22:03:03.584: INFO: (2) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 15.371591ms)
Apr 29 22:03:03.585: INFO: (2) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 15.662495ms)
Apr 29 22:03:03.585: INFO: (2) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 16.1101ms)
Apr 29 22:03:03.586: INFO: (2) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 16.1183ms)
Apr 29 22:03:03.586: INFO: (2) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 16.1328ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 18.577231ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 18.028524ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 18.360628ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 18.714833ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 18.131325ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 18.734533ms)
Apr 29 22:03:03.588: INFO: (2) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 18.872034ms)
Apr 29 22:03:03.589: INFO: (2) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 18.56553ms)
Apr 29 22:03:03.592: INFO: (2) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 22.265977ms)
Apr 29 22:03:03.593: INFO: (2) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 23.599994ms)
Apr 29 22:03:03.593: INFO: (2) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 23.452992ms)
Apr 29 22:03:03.607: INFO: (3) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 12.84996ms)
Apr 29 22:03:03.609: INFO: (3) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 14.584981ms)
Apr 29 22:03:03.609: INFO: (3) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 14.719883ms)
Apr 29 22:03:03.610: INFO: (3) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 16.305602ms)
Apr 29 22:03:03.611: INFO: (3) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 15.859497ms)
Apr 29 22:03:03.611: INFO: (3) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 16.785509ms)
Apr 29 22:03:03.612: INFO: (3) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 18.879734ms)
Apr 29 22:03:03.613: INFO: (3) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 18.867034ms)
Apr 29 22:03:03.613: INFO: (3) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 18.659032ms)
Apr 29 22:03:03.614: INFO: (3) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 19.504842ms)
Apr 29 22:03:03.614: INFO: (3) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 20.187851ms)
Apr 29 22:03:03.614: INFO: (3) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 21.051462ms)
Apr 29 22:03:03.615: INFO: (3) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 20.572356ms)
Apr 29 22:03:03.615: INFO: (3) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 21.370565ms)
Apr 29 22:03:03.615: INFO: (3) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 20.502054ms)
Apr 29 22:03:03.615: INFO: (3) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 20.722058ms)
Apr 29 22:03:03.635: INFO: (4) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 19.161438ms)
Apr 29 22:03:03.635: INFO: (4) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 18.45523ms)
Apr 29 22:03:03.635: INFO: (4) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 19.046537ms)
Apr 29 22:03:03.635: INFO: (4) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 19.127838ms)
Apr 29 22:03:03.635: INFO: (4) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 19.651944ms)
Apr 29 22:03:03.636: INFO: (4) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 20.001649ms)
Apr 29 22:03:03.637: INFO: (4) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 20.510955ms)
Apr 29 22:03:03.638: INFO: (4) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 21.958773ms)
Apr 29 22:03:03.638: INFO: (4) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 21.68997ms)
Apr 29 22:03:03.638: INFO: (4) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 22.653282ms)
Apr 29 22:03:03.638: INFO: (4) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 21.900472ms)
Apr 29 22:03:03.639: INFO: (4) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 22.074074ms)
Apr 29 22:03:03.638: INFO: (4) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 22.807683ms)
Apr 29 22:03:03.639: INFO: (4) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 22.705582ms)
Apr 29 22:03:03.639: INFO: (4) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 22.784883ms)
Apr 29 22:03:03.639: INFO: (4) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 22.446778ms)
Apr 29 22:03:03.646: INFO: (5) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 7.032187ms)
Apr 29 22:03:03.651: INFO: (5) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 11.28614ms)
Apr 29 22:03:03.651: INFO: (5) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 11.965248ms)
Apr 29 22:03:03.653: INFO: (5) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 12.90286ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 17.178213ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 17.436416ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 17.506318ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.622219ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.409117ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 17.75812ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 17.551319ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 17.882423ms)
Apr 29 22:03:03.657: INFO: (5) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 17.744521ms)
Apr 29 22:03:03.658: INFO: (5) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 19.123637ms)
Apr 29 22:03:03.659: INFO: (5) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 19.025536ms)
Apr 29 22:03:03.659: INFO: (5) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 19.410041ms)
Apr 29 22:03:03.674: INFO: (6) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 14.267877ms)
Apr 29 22:03:03.674: INFO: (6) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 14.032375ms)
Apr 29 22:03:03.675: INFO: (6) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 14.370079ms)
Apr 29 22:03:03.676: INFO: (6) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 16.396504ms)
Apr 29 22:03:03.676: INFO: (6) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 16.510805ms)
Apr 29 22:03:03.676: INFO: (6) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 17.250814ms)
Apr 29 22:03:03.678: INFO: (6) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 18.362628ms)
Apr 29 22:03:03.678: INFO: (6) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 17.835322ms)
Apr 29 22:03:03.678: INFO: (6) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 18.764733ms)
Apr 29 22:03:03.680: INFO: (6) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 20.269951ms)
Apr 29 22:03:03.680: INFO: (6) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 20.346453ms)
Apr 29 22:03:03.680: INFO: (6) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 20.569756ms)
Apr 29 22:03:03.680: INFO: (6) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 20.749358ms)
Apr 29 22:03:03.681: INFO: (6) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 21.016561ms)
Apr 29 22:03:03.681: INFO: (6) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 21.459667ms)
Apr 29 22:03:03.682: INFO: (6) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 22.545881ms)
Apr 29 22:03:03.696: INFO: (7) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 13.969173ms)
Apr 29 22:03:03.696: INFO: (7) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 14.003174ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 14.202777ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 14.398179ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 14.708683ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 14.908485ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 15.479492ms)
Apr 29 22:03:03.697: INFO: (7) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 15.501693ms)
Apr 29 22:03:03.698: INFO: (7) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 15.534593ms)
Apr 29 22:03:03.698: INFO: (7) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 15.538993ms)
Apr 29 22:03:03.699: INFO: (7) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 16.980711ms)
Apr 29 22:03:03.700: INFO: (7) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 17.301815ms)
Apr 29 22:03:03.702: INFO: (7) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 20.04455ms)
Apr 29 22:03:03.703: INFO: (7) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 20.739058ms)
Apr 29 22:03:03.703: INFO: (7) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 20.764558ms)
Apr 29 22:03:03.703: INFO: (7) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 20.835859ms)
Apr 29 22:03:03.711: INFO: (8) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 8.110501ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 16.970311ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 17.333416ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 17.333715ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.569818ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 17.830222ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 17.013612ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.788621ms)
Apr 29 22:03:03.721: INFO: (8) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 17.818121ms)
Apr 29 22:03:03.722: INFO: (8) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 17.902823ms)
Apr 29 22:03:03.727: INFO: (8) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 22.890184ms)
Apr 29 22:03:03.732: INFO: (8) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 27.911047ms)
Apr 29 22:03:03.733: INFO: (8) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 29.306364ms)
Apr 29 22:03:03.733: INFO: (8) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 29.391165ms)
Apr 29 22:03:03.733: INFO: (8) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 29.495667ms)
Apr 29 22:03:03.733: INFO: (8) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 29.996373ms)
Apr 29 22:03:03.747: INFO: (9) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 13.452767ms)
Apr 29 22:03:03.754: INFO: (9) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 19.648844ms)
Apr 29 22:03:03.758: INFO: (9) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 23.661494ms)
Apr 29 22:03:03.758: INFO: (9) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 24.417904ms)
Apr 29 22:03:03.758: INFO: (9) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 23.776796ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 24.360703ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 24.223401ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 24.060399ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 25.254214ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 24.737507ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 24.90861ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 25.297014ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 24.661807ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 24.93191ms)
Apr 29 22:03:03.759: INFO: (9) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 25.690219ms)
Apr 29 22:03:03.760: INFO: (9) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 25.542717ms)
Apr 29 22:03:03.769: INFO: (10) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 9.377017ms)
Apr 29 22:03:03.775: INFO: (10) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 14.751483ms)
Apr 29 22:03:03.775: INFO: (10) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 14.48578ms)
Apr 29 22:03:03.775: INFO: (10) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 15.196289ms)
Apr 29 22:03:03.775: INFO: (10) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 15.344891ms)
Apr 29 22:03:03.778: INFO: (10) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 17.871422ms)
Apr 29 22:03:03.779: INFO: (10) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 18.235427ms)
Apr 29 22:03:03.779: INFO: (10) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 18.317028ms)
Apr 29 22:03:03.779: INFO: (10) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 18.911735ms)
Apr 29 22:03:03.779: INFO: (10) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 18.894335ms)
Apr 29 22:03:03.779: INFO: (10) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 19.480143ms)
Apr 29 22:03:03.782: INFO: (10) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 22.418779ms)
Apr 29 22:03:03.782: INFO: (10) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 22.52238ms)
Apr 29 22:03:03.782: INFO: (10) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 22.212476ms)
Apr 29 22:03:03.783: INFO: (10) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 22.699982ms)
Apr 29 22:03:03.783: INFO: (10) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 22.611581ms)
Apr 29 22:03:03.795: INFO: (11) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 11.745646ms)
Apr 29 22:03:03.796: INFO: (11) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 12.788459ms)
Apr 29 22:03:03.796: INFO: (11) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 12.955861ms)
Apr 29 22:03:03.796: INFO: (11) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 12.930061ms)
Apr 29 22:03:03.798: INFO: (11) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 14.398979ms)
Apr 29 22:03:03.800: INFO: (11) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 16.182601ms)
Apr 29 22:03:03.800: INFO: (11) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 16.558606ms)
Apr 29 22:03:03.800: INFO: (11) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 16.531905ms)
Apr 29 22:03:03.800: INFO: (11) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 17.266114ms)
Apr 29 22:03:03.800: INFO: (11) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.190314ms)
Apr 29 22:03:03.801: INFO: (11) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 17.874922ms)
Apr 29 22:03:03.802: INFO: (11) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 18.610032ms)
Apr 29 22:03:03.804: INFO: (11) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 20.238952ms)
Apr 29 22:03:03.804: INFO: (11) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 20.584356ms)
Apr 29 22:03:03.804: INFO: (11) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 20.691057ms)
Apr 29 22:03:03.804: INFO: (11) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 20.631657ms)
Apr 29 22:03:03.812: INFO: (12) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 7.785597ms)
Apr 29 22:03:03.814: INFO: (12) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 9.106913ms)
Apr 29 22:03:03.819: INFO: (12) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 14.376178ms)
Apr 29 22:03:03.819: INFO: (12) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 14.46028ms)
Apr 29 22:03:03.819: INFO: (12) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 14.697483ms)
Apr 29 22:03:03.820: INFO: (12) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 15.724796ms)
Apr 29 22:03:03.820: INFO: (12) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 15.883498ms)
Apr 29 22:03:03.821: INFO: (12) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 16.238902ms)
Apr 29 22:03:03.821: INFO: (12) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 15.985499ms)
Apr 29 22:03:03.822: INFO: (12) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 17.601619ms)
Apr 29 22:03:03.823: INFO: (12) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.260014ms)
Apr 29 22:03:03.823: INFO: (12) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 18.091924ms)
Apr 29 22:03:03.823: INFO: (12) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 18.047025ms)
Apr 29 22:03:03.825: INFO: (12) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 19.609643ms)
Apr 29 22:03:03.825: INFO: (12) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 20.405454ms)
Apr 29 22:03:03.825: INFO: (12) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 19.787046ms)
Apr 29 22:03:03.834: INFO: (13) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 8.671308ms)
Apr 29 22:03:03.841: INFO: (13) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 15.105487ms)
Apr 29 22:03:03.841: INFO: (13) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 14.988786ms)
Apr 29 22:03:03.841: INFO: (13) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 15.426292ms)
Apr 29 22:03:03.842: INFO: (13) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 16.639207ms)
Apr 29 22:03:03.842: INFO: (13) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 16.563006ms)
Apr 29 22:03:03.842: INFO: (13) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 16.501405ms)
Apr 29 22:03:03.843: INFO: (13) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 17.528718ms)
Apr 29 22:03:03.843: INFO: (13) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 17.005411ms)
Apr 29 22:03:03.843: INFO: (13) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 17.71422ms)
Apr 29 22:03:03.844: INFO: (13) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 18.388829ms)
Apr 29 22:03:03.844: INFO: (13) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 18.266827ms)
Apr 29 22:03:03.845: INFO: (13) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 18.910935ms)
Apr 29 22:03:03.846: INFO: (13) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 19.835847ms)
Apr 29 22:03:03.848: INFO: (13) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 21.759371ms)
Apr 29 22:03:03.849: INFO: (13) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 22.56558ms)
Apr 29 22:03:03.856: INFO: (14) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 7.801097ms)
Apr 29 22:03:03.859: INFO: (14) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 10.201927ms)
Apr 29 22:03:03.861: INFO: (14) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 11.720746ms)
Apr 29 22:03:03.861: INFO: (14) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 11.390441ms)
Apr 29 22:03:03.861: INFO: (14) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 12.12535ms)
Apr 29 22:03:03.861: INFO: (14) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 12.08625ms)
Apr 29 22:03:03.863: INFO: (14) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 14.617982ms)
Apr 29 22:03:03.864: INFO: (14) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 14.812584ms)
Apr 29 22:03:03.864: INFO: (14) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 15.207689ms)
Apr 29 22:03:03.865: INFO: (14) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 15.576294ms)
Apr 29 22:03:03.867: INFO: (14) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 18.295528ms)
Apr 29 22:03:03.871: INFO: (14) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 22.350078ms)
Apr 29 22:03:03.871: INFO: (14) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 22.50828ms)
Apr 29 22:03:03.871: INFO: (14) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 22.026374ms)
Apr 29 22:03:03.872: INFO: (14) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 22.56488ms)
Apr 29 22:03:03.874: INFO: (14) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 24.505805ms)
Apr 29 22:03:03.884: INFO: (15) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 9.789321ms)
Apr 29 22:03:03.885: INFO: (15) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 10.896235ms)
Apr 29 22:03:03.885: INFO: (15) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 10.801135ms)
Apr 29 22:03:03.885: INFO: (15) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 10.589732ms)
Apr 29 22:03:03.886: INFO: (15) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 11.328841ms)
Apr 29 22:03:03.887: INFO: (15) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 12.965861ms)
Apr 29 22:03:03.888: INFO: (15) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 13.554369ms)
Apr 29 22:03:03.888: INFO: (15) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 13.821872ms)
Apr 29 22:03:03.888: INFO: (15) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 14.087475ms)
Apr 29 22:03:03.889: INFO: (15) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 15.063688ms)
Apr 29 22:03:03.889: INFO: (15) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 15.411792ms)
Apr 29 22:03:03.892: INFO: (15) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 17.387116ms)
Apr 29 22:03:03.896: INFO: (15) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 21.316565ms)
Apr 29 22:03:03.896: INFO: (15) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 21.275364ms)
Apr 29 22:03:03.896: INFO: (15) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 21.893372ms)
Apr 29 22:03:03.896: INFO: (15) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 22.106174ms)
Apr 29 22:03:03.917: INFO: (16) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 20.663057ms)
Apr 29 22:03:03.917: INFO: (16) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 19.692245ms)
Apr 29 22:03:03.920: INFO: (16) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 22.462279ms)
Apr 29 22:03:03.920: INFO: (16) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 23.176488ms)
Apr 29 22:03:03.921: INFO: (16) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 24.365703ms)
Apr 29 22:03:03.922: INFO: (16) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 24.708407ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 32.750407ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 32.394002ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 33.052611ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 32.717107ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 33.480316ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 33.162412ms)
Apr 29 22:03:03.930: INFO: (16) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 33.506417ms)
Apr 29 22:03:03.931: INFO: (16) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 33.035411ms)
Apr 29 22:03:03.931: INFO: (16) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 33.967322ms)
Apr 29 22:03:03.931: INFO: (16) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 34.335826ms)
Apr 29 22:03:03.943: INFO: (17) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 11.400642ms)
Apr 29 22:03:03.949: INFO: (17) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 16.820509ms)
Apr 29 22:03:03.951: INFO: (17) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 18.44643ms)
Apr 29 22:03:03.951: INFO: (17) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 18.886935ms)
Apr 29 22:03:03.953: INFO: (17) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 20.507255ms)
Apr 29 22:03:03.954: INFO: (17) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 22.223376ms)
Apr 29 22:03:03.955: INFO: (17) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 23.143288ms)
Apr 29 22:03:03.955: INFO: (17) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 22.928985ms)
Apr 29 22:03:03.957: INFO: (17) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 25.439516ms)
Apr 29 22:03:03.960: INFO: (17) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 28.268151ms)
Apr 29 22:03:03.960: INFO: (17) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 27.33304ms)
Apr 29 22:03:03.960: INFO: (17) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 28.412053ms)
Apr 29 22:03:03.961: INFO: (17) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 29.054861ms)
Apr 29 22:03:03.962: INFO: (17) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 29.938972ms)
Apr 29 22:03:03.962: INFO: (17) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 30.199975ms)
Apr 29 22:03:03.962: INFO: (17) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 30.374877ms)
Apr 29 22:03:03.985: INFO: (18) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 22.144175ms)
Apr 29 22:03:03.985: INFO: (18) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 22.582681ms)
Apr 29 22:03:03.987: INFO: (18) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 24.117ms)
Apr 29 22:03:03.993: INFO: (18) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 30.041574ms)
Apr 29 22:03:03.993: INFO: (18) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 29.850471ms)
Apr 29 22:03:03.993: INFO: (18) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 30.146875ms)
Apr 29 22:03:03.997: INFO: (18) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 34.320327ms)
Apr 29 22:03:03.998: INFO: (18) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 35.090136ms)
Apr 29 22:03:03.998: INFO: (18) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 35.190237ms)
Apr 29 22:03:03.999: INFO: (18) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 35.763945ms)
Apr 29 22:03:03.999: INFO: (18) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 36.22765ms)
Apr 29 22:03:04.000: INFO: (18) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 36.650455ms)
Apr 29 22:03:04.000: INFO: (18) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 37.158461ms)
Apr 29 22:03:04.000: INFO: (18) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 36.99976ms)
Apr 29 22:03:04.001: INFO: (18) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 37.879871ms)
Apr 29 22:03:04.001: INFO: (18) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 38.141774ms)
Apr 29 22:03:04.013: INFO: (19) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 12.012849ms)
Apr 29 22:03:04.017: INFO: (19) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq/proxy/rewriteme">test</a> (200; 14.764283ms)
Apr 29 22:03:04.017: INFO: (19) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 15.437391ms)
Apr 29 22:03:04.018: INFO: (19) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:460/proxy/: tls baz (200; 15.863897ms)
Apr 29 22:03:04.018: INFO: (19) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:462/proxy/: tls qux (200; 16.376603ms)
Apr 29 22:03:04.018: INFO: (19) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">... (200; 16.441604ms)
Apr 29 22:03:04.019: INFO: (19) /api/v1/namespaces/proxy-8399/pods/http:proxy-service-2lj2p-sswpq:160/proxy/: foo (200; 17.793721ms)
Apr 29 22:03:04.019: INFO: (19) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:1080/proxy/rewriteme">test<... (200; 18.181826ms)
Apr 29 22:03:04.020: INFO: (19) /api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/: <a href="/api/v1/namespaces/proxy-8399/pods/https:proxy-service-2lj2p-sswpq:443/proxy/tlsrewritem... (200; 18.289827ms)
Apr 29 22:03:04.021: INFO: (19) /api/v1/namespaces/proxy-8399/pods/proxy-service-2lj2p-sswpq:162/proxy/: bar (200; 20.299552ms)
Apr 29 22:03:04.023: INFO: (19) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname1/proxy/: foo (200; 22.077974ms)
Apr 29 22:03:04.025: INFO: (19) /api/v1/namespaces/proxy-8399/services/http:proxy-service-2lj2p:portname2/proxy/: bar (200; 23.875596ms)
Apr 29 22:03:04.025: INFO: (19) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname1/proxy/: foo (200; 23.998898ms)
Apr 29 22:03:04.026: INFO: (19) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname1/proxy/: tls baz (200; 24.832908ms)
Apr 29 22:03:04.026: INFO: (19) /api/v1/namespaces/proxy-8399/services/proxy-service-2lj2p:portname2/proxy/: bar (200; 24.612406ms)
Apr 29 22:03:04.026: INFO: (19) /api/v1/namespaces/proxy-8399/services/https:proxy-service-2lj2p:tlsportname2/proxy/: tls qux (200; 24.587406ms)
STEP: deleting ReplicationController proxy-service-2lj2p in namespace proxy-8399, will wait for the garbage collector to delete the pods
Apr 29 22:03:04.093: INFO: Deleting ReplicationController proxy-service-2lj2p took: 12.825659ms
Apr 29 22:03:04.694: INFO: Terminating ReplicationController proxy-service-2lj2p pods took: 600.096952ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:06.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8399" for this suite.

• [SLOW TEST:10.661 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":21,"skipped":376,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:06.722: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1144
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 22:03:06.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 22:03:07.050: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 22:03:09.068: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:03:11.062: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:03:13.065: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:03:15.059: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:03:17.061: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:03:19.062: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 22:03:19.070: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:03:21.084: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:03:23.082: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:03:25.080: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:03:27.080: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 29 22:03:27.090: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 29 22:03:31.136: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 29 22:03:31.136: INFO: Breadth first check of 10.244.2.15 on host 10.240.0.5...
Apr 29 22:03:31.140: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.16:9080/dial?request=hostname&protocol=http&host=10.244.2.15&port=8080&tries=1'] Namespace:pod-network-test-1144 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:03:31.140: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:03:31.308: INFO: Waiting for responses: map[]
Apr 29 22:03:31.308: INFO: reached 10.244.2.15 after 0/1 tries
Apr 29 22:03:31.308: INFO: Breadth first check of 10.244.3.10 on host 10.240.0.4...
Apr 29 22:03:31.314: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.16:9080/dial?request=hostname&protocol=http&host=10.244.3.10&port=8080&tries=1'] Namespace:pod-network-test-1144 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:03:31.314: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:03:31.501: INFO: Waiting for responses: map[]
Apr 29 22:03:31.501: INFO: reached 10.244.3.10 after 0/1 tries
Apr 29 22:03:31.501: INFO: Breadth first check of 10.244.4.20 on host 10.240.0.6...
Apr 29 22:03:31.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.16:9080/dial?request=hostname&protocol=http&host=10.244.4.20&port=8080&tries=1'] Namespace:pod-network-test-1144 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:03:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:03:31.665: INFO: Waiting for responses: map[]
Apr 29 22:03:31.665: INFO: reached 10.244.4.20 after 0/1 tries
Apr 29 22:03:31.665: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:31.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1144" for this suite.

• [SLOW TEST:24.965 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":382,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:31.687: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:31.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3267" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":23,"skipped":410,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:31.953: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:34.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3707" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":24,"skipped":416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:34.311: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:03:34.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0" in namespace "projected-7747" to be "Succeeded or Failed"
Apr 29 22:03:34.517: INFO: Pod "downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241052ms
Apr 29 22:03:36.529: INFO: Pod "downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015839768s
Apr 29 22:03:38.539: INFO: Pod "downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025484827s
STEP: Saw pod success
Apr 29 22:03:38.539: INFO: Pod "downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0" satisfied condition "Succeeded or Failed"
Apr 29 22:03:38.542: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0 container client-container: <nil>
STEP: delete the pod
Apr 29 22:03:38.597: INFO: Waiting for pod downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0 to disappear
Apr 29 22:03:38.601: INFO: Pod downwardapi-volume-8928333d-86bb-46dc-ba80-04cf103b85d0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:38.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7747" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":487,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:38.614: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1545
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:03:38.793: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 22:03:42.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-1545 --namespace=crd-publish-openapi-1545 create -f -'
Apr 29 22:03:43.348: INFO: stderr: ""
Apr 29 22:03:43.348: INFO: stdout: "e2e-test-crd-publish-openapi-4204-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 22:03:43.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-1545 --namespace=crd-publish-openapi-1545 delete e2e-test-crd-publish-openapi-4204-crds test-cr'
Apr 29 22:03:43.483: INFO: stderr: ""
Apr 29 22:03:43.484: INFO: stdout: "e2e-test-crd-publish-openapi-4204-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 29 22:03:43.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-1545 --namespace=crd-publish-openapi-1545 apply -f -'
Apr 29 22:03:43.835: INFO: stderr: ""
Apr 29 22:03:43.835: INFO: stdout: "e2e-test-crd-publish-openapi-4204-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 22:03:43.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-1545 --namespace=crd-publish-openapi-1545 delete e2e-test-crd-publish-openapi-4204-crds test-cr'
Apr 29 22:03:43.961: INFO: stderr: ""
Apr 29 22:03:43.961: INFO: stdout: "e2e-test-crd-publish-openapi-4204-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 22:03:43.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-1545 explain e2e-test-crd-publish-openapi-4204-crds'
Apr 29 22:03:44.183: INFO: stderr: ""
Apr 29 22:03:44.183: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4204-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:47.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1545" for this suite.

• [SLOW TEST:9.397 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":26,"skipped":506,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:48.011: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Apr 29 22:03:48.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-6497 create -f -'
Apr 29 22:03:48.530: INFO: stderr: ""
Apr 29 22:03:48.530: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Apr 29 22:03:48.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-6497 diff -f -'
Apr 29 22:03:49.042: INFO: rc: 1
Apr 29 22:03:49.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-6497 delete -f -'
Apr 29 22:03:49.156: INFO: stderr: ""
Apr 29 22:03:49.156: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6497" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":27,"skipped":513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-2ab7ac05-7d66-43d7-a1b9-2937567ea7e0
STEP: Creating a pod to test consume configMaps
Apr 29 22:03:49.375: INFO: Waiting up to 5m0s for pod "pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410" in namespace "configmap-8253" to be "Succeeded or Failed"
Apr 29 22:03:49.379: INFO: Pod "pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410": Phase="Pending", Reason="", readiness=false. Elapsed: 4.244251ms
Apr 29 22:03:51.388: INFO: Pod "pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013588984s
Apr 29 22:03:53.401: INFO: Pod "pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026622926s
STEP: Saw pod success
Apr 29 22:03:53.402: INFO: Pod "pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410" satisfied condition "Succeeded or Failed"
Apr 29 22:03:53.404: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:03:53.458: INFO: Waiting for pod pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410 to disappear
Apr 29 22:03:53.461: INFO: Pod pod-configmaps-62be9b53-f0c2-4721-a4d1-e601c3cca410 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:53.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8253" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":28,"skipped":548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:53.478: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:03:53.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-308" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":29,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:03:53.704: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 29 22:03:53.880: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 22:03:53.890: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 22:03:53.894: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-0 before test
Apr 29 22:03:53.903: INFO: azure-ip-masq-agent-xstdb from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:03:53.903: INFO: csi-secrets-store-lwtxb from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:03:53.903: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:03:53.903: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:03:53.903: INFO: csi-secrets-store-provider-azure-mhccg from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:03:53.903: INFO: kube-proxy-zfgdr from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:03:53.903: INFO: metrics-server-6c8cc7585b-khh4j from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container metrics-server ready: true, restart count 1
Apr 29 22:03:53.903: INFO: httpd-deployment-86bff9b6d7-t9rzx from kubectl-6497 started at 2021-04-29 22:03:48 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container httpd ready: false, restart count 0
Apr 29 22:03:53.903: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:03:53.903: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:03:53.903: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:03:53.903: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-1 before test
Apr 29 22:03:53.913: INFO: azure-ip-masq-agent-vsvkh from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:03:53.913: INFO: csi-secrets-store-6rvtw from kube-system started at 2021-04-29 20:49:00 +0000 UTC (3 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:03:53.913: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:03:53.913: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:03:53.913: INFO: csi-secrets-store-provider-azure-mvwgl from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:03:53.913: INFO: kube-proxy-kf9sp from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:03:53.913: INFO: sonobuoy from sonobuoy started at 2021-04-29 21:52:30 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 22:03:53.913: INFO: sonobuoy-e2e-job-b783d86494674f2c from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container e2e ready: true, restart count 0
Apr 29 22:03:53.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:03:53.913: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:03:53.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:03:53.913: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:03:53.913: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-2 before test
Apr 29 22:03:53.922: INFO: azure-ip-masq-agent-twx92 from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:03:53.922: INFO: coredns-6bbfb77cb5-rjkn6 from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container coredns ready: true, restart count 0
Apr 29 22:03:53.922: INFO: csi-secrets-store-7k84m from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:03:53.922: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:03:53.922: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:03:53.922: INFO: csi-secrets-store-provider-azure-9zz42 from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:03:53.922: INFO: kube-proxy-2zjsk from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:03:53.922: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:03:53.922: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:03:53.922: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c481aa78-d228-4d91-90ad-1ee27a810740 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.240.0.6 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.240.0.6 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 29 22:04:12.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.6 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321
Apr 29 22:04:12.343: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.6:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321 UDP
Apr 29 22:04:12.509: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.6 54321] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:12.509: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 29 22:04:17.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.6 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321
Apr 29 22:04:17.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.6:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:17.834: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321 UDP
Apr 29 22:04:18.004: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.6 54321] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:18.005: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 29 22:04:23.187: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.6 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:23.187: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321
Apr 29 22:04:23.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.6:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:23.345: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321 UDP
Apr 29 22:04:23.513: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.6 54321] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:23.514: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 29 22:04:28.680: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.6 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:28.680: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321
Apr 29 22:04:28.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.6:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:28.846: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321 UDP
Apr 29 22:04:29.021: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.6 54321] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:29.021: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Apr 29 22:04:34.190: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.6 http://127.0.0.1:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:34.190: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321
Apr 29 22:04:34.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.6:54321/hostname] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:34.378: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.6, port: 54321 UDP
Apr 29 22:04:34.539: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.6 54321] Namespace:sched-pred-9085 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:04:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: removing the label kubernetes.io/e2e-c481aa78-d228-4d91-90ad-1ee27a810740 off the node k8s-linuxpool-38811533-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c481aa78-d228-4d91-90ad-1ee27a810740
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:39.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9085" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:46.067 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":30,"skipped":639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 22:04:39.988: INFO: Waiting up to 5m0s for pod "pod-d1e45298-af16-4704-a55b-819ea60ee583" in namespace "emptydir-2075" to be "Succeeded or Failed"
Apr 29 22:04:39.995: INFO: Pod "pod-d1e45298-af16-4704-a55b-819ea60ee583": Phase="Pending", Reason="", readiness=false. Elapsed: 6.699877ms
Apr 29 22:04:42.005: INFO: Pod "pod-d1e45298-af16-4704-a55b-819ea60ee583": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017560208s
Apr 29 22:04:44.018: INFO: Pod "pod-d1e45298-af16-4704-a55b-819ea60ee583": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029631922s
STEP: Saw pod success
Apr 29 22:04:44.018: INFO: Pod "pod-d1e45298-af16-4704-a55b-819ea60ee583" satisfied condition "Succeeded or Failed"
Apr 29 22:04:44.026: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-d1e45298-af16-4704-a55b-819ea60ee583 container test-container: <nil>
STEP: delete the pod
Apr 29 22:04:44.055: INFO: Waiting for pod pod-d1e45298-af16-4704-a55b-819ea60ee583 to disappear
Apr 29 22:04:44.060: INFO: Pod pod-d1e45298-af16-4704-a55b-819ea60ee583 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:44.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2075" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":662,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:44.075: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:04:44.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-5826 version'
Apr 29 22:04:44.408: INFO: stderr: ""
Apr 29 22:04:44.408: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"19f2c5b389f3b63abae6ee1ced0af4ffb9924f40\", GitTreeState:\"clean\", BuildDate:\"2021-04-26T22:59:56Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:44.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5826" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":32,"skipped":662,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:44.420: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3157
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3157
I0429 22:04:44.653968      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3157, replica count: 2
I0429 22:04:47.704616      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:04:47.704: INFO: Creating new exec pod
Apr 29 22:04:52.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3157 exec execpodpmr59 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 29 22:04:53.059: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 29 22:04:53.059: INFO: stdout: ""
Apr 29 22:04:53.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3157 exec execpodpmr59 -- /bin/sh -x -c nc -zv -t -w 2 10.0.105.19 80'
Apr 29 22:04:53.343: INFO: stderr: "+ nc -zv -t -w 2 10.0.105.19 80\nConnection to 10.0.105.19 80 port [tcp/http] succeeded!\n"
Apr 29 22:04:53.343: INFO: stdout: ""
Apr 29 22:04:53.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3157 exec execpodpmr59 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.6 31238'
Apr 29 22:04:53.604: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.6 31238\nConnection to 10.240.0.6 31238 port [tcp/31238] succeeded!\n"
Apr 29 22:04:53.604: INFO: stdout: ""
Apr 29 22:04:53.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3157 exec execpodpmr59 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.5 31238'
Apr 29 22:04:53.864: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.5 31238\nConnection to 10.240.0.5 31238 port [tcp/31238] succeeded!\n"
Apr 29 22:04:53.864: INFO: stdout: ""
Apr 29 22:04:53.864: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3157" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.510 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":33,"skipped":677,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:53.931: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:04:54.190: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9" in namespace "downward-api-8740" to be "Succeeded or Failed"
Apr 29 22:04:54.193: INFO: Pod "downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683131ms
Apr 29 22:04:56.227: INFO: Pod "downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037162717s
Apr 29 22:04:58.237: INFO: Pod "downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04699579s
STEP: Saw pod success
Apr 29 22:04:58.237: INFO: Pod "downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9" satisfied condition "Succeeded or Failed"
Apr 29 22:04:58.241: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9 container client-container: <nil>
STEP: delete the pod
Apr 29 22:04:58.294: INFO: Waiting for pod downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9 to disappear
Apr 29 22:04:58.296: INFO: Pod downwardapi-volume-c4f595f7-8543-4502-b8c1-fbf4dbd828d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:58.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8740" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:04:58.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-251" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":35,"skipped":701,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:04:58.529: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:04:58.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838" in namespace "downward-api-9300" to be "Succeeded or Failed"
Apr 29 22:04:58.712: INFO: Pod "downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838": Phase="Pending", Reason="", readiness=false. Elapsed: 3.173536ms
Apr 29 22:05:00.724: INFO: Pod "downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015118496s
Apr 29 22:05:02.732: INFO: Pod "downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023245182s
STEP: Saw pod success
Apr 29 22:05:02.732: INFO: Pod "downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838" satisfied condition "Succeeded or Failed"
Apr 29 22:05:02.735: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838 container client-container: <nil>
STEP: delete the pod
Apr 29 22:05:02.758: INFO: Waiting for pod downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838 to disappear
Apr 29 22:05:02.761: INFO: Pod downwardapi-volume-934e19dc-04b1-4626-b8d3-5f905bc1a838 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9300" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":704,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:02.774: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2616
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-ae66ca7e-df36-413a-a0cc-3f958ec25c5b
STEP: Creating a pod to test consume secrets
Apr 29 22:05:02.989: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228" in namespace "projected-2616" to be "Succeeded or Failed"
Apr 29 22:05:02.992: INFO: Pod "pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228": Phase="Pending", Reason="", readiness=false. Elapsed: 3.153736ms
Apr 29 22:05:05.005: INFO: Pod "pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015525737s
Apr 29 22:05:07.017: INFO: Pod "pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02804491s
STEP: Saw pod success
Apr 29 22:05:07.017: INFO: Pod "pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228" satisfied condition "Succeeded or Failed"
Apr 29 22:05:07.020: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:05:07.067: INFO: Waiting for pod pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228 to disappear
Apr 29 22:05:07.071: INFO: Pod pod-projected-secrets-2c59a459-facb-453b-8380-26fd78ed0228 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:07.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2616" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":717,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:07.081: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6632
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-736466f4-9294-4999-a030-b10a9637464d
STEP: Creating secret with name secret-projected-all-test-volume-8f2a6562-4340-43e1-9623-0f38fca4f703
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 29 22:05:07.304: INFO: Waiting up to 5m0s for pod "projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf" in namespace "projected-6632" to be "Succeeded or Failed"
Apr 29 22:05:07.308: INFO: Pod "projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349038ms
Apr 29 22:05:09.317: INFO: Pod "projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013116847s
STEP: Saw pod success
Apr 29 22:05:09.318: INFO: Pod "projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf" satisfied condition "Succeeded or Failed"
Apr 29 22:05:09.321: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 29 22:05:09.346: INFO: Waiting for pod projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf to disappear
Apr 29 22:05:09.349: INFO: Pod projected-volume-5b6b0a94-8901-41c4-b9fc-776914d557bf no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:09.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6632" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":38,"skipped":720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:09.359: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-7247
STEP: creating replication controller nodeport-test in namespace services-7247
I0429 22:05:09.582420      21 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7247, replica count: 2
Apr 29 22:05:12.632: INFO: Creating new exec pod
I0429 22:05:12.632763      21 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:05:17.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-7247 exec execpodmrkcs -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 29 22:05:17.969: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 29 22:05:17.969: INFO: stdout: ""
Apr 29 22:05:17.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-7247 exec execpodmrkcs -- /bin/sh -x -c nc -zv -t -w 2 10.0.146.16 80'
Apr 29 22:05:18.243: INFO: stderr: "+ nc -zv -t -w 2 10.0.146.16 80\nConnection to 10.0.146.16 80 port [tcp/http] succeeded!\n"
Apr 29 22:05:18.243: INFO: stdout: ""
Apr 29 22:05:18.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-7247 exec execpodmrkcs -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.4 32539'
Apr 29 22:05:18.517: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.4 32539\nConnection to 10.240.0.4 32539 port [tcp/32539] succeeded!\n"
Apr 29 22:05:18.518: INFO: stdout: ""
Apr 29 22:05:18.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-7247 exec execpodmrkcs -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.5 32539'
Apr 29 22:05:18.833: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.5 32539\nConnection to 10.240.0.5 32539 port [tcp/32539] succeeded!\n"
Apr 29 22:05:18.833: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:18.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7247" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.500 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":39,"skipped":744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:18.860: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 22:05:19.068: INFO: Waiting up to 5m0s for pod "pod-7bd5bc76-165d-4eda-930b-9d4bc4319555" in namespace "emptydir-3226" to be "Succeeded or Failed"
Apr 29 22:05:19.071: INFO: Pod "pod-7bd5bc76-165d-4eda-930b-9d4bc4319555": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164236ms
Apr 29 22:05:21.082: INFO: Pod "pod-7bd5bc76-165d-4eda-930b-9d4bc4319555": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014485392s
Apr 29 22:05:23.094: INFO: Pod "pod-7bd5bc76-165d-4eda-930b-9d4bc4319555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026370125s
STEP: Saw pod success
Apr 29 22:05:23.094: INFO: Pod "pod-7bd5bc76-165d-4eda-930b-9d4bc4319555" satisfied condition "Succeeded or Failed"
Apr 29 22:05:23.099: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-7bd5bc76-165d-4eda-930b-9d4bc4319555 container test-container: <nil>
STEP: delete the pod
Apr 29 22:05:23.125: INFO: Waiting for pod pod-7bd5bc76-165d-4eda-930b-9d4bc4319555 to disappear
Apr 29 22:05:23.128: INFO: Pod pod-7bd5bc76-165d-4eda-930b-9d4bc4319555 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:23.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3226" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":772,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:23.139: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5912
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr 29 22:05:23.317: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Apr 29 22:05:23.783: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Apr 29 22:05:25.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:27.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:29.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:31.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:33.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:35.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:37.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:39.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:41.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755330723, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:05:44.994: INFO: Waited 1.122040406s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:45.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5912" for this suite.

• [SLOW TEST:22.710 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":41,"skipped":776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:45.849: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 29 22:05:52.154: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:52.154: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:52.308: INFO: Exec stderr: ""
Apr 29 22:05:52.308: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:52.308: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:52.456: INFO: Exec stderr: ""
Apr 29 22:05:52.457: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:52.457: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:52.635: INFO: Exec stderr: ""
Apr 29 22:05:52.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:52.635: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:52.814: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 29 22:05:52.814: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.039: INFO: Exec stderr: ""
Apr 29 22:05:53.039: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:53.039: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.192: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 29 22:05:53.192: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:53.192: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.347: INFO: Exec stderr: ""
Apr 29 22:05:53.347: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.507: INFO: Exec stderr: ""
Apr 29 22:05:53.507: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.657: INFO: Exec stderr: ""
Apr 29 22:05:53.657: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6713 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:05:53.657: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:05:53.815: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:53.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6713" for this suite.

• [SLOW TEST:7.983 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":803,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:53.833: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9802
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 22:05:54.072: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Apr 29 22:05:54.076: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 22:05:54.094: INFO: waiting for watch events with expected annotations
Apr 29 22:05:54.094: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9802" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":43,"skipped":821,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:54.156: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-d6b7aeee-d312-4b04-94b6-dbfec575a84e
STEP: Creating a pod to test consume configMaps
Apr 29 22:05:54.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9" in namespace "projected-5515" to be "Succeeded or Failed"
Apr 29 22:05:54.338: INFO: Pod "pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.082134ms
Apr 29 22:05:56.454: INFO: Pod "pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119091962s
Apr 29 22:05:58.471: INFO: Pod "pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.136092265s
STEP: Saw pod success
Apr 29 22:05:58.471: INFO: Pod "pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9" satisfied condition "Succeeded or Failed"
Apr 29 22:05:58.474: INFO: Trying to get logs from node k8s-linuxpool-38811533-1 pod pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:05:58.561: INFO: Waiting for pod pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9 to disappear
Apr 29 22:05:58.566: INFO: Pod pod-projected-configmaps-a766f08a-d239-4332-acd0-facea31bbca9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:05:58.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5515" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":44,"skipped":824,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:05:58.582: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 29 22:05:58.782: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 22:05:58.790: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 22:05:58.794: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-0 before test
Apr 29 22:05:58.802: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-6713 started at 2021-04-29 22:05:50 +0000 UTC (2 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container busybox-1 ready: true, restart count 0
Apr 29 22:05:58.803: INFO: 	Container busybox-2 ready: true, restart count 0
Apr 29 22:05:58.803: INFO: azure-ip-masq-agent-xstdb from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:05:58.803: INFO: csi-secrets-store-lwtxb from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:05:58.803: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:05:58.803: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:05:58.803: INFO: csi-secrets-store-provider-azure-mhccg from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:05:58.803: INFO: kube-proxy-zfgdr from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:05:58.803: INFO: metrics-server-6c8cc7585b-khh4j from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container metrics-server ready: true, restart count 1
Apr 29 22:05:58.803: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:05:58.803: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:05:58.803: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:05:58.803: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-1 before test
Apr 29 22:05:58.813: INFO: azure-ip-masq-agent-vsvkh from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:05:58.813: INFO: csi-secrets-store-6rvtw from kube-system started at 2021-04-29 20:49:00 +0000 UTC (3 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:05:58.813: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:05:58.813: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:05:58.813: INFO: csi-secrets-store-provider-azure-mvwgl from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:05:58.813: INFO: kube-proxy-kf9sp from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:05:58.813: INFO: sonobuoy from sonobuoy started at 2021-04-29 21:52:30 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 22:05:58.813: INFO: sonobuoy-e2e-job-b783d86494674f2c from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:05:58.813: INFO: 	Container e2e ready: true, restart count 0
Apr 29 22:05:58.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:05:58.814: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:05:58.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:05:58.814: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:05:58.814: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-2 before test
Apr 29 22:05:58.825: INFO: test-pod from e2e-kubelet-etc-hosts-6713 started at 2021-04-29 22:05:46 +0000 UTC (3 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container busybox-1 ready: true, restart count 0
Apr 29 22:05:58.825: INFO: 	Container busybox-2 ready: true, restart count 0
Apr 29 22:05:58.825: INFO: 	Container busybox-3 ready: true, restart count 0
Apr 29 22:05:58.825: INFO: azure-ip-masq-agent-twx92 from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:05:58.825: INFO: coredns-6bbfb77cb5-rjkn6 from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container coredns ready: true, restart count 0
Apr 29 22:05:58.825: INFO: csi-secrets-store-7k84m from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:05:58.825: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:05:58.825: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:05:58.825: INFO: csi-secrets-store-provider-azure-9zz42 from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:05:58.825: INFO: kube-proxy-2zjsk from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:05:58.825: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:05:58.825: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:05:58.825: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4a8b9faf-b457-4b8f-a85e-37a1b510d563 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.240.0.5 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4a8b9faf-b457-4b8f-a85e-37a1b510d563 off the node k8s-linuxpool-38811533-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4a8b9faf-b457-4b8f-a85e-37a1b510d563
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:11:07.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9660" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.514 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":45,"skipped":827,"failed":0}
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:11:07.097: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:11:07.348: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b6e202aa-d74b-42d3-ad98-b3f5accc4b27", Controller:(*bool)(0xc002347fbe), BlockOwnerDeletion:(*bool)(0xc002347fbf)}}
Apr 29 22:11:07.360: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d20afeef-9fed-4f08-afa5-8f7de5da0d5a", Controller:(*bool)(0xc003c3c416), BlockOwnerDeletion:(*bool)(0xc003c3c417)}}
Apr 29 22:11:07.379: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c3ad402f-f835-48a5-904c-baac0071ab33", Controller:(*bool)(0xc0052ff0d6), BlockOwnerDeletion:(*bool)(0xc0052ff0d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:11:12.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9919" for this suite.

• [SLOW TEST:5.323 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":46,"skipped":827,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:11:12.419: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:11:12.593: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 29 22:11:12.603: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 29 22:11:17.618: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 22:11:17.618: INFO: Creating deployment "test-rolling-update-deployment"
Apr 29 22:11:17.630: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 29 22:11:17.636: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 29 22:11:19.648: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 29 22:11:19.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331077, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331077, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:11:21.661: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 22:11:21.670: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3991  f10c2400-aeef-4267-9d2e-af84f22c89ac 13440 1 2021-04-29 22:11:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-04-29 22:11:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 22:11:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00375e768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-29 22:11:17 +0000 UTC,LastTransitionTime:2021-04-29 22:11:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-04-29 22:11:20 +0000 UTC,LastTransitionTime:2021-04-29 22:11:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 22:11:21.674: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-3991  8b59bce4-79b5-4aa3-ae79-3ff65dfb1143 13429 1 2021-04-29 22:11:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f10c2400-aeef-4267-9d2e-af84f22c89ac 0xc00375ec37 0xc00375ec38}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:11:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10c2400-aeef-4267-9d2e-af84f22c89ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00375ecc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 22:11:21.674: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 29 22:11:21.674: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3991  ce4eee9a-909b-4663-a56b-9f1b207ec5e9 13439 2 2021-04-29 22:11:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f10c2400-aeef-4267-9d2e-af84f22c89ac 0xc00375eb27 0xc00375eb28}] []  [{e2e.test Update apps/v1 2021-04-29 22:11:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 22:11:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f10c2400-aeef-4267-9d2e-af84f22c89ac\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00375ebc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 22:11:21.677: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-h2kwn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-h2kwn test-rolling-update-deployment-6b6bf9df46- deployment-3991  2a9b5d07-78b6-46d7-8124-e0cb49a1a4d4 13428 0 2021-04-29 22:11:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 8b59bce4-79b5-4aa3-ae79-3ff65dfb1143 0xc00375f0e7 0xc00375f0e8}] []  [{kube-controller-manager Update v1 2021-04-29 22:11:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8b59bce4-79b5-4aa3-ae79-3ff65dfb1143\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 22:11:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4m7bp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4m7bp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4m7bp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:11:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:11:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:11:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:11:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.28,StartTime:2021-04-29 22:11:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 22:11:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://c8200d5be833edf2a131fc00d8cebcd5fe9445283bceba28999add16be63d184,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:11:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3991" for this suite.

• [SLOW TEST:9.273 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":47,"skipped":830,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:11:21.692: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3490
STEP: creating service affinity-nodeport-transition in namespace services-3490
STEP: creating replication controller affinity-nodeport-transition in namespace services-3490
I0429 22:11:21.916435      21 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3490, replica count: 3
I0429 22:11:24.966806      21 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:11:24.987: INFO: Creating new exec pod
Apr 29 22:11:30.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Apr 29 22:11:30.338: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 29 22:11:30.338: INFO: stdout: ""
Apr 29 22:11:30.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c nc -zv -t -w 2 10.0.78.94 80'
Apr 29 22:11:30.613: INFO: stderr: "+ nc -zv -t -w 2 10.0.78.94 80\nConnection to 10.0.78.94 80 port [tcp/http] succeeded!\n"
Apr 29 22:11:30.613: INFO: stdout: ""
Apr 29 22:11:30.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.6 31367'
Apr 29 22:11:30.894: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.6 31367\nConnection to 10.240.0.6 31367 port [tcp/31367] succeeded!\n"
Apr 29 22:11:30.894: INFO: stdout: ""
Apr 29 22:11:30.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.5 31367'
Apr 29 22:11:31.171: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.5 31367\nConnection to 10.240.0.5 31367 port [tcp/31367] succeeded!\n"
Apr 29 22:11:31.171: INFO: stdout: ""
Apr 29 22:11:31.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.5:31367/ ; done'
Apr 29 22:11:31.590: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n"
Apr 29 22:11:31.590: INFO: stdout: "\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-h6cqx\naffinity-nodeport-transition-h6cqx\naffinity-nodeport-transition-cbfpr\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-cbfpr"
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-h6cqx
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-h6cqx
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:31.590: INFO: Received response from host: affinity-nodeport-transition-cbfpr
Apr 29 22:11:31.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3490 exec execpod-affinityrpwdp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.5:31367/ ; done'
Apr 29 22:11:32.034: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:31367/\n"
Apr 29 22:11:32.034: INFO: stdout: "\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq\naffinity-nodeport-transition-g97sq"
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Received response from host: affinity-nodeport-transition-g97sq
Apr 29 22:11:32.034: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3490, will wait for the garbage collector to delete the pods
Apr 29 22:11:32.125: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.379326ms
Apr 29 22:11:32.225: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.186545ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:11:41.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3490" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:19.900 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":48,"skipped":835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:11:41.595: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2466
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 29 22:11:41.772: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:11:45.556: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:12:00.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2466" for this suite.

• [SLOW TEST:18.837 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":49,"skipped":927,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:12:00.434: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:12:16.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4473" for this suite.

• [SLOW TEST:16.407 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":50,"skipped":935,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:12:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-sk2g
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 22:12:17.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sk2g" in namespace "subpath-8688" to be "Succeeded or Failed"
Apr 29 22:12:17.101: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.19843ms
Apr 29 22:12:19.117: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019243549s
Apr 29 22:12:21.128: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 4.030625111s
Apr 29 22:12:23.140: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 6.042440865s
Apr 29 22:12:25.151: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 8.053289798s
Apr 29 22:12:27.160: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 10.062320801s
Apr 29 22:12:29.168: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 12.070230082s
Apr 29 22:12:31.181: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 14.083262197s
Apr 29 22:12:33.192: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 16.094191181s
Apr 29 22:12:35.203: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 18.105658757s
Apr 29 22:12:37.211: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Running", Reason="", readiness=true. Elapsed: 20.113654989s
Apr 29 22:12:39.220: INFO: Pod "pod-subpath-test-secret-sk2g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.122319515s
STEP: Saw pod success
Apr 29 22:12:39.220: INFO: Pod "pod-subpath-test-secret-sk2g" satisfied condition "Succeeded or Failed"
Apr 29 22:12:39.223: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-subpath-test-secret-sk2g container test-container-subpath-secret-sk2g: <nil>
STEP: delete the pod
Apr 29 22:12:39.282: INFO: Waiting for pod pod-subpath-test-secret-sk2g to disappear
Apr 29 22:12:39.285: INFO: Pod pod-subpath-test-secret-sk2g no longer exists
STEP: Deleting pod pod-subpath-test-secret-sk2g
Apr 29 22:12:39.285: INFO: Deleting pod "pod-subpath-test-secret-sk2g" in namespace "subpath-8688"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:12:39.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8688" for this suite.

• [SLOW TEST:22.456 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":51,"skipped":943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:12:39.303: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9582
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:13:06.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9582" for this suite.

• [SLOW TEST:27.619 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":992,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:13:06.922: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 29 22:13:11.690: INFO: Successfully updated pod "annotationupdate3478cf73-0dc7-4690-83fb-306b357ddd1a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:13:13.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9742" for this suite.

• [SLOW TEST:6.816 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":994,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:13:13.739: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:13:19.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8321" for this suite.

• [SLOW TEST:5.604 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":54,"skipped":1004,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:13:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6985
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Apr 29 22:13:19.539: INFO: created test-pod-1
Apr 29 22:13:19.558: INFO: created test-pod-2
Apr 29 22:13:19.569: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:13:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6985" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":55,"skipped":1022,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:13:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-91925562-3656-4736-b6bd-04d1f55e8b65
STEP: Creating a pod to test consume secrets
Apr 29 22:13:19.869: INFO: Waiting up to 5m0s for pod "pod-secrets-0184315b-36ae-404d-8746-26b93e746618" in namespace "secrets-1958" to be "Succeeded or Failed"
Apr 29 22:13:19.872: INFO: Pod "pod-secrets-0184315b-36ae-404d-8746-26b93e746618": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095728ms
Apr 29 22:13:21.883: INFO: Pod "pod-secrets-0184315b-36ae-404d-8746-26b93e746618": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013709523s
Apr 29 22:13:23.894: INFO: Pod "pod-secrets-0184315b-36ae-404d-8746-26b93e746618": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024716511s
STEP: Saw pod success
Apr 29 22:13:23.894: INFO: Pod "pod-secrets-0184315b-36ae-404d-8746-26b93e746618" satisfied condition "Succeeded or Failed"
Apr 29 22:13:23.897: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-0184315b-36ae-404d-8746-26b93e746618 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:13:23.928: INFO: Waiting for pod pod-secrets-0184315b-36ae-404d-8746-26b93e746618 to disappear
Apr 29 22:13:23.931: INFO: Pod pod-secrets-0184315b-36ae-404d-8746-26b93e746618 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:13:23.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1958" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":1038,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:13:23.943: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3115
Apr 29 22:13:26.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 29 22:13:26.439: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 29 22:13:26.439: INFO: stdout: "iptables"
Apr 29 22:13:26.439: INFO: proxyMode: iptables
Apr 29 22:13:26.458: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 29 22:13:26.463: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3115
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3115
I0429 22:13:26.488128      21 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3115, replica count: 3
I0429 22:13:29.538420      21 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:13:29.558: INFO: Creating new exec pod
Apr 29 22:13:34.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Apr 29 22:13:34.875: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Apr 29 22:13:34.875: INFO: stdout: ""
Apr 29 22:13:34.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c nc -zv -t -w 2 10.0.49.147 80'
Apr 29 22:13:35.128: INFO: stderr: "+ nc -zv -t -w 2 10.0.49.147 80\nConnection to 10.0.49.147 80 port [tcp/http] succeeded!\n"
Apr 29 22:13:35.128: INFO: stdout: ""
Apr 29 22:13:35.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.4 32521'
Apr 29 22:13:35.403: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.4 32521\nConnection to 10.240.0.4 32521 port [tcp/32521] succeeded!\n"
Apr 29 22:13:35.403: INFO: stdout: ""
Apr 29 22:13:35.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.6 32521'
Apr 29 22:13:35.661: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.6 32521\nConnection to 10.240.0.6 32521 port [tcp/32521] succeeded!\n"
Apr 29 22:13:35.661: INFO: stdout: ""
Apr 29 22:13:35.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.5:32521/ ; done'
Apr 29 22:13:36.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n"
Apr 29 22:13:36.003: INFO: stdout: "\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt\naffinity-nodeport-timeout-xxnlt"
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Received response from host: affinity-nodeport-timeout-xxnlt
Apr 29 22:13:36.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.5:32521/'
Apr 29 22:13:36.286: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n"
Apr 29 22:13:36.286: INFO: stdout: "affinity-nodeport-timeout-xxnlt"
Apr 29 22:13:56.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3115 exec execpod-affinityhtx4q -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.5:32521/'
Apr 29 22:13:56.944: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.5:32521/\n"
Apr 29 22:13:56.944: INFO: stdout: "affinity-nodeport-timeout-9b4xm"
Apr 29 22:13:56.944: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3115, will wait for the garbage collector to delete the pods
Apr 29 22:13:57.073: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 11.720106ms
Apr 29 22:13:57.773: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.121102ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:14:12.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3115" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:48.310 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":57,"skipped":1049,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:14:12.253: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Apr 29 22:14:12.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 29 22:14:12.610: INFO: stderr: ""
Apr 29 22:14:12.610: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Apr 29 22:14:12.610: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 29 22:14:12.610: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8686" to be "running and ready, or succeeded"
Apr 29 22:14:12.614: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.639533ms
Apr 29 22:14:14.626: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015412859s
Apr 29 22:14:16.636: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.026181066s
Apr 29 22:14:16.636: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 29 22:14:16.637: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 29 22:14:16.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator'
Apr 29 22:14:16.789: INFO: stderr: ""
Apr 29 22:14:16.789: INFO: stdout: "I0429 22:14:13.989776       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/b4r5 571\nI0429 22:14:14.189909       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/qsnt 378\nI0429 22:14:14.389857       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bpc 256\nI0429 22:14:14.589870       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/dcw 455\nI0429 22:14:14.789855       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kgq 370\nI0429 22:14:14.989856       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/n96 205\nI0429 22:14:15.189858       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/x2j 536\nI0429 22:14:15.389896       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/plzg 266\nI0429 22:14:15.589855       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/4r5j 576\nI0429 22:14:15.789848       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/w7f 529\nI0429 22:14:15.989888       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/ssj5 436\nI0429 22:14:16.189862       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/vft 235\nI0429 22:14:16.389888       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/sts 556\nI0429 22:14:16.589816       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/6g5x 555\nI0429 22:14:16.789871       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/h599 551\n"
STEP: limiting log lines
Apr 29 22:14:16.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator --tail=1'
Apr 29 22:14:16.896: INFO: stderr: ""
Apr 29 22:14:16.896: INFO: stdout: "I0429 22:14:16.789871       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/h599 551\n"
Apr 29 22:14:16.896: INFO: got output "I0429 22:14:16.789871       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/h599 551\n"
STEP: limiting log bytes
Apr 29 22:14:16.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator --limit-bytes=1'
Apr 29 22:14:17.010: INFO: stderr: ""
Apr 29 22:14:17.010: INFO: stdout: "I"
Apr 29 22:14:17.010: INFO: got output "I"
STEP: exposing timestamps
Apr 29 22:14:17.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 29 22:14:17.120: INFO: stderr: ""
Apr 29 22:14:17.120: INFO: stdout: "2021-04-29T22:14:16.990102970Z I0429 22:14:16.989906       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/9mk 293\n"
Apr 29 22:14:17.120: INFO: got output "2021-04-29T22:14:16.990102970Z I0429 22:14:16.989906       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/9mk 293\n"
STEP: restricting to a time range
Apr 29 22:14:19.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator --since=1s'
Apr 29 22:14:19.739: INFO: stderr: ""
Apr 29 22:14:19.739: INFO: stdout: "I0429 22:14:18.789885       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/xnpj 396\nI0429 22:14:18.989851       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/jtqq 539\nI0429 22:14:19.189863       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/pcj 596\nI0429 22:14:19.389833       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/bdb2 449\nI0429 22:14:19.589869       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/ftt4 232\n"
Apr 29 22:14:19.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 logs logs-generator logs-generator --since=24h'
Apr 29 22:14:19.882: INFO: stderr: ""
Apr 29 22:14:19.882: INFO: stdout: "I0429 22:14:13.989776       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/b4r5 571\nI0429 22:14:14.189909       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/qsnt 378\nI0429 22:14:14.389857       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/bpc 256\nI0429 22:14:14.589870       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/dcw 455\nI0429 22:14:14.789855       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/kgq 370\nI0429 22:14:14.989856       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/n96 205\nI0429 22:14:15.189858       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/x2j 536\nI0429 22:14:15.389896       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/plzg 266\nI0429 22:14:15.589855       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/4r5j 576\nI0429 22:14:15.789848       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/w7f 529\nI0429 22:14:15.989888       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/ssj5 436\nI0429 22:14:16.189862       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/vft 235\nI0429 22:14:16.389888       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/sts 556\nI0429 22:14:16.589816       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/6g5x 555\nI0429 22:14:16.789871       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/h599 551\nI0429 22:14:16.989906       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/9mk 293\nI0429 22:14:17.189865       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/kr8 552\nI0429 22:14:17.389858       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/qrm 545\nI0429 22:14:17.589952       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/4cz 268\nI0429 22:14:17.789859       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/72l 516\nI0429 22:14:17.989896       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/xgx9 247\nI0429 22:14:18.189869       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/plm 544\nI0429 22:14:18.389865       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/8w5 389\nI0429 22:14:18.589864       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/9wp 421\nI0429 22:14:18.789885       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/xnpj 396\nI0429 22:14:18.989851       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/jtqq 539\nI0429 22:14:19.189863       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/pcj 596\nI0429 22:14:19.389833       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/bdb2 449\nI0429 22:14:19.589869       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/kube-system/pods/ftt4 232\nI0429 22:14:19.790273       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/95xb 244\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Apr 29 22:14:19.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-8686 delete pod logs-generator'
Apr 29 22:14:31.810: INFO: stderr: ""
Apr 29 22:14:31.810: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:14:31.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8686" for this suite.

• [SLOW TEST:19.581 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":58,"skipped":1050,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:14:31.834: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6030
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-520d9ed3-8f93-4f12-968d-0748707b0ebd
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-520d9ed3-8f93-4f12-968d-0748707b0ebd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:15:48.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6030" for this suite.

• [SLOW TEST:76.655 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":59,"skipped":1068,"failed":0}
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:15:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5446
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-5446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5446 to expose endpoints map[]
Apr 29 22:15:48.742: INFO: successfully validated that service endpoint-test2 in namespace services-5446 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5446 to expose endpoints map[pod1:[80]]
Apr 29 22:15:50.810: INFO: successfully validated that service endpoint-test2 in namespace services-5446 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-5446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5446 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 29 22:15:53.855: INFO: successfully validated that service endpoint-test2 in namespace services-5446 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-5446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5446 to expose endpoints map[pod2:[80]]
Apr 29 22:15:53.897: INFO: successfully validated that service endpoint-test2 in namespace services-5446 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-5446
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5446 to expose endpoints map[]
Apr 29 22:15:54.943: INFO: successfully validated that service endpoint-test2 in namespace services-5446 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:15:54.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5446" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.522 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":60,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:15:55.014: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-505
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:15:55.203: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:15:55.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-505" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":61,"skipped":1104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:15:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2441
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:15:56.342: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 29 22:16:00.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 create -f -'
Apr 29 22:16:01.473: INFO: stderr: ""
Apr 29 22:16:01.473: INFO: stdout: "e2e-test-crd-publish-openapi-3189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 29 22:16:01.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 delete e2e-test-crd-publish-openapi-3189-crds test-foo'
Apr 29 22:16:01.622: INFO: stderr: ""
Apr 29 22:16:01.622: INFO: stdout: "e2e-test-crd-publish-openapi-3189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 29 22:16:01.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 apply -f -'
Apr 29 22:16:01.952: INFO: stderr: ""
Apr 29 22:16:01.952: INFO: stdout: "e2e-test-crd-publish-openapi-3189-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 29 22:16:01.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 delete e2e-test-crd-publish-openapi-3189-crds test-foo'
Apr 29 22:16:02.233: INFO: stderr: ""
Apr 29 22:16:02.233: INFO: stdout: "e2e-test-crd-publish-openapi-3189-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 29 22:16:02.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 create -f -'
Apr 29 22:16:02.586: INFO: rc: 1
Apr 29 22:16:02.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 apply -f -'
Apr 29 22:16:02.890: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 29 22:16:02.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 create -f -'
Apr 29 22:16:03.181: INFO: rc: 1
Apr 29 22:16:03.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 --namespace=crd-publish-openapi-2441 apply -f -'
Apr 29 22:16:03.476: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 29 22:16:03.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 explain e2e-test-crd-publish-openapi-3189-crds'
Apr 29 22:16:03.773: INFO: stderr: ""
Apr 29 22:16:03.773: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 29 22:16:03.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 explain e2e-test-crd-publish-openapi-3189-crds.metadata'
Apr 29 22:16:04.092: INFO: stderr: ""
Apr 29 22:16:04.092: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 29 22:16:04.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 explain e2e-test-crd-publish-openapi-3189-crds.spec'
Apr 29 22:16:04.423: INFO: stderr: ""
Apr 29 22:16:04.423: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 29 22:16:04.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 explain e2e-test-crd-publish-openapi-3189-crds.spec.bars'
Apr 29 22:16:04.648: INFO: stderr: ""
Apr 29 22:16:04.648: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3189-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 29 22:16:04.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2441 explain e2e-test-crd-publish-openapi-3189-crds.spec.bars2'
Apr 29 22:16:04.881: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:16:08.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2441" for this suite.

• [SLOW TEST:12.872 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":62,"skipped":1131,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:16:08.767: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 22:16:08.995: INFO: Waiting up to 5m0s for pod "pod-a1fc85b2-6520-4a14-8d7b-90895686152b" in namespace "emptydir-3895" to be "Succeeded or Failed"
Apr 29 22:16:08.999: INFO: Pod "pod-a1fc85b2-6520-4a14-8d7b-90895686152b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729133ms
Apr 29 22:16:11.012: INFO: Pod "pod-a1fc85b2-6520-4a14-8d7b-90895686152b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016886347s
Apr 29 22:16:13.023: INFO: Pod "pod-a1fc85b2-6520-4a14-8d7b-90895686152b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027929135s
STEP: Saw pod success
Apr 29 22:16:13.023: INFO: Pod "pod-a1fc85b2-6520-4a14-8d7b-90895686152b" satisfied condition "Succeeded or Failed"
Apr 29 22:16:13.026: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-a1fc85b2-6520-4a14-8d7b-90895686152b container test-container: <nil>
STEP: delete the pod
Apr 29 22:16:13.064: INFO: Waiting for pod pod-a1fc85b2-6520-4a14-8d7b-90895686152b to disappear
Apr 29 22:16:13.068: INFO: Pod pod-a1fc85b2-6520-4a14-8d7b-90895686152b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:16:13.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3895" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":63,"skipped":1140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:16:13.079: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Apr 29 22:16:13.261: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 22:17:13.328: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:17:13.331: INFO: Starting informer...
STEP: Starting pods...
Apr 29 22:17:13.557: INFO: Pod1 is running on k8s-linuxpool-38811533-0. Tainting Node
Apr 29 22:17:15.791: INFO: Pod2 is running on k8s-linuxpool-38811533-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 29 22:17:22.522: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 29 22:17:42.652: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:17:42.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5010" for this suite.

• [SLOW TEST:89.613 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":64,"skipped":1164,"failed":0}
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:17:42.693: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Apr 29 22:17:42.942: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:42.942: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:42.956: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:42.956: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:42.977: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:42.977: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:43.132: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:43.132: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 29 22:17:45.037: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 29 22:17:45.037: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 29 22:17:45.455: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Apr 29 22:17:45.490: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Apr 29 22:17:45.492: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.492: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.492: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.492: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 0
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.493: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.514: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.514: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.676: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.676: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.676: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.676: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 2
Apr 29 22:17:45.677: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
STEP: listing Deployments
Apr 29 22:17:45.684: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Apr 29 22:17:45.698: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Apr 29 22:17:45.703: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.738: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.790: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.822: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.851: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.883: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.903: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 29 22:17:45.917: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
Apr 29 22:17:48.191: INFO: observed Deployment test-deployment in namespace deployment-609 with ReadyReplicas 1
STEP: deleting the Deployment
Apr 29 22:17:48.216: INFO: observed event type MODIFIED
Apr 29 22:17:48.216: INFO: observed event type MODIFIED
Apr 29 22:17:48.216: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
Apr 29 22:17:48.217: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 22:17:48.235: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 29 22:17:48.243: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-609  db8ca03a-aac6-4d81-9779-a6943addbea4 15306 3 2021-04-29 22:17:45 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 02a78774-1e78-47c2-80a8-0b36bf0fbe41 0xc004620507 0xc004620508}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:17:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02a78774-1e78-47c2-80a8-0b36bf0fbe41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004620590 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 29 22:17:48.247: INFO: pod: "test-deployment-768947d6f5-ds4xg":
&Pod{ObjectMeta:{test-deployment-768947d6f5-ds4xg test-deployment-768947d6f5- deployment-609  d778eaf5-5c8f-4231-bf1f-69f8bc6a16c6 15307 0 2021-04-29 22:17:48 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 db8ca03a-aac6-4d81-9779-a6943addbea4 0xc004620b97 0xc004620b98}] []  [{kube-controller-manager Update v1 2021-04-29 22:17:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db8ca03a-aac6-4d81-9779-a6943addbea4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xgb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xgb9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xgb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 29 22:17:48.248: INFO: pod: "test-deployment-768947d6f5-jzvbv":
&Pod{ObjectMeta:{test-deployment-768947d6f5-jzvbv test-deployment-768947d6f5- deployment-609  8f4a55ed-5bff-4722-96e3-0dcdd73460db 15289 0 2021-04-29 22:17:45 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-768947d6f5 db8ca03a-aac6-4d81-9779-a6943addbea4 0xc004620d67 0xc004620d68}] []  [{kube-controller-manager Update v1 2021-04-29 22:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db8ca03a-aac6-4d81-9779-a6943addbea4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 22:17:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xgb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xgb9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xgb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.46,StartTime:2021-04-29 22:17:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 22:17:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0f3136c78c5c0b8ee6626d569ec3eda677e5457828c94b68ba5babbc7bfa9c38,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 29 22:17:48.248: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-609  551fedfd-8db5-43d1-8c31-c325ed75af11 15305 4 2021-04-29 22:17:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 02a78774-1e78-47c2-80a8-0b36bf0fbe41 0xc004620657 0xc004620658}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02a78774-1e78-47c2-80a8-0b36bf0fbe41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046207a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 29 22:17:48.251: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-609  685f3189-2b66-4fbe-b578-ec252a8bc98b 15216 2 2021-04-29 22:17:42 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 02a78774-1e78-47c2-80a8-0b36bf0fbe41 0xc004620817 0xc004620818}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:17:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02a78774-1e78-47c2-80a8-0b36bf0fbe41\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004620880 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Apr 29 22:17:48.255: INFO: pod: "test-deployment-8b6954bfb-q5dtx":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-q5dtx test-deployment-8b6954bfb- deployment-609  b4f35157-107f-4a91-800d-feef7f27ffbd 15178 0 2021-04-29 22:17:42 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 685f3189-2b66-4fbe-b578-ec252a8bc98b 0xc00464e5b7 0xc00464e5b8}] []  [{kube-controller-manager Update v1 2021-04-29 22:17:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"685f3189-2b66-4fbe-b578-ec252a8bc98b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 22:17:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xgb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xgb9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xgb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:17:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.4.45,StartTime:2021-04-29 22:17:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 22:17:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://771e3733b0044464f367be0e3377a7de308dbfa9e26b7f73dd0cf8146afa12ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:17:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-609" for this suite.

• [SLOW TEST:5.582 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":65,"skipped":1164,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:17:48.275: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 22:17:56.662: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 22:17:56.667: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 22:17:58.667: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 22:17:58.678: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 22:18:00.667: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 22:18:00.676: INFO: Pod pod-with-poststart-http-hook still exists
Apr 29 22:18:02.667: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 29 22:18:02.676: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:18:02.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2753" for this suite.

• [SLOW TEST:14.416 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":66,"skipped":1174,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:18:02.691: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:18:03.703: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:18:05.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331483, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331483, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331483, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:18:08.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:18:08.791: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:18:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5598" for this suite.
STEP: Destroying namespace "webhook-5598-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.506 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":67,"skipped":1176,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:18:10.199: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4113.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4113.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4113.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4113.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4113.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4113.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:18:26.594: INFO: DNS probes using dns-4113/dns-test-b3182e6d-564f-4360-9a8c-721a1a63f922 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:18:26.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4113" for this suite.

• [SLOW TEST:16.456 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":68,"skipped":1184,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:18:26.656: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Apr 29 22:18:27.373: INFO: created pod pod-service-account-defaultsa
Apr 29 22:18:27.373: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 29 22:18:27.379: INFO: created pod pod-service-account-mountsa
Apr 29 22:18:27.379: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 29 22:18:27.387: INFO: created pod pod-service-account-nomountsa
Apr 29 22:18:27.387: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 29 22:18:27.392: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 29 22:18:27.392: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 29 22:18:27.408: INFO: created pod pod-service-account-mountsa-mountspec
Apr 29 22:18:27.409: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 29 22:18:27.415: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 29 22:18:27.416: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 29 22:18:27.425: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 29 22:18:27.425: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 29 22:18:27.443: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 29 22:18:27.443: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 29 22:18:27.477: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 29 22:18:27.477: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:18:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1066" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":69,"skipped":1187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:18:27.510: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:18:38.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3622" for this suite.

• [SLOW TEST:11.338 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":70,"skipped":1227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:18:38.852: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-4bbd276b-dfdb-478b-ad8f-5ce4bef131be in namespace container-probe-7526
Apr 29 22:18:43.084: INFO: Started pod busybox-4bbd276b-dfdb-478b-ad8f-5ce4bef131be in namespace container-probe-7526
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 22:18:43.088: INFO: Initial restart count of pod busybox-4bbd276b-dfdb-478b-ad8f-5ce4bef131be is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:22:44.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7526" for this suite.

• [SLOW TEST:245.595 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1318,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:22:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-4686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 22:22:44.685: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 22:22:44.701: INFO: waiting for watch events with expected annotations
Apr 29 22:22:44.701: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:22:44.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-4686" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":72,"skipped":1326,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:22:44.747: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7220
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 29 22:22:44.925: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 29 22:22:59.501: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:23:03.284: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:18.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7220" for this suite.

• [SLOW TEST:33.400 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":73,"skipped":1329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:18.148: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 22:23:18.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-5169 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 29 22:23:18.458: INFO: stderr: ""
Apr 29 22:23:18.458: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Apr 29 22:23:18.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-5169 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Apr 29 22:23:18.776: INFO: stderr: ""
Apr 29 22:23:18.776: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Apr 29 22:23:18.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-5169 delete pods e2e-test-httpd-pod'
Apr 29 22:23:21.795: INFO: stderr: ""
Apr 29 22:23:21.795: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:21.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5169" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":74,"skipped":1405,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:21.823: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:23:22.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d" in namespace "downward-api-433" to be "Succeeded or Failed"
Apr 29 22:23:22.035: INFO: Pod "downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529129ms
Apr 29 22:23:24.045: INFO: Pod "downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013824647s
STEP: Saw pod success
Apr 29 22:23:24.045: INFO: Pod "downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d" satisfied condition "Succeeded or Failed"
Apr 29 22:23:24.048: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d container client-container: <nil>
STEP: delete the pod
Apr 29 22:23:24.112: INFO: Waiting for pod downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d to disappear
Apr 29 22:23:24.115: INFO: Pod downwardapi-volume-57935d90-1d1b-4a01-91ca-7ec49e41052d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:24.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-433" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":75,"skipped":1418,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:24.136: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-1215/configmap-test-607e3963-a4c0-45bf-a188-8ed4d6c49d5f
STEP: Creating a pod to test consume configMaps
Apr 29 22:23:24.333: INFO: Waiting up to 5m0s for pod "pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992" in namespace "configmap-1215" to be "Succeeded or Failed"
Apr 29 22:23:24.336: INFO: Pod "pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.926524ms
Apr 29 22:23:26.347: INFO: Pod "pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013387639s
Apr 29 22:23:28.357: INFO: Pod "pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023925052s
STEP: Saw pod success
Apr 29 22:23:28.357: INFO: Pod "pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992" satisfied condition "Succeeded or Failed"
Apr 29 22:23:28.361: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992 container env-test: <nil>
STEP: delete the pod
Apr 29 22:23:28.396: INFO: Waiting for pod pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992 to disappear
Apr 29 22:23:28.399: INFO: Pod pod-configmaps-511a4c7f-156d-4421-844a-5b302ab3b992 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:28.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1215" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1419,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:28.413: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:28.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-829" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":77,"skipped":1462,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:28.705: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6437
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 29 22:23:28.887: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:50.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6437" for this suite.

• [SLOW TEST:22.129 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":78,"skipped":1483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:50.835: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:51.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7000" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":79,"skipped":1511,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:51.082: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Apr 29 22:23:55.293: INFO: Pod pod-hostip-73e7c8bf-edfa-45bb-9d31-fff595af2136 has hostIP: 10.240.0.5
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:55.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3970" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":80,"skipped":1524,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:55.314: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-99e57f13-b5a4-47ad-8737-4e595911898f
STEP: Creating a pod to test consume configMaps
Apr 29 22:23:55.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5" in namespace "configmap-2758" to be "Succeeded or Failed"
Apr 29 22:23:55.528: INFO: Pod "pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.452728ms
Apr 29 22:23:57.546: INFO: Pod "pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021487953s
Apr 29 22:23:59.555: INFO: Pod "pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029668996s
STEP: Saw pod success
Apr 29 22:23:59.555: INFO: Pod "pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5" satisfied condition "Succeeded or Failed"
Apr 29 22:23:59.557: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:23:59.591: INFO: Waiting for pod pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5 to disappear
Apr 29 22:23:59.595: INFO: Pod pod-configmaps-8ba6cc6f-4a3e-46e5-8dac-15ac080977c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:23:59.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2758" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1533,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:23:59.606: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 22:24:02.827: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:02.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4901" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1538,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:02.854: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:07.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6212" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1539,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:07.092: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:24:07.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591" in namespace "downward-api-3622" to be "Succeeded or Failed"
Apr 29 22:24:07.291: INFO: Pod "downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591": Phase="Pending", Reason="", readiness=false. Elapsed: 3.234826ms
Apr 29 22:24:09.300: INFO: Pod "downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013024066s
Apr 29 22:24:11.311: INFO: Pod "downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02369381s
STEP: Saw pod success
Apr 29 22:24:11.311: INFO: Pod "downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591" satisfied condition "Succeeded or Failed"
Apr 29 22:24:11.314: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591 container client-container: <nil>
STEP: delete the pod
Apr 29 22:24:11.342: INFO: Waiting for pod downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591 to disappear
Apr 29 22:24:11.345: INFO: Pod downwardapi-volume-023d5840-a42c-4bc0-9fbf-d6115ba6f591 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:11.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3622" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1554,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:11.356: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Apr 29 22:24:11.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-1378 cluster-info'
Apr 29 22:24:11.657: INFO: stderr: ""
Apr 29 22:24:11.657: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:11.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1378" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":85,"skipped":1575,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:11.669: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-212
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-3aa1c463-d1cc-46c9-8817-4c69f7e5053f
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:15.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-212" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1589,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:15.920: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:20.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4909" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1600,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:20.180: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:24:21.079: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 29 22:24:23.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331861, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331861, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331861, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755331861, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:24:26.128: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:24:26.136: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2366-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:24:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8523" for this suite.
STEP: Destroying namespace "webhook-8523-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.452 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":88,"skipped":1604,"failed":0}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:24:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-44
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-44
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 29 22:24:27.963: INFO: Found 0 stateful pods, waiting for 3
Apr 29 22:24:37.990: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:24:37.990: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:24:37.990: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:24:38.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-44 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:24:38.310: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:24:38.310: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:24:38.310: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 29 22:24:48.379: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 29 22:24:58.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-44 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:24:58.684: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:24:58.684: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:24:58.684: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:25:08.726: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:25:08.726: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:08.726: INFO: Waiting for Pod statefulset-44/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:18.754: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:25:18.754: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:18.754: INFO: Waiting for Pod statefulset-44/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:28.755: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:25:28.755: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:38.751: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:25:38.751: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 22:25:48.752: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 29 22:25:58.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-44 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:25:59.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:25:59.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:25:59.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:26:09.094: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 29 22:26:19.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-44 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:26:19.677: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:26:19.677: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:26:19.677: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:26:29.768: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:26:29.768: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 29 22:26:29.768: INFO: Waiting for Pod statefulset-44/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 29 22:26:29.768: INFO: Waiting for Pod statefulset-44/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 29 22:26:39.788: INFO: Waiting for StatefulSet statefulset-44/ss2 to complete update
Apr 29 22:26:39.788: INFO: Waiting for Pod statefulset-44/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 29 22:26:39.788: INFO: Waiting for Pod statefulset-44/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 22:26:49.790: INFO: Deleting all statefulset in ns statefulset-44
Apr 29 22:26:49.793: INFO: Scaling statefulset ss2 to 0
Apr 29 22:27:19.834: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:27:19.838: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:27:19.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-44" for this suite.

• [SLOW TEST:172.237 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":89,"skipped":1607,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:27:19.871: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 29 22:27:25.136: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:27:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6811" for this suite.

• [SLOW TEST:6.307 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":90,"skipped":1615,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:27:26.179: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3641
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3641
STEP: creating replication controller externalsvc in namespace services-3641
I0429 22:27:26.421501      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3641, replica count: 2
I0429 22:27:29.471825      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 29 22:27:29.514: INFO: Creating new exec pod
Apr 29 22:27:33.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-3641 exec execpodzctw9 -- /bin/sh -x -c nslookup nodeport-service.services-3641.svc.cluster.local'
Apr 29 22:27:33.834: INFO: stderr: "+ nslookup nodeport-service.services-3641.svc.cluster.local\n"
Apr 29 22:27:33.834: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-3641.svc.cluster.local\tcanonical name = externalsvc.services-3641.svc.cluster.local.\nName:\texternalsvc.services-3641.svc.cluster.local\nAddress: 10.0.115.43\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3641, will wait for the garbage collector to delete the pods
Apr 29 22:27:33.905: INFO: Deleting ReplicationController externalsvc took: 15.161062ms
Apr 29 22:27:34.005: INFO: Terminating ReplicationController externalsvc pods took: 100.423429ms
Apr 29 22:27:41.951: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:27:41.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3641" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.809 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":91,"skipped":1616,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:27:41.989: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3245
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-188db378-9f0a-443b-bfbd-4e31e9f6ac96
STEP: Creating secret with name s-test-opt-upd-9f7b9511-b751-43d6-a6ba-5d8bdbcf8e00
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-188db378-9f0a-443b-bfbd-4e31e9f6ac96
STEP: Updating secret s-test-opt-upd-9f7b9511-b751-43d6-a6ba-5d8bdbcf8e00
STEP: Creating secret with name s-test-opt-create-b215831f-dbcb-416b-97de-9bb4ee11b97f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:29:02.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3245" for this suite.

• [SLOW TEST:80.870 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":92,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:29:02.860: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9795
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 29 22:29:03.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 create -f -'
Apr 29 22:29:03.427: INFO: stderr: ""
Apr 29 22:29:03.427: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 22:29:03.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:03.537: INFO: stderr: ""
Apr 29 22:29:03.537: INFO: stdout: "update-demo-nautilus-2cb2b update-demo-nautilus-b6qp7 "
Apr 29 22:29:03.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-2cb2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:03.647: INFO: stderr: ""
Apr 29 22:29:03.647: INFO: stdout: ""
Apr 29 22:29:03.647: INFO: update-demo-nautilus-2cb2b is created but not running
Apr 29 22:29:08.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:08.771: INFO: stderr: ""
Apr 29 22:29:08.771: INFO: stdout: "update-demo-nautilus-2cb2b update-demo-nautilus-b6qp7 "
Apr 29 22:29:08.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-2cb2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:08.885: INFO: stderr: ""
Apr 29 22:29:08.885: INFO: stdout: "true"
Apr 29 22:29:08.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-2cb2b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:09.006: INFO: stderr: ""
Apr 29 22:29:09.006: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:09.006: INFO: validating pod update-demo-nautilus-2cb2b
Apr 29 22:29:09.011: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:09.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:09.011: INFO: update-demo-nautilus-2cb2b is verified up and running
Apr 29 22:29:09.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:09.109: INFO: stderr: ""
Apr 29 22:29:09.109: INFO: stdout: "true"
Apr 29 22:29:09.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:09.214: INFO: stderr: ""
Apr 29 22:29:09.214: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:09.214: INFO: validating pod update-demo-nautilus-b6qp7
Apr 29 22:29:09.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:09.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:09.220: INFO: update-demo-nautilus-b6qp7 is verified up and running
STEP: scaling down the replication controller
Apr 29 22:29:09.222: INFO: scanned /root for discovery docs: <nil>
Apr 29 22:29:09.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 29 22:29:10.366: INFO: stderr: ""
Apr 29 22:29:10.366: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 22:29:10.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:10.474: INFO: stderr: ""
Apr 29 22:29:10.474: INFO: stdout: "update-demo-nautilus-2cb2b update-demo-nautilus-b6qp7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 22:29:15.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:15.577: INFO: stderr: ""
Apr 29 22:29:15.577: INFO: stdout: "update-demo-nautilus-2cb2b update-demo-nautilus-b6qp7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 22:29:20.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:20.693: INFO: stderr: ""
Apr 29 22:29:20.693: INFO: stdout: "update-demo-nautilus-2cb2b update-demo-nautilus-b6qp7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 29 22:29:25.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:25.811: INFO: stderr: ""
Apr 29 22:29:25.811: INFO: stdout: "update-demo-nautilus-b6qp7 "
Apr 29 22:29:25.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:25.921: INFO: stderr: ""
Apr 29 22:29:25.921: INFO: stdout: "true"
Apr 29 22:29:25.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:26.021: INFO: stderr: ""
Apr 29 22:29:26.021: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:26.021: INFO: validating pod update-demo-nautilus-b6qp7
Apr 29 22:29:26.025: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:26.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:26.025: INFO: update-demo-nautilus-b6qp7 is verified up and running
STEP: scaling up the replication controller
Apr 29 22:29:26.027: INFO: scanned /root for discovery docs: <nil>
Apr 29 22:29:26.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 29 22:29:27.181: INFO: stderr: ""
Apr 29 22:29:27.181: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 22:29:27.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:27.288: INFO: stderr: ""
Apr 29 22:29:27.288: INFO: stdout: "update-demo-nautilus-b6qp7 update-demo-nautilus-fgrsv "
Apr 29 22:29:27.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:27.392: INFO: stderr: ""
Apr 29 22:29:27.392: INFO: stdout: "true"
Apr 29 22:29:27.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:27.496: INFO: stderr: ""
Apr 29 22:29:27.496: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:27.496: INFO: validating pod update-demo-nautilus-b6qp7
Apr 29 22:29:27.500: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:27.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:27.500: INFO: update-demo-nautilus-b6qp7 is verified up and running
Apr 29 22:29:27.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-fgrsv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:27.602: INFO: stderr: ""
Apr 29 22:29:27.602: INFO: stdout: ""
Apr 29 22:29:27.602: INFO: update-demo-nautilus-fgrsv is created but not running
Apr 29 22:29:32.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 22:29:32.715: INFO: stderr: ""
Apr 29 22:29:32.715: INFO: stdout: "update-demo-nautilus-b6qp7 update-demo-nautilus-fgrsv "
Apr 29 22:29:32.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:32.834: INFO: stderr: ""
Apr 29 22:29:32.834: INFO: stdout: "true"
Apr 29 22:29:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-b6qp7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:32.965: INFO: stderr: ""
Apr 29 22:29:32.965: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:32.965: INFO: validating pod update-demo-nautilus-b6qp7
Apr 29 22:29:32.976: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:32.976: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:32.976: INFO: update-demo-nautilus-b6qp7 is verified up and running
Apr 29 22:29:32.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-fgrsv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 22:29:33.072: INFO: stderr: ""
Apr 29 22:29:33.072: INFO: stdout: "true"
Apr 29 22:29:33.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods update-demo-nautilus-fgrsv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 22:29:33.189: INFO: stderr: ""
Apr 29 22:29:33.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 22:29:33.189: INFO: validating pod update-demo-nautilus-fgrsv
Apr 29 22:29:33.195: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 22:29:33.195: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 22:29:33.195: INFO: update-demo-nautilus-fgrsv is verified up and running
STEP: using delete to clean up resources
Apr 29 22:29:33.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 delete --grace-period=0 --force -f -'
Apr 29 22:29:33.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 22:29:33.303: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 22:29:33.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get rc,svc -l name=update-demo --no-headers'
Apr 29 22:29:33.406: INFO: stderr: "No resources found in kubectl-9795 namespace.\n"
Apr 29 22:29:33.406: INFO: stdout: ""
Apr 29 22:29:33.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 22:29:33.546: INFO: stderr: ""
Apr 29 22:29:33.546: INFO: stdout: "update-demo-nautilus-b6qp7\nupdate-demo-nautilus-fgrsv\n"
Apr 29 22:29:34.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get rc,svc -l name=update-demo --no-headers'
Apr 29 22:29:34.170: INFO: stderr: "No resources found in kubectl-9795 namespace.\n"
Apr 29 22:29:34.170: INFO: stdout: ""
Apr 29 22:29:34.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9795 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 22:29:34.279: INFO: stderr: ""
Apr 29 22:29:34.279: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:29:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9795" for this suite.

• [SLOW TEST:31.435 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":93,"skipped":1651,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:29:34.295: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-745f95d0-12b2-4a5c-8e79-2ae6a4f34fe6 in namespace container-probe-8015
Apr 29 22:29:38.501: INFO: Started pod busybox-745f95d0-12b2-4a5c-8e79-2ae6a4f34fe6 in namespace container-probe-8015
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 22:29:38.504: INFO: Initial restart count of pod busybox-745f95d0-12b2-4a5c-8e79-2ae6a4f34fe6 is 0
Apr 29 22:30:32.801: INFO: Restart count of pod container-probe-8015/busybox-745f95d0-12b2-4a5c-8e79-2ae6a4f34fe6 is now 1 (54.297154404s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:30:32.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8015" for this suite.

• [SLOW TEST:58.533 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":94,"skipped":1659,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:30:32.828: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-636
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Apr 29 22:30:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:30:52.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-636" for this suite.

• [SLOW TEST:19.745 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":95,"skipped":1659,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:30:52.573: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-7cade7fd-aacf-47ed-91e7-78266717f461
STEP: Creating a pod to test consume secrets
Apr 29 22:30:52.771: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7" in namespace "projected-6652" to be "Succeeded or Failed"
Apr 29 22:30:52.774: INFO: Pod "pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936843ms
Apr 29 22:30:54.785: INFO: Pod "pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013426483s
Apr 29 22:30:56.806: INFO: Pod "pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034847535s
STEP: Saw pod success
Apr 29 22:30:56.806: INFO: Pod "pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7" satisfied condition "Succeeded or Failed"
Apr 29 22:30:56.817: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:30:56.899: INFO: Waiting for pod pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7 to disappear
Apr 29 22:30:56.904: INFO: Pod pod-projected-secrets-8b6f58f3-4f32-4a30-b96a-c87cf8ba74f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:30:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6652" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1661,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:30:56.931: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-605fcffa-4d07-4936-b977-6c3e36fb3c3b
STEP: Creating a pod to test consume configMaps
Apr 29 22:30:57.257: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd" in namespace "projected-298" to be "Succeeded or Failed"
Apr 29 22:30:57.277: INFO: Pod "pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.40118ms
Apr 29 22:30:59.285: INFO: Pod "pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02784499s
Apr 29 22:31:01.295: INFO: Pod "pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038188183s
STEP: Saw pod success
Apr 29 22:31:01.295: INFO: Pod "pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd" satisfied condition "Succeeded or Failed"
Apr 29 22:31:01.298: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:31:01.328: INFO: Waiting for pod pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd to disappear
Apr 29 22:31:01.331: INFO: Pod pod-projected-configmaps-a5dfba90-0d51-45c9-a468-a5cb20feaffd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:01.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-298" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1664,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:01.344: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7579
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 22:31:01.552: INFO: Waiting up to 5m0s for pod "pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f" in namespace "emptydir-7579" to be "Succeeded or Failed"
Apr 29 22:31:01.555: INFO: Pod "pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.002343ms
Apr 29 22:31:03.566: INFO: Pod "pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014161897s
Apr 29 22:31:05.579: INFO: Pod "pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027210735s
STEP: Saw pod success
Apr 29 22:31:05.579: INFO: Pod "pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f" satisfied condition "Succeeded or Failed"
Apr 29 22:31:05.583: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f container test-container: <nil>
STEP: delete the pod
Apr 29 22:31:05.617: INFO: Waiting for pod pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f to disappear
Apr 29 22:31:05.619: INFO: Pod pod-a8bcc163-1bd8-41d7-a4c9-483e4462787f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:05.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7579" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":98,"skipped":1680,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:05.631: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:31:06.783: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:31:08.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332266, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332266, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332266, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332266, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:31:11.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:11.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3271" for this suite.
STEP: Destroying namespace "webhook-3271-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.312 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":99,"skipped":1683,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:11.943: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:12.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9799" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":100,"skipped":1702,"failed":0}

------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:12.185: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-1339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr 29 22:31:12.385: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr 29 22:31:12.393: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 29 22:31:12.393: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr 29 22:31:12.407: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 29 22:31:12.407: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr 29 22:31:12.423: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 29 22:31:12.423: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr 29 22:31:19.492: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:19.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-1339" for this suite.

• [SLOW TEST:7.337 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":101,"skipped":1702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:19.523: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:31:19.705: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7886
I0429 22:31:19.737352      21 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7886, replica count: 1
I0429 22:31:20.787846      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 22:31:21.787931      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 22:31:22.788174      21 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:31:22.910: INFO: Created: latency-svc-lwb68
Apr 29 22:31:22.939: INFO: Got endpoints: latency-svc-lwb68 [51.098824ms]
Apr 29 22:31:22.963: INFO: Created: latency-svc-7zjpq
Apr 29 22:31:22.966: INFO: Got endpoints: latency-svc-7zjpq [26.739779ms]
Apr 29 22:31:22.971: INFO: Created: latency-svc-smghk
Apr 29 22:31:22.985: INFO: Got endpoints: latency-svc-smghk [44.238928ms]
Apr 29 22:31:22.993: INFO: Created: latency-svc-qkrkq
Apr 29 22:31:23.007: INFO: Created: latency-svc-wp6dv
Apr 29 22:31:23.007: INFO: Got endpoints: latency-svc-qkrkq [66.643045ms]
Apr 29 22:31:23.015: INFO: Created: latency-svc-v9gjz
Apr 29 22:31:23.023: INFO: Got endpoints: latency-svc-wp6dv [81.80856ms]
Apr 29 22:31:23.024: INFO: Got endpoints: latency-svc-v9gjz [83.700387ms]
Apr 29 22:31:23.031: INFO: Created: latency-svc-kxnm6
Apr 29 22:31:23.042: INFO: Got endpoints: latency-svc-kxnm6 [102.147348ms]
Apr 29 22:31:23.045: INFO: Created: latency-svc-z2txb
Apr 29 22:31:23.071: INFO: Created: latency-svc-dzs4z
Apr 29 22:31:23.073: INFO: Got endpoints: latency-svc-z2txb [131.681367ms]
Apr 29 22:31:23.094: INFO: Created: latency-svc-gvjw9
Apr 29 22:31:23.097: INFO: Got endpoints: latency-svc-dzs4z [155.83971ms]
Apr 29 22:31:23.101: INFO: Created: latency-svc-jdqxt
Apr 29 22:31:23.115: INFO: Got endpoints: latency-svc-gvjw9 [174.529074ms]
Apr 29 22:31:23.123: INFO: Got endpoints: latency-svc-jdqxt [181.766176ms]
Apr 29 22:31:23.125: INFO: Created: latency-svc-zs72g
Apr 29 22:31:23.135: INFO: Created: latency-svc-tdzst
Apr 29 22:31:23.142: INFO: Got endpoints: latency-svc-zs72g [201.212952ms]
Apr 29 22:31:23.148: INFO: Created: latency-svc-4hx4j
Apr 29 22:31:23.149: INFO: Got endpoints: latency-svc-tdzst [207.850646ms]
Apr 29 22:31:23.162: INFO: Created: latency-svc-z5857
Apr 29 22:31:23.167: INFO: Got endpoints: latency-svc-4hx4j [225.06849ms]
Apr 29 22:31:23.172: INFO: Created: latency-svc-s4f59
Apr 29 22:31:23.173: INFO: Got endpoints: latency-svc-z5857 [231.753285ms]
Apr 29 22:31:23.185: INFO: Got endpoints: latency-svc-s4f59 [243.093045ms]
Apr 29 22:31:23.199: INFO: Created: latency-svc-rrx75
Apr 29 22:31:23.199: INFO: Created: latency-svc-h5w6r
Apr 29 22:31:23.203: INFO: Got endpoints: latency-svc-h5w6r [236.267949ms]
Apr 29 22:31:23.219: INFO: Got endpoints: latency-svc-rrx75 [234.445523ms]
Apr 29 22:31:23.230: INFO: Created: latency-svc-p2dxk
Apr 29 22:31:23.235: INFO: Got endpoints: latency-svc-p2dxk [228.409737ms]
Apr 29 22:31:23.247: INFO: Created: latency-svc-wcs89
Apr 29 22:31:23.259: INFO: Got endpoints: latency-svc-wcs89 [235.426536ms]
Apr 29 22:31:23.261: INFO: Created: latency-svc-l85vz
Apr 29 22:31:23.270: INFO: Got endpoints: latency-svc-l85vz [246.098088ms]
Apr 29 22:31:23.275: INFO: Created: latency-svc-kwj2j
Apr 29 22:31:23.285: INFO: Got endpoints: latency-svc-kwj2j [242.832041ms]
Apr 29 22:31:23.291: INFO: Created: latency-svc-l6k5j
Apr 29 22:31:23.311: INFO: Got endpoints: latency-svc-l6k5j [237.75387ms]
Apr 29 22:31:23.311: INFO: Created: latency-svc-psthn
Apr 29 22:31:23.319: INFO: Got endpoints: latency-svc-psthn [221.56174ms]
Apr 29 22:31:23.335: INFO: Created: latency-svc-zc22n
Apr 29 22:31:23.348: INFO: Got endpoints: latency-svc-zc22n [232.608796ms]
Apr 29 22:31:23.357: INFO: Created: latency-svc-vtf7q
Apr 29 22:31:23.364: INFO: Got endpoints: latency-svc-vtf7q [241.204018ms]
Apr 29 22:31:23.372: INFO: Created: latency-svc-vwrhc
Apr 29 22:31:23.394: INFO: Got endpoints: latency-svc-vwrhc [251.301562ms]
Apr 29 22:31:23.403: INFO: Created: latency-svc-sfx5n
Apr 29 22:31:23.413: INFO: Got endpoints: latency-svc-sfx5n [263.84634ms]
Apr 29 22:31:23.417: INFO: Created: latency-svc-pdvcd
Apr 29 22:31:23.429: INFO: Created: latency-svc-q58nk
Apr 29 22:31:23.441: INFO: Created: latency-svc-zhh4v
Apr 29 22:31:23.443: INFO: Got endpoints: latency-svc-pdvcd [276.629721ms]
Apr 29 22:31:23.448: INFO: Got endpoints: latency-svc-q58nk [275.037398ms]
Apr 29 22:31:23.455: INFO: Created: latency-svc-668ks
Apr 29 22:31:23.462: INFO: Got endpoints: latency-svc-zhh4v [276.920725ms]
Apr 29 22:31:23.467: INFO: Got endpoints: latency-svc-668ks [264.708451ms]
Apr 29 22:31:23.470: INFO: Created: latency-svc-2vt4q
Apr 29 22:31:23.488: INFO: Got endpoints: latency-svc-2vt4q [268.314003ms]
Apr 29 22:31:23.488: INFO: Created: latency-svc-8hb4m
Apr 29 22:31:23.494: INFO: Got endpoints: latency-svc-8hb4m [258.015057ms]
Apr 29 22:31:23.496: INFO: Created: latency-svc-jqj8k
Apr 29 22:31:23.508: INFO: Created: latency-svc-cxrbs
Apr 29 22:31:23.508: INFO: Got endpoints: latency-svc-jqj8k [249.721539ms]
Apr 29 22:31:23.519: INFO: Created: latency-svc-5s6gl
Apr 29 22:31:23.530: INFO: Got endpoints: latency-svc-cxrbs [259.555578ms]
Apr 29 22:31:23.537: INFO: Got endpoints: latency-svc-5s6gl [251.573665ms]
Apr 29 22:31:23.540: INFO: Created: latency-svc-4x9fz
Apr 29 22:31:23.560: INFO: Got endpoints: latency-svc-4x9fz [249.72434ms]
Apr 29 22:31:23.561: INFO: Created: latency-svc-75c76
Apr 29 22:31:23.570: INFO: Created: latency-svc-nx7w9
Apr 29 22:31:23.578: INFO: Got endpoints: latency-svc-75c76 [258.839268ms]
Apr 29 22:31:23.580: INFO: Got endpoints: latency-svc-nx7w9 [232.10819ms]
Apr 29 22:31:23.585: INFO: Created: latency-svc-g5r87
Apr 29 22:31:23.596: INFO: Created: latency-svc-vppzc
Apr 29 22:31:23.600: INFO: Got endpoints: latency-svc-g5r87 [235.883843ms]
Apr 29 22:31:23.612: INFO: Created: latency-svc-sx7c2
Apr 29 22:31:23.625: INFO: Got endpoints: latency-svc-vppzc [231.416779ms]
Apr 29 22:31:23.630: INFO: Got endpoints: latency-svc-sx7c2 [216.987475ms]
Apr 29 22:31:23.631: INFO: Created: latency-svc-mq76b
Apr 29 22:31:23.647: INFO: Created: latency-svc-xsw4n
Apr 29 22:31:23.647: INFO: Got endpoints: latency-svc-mq76b [203.613886ms]
Apr 29 22:31:23.659: INFO: Created: latency-svc-q7z9k
Apr 29 22:31:23.669: INFO: Got endpoints: latency-svc-xsw4n [221.079933ms]
Apr 29 22:31:23.670: INFO: Created: latency-svc-7zzmx
Apr 29 22:31:23.683: INFO: Created: latency-svc-vspcc
Apr 29 22:31:23.698: INFO: Created: latency-svc-cthtl
Apr 29 22:31:23.704: INFO: Created: latency-svc-lgnhp
Apr 29 22:31:23.716: INFO: Created: latency-svc-9jdlm
Apr 29 22:31:23.718: INFO: Got endpoints: latency-svc-q7z9k [256.155931ms]
Apr 29 22:31:23.727: INFO: Created: latency-svc-8b27k
Apr 29 22:31:23.745: INFO: Created: latency-svc-kphzx
Apr 29 22:31:23.754: INFO: Created: latency-svc-qf6nm
Apr 29 22:31:23.761: INFO: Created: latency-svc-gxtcc
Apr 29 22:31:23.768: INFO: Got endpoints: latency-svc-7zzmx [300.251355ms]
Apr 29 22:31:23.772: INFO: Created: latency-svc-bhxtp
Apr 29 22:31:23.782: INFO: Created: latency-svc-g9z85
Apr 29 22:31:23.791: INFO: Created: latency-svc-kzcmv
Apr 29 22:31:23.809: INFO: Created: latency-svc-dhtx6
Apr 29 22:31:23.819: INFO: Got endpoints: latency-svc-vspcc [331.133693ms]
Apr 29 22:31:23.823: INFO: Created: latency-svc-65fxx
Apr 29 22:31:23.836: INFO: Created: latency-svc-j96jm
Apr 29 22:31:23.848: INFO: Created: latency-svc-5vtwc
Apr 29 22:31:23.856: INFO: Created: latency-svc-684gg
Apr 29 22:31:23.863: INFO: Got endpoints: latency-svc-cthtl [369.798841ms]
Apr 29 22:31:23.885: INFO: Created: latency-svc-7wwwv
Apr 29 22:31:23.914: INFO: Got endpoints: latency-svc-lgnhp [406.136456ms]
Apr 29 22:31:23.940: INFO: Created: latency-svc-ctvlt
Apr 29 22:31:23.967: INFO: Got endpoints: latency-svc-9jdlm [436.731289ms]
Apr 29 22:31:23.988: INFO: Created: latency-svc-sgzc4
Apr 29 22:31:24.015: INFO: Got endpoints: latency-svc-8b27k [478.148877ms]
Apr 29 22:31:24.035: INFO: Created: latency-svc-6w54t
Apr 29 22:31:24.065: INFO: Got endpoints: latency-svc-kphzx [504.853555ms]
Apr 29 22:31:24.089: INFO: Created: latency-svc-mgkcp
Apr 29 22:31:24.116: INFO: Got endpoints: latency-svc-qf6nm [538.097325ms]
Apr 29 22:31:24.132: INFO: Created: latency-svc-59xxs
Apr 29 22:31:24.171: INFO: Got endpoints: latency-svc-gxtcc [591.096076ms]
Apr 29 22:31:24.186: INFO: Created: latency-svc-74lsv
Apr 29 22:31:24.218: INFO: Got endpoints: latency-svc-bhxtp [618.430762ms]
Apr 29 22:31:24.241: INFO: Created: latency-svc-qpkgn
Apr 29 22:31:24.277: INFO: Got endpoints: latency-svc-g9z85 [652.032338ms]
Apr 29 22:31:24.302: INFO: Created: latency-svc-rjwfk
Apr 29 22:31:24.326: INFO: Got endpoints: latency-svc-kzcmv [695.165049ms]
Apr 29 22:31:24.347: INFO: Created: latency-svc-j84jt
Apr 29 22:31:24.363: INFO: Got endpoints: latency-svc-dhtx6 [716.272748ms]
Apr 29 22:31:24.383: INFO: Created: latency-svc-qpqxx
Apr 29 22:31:24.414: INFO: Got endpoints: latency-svc-65fxx [744.911253ms]
Apr 29 22:31:24.431: INFO: Created: latency-svc-zxhn9
Apr 29 22:31:24.468: INFO: Got endpoints: latency-svc-j96jm [749.66122ms]
Apr 29 22:31:24.480: INFO: Created: latency-svc-q8588
Apr 29 22:31:24.514: INFO: Got endpoints: latency-svc-5vtwc [746.440874ms]
Apr 29 22:31:24.531: INFO: Created: latency-svc-gvmrg
Apr 29 22:31:24.567: INFO: Got endpoints: latency-svc-684gg [747.869193ms]
Apr 29 22:31:24.585: INFO: Created: latency-svc-xzfj4
Apr 29 22:31:24.615: INFO: Got endpoints: latency-svc-7wwwv [751.183939ms]
Apr 29 22:31:24.639: INFO: Created: latency-svc-sxcgb
Apr 29 22:31:24.669: INFO: Got endpoints: latency-svc-ctvlt [754.551587ms]
Apr 29 22:31:24.688: INFO: Created: latency-svc-j9nhq
Apr 29 22:31:24.713: INFO: Got endpoints: latency-svc-sgzc4 [745.835963ms]
Apr 29 22:31:24.739: INFO: Created: latency-svc-n557j
Apr 29 22:31:24.768: INFO: Got endpoints: latency-svc-6w54t [753.031565ms]
Apr 29 22:31:24.795: INFO: Created: latency-svc-p4jm4
Apr 29 22:31:24.813: INFO: Got endpoints: latency-svc-mgkcp [747.804991ms]
Apr 29 22:31:24.839: INFO: Created: latency-svc-22jmv
Apr 29 22:31:24.869: INFO: Got endpoints: latency-svc-59xxs [753.019264ms]
Apr 29 22:31:24.884: INFO: Created: latency-svc-mghrv
Apr 29 22:31:24.948: INFO: Got endpoints: latency-svc-74lsv [776.350495ms]
Apr 29 22:31:24.976: INFO: Got endpoints: latency-svc-qpkgn [757.194923ms]
Apr 29 22:31:25.002: INFO: Created: latency-svc-86ksq
Apr 29 22:31:25.007: INFO: Created: latency-svc-vjb2g
Apr 29 22:31:25.038: INFO: Got endpoints: latency-svc-rjwfk [760.971777ms]
Apr 29 22:31:25.079: INFO: Got endpoints: latency-svc-j84jt [753.48817ms]
Apr 29 22:31:25.085: INFO: Created: latency-svc-2n6b5
Apr 29 22:31:25.101: INFO: Created: latency-svc-99cz7
Apr 29 22:31:25.114: INFO: Got endpoints: latency-svc-qpqxx [750.831132ms]
Apr 29 22:31:25.144: INFO: Created: latency-svc-sxxct
Apr 29 22:31:25.166: INFO: Got endpoints: latency-svc-zxhn9 [751.633243ms]
Apr 29 22:31:25.180: INFO: Created: latency-svc-gd25h
Apr 29 22:31:25.214: INFO: Got endpoints: latency-svc-q8588 [746.158765ms]
Apr 29 22:31:25.235: INFO: Created: latency-svc-n566t
Apr 29 22:31:25.265: INFO: Got endpoints: latency-svc-gvmrg [751.166335ms]
Apr 29 22:31:25.278: INFO: Created: latency-svc-9xfvp
Apr 29 22:31:25.323: INFO: Got endpoints: latency-svc-xzfj4 [755.175592ms]
Apr 29 22:31:25.342: INFO: Created: latency-svc-qf8zp
Apr 29 22:31:25.365: INFO: Got endpoints: latency-svc-sxcgb [750.508325ms]
Apr 29 22:31:25.385: INFO: Created: latency-svc-ncbd4
Apr 29 22:31:25.415: INFO: Got endpoints: latency-svc-j9nhq [746.102462ms]
Apr 29 22:31:25.436: INFO: Created: latency-svc-kdlrc
Apr 29 22:31:25.471: INFO: Got endpoints: latency-svc-n557j [758.149332ms]
Apr 29 22:31:25.488: INFO: Created: latency-svc-xpssh
Apr 29 22:31:25.518: INFO: Got endpoints: latency-svc-p4jm4 [749.991216ms]
Apr 29 22:31:25.530: INFO: Created: latency-svc-g2t7g
Apr 29 22:31:25.567: INFO: Got endpoints: latency-svc-22jmv [753.973272ms]
Apr 29 22:31:25.581: INFO: Created: latency-svc-ztrxt
Apr 29 22:31:25.614: INFO: Got endpoints: latency-svc-mghrv [745.107546ms]
Apr 29 22:31:25.633: INFO: Created: latency-svc-rjcqn
Apr 29 22:31:25.665: INFO: Got endpoints: latency-svc-86ksq [716.579741ms]
Apr 29 22:31:25.688: INFO: Created: latency-svc-qb84l
Apr 29 22:31:25.714: INFO: Got endpoints: latency-svc-vjb2g [738.343249ms]
Apr 29 22:31:25.734: INFO: Created: latency-svc-wq7fp
Apr 29 22:31:25.766: INFO: Got endpoints: latency-svc-2n6b5 [727.309593ms]
Apr 29 22:31:25.783: INFO: Created: latency-svc-5fks8
Apr 29 22:31:25.820: INFO: Got endpoints: latency-svc-99cz7 [740.516379ms]
Apr 29 22:31:25.841: INFO: Created: latency-svc-bk784
Apr 29 22:31:25.866: INFO: Got endpoints: latency-svc-sxxct [751.220931ms]
Apr 29 22:31:25.886: INFO: Created: latency-svc-7fdch
Apr 29 22:31:25.925: INFO: Got endpoints: latency-svc-gd25h [758.842139ms]
Apr 29 22:31:25.944: INFO: Created: latency-svc-d82bm
Apr 29 22:31:25.965: INFO: Got endpoints: latency-svc-n566t [751.126829ms]
Apr 29 22:31:25.992: INFO: Created: latency-svc-kdhqk
Apr 29 22:31:26.013: INFO: Got endpoints: latency-svc-9xfvp [747.904283ms]
Apr 29 22:31:26.033: INFO: Created: latency-svc-zn9pq
Apr 29 22:31:26.071: INFO: Got endpoints: latency-svc-qf8zp [748.36469ms]
Apr 29 22:31:26.110: INFO: Created: latency-svc-v5zv9
Apr 29 22:31:26.117: INFO: Got endpoints: latency-svc-ncbd4 [751.524034ms]
Apr 29 22:31:26.131: INFO: Created: latency-svc-56s5p
Apr 29 22:31:26.165: INFO: Got endpoints: latency-svc-kdlrc [750.212815ms]
Apr 29 22:31:26.183: INFO: Created: latency-svc-mztn4
Apr 29 22:31:26.214: INFO: Got endpoints: latency-svc-xpssh [742.624007ms]
Apr 29 22:31:26.229: INFO: Created: latency-svc-h7ngr
Apr 29 22:31:26.264: INFO: Got endpoints: latency-svc-g2t7g [746.183657ms]
Apr 29 22:31:26.294: INFO: Created: latency-svc-5hns4
Apr 29 22:31:26.317: INFO: Got endpoints: latency-svc-ztrxt [750.239213ms]
Apr 29 22:31:26.345: INFO: Created: latency-svc-df6jx
Apr 29 22:31:26.372: INFO: Got endpoints: latency-svc-rjcqn [758.226027ms]
Apr 29 22:31:26.393: INFO: Created: latency-svc-kmpmk
Apr 29 22:31:26.423: INFO: Got endpoints: latency-svc-qb84l [758.258427ms]
Apr 29 22:31:26.465: INFO: Got endpoints: latency-svc-wq7fp [750.396414ms]
Apr 29 22:31:26.519: INFO: Got endpoints: latency-svc-5fks8 [753.731762ms]
Apr 29 22:31:26.541: INFO: Created: latency-svc-sn5vv
Apr 29 22:31:26.556: INFO: Created: latency-svc-gsr55
Apr 29 22:31:26.565: INFO: Created: latency-svc-695pb
Apr 29 22:31:26.591: INFO: Got endpoints: latency-svc-bk784 [771.485612ms]
Apr 29 22:31:26.604: INFO: Created: latency-svc-htnkv
Apr 29 22:31:26.619: INFO: Got endpoints: latency-svc-7fdch [753.590858ms]
Apr 29 22:31:26.634: INFO: Created: latency-svc-cjbf6
Apr 29 22:31:26.664: INFO: Got endpoints: latency-svc-d82bm [738.864449ms]
Apr 29 22:31:26.679: INFO: Created: latency-svc-9dp9q
Apr 29 22:31:26.722: INFO: Got endpoints: latency-svc-kdhqk [756.6045ms]
Apr 29 22:31:26.738: INFO: Created: latency-svc-66lts
Apr 29 22:31:26.765: INFO: Got endpoints: latency-svc-zn9pq [751.385526ms]
Apr 29 22:31:26.782: INFO: Created: latency-svc-9g5qx
Apr 29 22:31:26.816: INFO: Got endpoints: latency-svc-v5zv9 [744.67003ms]
Apr 29 22:31:26.830: INFO: Created: latency-svc-pfhlj
Apr 29 22:31:26.869: INFO: Got endpoints: latency-svc-56s5p [752.42204ms]
Apr 29 22:31:26.882: INFO: Created: latency-svc-8rpqh
Apr 29 22:31:26.914: INFO: Got endpoints: latency-svc-mztn4 [748.188981ms]
Apr 29 22:31:26.936: INFO: Created: latency-svc-rvpll
Apr 29 22:31:26.967: INFO: Got endpoints: latency-svc-h7ngr [753.270753ms]
Apr 29 22:31:27.008: INFO: Created: latency-svc-bgjh9
Apr 29 22:31:27.019: INFO: Got endpoints: latency-svc-5hns4 [754.374768ms]
Apr 29 22:31:27.038: INFO: Created: latency-svc-nrwpc
Apr 29 22:31:27.065: INFO: Got endpoints: latency-svc-df6jx [747.763873ms]
Apr 29 22:31:27.079: INFO: Created: latency-svc-2nldd
Apr 29 22:31:27.116: INFO: Got endpoints: latency-svc-kmpmk [743.691616ms]
Apr 29 22:31:27.129: INFO: Created: latency-svc-4kms2
Apr 29 22:31:27.164: INFO: Got endpoints: latency-svc-sn5vv [741.353882ms]
Apr 29 22:31:27.183: INFO: Created: latency-svc-rhskc
Apr 29 22:31:27.214: INFO: Got endpoints: latency-svc-gsr55 [749.000789ms]
Apr 29 22:31:27.227: INFO: Created: latency-svc-z4mc8
Apr 29 22:31:27.263: INFO: Got endpoints: latency-svc-695pb [743.770315ms]
Apr 29 22:31:27.277: INFO: Created: latency-svc-ms4l2
Apr 29 22:31:27.315: INFO: Got endpoints: latency-svc-htnkv [723.555829ms]
Apr 29 22:31:27.332: INFO: Created: latency-svc-bknmv
Apr 29 22:31:27.367: INFO: Got endpoints: latency-svc-cjbf6 [747.967574ms]
Apr 29 22:31:27.383: INFO: Created: latency-svc-vkkkd
Apr 29 22:31:27.414: INFO: Got endpoints: latency-svc-9dp9q [750.413007ms]
Apr 29 22:31:27.429: INFO: Created: latency-svc-4dwph
Apr 29 22:31:27.464: INFO: Got endpoints: latency-svc-66lts [741.770284ms]
Apr 29 22:31:27.476: INFO: Created: latency-svc-wtgmm
Apr 29 22:31:27.526: INFO: Got endpoints: latency-svc-9g5qx [761.219759ms]
Apr 29 22:31:27.553: INFO: Created: latency-svc-phmqj
Apr 29 22:31:27.565: INFO: Got endpoints: latency-svc-pfhlj [748.737082ms]
Apr 29 22:31:27.577: INFO: Created: latency-svc-lldmp
Apr 29 22:31:27.616: INFO: Got endpoints: latency-svc-8rpqh [746.405049ms]
Apr 29 22:31:27.638: INFO: Created: latency-svc-2jp65
Apr 29 22:31:27.664: INFO: Got endpoints: latency-svc-rvpll [749.873197ms]
Apr 29 22:31:27.685: INFO: Created: latency-svc-h99n7
Apr 29 22:31:27.716: INFO: Got endpoints: latency-svc-bgjh9 [748.168473ms]
Apr 29 22:31:27.729: INFO: Created: latency-svc-d9mzn
Apr 29 22:31:27.764: INFO: Got endpoints: latency-svc-nrwpc [744.851825ms]
Apr 29 22:31:27.777: INFO: Created: latency-svc-5pqqs
Apr 29 22:31:27.817: INFO: Got endpoints: latency-svc-2nldd [751.827224ms]
Apr 29 22:31:27.833: INFO: Created: latency-svc-q8c78
Apr 29 22:31:27.864: INFO: Got endpoints: latency-svc-4kms2 [747.450962ms]
Apr 29 22:31:27.887: INFO: Created: latency-svc-dktwx
Apr 29 22:31:27.914: INFO: Got endpoints: latency-svc-rhskc [749.315389ms]
Apr 29 22:31:27.949: INFO: Created: latency-svc-mld9q
Apr 29 22:31:27.972: INFO: Got endpoints: latency-svc-z4mc8 [757.877809ms]
Apr 29 22:31:27.999: INFO: Created: latency-svc-kc5sg
Apr 29 22:31:28.026: INFO: Got endpoints: latency-svc-ms4l2 [763.125084ms]
Apr 29 22:31:28.043: INFO: Created: latency-svc-qs9kl
Apr 29 22:31:28.068: INFO: Got endpoints: latency-svc-bknmv [753.100541ms]
Apr 29 22:31:28.084: INFO: Created: latency-svc-gt8fv
Apr 29 22:31:28.114: INFO: Got endpoints: latency-svc-vkkkd [747.126556ms]
Apr 29 22:31:28.127: INFO: Created: latency-svc-g2b48
Apr 29 22:31:28.174: INFO: Got endpoints: latency-svc-4dwph [759.820635ms]
Apr 29 22:31:28.190: INFO: Created: latency-svc-vnz5n
Apr 29 22:31:28.222: INFO: Got endpoints: latency-svc-wtgmm [758.02731ms]
Apr 29 22:31:28.241: INFO: Created: latency-svc-v6l6d
Apr 29 22:31:28.264: INFO: Got endpoints: latency-svc-phmqj [737.444918ms]
Apr 29 22:31:28.288: INFO: Created: latency-svc-57mlp
Apr 29 22:31:28.320: INFO: Got endpoints: latency-svc-lldmp [754.977265ms]
Apr 29 22:31:28.343: INFO: Created: latency-svc-nr7kk
Apr 29 22:31:28.365: INFO: Got endpoints: latency-svc-2jp65 [748.598674ms]
Apr 29 22:31:28.389: INFO: Created: latency-svc-dm69f
Apr 29 22:31:28.416: INFO: Got endpoints: latency-svc-h99n7 [751.922921ms]
Apr 29 22:31:28.434: INFO: Created: latency-svc-fvc8n
Apr 29 22:31:28.465: INFO: Got endpoints: latency-svc-d9mzn [749.100581ms]
Apr 29 22:31:28.479: INFO: Created: latency-svc-cr2sx
Apr 29 22:31:28.520: INFO: Got endpoints: latency-svc-5pqqs [755.999677ms]
Apr 29 22:31:28.534: INFO: Created: latency-svc-5wqxb
Apr 29 22:31:28.566: INFO: Got endpoints: latency-svc-q8c78 [748.393569ms]
Apr 29 22:31:28.582: INFO: Created: latency-svc-5bzp6
Apr 29 22:31:28.615: INFO: Got endpoints: latency-svc-dktwx [751.28321ms]
Apr 29 22:31:28.629: INFO: Created: latency-svc-dm9rr
Apr 29 22:31:28.667: INFO: Got endpoints: latency-svc-mld9q [753.40174ms]
Apr 29 22:31:28.685: INFO: Created: latency-svc-whhvz
Apr 29 22:31:28.715: INFO: Got endpoints: latency-svc-kc5sg [742.947191ms]
Apr 29 22:31:28.733: INFO: Created: latency-svc-ldp47
Apr 29 22:31:28.768: INFO: Got endpoints: latency-svc-qs9kl [741.705473ms]
Apr 29 22:31:28.783: INFO: Created: latency-svc-c7fls
Apr 29 22:31:28.815: INFO: Got endpoints: latency-svc-gt8fv [747.107349ms]
Apr 29 22:31:28.833: INFO: Created: latency-svc-zhm8l
Apr 29 22:31:28.865: INFO: Got endpoints: latency-svc-g2b48 [750.814002ms]
Apr 29 22:31:28.881: INFO: Created: latency-svc-vhf4q
Apr 29 22:31:28.932: INFO: Got endpoints: latency-svc-vnz5n [758.323607ms]
Apr 29 22:31:28.973: INFO: Got endpoints: latency-svc-v6l6d [751.204207ms]
Apr 29 22:31:28.981: INFO: Created: latency-svc-t6brf
Apr 29 22:31:28.995: INFO: Created: latency-svc-6lxlw
Apr 29 22:31:29.043: INFO: Got endpoints: latency-svc-57mlp [779.107601ms]
Apr 29 22:31:29.067: INFO: Got endpoints: latency-svc-nr7kk [747.514955ms]
Apr 29 22:31:29.070: INFO: Created: latency-svc-vdg6d
Apr 29 22:31:29.090: INFO: Created: latency-svc-8vvqf
Apr 29 22:31:29.120: INFO: Got endpoints: latency-svc-dm69f [755.643569ms]
Apr 29 22:31:29.146: INFO: Created: latency-svc-w2w2x
Apr 29 22:31:29.167: INFO: Got endpoints: latency-svc-fvc8n [750.751199ms]
Apr 29 22:31:29.184: INFO: Created: latency-svc-mqkwx
Apr 29 22:31:29.215: INFO: Got endpoints: latency-svc-cr2sx [749.606583ms]
Apr 29 22:31:29.230: INFO: Created: latency-svc-tm22p
Apr 29 22:31:29.266: INFO: Got endpoints: latency-svc-5wqxb [745.686627ms]
Apr 29 22:31:29.278: INFO: Created: latency-svc-27hpv
Apr 29 22:31:29.314: INFO: Got endpoints: latency-svc-5bzp6 [748.556866ms]
Apr 29 22:31:29.329: INFO: Created: latency-svc-88rlv
Apr 29 22:31:29.370: INFO: Got endpoints: latency-svc-dm9rr [754.770254ms]
Apr 29 22:31:29.388: INFO: Created: latency-svc-l8jj4
Apr 29 22:31:29.416: INFO: Got endpoints: latency-svc-whhvz [748.579366ms]
Apr 29 22:31:29.432: INFO: Created: latency-svc-67qww
Apr 29 22:31:29.465: INFO: Got endpoints: latency-svc-ldp47 [749.353676ms]
Apr 29 22:31:29.478: INFO: Created: latency-svc-c8zxj
Apr 29 22:31:29.542: INFO: Got endpoints: latency-svc-c7fls [773.402415ms]
Apr 29 22:31:29.560: INFO: Created: latency-svc-kx7th
Apr 29 22:31:29.564: INFO: Got endpoints: latency-svc-zhm8l [749.021471ms]
Apr 29 22:31:29.583: INFO: Created: latency-svc-vmbqp
Apr 29 22:31:29.615: INFO: Got endpoints: latency-svc-vhf4q [750.144286ms]
Apr 29 22:31:29.631: INFO: Created: latency-svc-tgtv8
Apr 29 22:31:29.670: INFO: Got endpoints: latency-svc-t6brf [737.101001ms]
Apr 29 22:31:29.682: INFO: Created: latency-svc-6lzh7
Apr 29 22:31:29.714: INFO: Got endpoints: latency-svc-6lxlw [740.827953ms]
Apr 29 22:31:29.731: INFO: Created: latency-svc-x7t8z
Apr 29 22:31:29.771: INFO: Got endpoints: latency-svc-vdg6d [727.941871ms]
Apr 29 22:31:29.786: INFO: Created: latency-svc-dbz5j
Apr 29 22:31:29.825: INFO: Got endpoints: latency-svc-8vvqf [757.987695ms]
Apr 29 22:31:29.841: INFO: Created: latency-svc-xkd7w
Apr 29 22:31:29.864: INFO: Got endpoints: latency-svc-w2w2x [743.309088ms]
Apr 29 22:31:29.880: INFO: Created: latency-svc-xxjk4
Apr 29 22:31:29.916: INFO: Got endpoints: latency-svc-mqkwx [749.80828ms]
Apr 29 22:31:29.940: INFO: Created: latency-svc-z9965
Apr 29 22:31:29.965: INFO: Got endpoints: latency-svc-tm22p [750.194686ms]
Apr 29 22:31:29.986: INFO: Created: latency-svc-mxz8g
Apr 29 22:31:30.017: INFO: Got endpoints: latency-svc-27hpv [751.304801ms]
Apr 29 22:31:30.040: INFO: Created: latency-svc-w7l8l
Apr 29 22:31:30.074: INFO: Got endpoints: latency-svc-88rlv [759.162411ms]
Apr 29 22:31:30.086: INFO: Created: latency-svc-jxfx4
Apr 29 22:31:30.114: INFO: Got endpoints: latency-svc-l8jj4 [743.352487ms]
Apr 29 22:31:30.132: INFO: Created: latency-svc-8zdjm
Apr 29 22:31:30.168: INFO: Got endpoints: latency-svc-67qww [752.491215ms]
Apr 29 22:31:30.182: INFO: Created: latency-svc-vgksk
Apr 29 22:31:30.214: INFO: Got endpoints: latency-svc-c8zxj [749.494473ms]
Apr 29 22:31:30.234: INFO: Created: latency-svc-5pz2z
Apr 29 22:31:30.269: INFO: Got endpoints: latency-svc-kx7th [727.136158ms]
Apr 29 22:31:30.285: INFO: Created: latency-svc-4cbj7
Apr 29 22:31:30.314: INFO: Got endpoints: latency-svc-vmbqp [750.159582ms]
Apr 29 22:31:30.328: INFO: Created: latency-svc-ltxh6
Apr 29 22:31:30.364: INFO: Got endpoints: latency-svc-tgtv8 [748.410056ms]
Apr 29 22:31:30.384: INFO: Created: latency-svc-mg9p7
Apr 29 22:31:30.414: INFO: Got endpoints: latency-svc-6lzh7 [744.384199ms]
Apr 29 22:31:30.432: INFO: Created: latency-svc-rlmdc
Apr 29 22:31:30.467: INFO: Got endpoints: latency-svc-x7t8z [752.713315ms]
Apr 29 22:31:30.493: INFO: Created: latency-svc-psp2h
Apr 29 22:31:30.514: INFO: Got endpoints: latency-svc-dbz5j [743.066279ms]
Apr 29 22:31:30.532: INFO: Created: latency-svc-tk9gh
Apr 29 22:31:30.566: INFO: Got endpoints: latency-svc-xkd7w [740.837147ms]
Apr 29 22:31:30.585: INFO: Created: latency-svc-r6bvd
Apr 29 22:31:30.615: INFO: Got endpoints: latency-svc-xxjk4 [751.049791ms]
Apr 29 22:31:30.639: INFO: Created: latency-svc-q9fbx
Apr 29 22:31:30.664: INFO: Got endpoints: latency-svc-z9965 [747.018034ms]
Apr 29 22:31:30.677: INFO: Created: latency-svc-mzzdp
Apr 29 22:31:30.724: INFO: Got endpoints: latency-svc-mxz8g [758.767198ms]
Apr 29 22:31:30.748: INFO: Created: latency-svc-jjvwf
Apr 29 22:31:30.764: INFO: Got endpoints: latency-svc-w7l8l [747.117434ms]
Apr 29 22:31:30.814: INFO: Got endpoints: latency-svc-jxfx4 [740.47324ms]
Apr 29 22:31:30.871: INFO: Got endpoints: latency-svc-8zdjm [757.582582ms]
Apr 29 22:31:30.916: INFO: Got endpoints: latency-svc-vgksk [747.645642ms]
Apr 29 22:31:30.968: INFO: Got endpoints: latency-svc-5pz2z [753.674526ms]
Apr 29 22:31:31.026: INFO: Got endpoints: latency-svc-4cbj7 [756.906572ms]
Apr 29 22:31:31.069: INFO: Got endpoints: latency-svc-ltxh6 [754.772341ms]
Apr 29 22:31:31.114: INFO: Got endpoints: latency-svc-mg9p7 [750.590081ms]
Apr 29 22:31:31.165: INFO: Got endpoints: latency-svc-rlmdc [750.427279ms]
Apr 29 22:31:31.217: INFO: Got endpoints: latency-svc-psp2h [749.601367ms]
Apr 29 22:31:31.266: INFO: Got endpoints: latency-svc-tk9gh [751.133388ms]
Apr 29 22:31:31.317: INFO: Got endpoints: latency-svc-r6bvd [750.353876ms]
Apr 29 22:31:31.378: INFO: Got endpoints: latency-svc-q9fbx [763.242358ms]
Apr 29 22:31:31.416: INFO: Got endpoints: latency-svc-mzzdp [752.189501ms]
Apr 29 22:31:31.475: INFO: Got endpoints: latency-svc-jjvwf [751.569392ms]
Apr 29 22:31:31.476: INFO: Latencies: [26.739779ms 44.238928ms 66.643045ms 81.80856ms 83.700387ms 102.147348ms 131.681367ms 155.83971ms 174.529074ms 181.766176ms 201.212952ms 203.613886ms 207.850646ms 216.987475ms 221.079933ms 221.56174ms 225.06849ms 228.409737ms 231.416779ms 231.753285ms 232.10819ms 232.608796ms 234.445523ms 235.426536ms 235.883843ms 236.267949ms 237.75387ms 241.204018ms 242.832041ms 243.093045ms 246.098088ms 249.721539ms 249.72434ms 251.301562ms 251.573665ms 256.155931ms 258.015057ms 258.839268ms 259.555578ms 263.84634ms 264.708451ms 268.314003ms 275.037398ms 276.629721ms 276.920725ms 300.251355ms 331.133693ms 369.798841ms 406.136456ms 436.731289ms 478.148877ms 504.853555ms 538.097325ms 591.096076ms 618.430762ms 652.032338ms 695.165049ms 716.272748ms 716.579741ms 723.555829ms 727.136158ms 727.309593ms 727.941871ms 737.101001ms 737.444918ms 738.343249ms 738.864449ms 740.47324ms 740.516379ms 740.827953ms 740.837147ms 741.353882ms 741.705473ms 741.770284ms 742.624007ms 742.947191ms 743.066279ms 743.309088ms 743.352487ms 743.691616ms 743.770315ms 744.384199ms 744.67003ms 744.851825ms 744.911253ms 745.107546ms 745.686627ms 745.835963ms 746.102462ms 746.158765ms 746.183657ms 746.405049ms 746.440874ms 747.018034ms 747.107349ms 747.117434ms 747.126556ms 747.450962ms 747.514955ms 747.645642ms 747.763873ms 747.804991ms 747.869193ms 747.904283ms 747.967574ms 748.168473ms 748.188981ms 748.36469ms 748.393569ms 748.410056ms 748.556866ms 748.579366ms 748.598674ms 748.737082ms 749.000789ms 749.021471ms 749.100581ms 749.315389ms 749.353676ms 749.494473ms 749.601367ms 749.606583ms 749.66122ms 749.80828ms 749.873197ms 749.991216ms 750.144286ms 750.159582ms 750.194686ms 750.212815ms 750.239213ms 750.353876ms 750.396414ms 750.413007ms 750.427279ms 750.508325ms 750.590081ms 750.751199ms 750.814002ms 750.831132ms 751.049791ms 751.126829ms 751.133388ms 751.166335ms 751.183939ms 751.204207ms 751.220931ms 751.28321ms 751.304801ms 751.385526ms 751.524034ms 751.569392ms 751.633243ms 751.827224ms 751.922921ms 752.189501ms 752.42204ms 752.491215ms 752.713315ms 753.019264ms 753.031565ms 753.100541ms 753.270753ms 753.40174ms 753.48817ms 753.590858ms 753.674526ms 753.731762ms 753.973272ms 754.374768ms 754.551587ms 754.770254ms 754.772341ms 754.977265ms 755.175592ms 755.643569ms 755.999677ms 756.6045ms 756.906572ms 757.194923ms 757.582582ms 757.877809ms 757.987695ms 758.02731ms 758.149332ms 758.226027ms 758.258427ms 758.323607ms 758.767198ms 758.842139ms 759.162411ms 759.820635ms 760.971777ms 761.219759ms 763.125084ms 763.242358ms 771.485612ms 773.402415ms 776.350495ms 779.107601ms]
Apr 29 22:31:31.476: INFO: 50 %ile: 747.763873ms
Apr 29 22:31:31.476: INFO: 90 %ile: 757.582582ms
Apr 29 22:31:31.476: INFO: 99 %ile: 776.350495ms
Apr 29 22:31:31.476: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:31:31.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7886" for this suite.

• [SLOW TEST:11.976 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":102,"skipped":1730,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:31:31.499: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3253
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3253, will wait for the garbage collector to delete the pods
Apr 29 22:31:35.764: INFO: Deleting Job.batch foo took: 15.083212ms
Apr 29 22:31:35.864: INFO: Terminating Job.batch foo pods took: 100.170307ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:32:11.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3253" for this suite.

• [SLOW TEST:40.400 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":103,"skipped":1733,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:32:11.901: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0429 22:32:52.193234      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 22:32:54.218: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 29 22:32:54.218: INFO: Deleting pod "simpletest.rc-4h8gq" in namespace "gc-6955"
Apr 29 22:32:54.239: INFO: Deleting pod "simpletest.rc-5hgfq" in namespace "gc-6955"
Apr 29 22:32:54.261: INFO: Deleting pod "simpletest.rc-62dh9" in namespace "gc-6955"
Apr 29 22:32:54.277: INFO: Deleting pod "simpletest.rc-8t24q" in namespace "gc-6955"
Apr 29 22:32:54.316: INFO: Deleting pod "simpletest.rc-k689f" in namespace "gc-6955"
Apr 29 22:32:54.330: INFO: Deleting pod "simpletest.rc-l6chw" in namespace "gc-6955"
Apr 29 22:32:54.368: INFO: Deleting pod "simpletest.rc-rbj5k" in namespace "gc-6955"
Apr 29 22:32:54.384: INFO: Deleting pod "simpletest.rc-rprrb" in namespace "gc-6955"
Apr 29 22:32:54.401: INFO: Deleting pod "simpletest.rc-vxtjb" in namespace "gc-6955"
Apr 29 22:32:54.415: INFO: Deleting pod "simpletest.rc-xxp88" in namespace "gc-6955"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:32:54.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6955" for this suite.

• [SLOW TEST:42.546 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":104,"skipped":1751,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:32:54.448: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:32:55.774: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:32:57.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332375, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332375, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332375, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332375, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:33:00.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:33:00.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2705" for this suite.
STEP: Destroying namespace "webhook-2705-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":105,"skipped":1754,"failed":0}
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:33:00.991: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2196
STEP: creating service affinity-nodeport in namespace services-2196
STEP: creating replication controller affinity-nodeport in namespace services-2196
I0429 22:33:01.337668      21 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2196, replica count: 3
I0429 22:33:04.387994      21 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:33:04.409: INFO: Creating new exec pod
Apr 29 22:33:09.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2196 exec execpod-affinityf25c5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Apr 29 22:33:09.704: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 29 22:33:09.704: INFO: stdout: ""
Apr 29 22:33:09.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2196 exec execpod-affinityf25c5 -- /bin/sh -x -c nc -zv -t -w 2 10.0.103.236 80'
Apr 29 22:33:09.992: INFO: stderr: "+ nc -zv -t -w 2 10.0.103.236 80\nConnection to 10.0.103.236 80 port [tcp/http] succeeded!\n"
Apr 29 22:33:09.992: INFO: stdout: ""
Apr 29 22:33:09.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2196 exec execpod-affinityf25c5 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.6 30477'
Apr 29 22:33:10.274: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.6 30477\nConnection to 10.240.0.6 30477 port [tcp/30477] succeeded!\n"
Apr 29 22:33:10.274: INFO: stdout: ""
Apr 29 22:33:10.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2196 exec execpod-affinityf25c5 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.5 30477'
Apr 29 22:33:10.562: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.5 30477\nConnection to 10.240.0.5 30477 port [tcp/30477] succeeded!\n"
Apr 29 22:33:10.562: INFO: stdout: ""
Apr 29 22:33:10.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2196 exec execpod-affinityf25c5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.5:30477/ ; done'
Apr 29 22:33:10.910: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.5:30477/\n"
Apr 29 22:33:10.910: INFO: stdout: "\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr\naffinity-nodeport-zdspr"
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Received response from host: affinity-nodeport-zdspr
Apr 29 22:33:10.910: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2196, will wait for the garbage collector to delete the pods
Apr 29 22:33:10.988: INFO: Deleting ReplicationController affinity-nodeport took: 7.462198ms
Apr 29 22:33:11.588: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.187793ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:33:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2196" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.298 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":106,"skipped":1754,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:33:22.297: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:33:22.868: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:33:24.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332402, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332402, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332402, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332402, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:33:27.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:33:40.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7496" for this suite.
STEP: Destroying namespace "webhook-7496-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.895 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":107,"skipped":1774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:33:40.193: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Apr 29 22:33:40.470: INFO: Waiting up to 5m0s for pod "client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203" in namespace "containers-8662" to be "Succeeded or Failed"
Apr 29 22:33:40.474: INFO: Pod "client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469245ms
Apr 29 22:33:42.486: INFO: Pod "client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016068314s
Apr 29 22:33:44.499: INFO: Pod "client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028693551s
STEP: Saw pod success
Apr 29 22:33:44.499: INFO: Pod "client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203" satisfied condition "Succeeded or Failed"
Apr 29 22:33:44.502: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:33:44.560: INFO: Waiting for pod client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203 to disappear
Apr 29 22:33:44.563: INFO: Pod client-containers-3e3124cf-bfb6-4719-a1fa-1a6f6096d203 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:33:44.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8662" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":108,"skipped":1811,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:33:44.573: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8976
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-8976
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8976
Apr 29 22:33:44.780: INFO: Found 0 stateful pods, waiting for 1
Apr 29 22:33:54.801: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 29 22:33:54.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:33:55.081: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:33:55.081: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:33:55.081: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:33:55.086: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 22:34:05.110: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:34:05.110: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:34:05.136: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:05.136: INFO: ss-0  k8s-linuxpool-38811533-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:05.136: INFO: 
Apr 29 22:34:05.136: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 29 22:34:06.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996346639s
Apr 29 22:34:07.152: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988143928s
Apr 29 22:34:08.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979688322s
Apr 29 22:34:09.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970784418s
Apr 29 22:34:10.176: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.964023948s
Apr 29 22:34:11.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956144673s
Apr 29 22:34:12.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947464794s
Apr 29 22:34:13.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939433732s
Apr 29 22:34:14.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 930.932171ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8976
Apr 29 22:34:15.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:34:15.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:34:15.496: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:34:15.496: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:34:15.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:34:15.787: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 22:34:15.787: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:34:15.787: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:34:15.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:34:16.047: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 22:34:16.047: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:34:16.047: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:34:16.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:34:16.054: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:34:16.054: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 29 22:34:16.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:34:16.351: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:34:16.351: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:34:16.351: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:34:16.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:34:16.638: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:34:16.638: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:34:16.638: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:34:16.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-8976 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:34:16.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:34:16.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:34:16.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:34:16.910: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:34:16.915: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 29 22:34:26.950: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:34:26.950: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:34:26.950: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:34:26.973: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:26.973: INFO: ss-0  k8s-linuxpool-38811533-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:26.973: INFO: ss-1  k8s-linuxpool-38811533-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:26.973: INFO: ss-2  k8s-linuxpool-38811533-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:26.973: INFO: 
Apr 29 22:34:26.973: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 22:34:27.982: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:27.983: INFO: ss-0  k8s-linuxpool-38811533-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:27.983: INFO: ss-1  k8s-linuxpool-38811533-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:27.983: INFO: ss-2  k8s-linuxpool-38811533-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:27.983: INFO: 
Apr 29 22:34:27.983: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 22:34:28.996: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:28.996: INFO: ss-0  k8s-linuxpool-38811533-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:28.996: INFO: ss-1  k8s-linuxpool-38811533-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:28.996: INFO: ss-2  k8s-linuxpool-38811533-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:28.996: INFO: 
Apr 29 22:34:28.996: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 22:34:30.016: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:30.016: INFO: ss-0  k8s-linuxpool-38811533-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:30.016: INFO: ss-1  k8s-linuxpool-38811533-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:30.016: INFO: ss-2  k8s-linuxpool-38811533-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:30.016: INFO: 
Apr 29 22:34:30.016: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 22:34:31.025: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:31.025: INFO: ss-0  k8s-linuxpool-38811533-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:33:44 +0000 UTC  }]
Apr 29 22:34:31.025: INFO: ss-1  k8s-linuxpool-38811533-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:31.025: INFO: ss-2  k8s-linuxpool-38811533-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:31.026: INFO: 
Apr 29 22:34:31.026: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 22:34:32.033: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Apr 29 22:34:32.033: INFO: ss-2  k8s-linuxpool-38811533-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:34:05 +0000 UTC  }]
Apr 29 22:34:32.033: INFO: 
Apr 29 22:34:32.033: INFO: StatefulSet ss has not reached scale 0, at 1
Apr 29 22:34:33.041: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.936151106s
Apr 29 22:34:34.049: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.928145299s
Apr 29 22:34:35.056: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.919860196s
Apr 29 22:34:36.065: INFO: Verifying statefulset ss doesn't scale past 0 for another 912.877117ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8976
Apr 29 22:34:37.073: INFO: Scaling statefulset ss to 0
Apr 29 22:34:37.095: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 22:34:37.097: INFO: Deleting all statefulset in ns statefulset-8976
Apr 29 22:34:37.100: INFO: Scaling statefulset ss to 0
Apr 29 22:34:37.109: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:34:37.111: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:34:37.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8976" for this suite.

• [SLOW TEST:52.562 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":109,"skipped":1812,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:34:37.136: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:34:37.808: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 29 22:34:39.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332477, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332477, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332477, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332477, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:34:42.853: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:34:42.861: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:34:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1717" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.055 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":110,"skipped":1824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:34:44.192: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-2873/secret-test-810e3019-47d6-4017-a41e-b629fce57a62
STEP: Creating a pod to test consume secrets
Apr 29 22:34:44.491: INFO: Waiting up to 5m0s for pod "pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293" in namespace "secrets-2873" to be "Succeeded or Failed"
Apr 29 22:34:44.495: INFO: Pod "pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293": Phase="Pending", Reason="", readiness=false. Elapsed: 3.886048ms
Apr 29 22:34:46.508: INFO: Pod "pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017239057s
Apr 29 22:34:48.533: INFO: Pod "pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042060879s
STEP: Saw pod success
Apr 29 22:34:48.533: INFO: Pod "pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293" satisfied condition "Succeeded or Failed"
Apr 29 22:34:48.537: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293 container env-test: <nil>
STEP: delete the pod
Apr 29 22:34:48.585: INFO: Waiting for pod pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293 to disappear
Apr 29 22:34:48.589: INFO: Pod pod-configmaps-08c4937f-9b51-4a78-98e6-c1f5cc427293 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:34:48.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2873" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":1858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:34:48.612: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 29 22:34:48.873: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 22:34:48.882: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 22:34:48.885: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-0 before test
Apr 29 22:34:48.895: INFO: azure-ip-masq-agent-xstdb from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.895: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:34:48.895: INFO: csi-secrets-store-cfj79 from kube-system started at 2021-04-29 22:17:42 +0000 UTC (3 container statuses recorded)
Apr 29 22:34:48.895: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:34:48.895: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:34:48.895: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:34:48.895: INFO: csi-secrets-store-provider-azure-qsz2p from kube-system started at 2021-04-29 22:17:42 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.895: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:34:48.895: INFO: kube-proxy-zfgdr from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.895: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:34:48.895: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:34:48.895: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:34:48.895: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:34:48.895: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-1 before test
Apr 29 22:34:48.908: INFO: azure-ip-masq-agent-vsvkh from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:34:48.908: INFO: csi-secrets-store-6rvtw from kube-system started at 2021-04-29 20:49:00 +0000 UTC (3 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:34:48.908: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:34:48.908: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:34:48.908: INFO: csi-secrets-store-provider-azure-mvwgl from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:34:48.908: INFO: kube-proxy-kf9sp from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:34:48.908: INFO: sonobuoy from sonobuoy started at 2021-04-29 21:52:30 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 22:34:48.908: INFO: sonobuoy-e2e-job-b783d86494674f2c from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container e2e ready: true, restart count 0
Apr 29 22:34:48.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:34:48.908: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:34:48.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:34:48.908: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:34:48.908: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-2 before test
Apr 29 22:34:48.919: INFO: azure-ip-masq-agent-twx92 from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.919: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:34:48.919: INFO: coredns-6bbfb77cb5-rjkn6 from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.919: INFO: 	Container coredns ready: true, restart count 0
Apr 29 22:34:48.919: INFO: csi-secrets-store-7k84m from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:34:48.919: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:34:48.919: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:34:48.919: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:34:48.919: INFO: csi-secrets-store-provider-azure-9zz42 from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.919: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:34:48.920: INFO: kube-proxy-2zjsk from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.920: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:34:48.920: INFO: metrics-server-6c8cc7585b-t8pbb from kube-system started at 2021-04-29 22:17:16 +0000 UTC (1 container statuses recorded)
Apr 29 22:34:48.920: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 22:34:48.920: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:34:48.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:34:48.920: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167a7507022612ec], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: true}, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.167a750702f646ed], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity, 3 node(s) had taint {node-role.kubernetes.io/master: true}, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:34:49.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6609" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":112,"skipped":1918,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:34:49.974: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 22:34:50.181: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:50.181: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:50.181: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:50.186: INFO: Number of nodes with available pods: 0
Apr 29 22:34:50.186: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 22:34:51.196: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:51.196: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:51.196: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:51.201: INFO: Number of nodes with available pods: 0
Apr 29 22:34:51.201: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 22:34:52.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:52.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:52.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:52.199: INFO: Number of nodes with available pods: 1
Apr 29 22:34:52.199: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 22:34:53.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.195: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.199: INFO: Number of nodes with available pods: 3
Apr 29 22:34:53.199: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 29 22:34:53.221: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.221: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.221: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:53.225: INFO: Number of nodes with available pods: 2
Apr 29 22:34:53.225: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:54.236: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:54.236: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:54.236: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:54.240: INFO: Number of nodes with available pods: 2
Apr 29 22:34:54.240: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:55.237: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:55.237: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:55.237: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:55.242: INFO: Number of nodes with available pods: 2
Apr 29 22:34:55.242: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:56.237: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:56.238: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:56.238: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:56.242: INFO: Number of nodes with available pods: 2
Apr 29 22:34:56.242: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:57.242: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:57.242: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:57.242: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:57.249: INFO: Number of nodes with available pods: 2
Apr 29 22:34:57.249: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:58.233: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:58.233: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:58.234: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:58.237: INFO: Number of nodes with available pods: 2
Apr 29 22:34:58.237: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:34:59.233: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:59.233: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:59.233: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 22:34:59.236: INFO: Number of nodes with available pods: 3
Apr 29 22:34:59.236: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8518, will wait for the garbage collector to delete the pods
Apr 29 22:34:59.303: INFO: Deleting DaemonSet.extensions daemon-set took: 10.445329ms
Apr 29 22:34:59.403: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.191735ms
Apr 29 22:35:12.116: INFO: Number of nodes with available pods: 0
Apr 29 22:35:12.116: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 22:35:12.120: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21975"},"items":null}

Apr 29 22:35:12.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21975"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:12.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8518" for this suite.

• [SLOW TEST:22.184 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":113,"skipped":1921,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:12.158: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:35:12.960: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 29 22:35:14.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332513, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332513, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332513, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332512, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:35:18.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:18.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9382" for this suite.
STEP: Destroying namespace "webhook-9382-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.060 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":114,"skipped":1927,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:18.219: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-2215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:35:18.819: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 29 22:35:18.821: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 29 22:35:18.821: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.821: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 29 22:35:18.821: INFO: Checking APIGroup: apps
Apr 29 22:35:18.822: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 29 22:35:18.822: INFO: Versions found [{apps/v1 v1}]
Apr 29 22:35:18.822: INFO: apps/v1 matches apps/v1
Apr 29 22:35:18.822: INFO: Checking APIGroup: events.k8s.io
Apr 29 22:35:18.823: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 29 22:35:18.823: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.823: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 29 22:35:18.823: INFO: Checking APIGroup: authentication.k8s.io
Apr 29 22:35:18.825: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 29 22:35:18.825: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.825: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 29 22:35:18.825: INFO: Checking APIGroup: authorization.k8s.io
Apr 29 22:35:18.826: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 29 22:35:18.826: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.826: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 29 22:35:18.826: INFO: Checking APIGroup: autoscaling
Apr 29 22:35:18.827: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Apr 29 22:35:18.827: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Apr 29 22:35:18.827: INFO: autoscaling/v1 matches autoscaling/v1
Apr 29 22:35:18.827: INFO: Checking APIGroup: batch
Apr 29 22:35:18.829: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 29 22:35:18.829: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Apr 29 22:35:18.829: INFO: batch/v1 matches batch/v1
Apr 29 22:35:18.829: INFO: Checking APIGroup: certificates.k8s.io
Apr 29 22:35:18.830: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 29 22:35:18.830: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.830: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 29 22:35:18.830: INFO: Checking APIGroup: networking.k8s.io
Apr 29 22:35:18.831: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 29 22:35:18.831: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.831: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 29 22:35:18.832: INFO: Checking APIGroup: extensions
Apr 29 22:35:18.833: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Apr 29 22:35:18.833: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Apr 29 22:35:18.833: INFO: extensions/v1beta1 matches extensions/v1beta1
Apr 29 22:35:18.833: INFO: Checking APIGroup: policy
Apr 29 22:35:18.834: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Apr 29 22:35:18.834: INFO: Versions found [{policy/v1beta1 v1beta1}]
Apr 29 22:35:18.834: INFO: policy/v1beta1 matches policy/v1beta1
Apr 29 22:35:18.834: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 29 22:35:18.835: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 29 22:35:18.835: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.835: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 29 22:35:18.835: INFO: Checking APIGroup: storage.k8s.io
Apr 29 22:35:18.837: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 29 22:35:18.837: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.837: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 29 22:35:18.837: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 29 22:35:18.838: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 29 22:35:18.838: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.838: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 29 22:35:18.838: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 29 22:35:18.839: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 29 22:35:18.839: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.839: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 29 22:35:18.839: INFO: Checking APIGroup: scheduling.k8s.io
Apr 29 22:35:18.841: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 29 22:35:18.841: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.841: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 29 22:35:18.841: INFO: Checking APIGroup: coordination.k8s.io
Apr 29 22:35:18.842: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 29 22:35:18.842: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.842: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 29 22:35:18.842: INFO: Checking APIGroup: node.k8s.io
Apr 29 22:35:18.843: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 29 22:35:18.843: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.843: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 29 22:35:18.843: INFO: Checking APIGroup: discovery.k8s.io
Apr 29 22:35:18.846: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Apr 29 22:35:18.846: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.846: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Apr 29 22:35:18.846: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 29 22:35:18.848: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Apr 29 22:35:18.848: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.848: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Apr 29 22:35:18.848: INFO: Checking APIGroup: secrets-store.csi.x-k8s.io
Apr 29 22:35:18.849: INFO: PreferredVersion.GroupVersion: secrets-store.csi.x-k8s.io/v1alpha1
Apr 29 22:35:18.849: INFO: Versions found [{secrets-store.csi.x-k8s.io/v1alpha1 v1alpha1}]
Apr 29 22:35:18.849: INFO: secrets-store.csi.x-k8s.io/v1alpha1 matches secrets-store.csi.x-k8s.io/v1alpha1
Apr 29 22:35:18.849: INFO: Checking APIGroup: metrics.k8s.io
Apr 29 22:35:18.851: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Apr 29 22:35:18.851: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Apr 29 22:35:18.851: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:18.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2215" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":115,"skipped":1940,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0429 22:35:25.141786      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 22:35:27.172: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:27.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8169" for this suite.

• [SLOW TEST:8.329 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":116,"skipped":1951,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:27.197: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-946" for this suite.

• [SLOW TEST:16.366 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":117,"skipped":1951,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:43.565: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-16
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:35:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:35:44.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-16" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":118,"skipped":1956,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:35:44.820: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-643
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-2e43c6de-15ff-41b6-81b6-76babe08c68a
STEP: Creating secret with name s-test-opt-upd-bbcc4d1f-66bf-4d52-b7a6-1cb00ca6cd03
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2e43c6de-15ff-41b6-81b6-76babe08c68a
STEP: Updating secret s-test-opt-upd-bbcc4d1f-66bf-4d52-b7a6-1cb00ca6cd03
STEP: Creating secret with name s-test-opt-create-daee6eb4-2b59-450e-b4d6-379ba3beecf1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:36:51.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-643" for this suite.

• [SLOW TEST:66.643 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":119,"skipped":1960,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:36:51.463: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:36:55.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3410" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":1960,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:36:55.717: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Apr 29 22:36:55.937: INFO: created test-event-1
Apr 29 22:36:55.963: INFO: created test-event-2
Apr 29 22:36:55.968: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Apr 29 22:36:55.972: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Apr 29 22:36:56.004: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:36:56.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7651" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":121,"skipped":1979,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:36:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:36:56.987: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 29 22:36:59.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332617, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332617, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332617, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332617, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:37:02.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 29 22:37:04.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=webhook-1981 attach --namespace=webhook-1981 to-be-attached-pod -i -c=container1'
Apr 29 22:37:04.509: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:37:04.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1981" for this suite.
STEP: Destroying namespace "webhook-1981-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.561 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":122,"skipped":1990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:37:04.599: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 29 22:37:09.338: INFO: Successfully updated pod "annotationupdated0e203bc-557e-429b-b9a0-cf0dccd25155"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:37:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6484" for this suite.

• [SLOW TEST:6.775 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2012,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:37:11.374: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 22:37:11.570: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 22:38:11.638: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:38:11.641: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Apr 29 22:38:15.883: INFO: found a healthy node: k8s-linuxpool-38811533-0
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:38:43.998: INFO: pods created so far: [1 1 1]
Apr 29 22:38:43.998: INFO: length of pods created so far: 3
Apr 29 22:38:52.026: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:38:59.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-970" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:38:59.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6506" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:107.810 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":124,"skipped":2021,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:38:59.185: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:39:01.398: INFO: Deleting pod "var-expansion-804a9f42-b619-4cd4-98ae-bd9bb150c8a4" in namespace "var-expansion-1347"
Apr 29 22:39:01.409: INFO: Wait up to 5m0s for pod "var-expansion-804a9f42-b619-4cd4-98ae-bd9bb150c8a4" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:13.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1347" for this suite.

• [SLOW TEST:14.257 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":125,"skipped":2022,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:13.443: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:39:13.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6" in namespace "projected-1518" to be "Succeeded or Failed"
Apr 29 22:39:13.657: INFO: Pod "downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.029333ms
Apr 29 22:39:15.669: INFO: Pod "downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01435644s
Apr 29 22:39:17.686: INFO: Pod "downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031952898s
STEP: Saw pod success
Apr 29 22:39:17.686: INFO: Pod "downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6" satisfied condition "Succeeded or Failed"
Apr 29 22:39:17.691: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6 container client-container: <nil>
STEP: delete the pod
Apr 29 22:39:17.775: INFO: Waiting for pod downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6 to disappear
Apr 29 22:39:17.778: INFO: Pod downwardapi-volume-ab8f4b10-ca55-4ddf-a0f1-9f1c981594d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:17.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1518" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2028,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:17.789: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Apr 29 22:39:17.976: INFO: Waiting up to 5m0s for pod "var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f" in namespace "var-expansion-4673" to be "Succeeded or Failed"
Apr 29 22:39:17.979: INFO: Pod "var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371637ms
Apr 29 22:39:19.995: INFO: Pod "var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019072055s
STEP: Saw pod success
Apr 29 22:39:19.995: INFO: Pod "var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f" satisfied condition "Succeeded or Failed"
Apr 29 22:39:19.998: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f container dapi-container: <nil>
STEP: delete the pod
Apr 29 22:39:20.022: INFO: Waiting for pod var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f to disappear
Apr 29 22:39:20.025: INFO: Pod var-expansion-a405f651-2840-4158-93da-a6c2ae6be49f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:20.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4673" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:20.037: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8288
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 29 22:39:20.247: INFO: Waiting up to 5m0s for pod "pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d" in namespace "emptydir-8288" to be "Succeeded or Failed"
Apr 29 22:39:20.252: INFO: Pod "pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.824253ms
Apr 29 22:39:22.260: INFO: Pod "pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012602866s
Apr 29 22:39:24.272: INFO: Pod "pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025213715s
STEP: Saw pod success
Apr 29 22:39:24.272: INFO: Pod "pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d" satisfied condition "Succeeded or Failed"
Apr 29 22:39:24.276: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d container test-container: <nil>
STEP: delete the pod
Apr 29 22:39:24.335: INFO: Waiting for pod pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d to disappear
Apr 29 22:39:24.338: INFO: Pod pod-77ceebf0-7a8b-4de5-8ae6-b5f55fbe341d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:24.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8288" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2070,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:24.349: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 22:39:27.581: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:27.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2059" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":129,"skipped":2070,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:27.613: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0429 22:39:29.389981      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 22:39:31.416: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:31.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8365" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":130,"skipped":2074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:31.428: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-1f8edf58-aa90-4561-92d4-573f69dbe456
STEP: Creating a pod to test consume configMaps
Apr 29 22:39:31.626: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8" in namespace "projected-4586" to be "Succeeded or Failed"
Apr 29 22:39:31.630: INFO: Pod "pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.190235ms
Apr 29 22:39:33.642: INFO: Pod "pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0151537s
Apr 29 22:39:35.652: INFO: Pod "pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025069427s
STEP: Saw pod success
Apr 29 22:39:35.652: INFO: Pod "pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8" satisfied condition "Succeeded or Failed"
Apr 29 22:39:35.655: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:39:35.692: INFO: Waiting for pod pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8 to disappear
Apr 29 22:39:35.695: INFO: Pod pod-projected-configmaps-a5d93e70-efae-49ef-8020-2f4905261ad8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:35.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4586" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":131,"skipped":2119,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:35.706: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-6186780c-5d34-4c93-96d3-e17fcc3c0fec
STEP: Creating a pod to test consume secrets
Apr 29 22:39:35.920: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79" in namespace "projected-1315" to be "Succeeded or Failed"
Apr 29 22:39:35.923: INFO: Pod "pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.875131ms
Apr 29 22:39:37.934: INFO: Pod "pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014261755s
Apr 29 22:39:39.945: INFO: Pod "pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024862455s
STEP: Saw pod success
Apr 29 22:39:39.945: INFO: Pod "pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79" satisfied condition "Succeeded or Failed"
Apr 29 22:39:39.948: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:39:39.982: INFO: Waiting for pod pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79 to disappear
Apr 29 22:39:39.985: INFO: Pod pod-projected-secrets-22f71d34-a99f-4546-bf74-279f95b36e79 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:39.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1315" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":132,"skipped":2120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:39.997: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:39:40.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:39:43.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:39:43.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8386" for this suite.
STEP: Destroying namespace "webhook-8386-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":133,"skipped":2147,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:39:43.903: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4009
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4009
STEP: creating service affinity-clusterip-transition in namespace services-4009
STEP: creating replication controller affinity-clusterip-transition in namespace services-4009
I0429 22:39:44.161005      21 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4009, replica count: 3
I0429 22:39:47.211399      21 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:39:47.226: INFO: Creating new exec pod
Apr 29 22:39:52.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4009 exec execpod-affinity57mkn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Apr 29 22:39:52.524: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 29 22:39:52.524: INFO: stdout: ""
Apr 29 22:39:52.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4009 exec execpod-affinity57mkn -- /bin/sh -x -c nc -zv -t -w 2 10.0.66.119 80'
Apr 29 22:39:52.812: INFO: stderr: "+ nc -zv -t -w 2 10.0.66.119 80\nConnection to 10.0.66.119 80 port [tcp/http] succeeded!\n"
Apr 29 22:39:52.812: INFO: stdout: ""
Apr 29 22:39:52.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4009 exec execpod-affinity57mkn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.66.119:80/ ; done'
Apr 29 22:39:53.203: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n"
Apr 29 22:39:53.203: INFO: stdout: "\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-d2tqj\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-2d284\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-d2tqj\naffinity-clusterip-transition-2d284\naffinity-clusterip-transition-d2tqj\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-2d284"
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-d2tqj
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-2d284
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-d2tqj
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-2d284
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-d2tqj
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.203: INFO: Received response from host: affinity-clusterip-transition-2d284
Apr 29 22:39:53.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4009 exec execpod-affinity57mkn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.66.119:80/ ; done'
Apr 29 22:39:53.632: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.66.119:80/\n"
Apr 29 22:39:53.632: INFO: stdout: "\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv\naffinity-clusterip-transition-tvhhv"
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Received response from host: affinity-clusterip-transition-tvhhv
Apr 29 22:39:53.632: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4009, will wait for the garbage collector to delete the pods
Apr 29 22:39:53.709: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.726583ms
Apr 29 22:39:53.809: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.192281ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:02.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4009" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.283 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":134,"skipped":2153,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:02.193: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-9130
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 22:40:02.394: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 22:40:02.438: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 22:40:04.451: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:40:06.450: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:40:08.449: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:40:10.448: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:40:12.446: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 22:40:14.448: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 22:40:14.455: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:40:16.466: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:40:18.465: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 22:40:20.464: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 29 22:40:20.470: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 29 22:40:24.524: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 29 22:40:24.524: INFO: Going to poll 10.244.2.122 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 29 22:40:24.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.122:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:40:24.527: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:40:24.686: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 29 22:40:24.686: INFO: Going to poll 10.244.3.32 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 29 22:40:24.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.32:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:40:24.691: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:40:24.843: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 29 22:40:24.843: INFO: Going to poll 10.244.4.68 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Apr 29 22:40:24.847: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.68:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9130 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 22:40:24.847: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 22:40:25.015: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:25.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9130" for this suite.

• [SLOW TEST:22.841 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2163,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:25.034: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8886
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 29 22:40:25.222: INFO: Waiting up to 5m0s for pod "pod-87ded27b-fd4b-4376-b060-fd4f8121c52d" in namespace "emptydir-8886" to be "Succeeded or Failed"
Apr 29 22:40:25.231: INFO: Pod "pod-87ded27b-fd4b-4376-b060-fd4f8121c52d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.028896ms
Apr 29 22:40:27.243: INFO: Pod "pod-87ded27b-fd4b-4376-b060-fd4f8121c52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020580943s
Apr 29 22:40:29.251: INFO: Pod "pod-87ded27b-fd4b-4376-b060-fd4f8121c52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029192145s
STEP: Saw pod success
Apr 29 22:40:29.251: INFO: Pod "pod-87ded27b-fd4b-4376-b060-fd4f8121c52d" satisfied condition "Succeeded or Failed"
Apr 29 22:40:29.255: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-87ded27b-fd4b-4376-b060-fd4f8121c52d container test-container: <nil>
STEP: delete the pod
Apr 29 22:40:29.280: INFO: Waiting for pod pod-87ded27b-fd4b-4376-b060-fd4f8121c52d to disappear
Apr 29 22:40:29.284: INFO: Pod pod-87ded27b-fd4b-4376-b060-fd4f8121c52d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8886" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2164,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:29.296: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-dcd352d3-d4f7-4880-8d09-609a898517ca
STEP: Creating a pod to test consume secrets
Apr 29 22:40:29.525: INFO: Waiting up to 5m0s for pod "pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4" in namespace "secrets-5820" to be "Succeeded or Failed"
Apr 29 22:40:29.529: INFO: Pod "pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333435ms
Apr 29 22:40:31.539: INFO: Pod "pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013297735s
STEP: Saw pod success
Apr 29 22:40:31.539: INFO: Pod "pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4" satisfied condition "Succeeded or Failed"
Apr 29 22:40:31.544: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:40:31.589: INFO: Waiting for pod pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4 to disappear
Apr 29 22:40:31.593: INFO: Pod pod-secrets-75f2ed02-a4ce-46b4-b310-29e80bf1bbe4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:31.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5820" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":137,"skipped":2170,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:40:32.363: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 29 22:40:34.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332832, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332832, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332832, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755332832, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:40:37.418: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:37.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7193" for this suite.
STEP: Destroying namespace "webhook-7193-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.973 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":138,"skipped":2172,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:37.583: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 22:40:37.816: INFO: Waiting up to 5m0s for pod "pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7" in namespace "emptydir-9847" to be "Succeeded or Failed"
Apr 29 22:40:37.819: INFO: Pod "pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157133ms
Apr 29 22:40:39.828: INFO: Pod "pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012458766s
Apr 29 22:40:41.835: INFO: Pod "pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019260158s
STEP: Saw pod success
Apr 29 22:40:41.835: INFO: Pod "pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7" satisfied condition "Succeeded or Failed"
Apr 29 22:40:41.838: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7 container test-container: <nil>
STEP: delete the pod
Apr 29 22:40:41.873: INFO: Waiting for pod pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7 to disappear
Apr 29 22:40:41.877: INFO: Pod pod-7eb01cea-ddcc-41fc-b5df-644f9de797a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:40:41.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9847" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2187,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:40:41.890: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Apr 29 22:40:42.099: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 22:41:42.164: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Apr 29 22:41:42.190: INFO: Created pod: pod0-sched-preemption-low-priority
Apr 29 22:41:42.210: INFO: Created pod: pod1-sched-preemption-medium-priority
Apr 29 22:41:42.262: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:42:46.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7163" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:124.566 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":140,"skipped":2190,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:42:46.459: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Apr 29 22:42:46.711: INFO: observed Pod pod-test in namespace pods-179 in phase Pending conditions []
Apr 29 22:42:46.715: INFO: observed Pod pod-test in namespace pods-179 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:42:46 +0000 UTC  }]
Apr 29 22:42:46.734: INFO: observed Pod pod-test in namespace pods-179 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:42:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:42:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:42:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-04-29 22:42:46 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Apr 29 22:42:49.097: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Apr 29 22:42:49.138: INFO: observed event type ADDED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
Apr 29 22:42:49.138: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:42:49.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-179" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":141,"skipped":2205,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:42:49.172: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-6903
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6903
STEP: Deleting pre-stop pod
Apr 29 22:43:00.418: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:00.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6903" for this suite.

• [SLOW TEST:11.286 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":142,"skipped":2218,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:00.458: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-6960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:43:00.650: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c" in namespace "security-context-test-6960" to be "Succeeded or Failed"
Apr 29 22:43:00.653: INFO: Pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.90393ms
Apr 29 22:43:02.658: INFO: Pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00781083s
Apr 29 22:43:04.668: INFO: Pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018516979s
Apr 29 22:43:04.668: INFO: Pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c" satisfied condition "Succeeded or Failed"
Apr 29 22:43:04.711: INFO: Got logs for pod "busybox-privileged-false-a8377b56-31b1-45e0-835c-570fedabd09c": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:04.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6960" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2232,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:04.727: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 29 22:43:04.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-1169 create -f -'
Apr 29 22:43:05.315: INFO: stderr: ""
Apr 29 22:43:05.315: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 22:43:06.324: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 22:43:06.324: INFO: Found 0 / 1
Apr 29 22:43:07.323: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 22:43:07.323: INFO: Found 0 / 1
Apr 29 22:43:08.326: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 22:43:08.326: INFO: Found 1 / 1
Apr 29 22:43:08.326: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 29 22:43:08.330: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 22:43:08.330: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 22:43:08.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-1169 patch pod agnhost-primary-8ckkq -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 29 22:43:08.468: INFO: stderr: ""
Apr 29 22:43:08.468: INFO: stdout: "pod/agnhost-primary-8ckkq patched\n"
STEP: checking annotations
Apr 29 22:43:08.474: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 22:43:08.474: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:08.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1169" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":144,"skipped":2251,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:08.492: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:16.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4652" for this suite.

• [SLOW TEST:8.219 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":145,"skipped":2255,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:16.711: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-53
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-1fa2275f-dbb8-4870-80e9-9b5fba404918
STEP: Creating a pod to test consume configMaps
Apr 29 22:43:16.929: INFO: Waiting up to 5m0s for pod "pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f" in namespace "configmap-53" to be "Succeeded or Failed"
Apr 29 22:43:16.932: INFO: Pod "pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135031ms
Apr 29 22:43:18.948: INFO: Pod "pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019233459s
Apr 29 22:43:20.960: INFO: Pod "pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03070853s
STEP: Saw pod success
Apr 29 22:43:20.960: INFO: Pod "pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f" satisfied condition "Succeeded or Failed"
Apr 29 22:43:20.963: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:43:20.992: INFO: Waiting for pod pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f to disappear
Apr 29 22:43:20.995: INFO: Pod pod-configmaps-42449bee-2f71-4d45-9946-35af6476086f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:20.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-53" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":146,"skipped":2256,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:21.009: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6536
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7067
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1485" for this suite.
STEP: Destroying namespace "nsdeletetest-6536" for this suite.
Apr 29 22:43:27.763: INFO: Namespace nsdeletetest-6536 was already deleted
STEP: Destroying namespace "nsdeletetest-7067" for this suite.

• [SLOW TEST:6.760 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":147,"skipped":2260,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:27.770: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:43:27.957: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 29 22:43:27.967: INFO: Number of nodes with available pods: 0
Apr 29 22:43:27.967: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 29 22:43:27.991: INFO: Number of nodes with available pods: 0
Apr 29 22:43:27.991: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:29.000: INFO: Number of nodes with available pods: 0
Apr 29 22:43:29.000: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:30.000: INFO: Number of nodes with available pods: 0
Apr 29 22:43:30.000: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:31.006: INFO: Number of nodes with available pods: 1
Apr 29 22:43:31.006: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 29 22:43:31.037: INFO: Number of nodes with available pods: 1
Apr 29 22:43:31.037: INFO: Number of running nodes: 0, number of available pods: 1
Apr 29 22:43:32.052: INFO: Number of nodes with available pods: 0
Apr 29 22:43:32.052: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 29 22:43:32.074: INFO: Number of nodes with available pods: 0
Apr 29 22:43:32.074: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:33.081: INFO: Number of nodes with available pods: 0
Apr 29 22:43:33.081: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:34.082: INFO: Number of nodes with available pods: 0
Apr 29 22:43:34.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:35.082: INFO: Number of nodes with available pods: 0
Apr 29 22:43:35.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:36.081: INFO: Number of nodes with available pods: 0
Apr 29 22:43:36.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:37.084: INFO: Number of nodes with available pods: 0
Apr 29 22:43:37.084: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:38.082: INFO: Number of nodes with available pods: 0
Apr 29 22:43:38.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:39.082: INFO: Number of nodes with available pods: 0
Apr 29 22:43:39.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:40.086: INFO: Number of nodes with available pods: 0
Apr 29 22:43:40.086: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:41.084: INFO: Number of nodes with available pods: 0
Apr 29 22:43:41.084: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:42.082: INFO: Number of nodes with available pods: 0
Apr 29 22:43:42.082: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:43.080: INFO: Number of nodes with available pods: 0
Apr 29 22:43:43.080: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 22:43:44.082: INFO: Number of nodes with available pods: 1
Apr 29 22:43:44.082: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-744, will wait for the garbage collector to delete the pods
Apr 29 22:43:44.153: INFO: Deleting DaemonSet.extensions daemon-set took: 10.631207ms
Apr 29 22:43:44.753: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.16494ms
Apr 29 22:43:47.668: INFO: Number of nodes with available pods: 0
Apr 29 22:43:47.668: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 22:43:47.670: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24989"},"items":null}

Apr 29 22:43:47.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24989"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:47.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-744" for this suite.

• [SLOW TEST:19.955 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":148,"skipped":2280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Apr 29 22:43:47.947: INFO: Waiting up to 5m0s for pod "var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825" in namespace "var-expansion-9595" to be "Succeeded or Failed"
Apr 29 22:43:47.956: INFO: Pod "var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825": Phase="Pending", Reason="", readiness=false. Elapsed: 8.726887ms
Apr 29 22:43:49.966: INFO: Pod "var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019222501s
Apr 29 22:43:51.978: INFO: Pod "var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031064519s
STEP: Saw pod success
Apr 29 22:43:51.978: INFO: Pod "var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825" satisfied condition "Succeeded or Failed"
Apr 29 22:43:51.981: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825 container dapi-container: <nil>
STEP: delete the pod
Apr 29 22:43:52.021: INFO: Waiting for pod var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825 to disappear
Apr 29 22:43:52.025: INFO: Pod var-expansion-f5c6ac16-9668-436a-b75e-e5907e373825 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:52.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9595" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":149,"skipped":2308,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:52.042: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1952.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:43:56.390: INFO: DNS probes using dns-1952/dns-test-9f224925-03ee-424f-b06f-6d76e0fbc464 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:43:56.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1952" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":150,"skipped":2320,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:43:56.441: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2325
STEP: creating service affinity-clusterip in namespace services-2325
STEP: creating replication controller affinity-clusterip in namespace services-2325
I0429 22:43:56.700491      21 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2325, replica count: 3
I0429 22:43:59.750796      21 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:43:59.766: INFO: Creating new exec pod
Apr 29 22:44:04.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2325 exec execpod-affinitywxzfx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Apr 29 22:44:05.086: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 29 22:44:05.086: INFO: stdout: ""
Apr 29 22:44:05.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2325 exec execpod-affinitywxzfx -- /bin/sh -x -c nc -zv -t -w 2 10.0.108.189 80'
Apr 29 22:44:05.345: INFO: stderr: "+ nc -zv -t -w 2 10.0.108.189 80\nConnection to 10.0.108.189 80 port [tcp/http] succeeded!\n"
Apr 29 22:44:05.346: INFO: stdout: ""
Apr 29 22:44:05.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-2325 exec execpod-affinitywxzfx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.108.189:80/ ; done'
Apr 29 22:44:05.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.108.189:80/\n"
Apr 29 22:44:05.692: INFO: stdout: "\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8\naffinity-clusterip-sbtc8"
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.692: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.693: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.693: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.693: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.693: INFO: Received response from host: affinity-clusterip-sbtc8
Apr 29 22:44:05.693: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2325, will wait for the garbage collector to delete the pods
Apr 29 22:44:05.784: INFO: Deleting ReplicationController affinity-clusterip took: 9.05459ms
Apr 29 22:44:05.885: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.251504ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:44:21.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2325" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:25.523 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":151,"skipped":2320,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:44:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3695.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3695.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3695.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3695.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3695.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3695.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:44:26.261: INFO: DNS probes using dns-3695/dns-test-243fd8b2-4222-4ee2-9b2e-a80bfc70e843 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:44:26.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3695" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":152,"skipped":2332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:44:26.300: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:44:26.489: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 29 22:44:31.509: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 22:44:31.509: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 29 22:44:33.516: INFO: Creating deployment "test-rollover-deployment"
Apr 29 22:44:33.524: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 29 22:44:35.541: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 29 22:44:35.547: INFO: Ensure that both replica sets have 1 created replica
Apr 29 22:44:35.554: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 29 22:44:35.566: INFO: Updating deployment test-rollover-deployment
Apr 29 22:44:35.566: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 29 22:44:37.579: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 29 22:44:37.586: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 29 22:44:37.592: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:37.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333075, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:39.605: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:39.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:41.605: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:41.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:43.607: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:43.607: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:45.608: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:45.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:47.606: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 22:44:47.606: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333077, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333073, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 22:44:49.605: INFO: 
Apr 29 22:44:49.605: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 22:44:49.614: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6395  8e1dff7a-94c7-4d57-aef5-6020e38def21 25434 2 2021-04-29 22:44:33 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-29 22:44:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 22:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aa58b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-04-29 22:44:33 +0000 UTC,LastTransitionTime:2021-04-29 22:44:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-04-29 22:44:48 +0000 UTC,LastTransitionTime:2021-04-29 22:44:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 22:44:49.617: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-6395  c7360996-9057-4786-9dcd-92587f72b45a 25424 2 2021-04-29 22:44:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8e1dff7a-94c7-4d57-aef5-6020e38def21 0xc002aa5d17 0xc002aa5d18}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:44:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e1dff7a-94c7-4d57-aef5-6020e38def21\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aa5da8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 22:44:49.617: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 29 22:44:49.617: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6395  173f6613-cabe-418e-9228-4156817226a0 25432 2 2021-04-29 22:44:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8e1dff7a-94c7-4d57-aef5-6020e38def21 0xc002aa5c07 0xc002aa5c08}] []  [{e2e.test Update apps/v1 2021-04-29 22:44:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 22:44:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e1dff7a-94c7-4d57-aef5-6020e38def21\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002aa5ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 22:44:49.618: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6395  85f7c2aa-5d15-456c-821e-c2be6910c627 25389 2 2021-04-29 22:44:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8e1dff7a-94c7-4d57-aef5-6020e38def21 0xc002aa5e17 0xc002aa5e18}] []  [{kube-controller-manager Update apps/v1 2021-04-29 22:44:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8e1dff7a-94c7-4d57-aef5-6020e38def21\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002aa5ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 22:44:49.625: INFO: Pod "test-rollover-deployment-668db69979-xtwjq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-xtwjq test-rollover-deployment-668db69979- deployment-6395  8cc91922-145c-43a2-99f0-8f1fdc6e4453 25402 0 2021-04-29 22:44:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 c7360996-9057-4786-9dcd-92587f72b45a 0xc0037f6677 0xc0037f6678}] []  [{kube-controller-manager Update v1 2021-04-29 22:44:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7360996-9057-4786-9dcd-92587f72b45a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 22:44:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vgwpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vgwpt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vgwpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:44:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:44:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:44:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 22:44:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.148,StartTime:2021-04-29 22:44:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 22:44:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://e27314218b9829138e5752108290ad531291b9473bffa5f053bb98c3b5af379f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:44:49.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6395" for this suite.

• [SLOW TEST:23.351 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":153,"skipped":2393,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:44:49.652: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:44:49.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d" in namespace "projected-1501" to be "Succeeded or Failed"
Apr 29 22:44:49.867: INFO: Pod "downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.671036ms
Apr 29 22:44:51.878: INFO: Pod "downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014845271s
Apr 29 22:44:53.884: INFO: Pod "downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021192548s
STEP: Saw pod success
Apr 29 22:44:53.884: INFO: Pod "downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d" satisfied condition "Succeeded or Failed"
Apr 29 22:44:53.887: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d container client-container: <nil>
STEP: delete the pod
Apr 29 22:44:53.920: INFO: Waiting for pod downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d to disappear
Apr 29 22:44:53.923: INFO: Pod downwardapi-volume-824343b1-42a5-4a93-a1b7-7c21c699480d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:44:53.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1501" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:44:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Apr 29 22:44:54.128: INFO: created test-podtemplate-1
Apr 29 22:44:54.132: INFO: created test-podtemplate-2
Apr 29 22:44:54.136: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Apr 29 22:44:54.139: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Apr 29 22:44:54.162: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:44:54.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6457" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":155,"skipped":2445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:44:54.178: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5
Apr 29 22:44:54.370: INFO: Pod name my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5: Found 0 pods out of 1
Apr 29 22:44:59.385: INFO: Pod name my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5: Found 1 pods out of 1
Apr 29 22:44:59.385: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5" are running
Apr 29 22:44:59.389: INFO: Pod "my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5-ckpg9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 22:44:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 22:44:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 22:44:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 22:44:54 +0000 UTC Reason: Message:}])
Apr 29 22:44:59.389: INFO: Trying to dial the pod
Apr 29 22:45:04.405: INFO: Controller my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5: Got expected result from replica 1 [my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5-ckpg9]: "my-hostname-basic-d7480cc5-a72b-4e91-9711-fa42a5779ec5-ckpg9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:04.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9183" for this suite.

• [SLOW TEST:10.241 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":156,"skipped":2470,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:04.420: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:45:04.611: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-6d64b4ec-8097-412e-86ea-cea7f49f79ec" in namespace "security-context-test-8797" to be "Succeeded or Failed"
Apr 29 22:45:04.614: INFO: Pod "busybox-readonly-false-6d64b4ec-8097-412e-86ea-cea7f49f79ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.806227ms
Apr 29 22:45:06.625: INFO: Pod "busybox-readonly-false-6d64b4ec-8097-412e-86ea-cea7f49f79ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013688295s
Apr 29 22:45:06.625: INFO: Pod "busybox-readonly-false-6d64b4ec-8097-412e-86ea-cea7f49f79ec" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:06.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8797" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":157,"skipped":2477,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:06.640: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a0b78efc-ef1e-4fbe-83a0-5bd72f6adf5e
STEP: Creating a pod to test consume configMaps
Apr 29 22:45:06.838: INFO: Waiting up to 5m0s for pod "pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c" in namespace "configmap-8913" to be "Succeeded or Failed"
Apr 29 22:45:06.841: INFO: Pod "pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511234ms
Apr 29 22:45:08.852: INFO: Pod "pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014022689s
Apr 29 22:45:10.862: INFO: Pod "pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024002631s
STEP: Saw pod success
Apr 29 22:45:10.862: INFO: Pod "pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c" satisfied condition "Succeeded or Failed"
Apr 29 22:45:10.864: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:45:10.891: INFO: Waiting for pod pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c to disappear
Apr 29 22:45:10.896: INFO: Pod pod-configmaps-279ea58a-5379-4c86-b113-aefbc2e1181c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:10.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8913" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2487,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:10.907: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:15.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3986" for this suite.

• [SLOW TEST:5.069 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":159,"skipped":2490,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:15.976: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 29 22:45:16.179: INFO: Waiting up to 5m0s for pod "downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f" in namespace "downward-api-524" to be "Succeeded or Failed"
Apr 29 22:45:16.182: INFO: Pod "downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.150731ms
Apr 29 22:45:18.193: INFO: Pod "downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013627547s
Apr 29 22:45:20.211: INFO: Pod "downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031316425s
STEP: Saw pod success
Apr 29 22:45:20.211: INFO: Pod "downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f" satisfied condition "Succeeded or Failed"
Apr 29 22:45:20.214: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f container dapi-container: <nil>
STEP: delete the pod
Apr 29 22:45:20.253: INFO: Waiting for pod downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f to disappear
Apr 29 22:45:20.256: INFO: Pod downward-api-4a3eddf5-9403-4e82-9133-21cb4b3d740f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:20.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-524" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":160,"skipped":2509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:20.269: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 29 22:45:20.476: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:23.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-949" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":161,"skipped":2583,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:23.759: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-e9282893-73bc-4d87-9602-1aad2a5fafce
STEP: Creating a pod to test consume configMaps
Apr 29 22:45:23.965: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8" in namespace "projected-4676" to be "Succeeded or Failed"
Apr 29 22:45:23.968: INFO: Pod "pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.04493ms
Apr 29 22:45:25.979: INFO: Pod "pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013676015s
Apr 29 22:45:27.996: INFO: Pod "pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030707055s
STEP: Saw pod success
Apr 29 22:45:27.996: INFO: Pod "pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8" satisfied condition "Succeeded or Failed"
Apr 29 22:45:28.000: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:45:28.042: INFO: Waiting for pod pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8 to disappear
Apr 29 22:45:28.046: INFO: Pod pod-projected-configmaps-12a76380-c5f7-4e2e-9715-a28f9be098f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:28.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4676" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:28.065: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:45:29.179: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:45:31.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333129, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333129, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333129, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333129, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:45:34.221: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 29 22:45:34.252: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-181" for this suite.
STEP: Destroying namespace "webhook-181-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.309 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":163,"skipped":2630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:34.376: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:45:38.595: INFO: DNS probes using dns-test-84f99c9d-87a3-4a59-91e7-0f257a88090e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:45:42.667: INFO: DNS probes using dns-test-e85f4d92-f33f-4164-91e2-893ba9fbf092 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3342.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3342.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 22:45:46.756: INFO: DNS probes using dns-test-8c137b09-b058-43ce-a664-40ddaa883878 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:46.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3342" for this suite.

• [SLOW TEST:12.455 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":164,"skipped":2720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:46.831: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-f7a8b533-e689-4623-b289-4ac0242894c9
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:47.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-240" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":165,"skipped":2752,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:47.028: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 29 22:45:47.194: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 22:45:47.203: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 22:45:47.207: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-0 before test
Apr 29 22:45:47.217: INFO: azure-ip-masq-agent-xstdb from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.217: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:45:47.217: INFO: csi-secrets-store-cfj79 from kube-system started at 2021-04-29 22:17:42 +0000 UTC (3 container statuses recorded)
Apr 29 22:45:47.217: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:45:47.217: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:45:47.217: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:45:47.217: INFO: csi-secrets-store-provider-azure-qsz2p from kube-system started at 2021-04-29 22:17:42 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.217: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:45:47.217: INFO: kube-proxy-zfgdr from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.217: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:45:47.217: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:45:47.217: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:45:47.217: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:45:47.217: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-1 before test
Apr 29 22:45:47.227: INFO: azure-ip-masq-agent-vsvkh from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.227: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:45:47.227: INFO: csi-secrets-store-6rvtw from kube-system started at 2021-04-29 20:49:00 +0000 UTC (3 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:45:47.228: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:45:47.228: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:45:47.228: INFO: csi-secrets-store-provider-azure-mvwgl from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:45:47.228: INFO: kube-proxy-kf9sp from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:45:47.228: INFO: sonobuoy from sonobuoy started at 2021-04-29 21:52:30 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 22:45:47.228: INFO: sonobuoy-e2e-job-b783d86494674f2c from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container e2e ready: true, restart count 0
Apr 29 22:45:47.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:45:47.228: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:45:47.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:45:47.228: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 22:45:47.228: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-2 before test
Apr 29 22:45:47.238: INFO: azure-ip-masq-agent-twx92 from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.238: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 22:45:47.238: INFO: coredns-6bbfb77cb5-rjkn6 from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.238: INFO: 	Container coredns ready: true, restart count 0
Apr 29 22:45:47.238: INFO: csi-secrets-store-7k84m from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 22:45:47.238: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 22:45:47.238: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 22:45:47.238: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 22:45:47.238: INFO: csi-secrets-store-provider-azure-9zz42 from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.238: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 22:45:47.238: INFO: kube-proxy-2zjsk from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.239: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 22:45:47.239: INFO: metrics-server-6c8cc7585b-t8pbb from kube-system started at 2021-04-29 22:17:16 +0000 UTC (1 container statuses recorded)
Apr 29 22:45:47.239: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 22:45:47.239: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 22:45:47.239: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 22:45:47.239: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b2ebe470-1119-4f81-b034-a38ae3e4d457 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b2ebe470-1119-4f81-b034-a38ae3e4d457 off the node k8s-linuxpool-38811533-0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b2ebe470-1119-4f81-b034-a38ae3e4d457
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:55.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3152" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.335 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":166,"skipped":2754,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:55.364: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:45:55.552: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb" in namespace "projected-3450" to be "Succeeded or Failed"
Apr 29 22:45:55.555: INFO: Pod "downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09303ms
Apr 29 22:45:57.575: INFO: Pod "downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023004682s
Apr 29 22:45:59.584: INFO: Pod "downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031874717s
STEP: Saw pod success
Apr 29 22:45:59.584: INFO: Pod "downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb" satisfied condition "Succeeded or Failed"
Apr 29 22:45:59.588: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb container client-container: <nil>
STEP: delete the pod
Apr 29 22:45:59.616: INFO: Waiting for pod downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb to disappear
Apr 29 22:45:59.619: INFO: Pod downwardapi-volume-de965201-e661-4983-b732-d8e0d1b652fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:45:59.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3450" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:45:59.631: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4440
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8575
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:46:31.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9847" for this suite.
STEP: Destroying namespace "nsdeletetest-4440" for this suite.
Apr 29 22:46:31.379: INFO: Namespace nsdeletetest-4440 was already deleted
STEP: Destroying namespace "nsdeletetest-8575" for this suite.

• [SLOW TEST:31.755 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":168,"skipped":2788,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:46:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:46:31.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57" in namespace "projected-9400" to be "Succeeded or Failed"
Apr 29 22:46:31.583: INFO: Pod "downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.967429ms
Apr 29 22:46:33.594: INFO: Pod "downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013774958s
Apr 29 22:46:35.603: INFO: Pod "downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02355677s
STEP: Saw pod success
Apr 29 22:46:35.604: INFO: Pod "downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57" satisfied condition "Succeeded or Failed"
Apr 29 22:46:35.607: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57 container client-container: <nil>
STEP: delete the pod
Apr 29 22:46:35.643: INFO: Waiting for pod downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57 to disappear
Apr 29 22:46:35.650: INFO: Pod downwardapi-volume-f3d908f2-3f44-4db2-803b-edbed1d00b57 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:46:35.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9400" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":2800,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:46:35.668: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8810
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8810
STEP: creating replication controller externalsvc in namespace services-8810
I0429 22:46:35.901770      21 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8810, replica count: 2
I0429 22:46:38.952100      21 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 29 22:46:38.995: INFO: Creating new exec pod
Apr 29 22:46:43.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-8810 exec execpodx45nq -- /bin/sh -x -c nslookup clusterip-service.services-8810.svc.cluster.local'
Apr 29 22:46:43.337: INFO: stderr: "+ nslookup clusterip-service.services-8810.svc.cluster.local\n"
Apr 29 22:46:43.337: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-8810.svc.cluster.local\tcanonical name = externalsvc.services-8810.svc.cluster.local.\nName:\texternalsvc.services-8810.svc.cluster.local\nAddress: 10.0.255.171\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8810, will wait for the garbage collector to delete the pods
Apr 29 22:46:43.410: INFO: Deleting ReplicationController externalsvc took: 16.139756ms
Apr 29 22:46:44.110: INFO: Terminating ReplicationController externalsvc pods took: 700.188987ms
Apr 29 22:46:51.856: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:46:51.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8810" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:16.232 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":170,"skipped":2804,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:46:51.904: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-470ffe71-02b2-463a-b79b-0d42be0fe535
STEP: Creating a pod to test consume secrets
Apr 29 22:46:52.118: INFO: Waiting up to 5m0s for pod "pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18" in namespace "secrets-5908" to be "Succeeded or Failed"
Apr 29 22:46:52.121: INFO: Pod "pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.745727ms
Apr 29 22:46:54.129: INFO: Pod "pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011613165s
Apr 29 22:46:56.140: INFO: Pod "pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022411315s
STEP: Saw pod success
Apr 29 22:46:56.140: INFO: Pod "pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18" satisfied condition "Succeeded or Failed"
Apr 29 22:46:56.143: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18 container secret-env-test: <nil>
STEP: delete the pod
Apr 29 22:46:56.183: INFO: Waiting for pod pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18 to disappear
Apr 29 22:46:56.190: INFO: Pod pod-secrets-d5464232-5398-48d6-a4bb-38f0e90f9e18 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:46:56.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5908" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2810,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:46:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 22:46:56.578: INFO: Waiting up to 5m0s for pod "pod-b320409d-5b49-4bd6-9d94-77a23e191864" in namespace "emptydir-1475" to be "Succeeded or Failed"
Apr 29 22:46:56.588: INFO: Pod "pod-b320409d-5b49-4bd6-9d94-77a23e191864": Phase="Pending", Reason="", readiness=false. Elapsed: 10.797105ms
Apr 29 22:46:58.598: INFO: Pod "pod-b320409d-5b49-4bd6-9d94-77a23e191864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020653737s
Apr 29 22:47:00.608: INFO: Pod "pod-b320409d-5b49-4bd6-9d94-77a23e191864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030722266s
STEP: Saw pod success
Apr 29 22:47:00.608: INFO: Pod "pod-b320409d-5b49-4bd6-9d94-77a23e191864" satisfied condition "Succeeded or Failed"
Apr 29 22:47:00.612: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-b320409d-5b49-4bd6-9d94-77a23e191864 container test-container: <nil>
STEP: delete the pod
Apr 29 22:47:00.642: INFO: Waiting for pod pod-b320409d-5b49-4bd6-9d94-77a23e191864 to disappear
Apr 29 22:47:00.645: INFO: Pod pod-b320409d-5b49-4bd6-9d94-77a23e191864 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:47:00.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1475" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":2814,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:47:00.657: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0429 22:47:11.008403      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 22:47:13.040: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 29 22:47:13.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-58z6f" in namespace "gc-480"
Apr 29 22:47:13.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vfzg" in namespace "gc-480"
Apr 29 22:47:13.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h8dk" in namespace "gc-480"
Apr 29 22:47:13.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-brd8j" in namespace "gc-480"
Apr 29 22:47:13.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhh8c" in namespace "gc-480"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:47:13.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-480" for this suite.

• [SLOW TEST:12.506 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":173,"skipped":2824,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:47:13.164: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2740
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2740
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2740
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2740
Apr 29 22:47:13.376: INFO: Found 0 stateful pods, waiting for 1
Apr 29 22:47:23.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 29 22:47:23.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:47:23.978: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:47:23.978: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:47:23.978: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:47:23.985: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 22:47:34.008: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:47:34.008: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:47:34.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
Apr 29 22:47:35.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995898052s
Apr 29 22:47:36.056: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989741586s
Apr 29 22:47:37.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973781927s
Apr 29 22:47:38.073: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964317433s
Apr 29 22:47:39.082: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956173752s
Apr 29 22:47:40.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947202665s
Apr 29 22:47:41.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.938486583s
Apr 29 22:47:42.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.929994504s
Apr 29 22:47:43.118: INFO: Verifying statefulset ss doesn't scale past 1 for another 921.988731ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2740
Apr 29 22:47:44.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:47:44.417: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:47:44.417: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:47:44.417: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:47:44.424: INFO: Found 1 stateful pods, waiting for 3
Apr 29 22:47:54.450: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:47:54.450: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 22:47:54.450: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 29 22:47:54.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:47:54.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:47:54.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:47:54.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:47:54.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:47:55.020: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:47:55.020: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:47:55.020: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:47:55.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 22:47:55.322: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 22:47:55.322: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 22:47:55.322: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 22:47:55.322: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:47:55.329: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 29 22:48:05.342: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:48:05.342: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:48:05.342: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 22:48:05.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Apr 29 22:48:06.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996299803s
Apr 29 22:48:07.374: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987067356s
Apr 29 22:48:08.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979402925s
Apr 29 22:48:09.390: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970964988s
Apr 29 22:48:10.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963197959s
Apr 29 22:48:11.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953073809s
Apr 29 22:48:12.416: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945236182s
Apr 29 22:48:13.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.937531758s
Apr 29 22:48:14.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.820826ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2740
Apr 29 22:48:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:48:15.762: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:48:15.762: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:48:15.762: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:48:15.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:48:16.082: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:48:16.082: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:48:16.082: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:48:16.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=statefulset-2740 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 22:48:16.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 22:48:16.386: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 22:48:16.386: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 22:48:16.386: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 22:48:46.417: INFO: Deleting all statefulset in ns statefulset-2740
Apr 29 22:48:46.420: INFO: Scaling statefulset ss to 0
Apr 29 22:48:46.432: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 22:48:46.434: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:48:46.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2740" for this suite.

• [SLOW TEST:93.293 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":174,"skipped":2833,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:48:46.457: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-e675a3cf-a667-480a-87c4-fbd3fd3a7d47 in namespace container-probe-3495
Apr 29 22:48:50.734: INFO: Started pod test-webserver-e675a3cf-a667-480a-87c4-fbd3fd3a7d47 in namespace container-probe-3495
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 22:48:50.737: INFO: Initial restart count of pod test-webserver-e675a3cf-a667-480a-87c4-fbd3fd3a7d47 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:52:52.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3495" for this suite.

• [SLOW TEST:245.665 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2847,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:52:52.123: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 22:52:52.349: INFO: Waiting up to 5m0s for pod "pod-6fb6d78c-2831-4697-9713-f7b9aecb216b" in namespace "emptydir-136" to be "Succeeded or Failed"
Apr 29 22:52:52.353: INFO: Pod "pod-6fb6d78c-2831-4697-9713-f7b9aecb216b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706135ms
Apr 29 22:52:54.363: INFO: Pod "pod-6fb6d78c-2831-4697-9713-f7b9aecb216b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013188093s
Apr 29 22:52:56.396: INFO: Pod "pod-6fb6d78c-2831-4697-9713-f7b9aecb216b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.046509068s
STEP: Saw pod success
Apr 29 22:52:56.396: INFO: Pod "pod-6fb6d78c-2831-4697-9713-f7b9aecb216b" satisfied condition "Succeeded or Failed"
Apr 29 22:52:56.400: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-6fb6d78c-2831-4697-9713-f7b9aecb216b container test-container: <nil>
STEP: delete the pod
Apr 29 22:52:56.469: INFO: Waiting for pod pod-6fb6d78c-2831-4697-9713-f7b9aecb216b to disappear
Apr 29 22:52:56.471: INFO: Pod pod-6fb6d78c-2831-4697-9713-f7b9aecb216b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:52:56.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-136" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":2848,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:52:56.484: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 29 22:53:01.249: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-506 pod-service-account-1a159522-62b9-485f-9aa9-ff762de1a37f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 29 22:53:01.517: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-506 pod-service-account-1a159522-62b9-485f-9aa9-ff762de1a37f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 29 22:53:01.796: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-506 pod-service-account-1a159522-62b9-485f-9aa9-ff762de1a37f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:53:02.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-506" for this suite.

• [SLOW TEST:5.612 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":177,"skipped":2854,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:53:02.096: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:53:02.876: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:53:04.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333582, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333582, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333583, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333582, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:53:07.928: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:53:08.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9995" for this suite.
STEP: Destroying namespace "webhook-9995-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.329 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":178,"skipped":2858,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:53:08.426: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 in namespace container-probe-6371
Apr 29 22:53:12.686: INFO: Started pod liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 in namespace container-probe-6371
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 22:53:12.688: INFO: Initial restart count of pod liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is 0
Apr 29 22:53:26.763: INFO: Restart count of pod container-probe-6371/liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is now 1 (14.074085877s elapsed)
Apr 29 22:53:46.871: INFO: Restart count of pod container-probe-6371/liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is now 2 (34.18226128s elapsed)
Apr 29 22:54:06.984: INFO: Restart count of pod container-probe-6371/liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is now 3 (54.295747825s elapsed)
Apr 29 22:54:27.098: INFO: Restart count of pod container-probe-6371/liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is now 4 (1m14.409096373s elapsed)
Apr 29 22:55:27.395: INFO: Restart count of pod container-probe-6371/liveness-19f35a01-1046-48b7-ae83-da3ad91f6666 is now 5 (2m14.706744771s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:55:27.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6371" for this suite.

• [SLOW TEST:139.002 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:55:27.431: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-6708
Apr 29 22:55:31.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Apr 29 22:55:31.948: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Apr 29 22:55:31.948: INFO: stdout: "iptables"
Apr 29 22:55:31.948: INFO: proxyMode: iptables
Apr 29 22:55:31.973: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Apr 29 22:55:31.978: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6708
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6708
I0429 22:55:32.019528      21 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6708, replica count: 3
I0429 22:55:35.069896      21 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 22:55:35.085: INFO: Creating new exec pod
Apr 29 22:55:40.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Apr 29 22:55:40.402: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Apr 29 22:55:40.402: INFO: stdout: ""
Apr 29 22:55:40.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c nc -zv -t -w 2 10.0.69.18 80'
Apr 29 22:55:40.661: INFO: stderr: "+ nc -zv -t -w 2 10.0.69.18 80\nConnection to 10.0.69.18 80 port [tcp/http] succeeded!\n"
Apr 29 22:55:40.661: INFO: stdout: ""
Apr 29 22:55:40.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.69.18:80/ ; done'
Apr 29 22:55:41.000: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n"
Apr 29 22:55:41.000: INFO: stdout: "\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw\naffinity-clusterip-timeout-g4stw"
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Received response from host: affinity-clusterip-timeout-g4stw
Apr 29 22:55:41.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.69.18:80/'
Apr 29 22:55:41.289: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n"
Apr 29 22:55:41.289: INFO: stdout: "affinity-clusterip-timeout-g4stw"
Apr 29 22:56:01.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.69.18:80/'
Apr 29 22:56:01.568: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n"
Apr 29 22:56:01.568: INFO: stdout: "affinity-clusterip-timeout-g4stw"
Apr 29 22:56:21.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-6708 exec execpod-affinitybkrjb -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.69.18:80/'
Apr 29 22:56:21.895: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.69.18:80/\n"
Apr 29 22:56:21.895: INFO: stdout: "affinity-clusterip-timeout-4gh87"
Apr 29 22:56:21.895: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6708, will wait for the garbage collector to delete the pods
Apr 29 22:56:21.977: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 10.440194ms
Apr 29 22:56:22.578: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.180757ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:56:31.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6708" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:64.529 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":180,"skipped":2895,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:56:31.966: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 22:56:32.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 22:56:34.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333792, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333792, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333792, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755333792, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 22:56:37.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:56:37.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8790" for this suite.
STEP: Destroying namespace "webhook-8790-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.831 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":181,"skipped":2910,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:56:37.799: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 22:56:38.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4843 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Apr 29 22:56:38.164: INFO: stderr: ""
Apr 29 22:56:38.164: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 29 22:56:43.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4843 get pod e2e-test-httpd-pod -o json'
Apr 29 22:56:43.320: INFO: stderr: ""
Apr 29 22:56:43.320: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-04-29T22:56:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-29T22:56:38Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.2.182\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-04-29T22:56:40Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4843\",\n        \"resourceVersion\": \"28994\",\n        \"uid\": \"edd7c9af-3413-4ad6-b239-04e8dc4afcac\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-42c2t\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-linuxpool-38811533-0\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-42c2t\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-42c2t\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-29T22:56:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-29T22:56:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-29T22:56:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-04-29T22:56:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2a6b362c636b2c8de11ead1f699054d953eae1e052d4469e20c01d4d8ce4dafb\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-04-29T22:56:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.182\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.2.182\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-04-29T22:56:38Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 29 22:56:43.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4843 replace -f -'
Apr 29 22:56:43.637: INFO: stderr: ""
Apr 29 22:56:43.637: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Apr 29 22:56:43.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4843 delete pods e2e-test-httpd-pod'
Apr 29 22:56:51.824: INFO: stderr: ""
Apr 29 22:56:51.825: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:56:51.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4843" for this suite.

• [SLOW TEST:14.048 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":182,"skipped":2912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:56:51.847: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Apr 29 22:56:52.067: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 22:57:52.134: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:57:52.137: INFO: Starting informer...
STEP: Starting pod...
Apr 29 22:57:52.358: INFO: Pod is running on k8s-linuxpool-38811533-0. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 29 22:57:52.384: INFO: Pod wasn't evicted. Proceeding
Apr 29 22:57:52.384: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 29 22:59:07.406: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7744" for this suite.

• [SLOW TEST:135.607 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":183,"skipped":2950,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:07.454: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:59:07.667: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:11.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4908" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":184,"skipped":2953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:11.799: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1677
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:59:16.076: INFO: Waiting up to 5m0s for pod "client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac" in namespace "pods-1677" to be "Succeeded or Failed"
Apr 29 22:59:16.084: INFO: Pod "client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac": Phase="Pending", Reason="", readiness=false. Elapsed: 7.067763ms
Apr 29 22:59:18.100: INFO: Pod "client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023536827s
Apr 29 22:59:20.114: INFO: Pod "client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037411167s
STEP: Saw pod success
Apr 29 22:59:20.114: INFO: Pod "client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac" satisfied condition "Succeeded or Failed"
Apr 29 22:59:20.122: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac container env3cont: <nil>
STEP: delete the pod
Apr 29 22:59:20.195: INFO: Waiting for pod client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac to disappear
Apr 29 22:59:20.199: INFO: Pod client-envvars-e798d95d-dc52-4554-bac8-5bc66fd100ac no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:20.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1677" for this suite.

• [SLOW TEST:8.415 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":185,"skipped":2981,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:20.214: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 22:59:25.021: INFO: Successfully updated pod "pod-update-e6eb9642-ffcb-49f7-89e0-ecaebf3ec563"
STEP: verifying the updated pod is in kubernetes
Apr 29 22:59:25.027: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:25.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6647" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":2983,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:25.039: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 22:59:25.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02" in namespace "downward-api-9671" to be "Succeeded or Failed"
Apr 29 22:59:25.250: INFO: Pod "downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.26263ms
Apr 29 22:59:27.263: INFO: Pod "downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016142754s
Apr 29 22:59:29.276: INFO: Pod "downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029141679s
STEP: Saw pod success
Apr 29 22:59:29.276: INFO: Pod "downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02" satisfied condition "Succeeded or Failed"
Apr 29 22:59:29.281: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02 container client-container: <nil>
STEP: delete the pod
Apr 29 22:59:29.342: INFO: Waiting for pod downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02 to disappear
Apr 29 22:59:29.349: INFO: Pod downwardapi-volume-d585dd63-2c3c-4843-8569-c380c2d1ab02 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:29.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9671" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":2986,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:29.369: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 22:59:37.676: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 22:59:37.681: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 22:59:39.681: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 22:59:39.696: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 22:59:41.681: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 22:59:41.693: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 22:59:43.681: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 22:59:43.690: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4514" for this suite.

• [SLOW TEST:14.339 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":188,"skipped":3086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:43.712: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-fbd82fcd-f6c6-45d0-b41f-3c011f6f0bf5
STEP: Creating a pod to test consume secrets
Apr 29 22:59:43.967: INFO: Waiting up to 5m0s for pod "pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c" in namespace "secrets-825" to be "Succeeded or Failed"
Apr 29 22:59:43.970: INFO: Pod "pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574632ms
Apr 29 22:59:45.987: INFO: Pod "pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020022375s
Apr 29 22:59:47.998: INFO: Pod "pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031301069s
STEP: Saw pod success
Apr 29 22:59:47.998: INFO: Pod "pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c" satisfied condition "Succeeded or Failed"
Apr 29 22:59:48.001: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 22:59:48.034: INFO: Waiting for pod pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c to disappear
Apr 29 22:59:48.037: INFO: Pod pod-secrets-2267f934-39f2-4e05-aed4-1607b957e64c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-825" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":189,"skipped":3198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:48.064: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-2923b182-1f4e-4865-8b58-7f09056bde2b
STEP: Creating a pod to test consume configMaps
Apr 29 22:59:48.266: INFO: Waiting up to 5m0s for pod "pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426" in namespace "configmap-6569" to be "Succeeded or Failed"
Apr 29 22:59:48.279: INFO: Pod "pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426": Phase="Pending", Reason="", readiness=false. Elapsed: 13.621822ms
Apr 29 22:59:50.290: INFO: Pod "pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024084108s
Apr 29 22:59:52.312: INFO: Pod "pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0466349s
STEP: Saw pod success
Apr 29 22:59:52.312: INFO: Pod "pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426" satisfied condition "Succeeded or Failed"
Apr 29 22:59:52.326: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 22:59:52.357: INFO: Waiting for pod pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426 to disappear
Apr 29 22:59:52.360: INFO: Pod pod-configmaps-659ca75a-02a2-451b-99d2-821c878af426 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 22:59:52.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6569" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3261,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 22:59:52.374: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2428
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 22:59:52.630: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 22:59:56.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 create -f -'
Apr 29 22:59:57.340: INFO: stderr: ""
Apr 29 22:59:57.341: INFO: stdout: "e2e-test-crd-publish-openapi-8559-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 22:59:57.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-8559-crds test-cr'
Apr 29 22:59:57.499: INFO: stderr: ""
Apr 29 22:59:57.499: INFO: stdout: "e2e-test-crd-publish-openapi-8559-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 29 22:59:57.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 apply -f -'
Apr 29 22:59:57.811: INFO: stderr: ""
Apr 29 22:59:57.811: INFO: stdout: "e2e-test-crd-publish-openapi-8559-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 22:59:57.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2428 --namespace=crd-publish-openapi-2428 delete e2e-test-crd-publish-openapi-8559-crds test-cr'
Apr 29 22:59:57.924: INFO: stderr: ""
Apr 29 22:59:57.924: INFO: stdout: "e2e-test-crd-publish-openapi-8559-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 22:59:57.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-2428 explain e2e-test-crd-publish-openapi-8559-crds'
Apr 29 22:59:58.170: INFO: stderr: ""
Apr 29 22:59:58.170: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8559-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:01.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2428" for this suite.

• [SLOW TEST:9.578 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":191,"skipped":3269,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:01.953: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:00:02.167: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 29 23:00:07.185: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 23:00:07.185: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 23:00:07.216: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8283  91ebbeaa-07a3-456e-8134-4124603aaa22 29859 1 2021-04-29 23:00:07 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-04-29 23:00:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0070f4748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 23:00:07.219: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:07.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8283" for this suite.

• [SLOW TEST:5.279 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":192,"skipped":3277,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:07.232: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Apr 29 23:00:07.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 create -f -'
Apr 29 23:00:07.817: INFO: stderr: ""
Apr 29 23:00:07.817: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 23:00:07.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 23:00:07.932: INFO: stderr: ""
Apr 29 23:00:07.932: INFO: stdout: "update-demo-nautilus-b4889 update-demo-nautilus-gbgx7 "
Apr 29 23:00:07.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods update-demo-nautilus-b4889 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 23:00:08.064: INFO: stderr: ""
Apr 29 23:00:08.064: INFO: stdout: ""
Apr 29 23:00:08.064: INFO: update-demo-nautilus-b4889 is created but not running
Apr 29 23:00:13.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 29 23:00:13.174: INFO: stderr: ""
Apr 29 23:00:13.174: INFO: stdout: "update-demo-nautilus-b4889 update-demo-nautilus-gbgx7 "
Apr 29 23:00:13.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods update-demo-nautilus-b4889 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 23:00:13.280: INFO: stderr: ""
Apr 29 23:00:13.280: INFO: stdout: "true"
Apr 29 23:00:13.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods update-demo-nautilus-b4889 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 23:00:13.383: INFO: stderr: ""
Apr 29 23:00:13.383: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:00:13.383: INFO: validating pod update-demo-nautilus-b4889
Apr 29 23:00:13.389: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:00:13.389: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:00:13.389: INFO: update-demo-nautilus-b4889 is verified up and running
Apr 29 23:00:13.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods update-demo-nautilus-gbgx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 29 23:00:13.498: INFO: stderr: ""
Apr 29 23:00:13.498: INFO: stdout: "true"
Apr 29 23:00:13.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods update-demo-nautilus-gbgx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 29 23:00:13.612: INFO: stderr: ""
Apr 29 23:00:13.612: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:00:13.612: INFO: validating pod update-demo-nautilus-gbgx7
Apr 29 23:00:13.617: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:00:13.617: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:00:13.617: INFO: update-demo-nautilus-gbgx7 is verified up and running
STEP: using delete to clean up resources
Apr 29 23:00:13.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 delete --grace-period=0 --force -f -'
Apr 29 23:00:13.735: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:13.735: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 23:00:13.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get rc,svc -l name=update-demo --no-headers'
Apr 29 23:00:13.841: INFO: stderr: "No resources found in kubectl-504 namespace.\n"
Apr 29 23:00:13.841: INFO: stdout: ""
Apr 29 23:00:13.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-504 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 23:00:13.947: INFO: stderr: ""
Apr 29 23:00:13.947: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:13.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-504" for this suite.

• [SLOW TEST:6.728 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":193,"skipped":3286,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:13.961: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:00:14.159: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e7e0f0cd-f0e6-480e-a079-3b5347ae6648" in namespace "security-context-test-3532" to be "Succeeded or Failed"
Apr 29 23:00:14.163: INFO: Pod "alpine-nnp-false-e7e0f0cd-f0e6-480e-a079-3b5347ae6648": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990136ms
Apr 29 23:00:16.175: INFO: Pod "alpine-nnp-false-e7e0f0cd-f0e6-480e-a079-3b5347ae6648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016052416s
Apr 29 23:00:18.186: INFO: Pod "alpine-nnp-false-e7e0f0cd-f0e6-480e-a079-3b5347ae6648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026976785s
Apr 29 23:00:18.186: INFO: Pod "alpine-nnp-false-e7e0f0cd-f0e6-480e-a079-3b5347ae6648" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:18.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3532" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:18.231: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:00:18.435: INFO: Creating deployment "test-recreate-deployment"
Apr 29 23:00:18.441: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 29 23:00:18.447: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Apr 29 23:00:20.468: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 29 23:00:20.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334018, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334018, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334018, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334018, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:00:22.480: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 29 23:00:22.492: INFO: Updating deployment test-recreate-deployment
Apr 29 23:00:22.492: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 23:00:22.637: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8472  46e70272-33a1-48f1-a303-edbcd346d248 30038 2 2021-04-29 23:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0074992e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-29 23:00:22 +0000 UTC,LastTransitionTime:2021-04-29 23:00:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-04-29 23:00:22 +0000 UTC,LastTransitionTime:2021-04-29 23:00:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 23:00:22.641: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-8472  6e074525-952f-4085-b173-02f4f0db3ea3 30035 1 2021-04-29 23:00:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 46e70272-33a1-48f1-a303-edbcd346d248 0xc007499830 0xc007499831}] []  [{kube-controller-manager Update apps/v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46e70272-33a1-48f1-a303-edbcd346d248\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0074998b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:00:22.641: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 29 23:00:22.642: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-8472  faba23fd-2362-4d05-b297-a8a29b8ec00a 30026 2 2021-04-29 23:00:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 46e70272-33a1-48f1-a303-edbcd346d248 0xc007499707 0xc007499708}] []  [{kube-controller-manager Update apps/v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46e70272-33a1-48f1-a303-edbcd346d248\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0074997b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:00:22.653: INFO: Pod "test-recreate-deployment-f79dd4667-tlg7h" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-tlg7h test-recreate-deployment-f79dd4667- deployment-8472  1396109c-1621-46c2-a4b2-c5e360e17d46 30037 0 2021-04-29 23:00:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 6e074525-952f-4085-b173-02f4f0db3ea3 0xc007499df0 0xc007499df1}] []  [{kube-controller-manager Update v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e074525-952f-4085-b173-02f4f0db3ea3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:00:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hkk5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hkk5k,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hkk5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:00:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:00:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:00:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:00:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:00:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:22.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8472" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":195,"skipped":3317,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:22.668: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Apr 29 23:00:22.856: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 29 23:00:22.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:23.228: INFO: stderr: ""
Apr 29 23:00:23.228: INFO: stdout: "service/agnhost-replica created\n"
Apr 29 23:00:23.228: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 29 23:00:23.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:23.661: INFO: stderr: ""
Apr 29 23:00:23.661: INFO: stdout: "service/agnhost-primary created\n"
Apr 29 23:00:23.662: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 29 23:00:23.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:23.903: INFO: stderr: ""
Apr 29 23:00:23.903: INFO: stdout: "service/frontend created\n"
Apr 29 23:00:23.904: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 29 23:00:23.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:24.228: INFO: stderr: ""
Apr 29 23:00:24.228: INFO: stdout: "deployment.apps/frontend created\n"
Apr 29 23:00:24.229: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 29 23:00:24.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:24.594: INFO: stderr: ""
Apr 29 23:00:24.594: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 29 23:00:24.594: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 29 23:00:24.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 create -f -'
Apr 29 23:00:24.874: INFO: stderr: ""
Apr 29 23:00:24.874: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Apr 29 23:00:24.874: INFO: Waiting for all frontend pods to be Running.
Apr 29 23:00:29.925: INFO: Waiting for frontend to serve content.
Apr 29 23:00:29.935: INFO: Trying to add a new entry to the guestbook.
Apr 29 23:00:29.948: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 29 23:00:29.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.158: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.158: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 23:00:30.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.301: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.301: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 23:00:30.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.421: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.421: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 23:00:30.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.543: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.543: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 23:00:30.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.685: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.685: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Apr 29 23:00:30.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-670 delete --grace-period=0 --force -f -'
Apr 29 23:00:30.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:00:30.827: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:30.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-670" for this suite.

• [SLOW TEST:8.200 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":196,"skipped":3328,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:30.869: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:42.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-646" for this suite.

• [SLOW TEST:11.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":197,"skipped":3346,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:42.170: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2933
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:47.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2933" for this suite.

• [SLOW TEST:5.264 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":198,"skipped":3359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:47.436: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 29 23:00:47.638: INFO: Waiting up to 5m0s for pod "pod-48bf3b15-1282-44bd-b21a-416362f54bd0" in namespace "emptydir-8178" to be "Succeeded or Failed"
Apr 29 23:00:47.641: INFO: Pod "pod-48bf3b15-1282-44bd-b21a-416362f54bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.853826ms
Apr 29 23:00:49.650: INFO: Pod "pod-48bf3b15-1282-44bd-b21a-416362f54bd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012210459s
Apr 29 23:00:51.659: INFO: Pod "pod-48bf3b15-1282-44bd-b21a-416362f54bd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021248088s
STEP: Saw pod success
Apr 29 23:00:51.659: INFO: Pod "pod-48bf3b15-1282-44bd-b21a-416362f54bd0" satisfied condition "Succeeded or Failed"
Apr 29 23:00:51.662: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-48bf3b15-1282-44bd-b21a-416362f54bd0 container test-container: <nil>
STEP: delete the pod
Apr 29 23:00:51.687: INFO: Waiting for pod pod-48bf3b15-1282-44bd-b21a-416362f54bd0 to disappear
Apr 29 23:00:51.689: INFO: Pod pod-48bf3b15-1282-44bd-b21a-416362f54bd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:00:51.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8178" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":199,"skipped":3394,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:00:51.707: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 29 23:00:56.453: INFO: Successfully updated pod "adopt-release-5xm7k"
STEP: Checking that the Job readopts the Pod
Apr 29 23:00:56.453: INFO: Waiting up to 15m0s for pod "adopt-release-5xm7k" in namespace "job-1688" to be "adopted"
Apr 29 23:00:56.456: INFO: Pod "adopt-release-5xm7k": Phase="Running", Reason="", readiness=true. Elapsed: 3.34743ms
Apr 29 23:00:58.470: INFO: Pod "adopt-release-5xm7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.016785594s
Apr 29 23:00:58.470: INFO: Pod "adopt-release-5xm7k" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 29 23:00:58.996: INFO: Successfully updated pod "adopt-release-5xm7k"
STEP: Checking that the Job releases the Pod
Apr 29 23:00:58.996: INFO: Waiting up to 15m0s for pod "adopt-release-5xm7k" in namespace "job-1688" to be "released"
Apr 29 23:00:59.001: INFO: Pod "adopt-release-5xm7k": Phase="Running", Reason="", readiness=true. Elapsed: 4.40014ms
Apr 29 23:01:01.020: INFO: Pod "adopt-release-5xm7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.023266951s
Apr 29 23:01:01.020: INFO: Pod "adopt-release-5xm7k" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:01:01.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1688" for this suite.

• [SLOW TEST:9.328 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":200,"skipped":3403,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:01:01.035: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 29 23:01:01.247: INFO: PodSpec: initContainers in spec.initContainers
Apr 29 23:01:41.203: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dd0a1c16-660b-41ca-8c9a-21951375706e", GenerateName:"", Namespace:"init-container-2528", SelfLink:"", UID:"57a2fdc1-0cd6-42d1-84c3-83d1f2ce4e93", ResourceVersion:"30645", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63755334061, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"247780197"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005d787a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005d787c0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc005d787e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005d78800)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ps9bx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007fe1500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ps9bx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ps9bx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ps9bx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007b262d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-38811533-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0030a6070), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007b26350)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007b26370)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007b26378), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007b2637c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00720dbd0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334061, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334061, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334061, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334061, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.5", PodIP:"10.244.2.203", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.2.203"}}, StartTime:(*v1.Time)(0xc005d78820), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030a6150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030a61c0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://633f88005172084950827a954aa76ce0ee76f218a179ec7587df8a695a9c3efc", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005d78860), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005d78840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc007b263ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:01:41.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2528" for this suite.

• [SLOW TEST:40.229 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":201,"skipped":3421,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:01:41.264: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a2c872ed-6e51-472c-9186-83d76bca00bb
STEP: Creating a pod to test consume configMaps
Apr 29 23:01:41.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879" in namespace "configmap-7507" to be "Succeeded or Failed"
Apr 29 23:01:41.511: INFO: Pod "pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879": Phase="Pending", Reason="", readiness=false. Elapsed: 5.415115ms
Apr 29 23:01:43.520: INFO: Pod "pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014618272s
Apr 29 23:01:45.530: INFO: Pod "pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025044976s
STEP: Saw pod success
Apr 29 23:01:45.531: INFO: Pod "pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879" satisfied condition "Succeeded or Failed"
Apr 29 23:01:45.534: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:01:45.560: INFO: Waiting for pod pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879 to disappear
Apr 29 23:01:45.564: INFO: Pod pod-configmaps-20f070f8-44a3-4986-b6cb-f6c86d71a879 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:01:45.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7507" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":202,"skipped":3421,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:01:45.576: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Apr 29 23:01:45.767: INFO: Waiting up to 5m0s for pod "var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b" in namespace "var-expansion-2527" to be "Succeeded or Failed"
Apr 29 23:01:45.773: INFO: Pod "var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.212817ms
Apr 29 23:01:47.785: INFO: Pod "var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01729207s
Apr 29 23:01:49.795: INFO: Pod "var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027674064s
STEP: Saw pod success
Apr 29 23:01:49.795: INFO: Pod "var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b" satisfied condition "Succeeded or Failed"
Apr 29 23:01:49.798: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:01:49.829: INFO: Waiting for pod var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b to disappear
Apr 29 23:01:49.832: INFO: Pod var-expansion-471a55cc-aa9d-491c-861d-2a4fd20e679b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:01:49.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2527" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3422,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:01:49.850: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-sdt9
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:01:50.057: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sdt9" in namespace "subpath-6142" to be "Succeeded or Failed"
Apr 29 23:01:50.071: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.003035ms
Apr 29 23:01:52.166: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 2.108443609s
Apr 29 23:01:54.181: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 4.123409108s
Apr 29 23:01:56.192: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 6.13462474s
Apr 29 23:01:58.215: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 8.157220746s
Apr 29 23:02:00.227: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 10.169674665s
Apr 29 23:02:02.257: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 12.199814076s
Apr 29 23:02:04.269: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 14.211642377s
Apr 29 23:02:06.285: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 16.227817232s
Apr 29 23:02:08.304: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 18.246326935s
Apr 29 23:02:10.314: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Running", Reason="", readiness=true. Elapsed: 20.256744656s
Apr 29 23:02:12.374: INFO: Pod "pod-subpath-test-configmap-sdt9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.316849663s
STEP: Saw pod success
Apr 29 23:02:12.374: INFO: Pod "pod-subpath-test-configmap-sdt9" satisfied condition "Succeeded or Failed"
Apr 29 23:02:12.378: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-subpath-test-configmap-sdt9 container test-container-subpath-configmap-sdt9: <nil>
STEP: delete the pod
Apr 29 23:02:12.403: INFO: Waiting for pod pod-subpath-test-configmap-sdt9 to disappear
Apr 29 23:02:12.406: INFO: Pod pod-subpath-test-configmap-sdt9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sdt9
Apr 29 23:02:12.406: INFO: Deleting pod "pod-subpath-test-configmap-sdt9" in namespace "subpath-6142"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:12.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6142" for this suite.

• [SLOW TEST:22.570 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":204,"skipped":3442,"failed":0}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:12.420: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-6063
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:12.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6063" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":205,"skipped":3443,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:12.711: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:02:12.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7" in namespace "projected-6975" to be "Succeeded or Failed"
Apr 29 23:02:12.897: INFO: Pod "downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005415ms
Apr 29 23:02:14.909: INFO: Pod "downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017587334s
Apr 29 23:02:16.932: INFO: Pod "downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040107724s
STEP: Saw pod success
Apr 29 23:02:16.932: INFO: Pod "downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7" satisfied condition "Succeeded or Failed"
Apr 29 23:02:16.938: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7 container client-container: <nil>
STEP: delete the pod
Apr 29 23:02:16.983: INFO: Waiting for pod downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7 to disappear
Apr 29 23:02:16.987: INFO: Pod downwardapi-volume-f693ee71-b6f3-426c-8232-a121f427c6f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:16.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6975" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:17.002: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-3434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:17.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3434" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":207,"skipped":3473,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:17.231: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 29 23:02:17.431: INFO: Waiting up to 5m0s for pod "downward-api-ccf85ccb-67cc-414c-923b-add40a67af69" in namespace "downward-api-6738" to be "Succeeded or Failed"
Apr 29 23:02:17.434: INFO: Pod "downward-api-ccf85ccb-67cc-414c-923b-add40a67af69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.29851ms
Apr 29 23:02:19.443: INFO: Pod "downward-api-ccf85ccb-67cc-414c-923b-add40a67af69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01195251s
Apr 29 23:02:21.455: INFO: Pod "downward-api-ccf85ccb-67cc-414c-923b-add40a67af69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024477061s
STEP: Saw pod success
Apr 29 23:02:21.455: INFO: Pod "downward-api-ccf85ccb-67cc-414c-923b-add40a67af69" satisfied condition "Succeeded or Failed"
Apr 29 23:02:21.458: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downward-api-ccf85ccb-67cc-414c-923b-add40a67af69 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:02:21.485: INFO: Waiting for pod downward-api-ccf85ccb-67cc-414c-923b-add40a67af69 to disappear
Apr 29 23:02:21.490: INFO: Pod downward-api-ccf85ccb-67cc-414c-923b-add40a67af69 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:21.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6738" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3493,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:21.522: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 29 23:02:29.785: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:29.789: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:31.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:31.800: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:33.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:33.800: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:35.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:35.803: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:37.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:37.803: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:39.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:39.800: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:41.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:41.817: INFO: Pod pod-with-prestop-http-hook still exists
Apr 29 23:02:43.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 29 23:02:43.800: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:43.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4577" for this suite.

• [SLOW TEST:22.305 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":209,"skipped":3504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Apr 29 23:02:44.028: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-6230 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:44.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6230" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":210,"skipped":3540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:44.154: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:02:44.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3875" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":211,"skipped":3604,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:02:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4315
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:03:44.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4315" for this suite.

• [SLOW TEST:60.205 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3612,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:03:44.606: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Apr 29 23:03:44.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 create -f -'
Apr 29 23:03:45.141: INFO: stderr: ""
Apr 29 23:03:45.141: INFO: stdout: "pod/pause created\n"
Apr 29 23:03:45.141: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 29 23:03:45.141: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9628" to be "running and ready"
Apr 29 23:03:45.145: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.161016ms
Apr 29 23:03:47.157: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015577936s
Apr 29 23:03:49.175: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.034260918s
Apr 29 23:03:49.175: INFO: Pod "pause" satisfied condition "running and ready"
Apr 29 23:03:49.175: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 29 23:03:49.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 label pods pause testing-label=testing-label-value'
Apr 29 23:03:49.318: INFO: stderr: ""
Apr 29 23:03:49.318: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 29 23:03:49.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 get pod pause -L testing-label'
Apr 29 23:03:49.418: INFO: stderr: ""
Apr 29 23:03:49.418: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 29 23:03:49.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 label pods pause testing-label-'
Apr 29 23:03:49.534: INFO: stderr: ""
Apr 29 23:03:49.534: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 29 23:03:49.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 get pod pause -L testing-label'
Apr 29 23:03:49.630: INFO: stderr: ""
Apr 29 23:03:49.630: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Apr 29 23:03:49.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 delete --grace-period=0 --force -f -'
Apr 29 23:03:49.769: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:03:49.769: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 29 23:03:49.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 get rc,svc -l name=pause --no-headers'
Apr 29 23:03:49.876: INFO: stderr: "No resources found in kubectl-9628 namespace.\n"
Apr 29 23:03:49.877: INFO: stdout: ""
Apr 29 23:03:49.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-9628 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 23:03:49.981: INFO: stderr: ""
Apr 29 23:03:49.981: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:03:49.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9628" for this suite.

• [SLOW TEST:5.390 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":213,"skipped":3613,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:03:49.998: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr 29 23:03:50.196: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7585  ea6afe52-16fe-43ab-937e-0cb0b9931eef 31248 0 2021-04-29 23:03:50 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-29 23:03:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d577t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d577t,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d577t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:03:50.200: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:03:52.223: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:03:54.209: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr 29 23:03:54.209: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7585 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:03:54.209: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Verifying customized DNS server is configured on pod...
Apr 29 23:03:54.395: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7585 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:03:54.395: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:03:54.552: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:03:54.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7585" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":214,"skipped":3633,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:03:54.590: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:01.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6156" for this suite.

• [SLOW TEST:7.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":215,"skipped":3650,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:01.827: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9184
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9184
STEP: Creating statefulset with conflicting port in namespace statefulset-9184
STEP: Waiting until pod test-pod will start running in namespace statefulset-9184
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9184
Apr 29 23:04:06.072: INFO: Observed stateful pod in namespace: statefulset-9184, name: ss-0, uid: 74431dac-525f-4a88-b5ef-732729b404ae, status phase: Failed. Waiting for statefulset controller to delete.
Apr 29 23:04:06.076: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9184
STEP: Removing pod with conflicting port in namespace statefulset-9184
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9184 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 23:04:10.135: INFO: Deleting all statefulset in ns statefulset-9184
Apr 29 23:04:10.138: INFO: Scaling statefulset ss to 0
Apr 29 23:04:20.179: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:04:20.182: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:20.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9184" for this suite.

• [SLOW TEST:18.425 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":216,"skipped":3686,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:20.255: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-30068df3-0b34-4d87-b47a-79941f60e84b
STEP: Creating a pod to test consume secrets
Apr 29 23:04:20.501: INFO: Waiting up to 5m0s for pod "pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147" in namespace "secrets-637" to be "Succeeded or Failed"
Apr 29 23:04:20.504: INFO: Pod "pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.582411ms
Apr 29 23:04:22.515: INFO: Pod "pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013810106s
Apr 29 23:04:24.523: INFO: Pod "pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021987419s
STEP: Saw pod success
Apr 29 23:04:24.523: INFO: Pod "pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147" satisfied condition "Succeeded or Failed"
Apr 29 23:04:24.526: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:04:24.585: INFO: Waiting for pod pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147 to disappear
Apr 29 23:04:24.588: INFO: Pod pod-secrets-613dcf2c-e96f-4a6a-91ad-864564a46147 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-637" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3705,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:24.604: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-736ad598-623f-4aac-9553-4ef3f65b12a7 in namespace container-probe-9101
Apr 29 23:04:26.825: INFO: Started pod liveness-736ad598-623f-4aac-9553-4ef3f65b12a7 in namespace container-probe-9101
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 23:04:26.829: INFO: Initial restart count of pod liveness-736ad598-623f-4aac-9553-4ef3f65b12a7 is 0
Apr 29 23:04:42.939: INFO: Restart count of pod container-probe-9101/liveness-736ad598-623f-4aac-9553-4ef3f65b12a7 is now 1 (16.1104413s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:42.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9101" for this suite.

• [SLOW TEST:18.367 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3724,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:42.970: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-b9b08d37-0556-407d-ba31-e0076e6799b1
STEP: Creating a pod to test consume configMaps
Apr 29 23:04:43.172: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512" in namespace "projected-628" to be "Succeeded or Failed"
Apr 29 23:04:43.178: INFO: Pod "pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512": Phase="Pending", Reason="", readiness=false. Elapsed: 5.666324ms
Apr 29 23:04:45.188: INFO: Pod "pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015792462s
Apr 29 23:04:47.200: INFO: Pod "pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027535438s
STEP: Saw pod success
Apr 29 23:04:47.200: INFO: Pod "pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512" satisfied condition "Succeeded or Failed"
Apr 29 23:04:47.203: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:04:47.231: INFO: Waiting for pod pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512 to disappear
Apr 29 23:04:47.236: INFO: Pod pod-projected-configmaps-73ebcb7f-b6fe-4506-bae8-3ee1d42a4512 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:47.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-628" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3743,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:47.246: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:04:47.452: INFO: Creating deployment "webserver-deployment"
Apr 29 23:04:47.457: INFO: Waiting for observed generation 1
Apr 29 23:04:49.468: INFO: Waiting for all required pods to come up
Apr 29 23:04:49.474: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 29 23:04:51.501: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 29 23:04:51.507: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 29 23:04:51.529: INFO: Updating deployment webserver-deployment
Apr 29 23:04:51.529: INFO: Waiting for observed generation 2
Apr 29 23:04:53.615: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 29 23:04:53.623: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 29 23:04:53.626: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:04:53.634: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 29 23:04:53.634: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 29 23:04:53.636: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:04:53.641: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 29 23:04:53.641: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 29 23:04:53.651: INFO: Updating deployment webserver-deployment
Apr 29 23:04:53.651: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:04:53.656: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 29 23:04:55.670: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Apr 29 23:04:55.676: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7521  9242d760-a03a-4e56-8560-bf9a2f22b4f2 32001 3 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f3fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-04-29 23:04:53 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-04-29 23:04:53 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 29 23:04:55.680: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7521  daedafab-d213-4536-8f06-f109b0f76ec9 31995 3 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9242d760-a03a-4e56-8560-bf9a2f22b4f2 0xc00491c187 0xc00491c188}] []  [{kube-controller-manager Update apps/v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9242d760-a03a-4e56-8560-bf9a2f22b4f2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00491c208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:04:55.680: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 29 23:04:55.680: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-7521  853cd5ec-12d5-4ca7-9e02-28cfcb762199 31996 3 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9242d760-a03a-4e56-8560-bf9a2f22b4f2 0xc00491c267 0xc00491c268}] []  [{kube-controller-manager Update apps/v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9242d760-a03a-4e56-8560-bf9a2f22b4f2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00491c2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:04:55.690: INFO: Pod "webserver-deployment-795d758f88-4jztm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4jztm webserver-deployment-795d758f88- deployment-7521  30e33961-6f1d-4143-90a0-22a24853e8a4 31997 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc0026c8667 0xc0026c8668}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.690: INFO: Pod "webserver-deployment-795d758f88-5hgnh" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5hgnh webserver-deployment-795d758f88- deployment-7521  232924b4-16ac-407c-a937-cba103492a57 32000 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc0026c8c70 0xc0026c8c71}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.691: INFO: Pod "webserver-deployment-795d758f88-662dp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-662dp webserver-deployment-795d758f88- deployment-7521  da8f04cd-7c2b-4398-8ba1-85eaeb25fffa 32020 0 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc0026c9220 0xc0026c9221}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.4.92,StartTime:2021-04-29 23:04:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.691: INFO: Pod "webserver-deployment-795d758f88-76z9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-76z9c webserver-deployment-795d758f88- deployment-7521  ce9ae58d-3320-4236-a1ae-35755757ce14 31850 0 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc0026c9880 0xc0026c9881}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.691: INFO: Pod "webserver-deployment-795d758f88-cd62c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cd62c webserver-deployment-795d758f88- deployment-7521  db39cc15-5b23-4750-85a0-718616f8a40c 31868 0 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc0026c9d20 0xc0026c9d21}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.691: INFO: Pod "webserver-deployment-795d758f88-dv568" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dv568 webserver-deployment-795d758f88- deployment-7521  cc715674-2904-4e7b-ac41-f5fed9c266ce 31981 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3c030 0xc003c3c031}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.692: INFO: Pod "webserver-deployment-795d758f88-fdjbw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fdjbw webserver-deployment-795d758f88- deployment-7521  aabfaa16-a071-444c-a444-f517f0af18d2 32030 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3c560 0xc003c3c561}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.692: INFO: Pod "webserver-deployment-795d758f88-kgbg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kgbg7 webserver-deployment-795d758f88- deployment-7521  b325de13-b980-4bba-90e1-b963f1cdb2e3 31993 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3c6f0 0xc003c3c6f1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.692: INFO: Pod "webserver-deployment-795d758f88-m2gk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m2gk2 webserver-deployment-795d758f88- deployment-7521  45ab62bf-6247-477f-a3ff-08cfbab1657a 32034 0 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3c880 0xc003c3c881}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.223,StartTime:2021-04-29 23:04:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.693: INFO: Pod "webserver-deployment-795d758f88-m5d6z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m5d6z webserver-deployment-795d758f88- deployment-7521  ec3f3328-2d6d-4385-9aa5-a0153087c148 32009 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3ca40 0xc003c3ca41}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.693: INFO: Pod "webserver-deployment-795d758f88-qjvtm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qjvtm webserver-deployment-795d758f88- deployment-7521  c1f8c859-38d8-4a0e-9fa6-14e65fdcad24 31963 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3cbd0 0xc003c3cbd1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.693: INFO: Pod "webserver-deployment-795d758f88-r6spr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-r6spr webserver-deployment-795d758f88- deployment-7521  bbdc8d84-ec0d-485b-86c7-2663ed2feb84 31878 0 2021-04-29 23:04:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3ce70 0xc003c3ce71}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.694: INFO: Pod "webserver-deployment-795d758f88-rlxqv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rlxqv webserver-deployment-795d758f88- deployment-7521  799070cb-463d-4b58-87f2-44cacf6dabe8 31970 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 daedafab-d213-4536-8f06-f109b0f76ec9 0xc003c3d0e0 0xc003c3d0e1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"daedafab-d213-4536-8f06-f109b0f76ec9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.694: INFO: Pod "webserver-deployment-dd94f59b7-2dhds" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2dhds webserver-deployment-dd94f59b7- deployment-7521  5caacd99-e8ec-4585-8508-14ea8c614489 31795 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc003c3d4f0 0xc003c3d4f1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.4.90,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://0bb3727029c801c07d9fca0a5de23135ef54e7c842f58c389348b805265f0bb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.694: INFO: Pod "webserver-deployment-dd94f59b7-4qc4s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4qc4s webserver-deployment-dd94f59b7- deployment-7521  4b9f164c-754f-4aab-a829-85762e225d2f 31988 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc003c3d8a0 0xc003c3d8a1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.694: INFO: Pod "webserver-deployment-dd94f59b7-4zl96" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4zl96 webserver-deployment-dd94f59b7- deployment-7521  c5402269-b6fe-400e-9632-a68af2c72301 31914 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc003c3dad0 0xc003c3dad1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.694: INFO: Pod "webserver-deployment-dd94f59b7-8444c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8444c webserver-deployment-dd94f59b7- deployment-7521  ae7e682f-d88d-4e7a-89dc-a60a98d37373 31992 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc003c3dd10 0xc003c3dd11}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.695: INFO: Pod "webserver-deployment-dd94f59b7-8f5gf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8f5gf webserver-deployment-dd94f59b7- deployment-7521  b5ad6635-b286-4d40-9924-25bd3b1cad5e 32011 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc003c3df40 0xc003c3df41}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.695: INFO: Pod "webserver-deployment-dd94f59b7-8gcgv" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8gcgv webserver-deployment-dd94f59b7- deployment-7521  e3fdb4ea-032d-4b69-99a1-c63bc4e5032d 31800 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067500c0 0xc0067500c1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.3.43,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5ab83ce3ab8d3c147aef11719fe12ced1f9dc52facdc410cf2cfdceb4f652186,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.695: INFO: Pod "webserver-deployment-dd94f59b7-d82zm" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d82zm webserver-deployment-dd94f59b7- deployment-7521  16b90104-579f-497b-9731-d89c1fa8123c 31921 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750250 0xc006750251}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.696: INFO: Pod "webserver-deployment-dd94f59b7-dbltg" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dbltg webserver-deployment-dd94f59b7- deployment-7521  4eb78a83-4750-4078-ad31-6a1673a81df7 31797 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067503c0 0xc0067503c1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.4.89,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e56c13e5236fe0fea489a44bf6f61bd27b866611080baf0c2ad60a8658370df9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.696: INFO: Pod "webserver-deployment-dd94f59b7-dx5rh" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dx5rh webserver-deployment-dd94f59b7- deployment-7521  9b55e7ad-a448-45b0-89d7-d98bbbbee44e 31931 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750550 0xc006750551}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.698: INFO: Pod "webserver-deployment-dd94f59b7-f5tgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-f5tgp webserver-deployment-dd94f59b7- deployment-7521  00127534-ecab-4799-9427-7ebbe48adb2b 31982 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067506c0 0xc0067506c1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.699: INFO: Pod "webserver-deployment-dd94f59b7-k8k6j" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-k8k6j webserver-deployment-dd94f59b7- deployment-7521  becef9d5-9feb-4fda-a15a-2acb6f335823 32007 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750830 0xc006750831}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.699: INFO: Pod "webserver-deployment-dd94f59b7-mwgzr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-mwgzr webserver-deployment-dd94f59b7- deployment-7521  6848641c-c747-40ea-aba9-5ed72d586ce8 31813 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067509b0 0xc0067509b1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.219,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6609a350d581279ac5b8adb4fbf5272378a6cc2a43f98f992e58beeecfc4c766,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.699: INFO: Pod "webserver-deployment-dd94f59b7-npfkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-npfkv webserver-deployment-dd94f59b7- deployment-7521  4f80732e-4f3b-4876-989b-3176e8f11da8 31933 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750b40 0xc006750b41}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.700: INFO: Pod "webserver-deployment-dd94f59b7-rm6m9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rm6m9 webserver-deployment-dd94f59b7- deployment-7521  3fce20ed-3efa-400e-a2ae-08c23db8c165 31811 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750cd0 0xc006750cd1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.221,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://20d3b3c749589dcd7cf0cd97003b6bbee7b01e221a30cd87d48d27ce379e2914,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.700: INFO: Pod "webserver-deployment-dd94f59b7-v2klr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-v2klr webserver-deployment-dd94f59b7- deployment-7521  56741eda-4171-4749-aef3-f161182772b3 31793 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750e60 0xc006750e61}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:10.244.4.91,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://428015fcba663e02a563036f933181e6ae440bb2bcec6f93422c18ce728150a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.700: INFO: Pod "webserver-deployment-dd94f59b7-vc8pg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vc8pg webserver-deployment-dd94f59b7- deployment-7521  a09ad446-3243-4b3b-a2c6-5797996645e8 32003 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006750ff0 0xc006750ff1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.701: INFO: Pod "webserver-deployment-dd94f59b7-vhqm7" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vhqm7 webserver-deployment-dd94f59b7- deployment-7521  d6be4ffc-7afb-4d1c-af49-4751acb73131 31944 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006751160 0xc006751161}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.6,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.701: INFO: Pod "webserver-deployment-dd94f59b7-vj8z5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vj8z5 webserver-deployment-dd94f59b7- deployment-7521  db1d2eae-dbfe-4d3b-8136-bb1486896602 31808 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067512d0 0xc0067512d1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.3.41,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://25e2b46e9b7b8d1e36d5d41a9100a84dc28ab8fbcee68f14697b86803d138ca8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.701: INFO: Pod "webserver-deployment-dd94f59b7-zdw2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zdw2m webserver-deployment-dd94f59b7- deployment-7521  79a1b16e-9359-4eee-955f-0c786d17c0d1 32018 0 2021-04-29 23:04:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc006751460 0xc006751461}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:,StartTime:2021-04-29 23:04:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:04:55.701: INFO: Pod "webserver-deployment-dd94f59b7-zhwjz" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zhwjz webserver-deployment-dd94f59b7- deployment-7521  70d6c242-3ec8-4155-aeb8-038e2498d947 31804 0 2021-04-29 23:04:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 853cd5ec-12d5-4ca7-9e02-28cfcb762199 0xc0067515d0 0xc0067515d1}] []  [{kube-controller-manager Update v1 2021-04-29 23:04:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"853cd5ec-12d5-4ca7-9e02-28cfcb762199\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:04:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g9rwg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g9rwg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g9rwg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:04:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.244.3.42,StartTime:2021-04-29 23:04:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:04:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7ed36616daa97caadc4d8f5c004c54cd1a274d06779c50fbfd2873c8577ec442,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.42,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:04:55.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7521" for this suite.

• [SLOW TEST:8.473 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":220,"skipped":3748,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:04:55.720: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Apr 29 23:04:55.968: INFO: Waiting up to 5m0s for pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c" in namespace "containers-5327" to be "Succeeded or Failed"
Apr 29 23:04:55.972: INFO: Pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052417ms
Apr 29 23:04:58.008: INFO: Pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039317455s
Apr 29 23:05:00.021: INFO: Pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052591028s
Apr 29 23:05:02.032: INFO: Pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063488019s
STEP: Saw pod success
Apr 29 23:05:02.032: INFO: Pod "client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c" satisfied condition "Succeeded or Failed"
Apr 29 23:05:02.038: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:05:02.078: INFO: Waiting for pod client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c to disappear
Apr 29 23:05:02.082: INFO: Pod client-containers-5761eeda-8a13-4e8e-bcfb-e58cf280c71c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:02.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5327" for this suite.

• [SLOW TEST:6.378 seconds]
[k8s.io] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3748,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:02.100: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Apr 29 23:05:02.343: INFO: Waiting up to 5m0s for pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899" in namespace "var-expansion-618" to be "Succeeded or Failed"
Apr 29 23:05:02.350: INFO: Pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899": Phase="Pending", Reason="", readiness=false. Elapsed: 6.665329ms
Apr 29 23:05:04.357: INFO: Pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013933838s
Apr 29 23:05:06.369: INFO: Pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025053393s
Apr 29 23:05:08.380: INFO: Pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03679878s
STEP: Saw pod success
Apr 29 23:05:08.380: INFO: Pod "var-expansion-29c89659-9eef-476b-8da5-ff150d10d899" satisfied condition "Succeeded or Failed"
Apr 29 23:05:08.383: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod var-expansion-29c89659-9eef-476b-8da5-ff150d10d899 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:05:08.412: INFO: Waiting for pod var-expansion-29c89659-9eef-476b-8da5-ff150d10d899 to disappear
Apr 29 23:05:08.414: INFO: Pod var-expansion-29c89659-9eef-476b-8da5-ff150d10d899 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:08.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-618" for this suite.

• [SLOW TEST:6.325 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3776,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:08.426: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 29 23:05:08.610: INFO: Waiting up to 5m0s for pod "downward-api-290aa0d1-472e-4135-accb-2016425b19f3" in namespace "downward-api-4808" to be "Succeeded or Failed"
Apr 29 23:05:08.613: INFO: Pod "downward-api-290aa0d1-472e-4135-accb-2016425b19f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.011213ms
Apr 29 23:05:10.625: INFO: Pod "downward-api-290aa0d1-472e-4135-accb-2016425b19f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015123834s
Apr 29 23:05:12.641: INFO: Pod "downward-api-290aa0d1-472e-4135-accb-2016425b19f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030575798s
STEP: Saw pod success
Apr 29 23:05:12.641: INFO: Pod "downward-api-290aa0d1-472e-4135-accb-2016425b19f3" satisfied condition "Succeeded or Failed"
Apr 29 23:05:12.644: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downward-api-290aa0d1-472e-4135-accb-2016425b19f3 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:05:12.672: INFO: Waiting for pod downward-api-290aa0d1-472e-4135-accb-2016425b19f3 to disappear
Apr 29 23:05:12.676: INFO: Pod downward-api-290aa0d1-472e-4135-accb-2016425b19f3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4808" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":223,"skipped":3779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:12.688: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:05:13.803: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 29 23:05:15.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334313, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334313, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334313, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334313, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:05:18.855: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:29.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2817" for this suite.
STEP: Destroying namespace "webhook-2817-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.462 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":224,"skipped":3832,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:29.151: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 29 23:05:33.954: INFO: Successfully updated pod "labelsupdate290216c6-6c0c-4ba4-9aed-4c2d0154185c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:36.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1013" for this suite.

• [SLOW TEST:6.903 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":225,"skipped":3840,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:36.057: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Apr 29 23:05:36.286: INFO: namespace kubectl-4674
Apr 29 23:05:36.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4674 create -f -'
Apr 29 23:05:36.605: INFO: stderr: ""
Apr 29 23:05:36.605: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 23:05:37.618: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:05:37.618: INFO: Found 0 / 1
Apr 29 23:05:38.612: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:05:38.612: INFO: Found 0 / 1
Apr 29 23:05:39.613: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:05:39.613: INFO: Found 1 / 1
Apr 29 23:05:39.613: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 23:05:39.616: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:05:39.616: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 23:05:39.616: INFO: wait on agnhost-primary startup in kubectl-4674 
Apr 29 23:05:39.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4674 logs agnhost-primary-g95tx agnhost-primary'
Apr 29 23:05:39.793: INFO: stderr: ""
Apr 29 23:05:39.793: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr 29 23:05:39.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4674 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 29 23:05:39.933: INFO: stderr: ""
Apr 29 23:05:39.933: INFO: stdout: "service/rm2 exposed\n"
Apr 29 23:05:39.938: INFO: Service rm2 in namespace kubectl-4674 found.
STEP: exposing service
Apr 29 23:05:41.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4674 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 29 23:05:42.081: INFO: stderr: ""
Apr 29 23:05:42.081: INFO: stdout: "service/rm3 exposed\n"
Apr 29 23:05:42.085: INFO: Service rm3 in namespace kubectl-4674 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:05:44.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4674" for this suite.

• [SLOW TEST:8.057 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":226,"skipped":3847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:05:44.115: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:06:12.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-728" for this suite.

• [SLOW TEST:28.310 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":227,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:06:12.425: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:06:12.977: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:06:14.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334373, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334373, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755334372, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:06:18.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:06:18.034: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7557-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:06:19.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1099" for this suite.
STEP: Destroying namespace "webhook-1099-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.045 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":228,"skipped":3888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:06:19.471: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Apr 29 23:08:20.235: INFO: Successfully updated pod "var-expansion-d5bd3353-e6bf-4f4a-8746-6eb4b8c5c540"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Apr 29 23:08:22.250: INFO: Deleting pod "var-expansion-d5bd3353-e6bf-4f4a-8746-6eb4b8c5c540" in namespace "var-expansion-2097"
Apr 29 23:08:22.263: INFO: Wait up to 5m0s for pod "var-expansion-d5bd3353-e6bf-4f4a-8746-6eb4b8c5c540" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:08:54.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2097" for this suite.

• [SLOW TEST:154.828 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":229,"skipped":3916,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:08:54.300: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:08:54.488: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb" in namespace "downward-api-5926" to be "Succeeded or Failed"
Apr 29 23:08:54.496: INFO: Pod "downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.382142ms
Apr 29 23:08:56.527: INFO: Pod "downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.038713188s
STEP: Saw pod success
Apr 29 23:08:56.527: INFO: Pod "downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb" satisfied condition "Succeeded or Failed"
Apr 29 23:08:56.536: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb container client-container: <nil>
STEP: delete the pod
Apr 29 23:08:56.657: INFO: Waiting for pod downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb to disappear
Apr 29 23:08:56.661: INFO: Pod downwardapi-volume-1a537f99-0873-42f8-bbd3-d9704553cefb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:08:56.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5926" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":230,"skipped":3936,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:08:56.673: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-0f80af87-fdcb-4690-ac37-79125ee94460
STEP: Creating a pod to test consume configMaps
Apr 29 23:08:56.930: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb" in namespace "projected-6602" to be "Succeeded or Failed"
Apr 29 23:08:56.935: INFO: Pod "pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.335931ms
Apr 29 23:08:58.945: INFO: Pod "pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015829079s
Apr 29 23:09:00.957: INFO: Pod "pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027684754s
STEP: Saw pod success
Apr 29 23:09:00.957: INFO: Pod "pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb" satisfied condition "Succeeded or Failed"
Apr 29 23:09:00.961: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:09:00.989: INFO: Waiting for pod pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb to disappear
Apr 29 23:09:00.992: INFO: Pod pod-projected-configmaps-2649ecb1-6f6e-4c01-bbef-8e64c1ad18fb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:09:00.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6602" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":231,"skipped":3937,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:09:01.007: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6452
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-6452
Apr 29 23:09:01.207: INFO: Found 0 stateful pods, waiting for 1
Apr 29 23:09:11.225: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 23:09:11.247: INFO: Deleting all statefulset in ns statefulset-6452
Apr 29 23:09:11.249: INFO: Scaling statefulset ss to 0
Apr 29 23:09:41.286: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:09:41.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:09:41.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6452" for this suite.

• [SLOW TEST:40.309 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":232,"skipped":3947,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:09:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-823.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.246_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-823.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-823.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-823.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-823.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.246_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 23:09:45.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.596: INFO: Unable to read wheezy_tcp@dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.600: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.606: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.633: INFO: Unable to read jessie_udp@dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.637: INFO: Unable to read jessie_tcp@dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.641: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.646: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local from pod dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f: the server could not find the requested resource (get pods dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f)
Apr 29 23:09:45.676: INFO: Lookups using dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f failed for: [wheezy_udp@dns-test-service.dns-823.svc.cluster.local wheezy_tcp@dns-test-service.dns-823.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local jessie_udp@dns-test-service.dns-823.svc.cluster.local jessie_tcp@dns-test-service.dns-823.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-823.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-823.svc.cluster.local]

Apr 29 23:09:50.757: INFO: DNS probes using dns-823/dns-test-ee00f7a1-1383-4102-8ba7-348ecb55168f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:09:50.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-823" for this suite.

• [SLOW TEST:9.604 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":233,"skipped":3948,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:09:50.921: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 29 23:09:55.139: INFO: &Pod{ObjectMeta:{send-events-8cf15772-0b0b-40f4-9c37-c86bbf29105b  events-5019  49e6e873-00bb-4533-b8a6-536f406e626c 33607 0 2021-04-29 23:09:51 +0000 UTC <nil> <nil> map[name:foo time:102017514] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-04-29 23:09:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-04-29 23:09:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xxkj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xxkj4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xxkj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-38811533-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:09:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:09:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:09:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-04-29 23:09:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.5,PodIP:10.244.2.247,StartTime:2021-04-29 23:09:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-04-29 23:09:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://a6ef87fd8e1febb7b388d409c2ad903782e76bd59b46aa59695eb61b4adfdeb1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 29 23:09:57.164: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 29 23:09:59.173: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:09:59.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5019" for this suite.

• [SLOW TEST:8.274 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":234,"skipped":3981,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:09:59.196: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:09:59.404: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 23:09:59.414: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:09:59.414: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:09:59.414: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:09:59.425: INFO: Number of nodes with available pods: 0
Apr 29 23:09:59.425: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:10:00.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:00.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:00.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:00.439: INFO: Number of nodes with available pods: 0
Apr 29 23:10:00.439: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:10:01.434: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:01.434: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:01.434: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:01.438: INFO: Number of nodes with available pods: 0
Apr 29 23:10:01.438: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:10:02.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:02.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:02.435: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:02.440: INFO: Number of nodes with available pods: 3
Apr 29 23:10:02.440: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 29 23:10:02.472: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:02.472: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:02.472: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:02.478: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:02.478: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:02.478: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:03.487: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:03.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:03.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:03.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:03.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:03.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:04.491: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:04.492: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:04.492: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:04.502: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:04.502: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:04.503: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:05.486: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:05.486: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:05.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:05.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:05.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:05.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:05.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:06.486: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:06.486: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:06.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:06.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:06.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:06.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:06.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:07.487: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:07.487: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:07.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:07.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:07.493: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:07.493: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:07.493: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:08.488: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:08.488: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:08.488: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:08.488: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:08.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:08.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:08.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:09.487: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:09.487: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:09.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:09.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:09.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:09.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:09.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:10.487: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:10.487: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:10.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:10.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:10.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:10.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:10.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:11.486: INFO: Wrong image for pod: daemon-set-pkpk4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:11.486: INFO: Pod daemon-set-pkpk4 is not available
Apr 29 23:10:11.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:11.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:11.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:11.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:11.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:12.487: INFO: Pod daemon-set-m44jh is not available
Apr 29 23:10:12.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:12.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:12.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:12.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:12.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:13.487: INFO: Pod daemon-set-m44jh is not available
Apr 29 23:10:13.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:13.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:13.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:13.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:13.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:14.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:14.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:14.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:14.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:14.492: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:15.489: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:15.489: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:15.489: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:15.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:15.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:15.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:16.484: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:16.484: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:16.484: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:16.488: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:16.488: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:16.489: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:17.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:17.486: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:17.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:17.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:17.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:17.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:18.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:18.487: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:18.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:18.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:18.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:18.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:19.486: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:19.486: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:19.486: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:19.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:19.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:19.490: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:20.487: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:20.487: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:20.487: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:20.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:20.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:20.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:21.490: INFO: Wrong image for pod: daemon-set-thccr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:21.490: INFO: Pod daemon-set-thccr is not available
Apr 29 23:10:21.490: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:21.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:21.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:21.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:22.488: INFO: Pod daemon-set-fcsr7 is not available
Apr 29 23:10:22.488: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:22.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:22.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:22.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:23.485: INFO: Pod daemon-set-fcsr7 is not available
Apr 29 23:10:23.485: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:23.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:23.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:23.491: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:24.489: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:24.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:24.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:24.494: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:25.490: INFO: Wrong image for pod: daemon-set-zr7tq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Apr 29 23:10:25.490: INFO: Pod daemon-set-zr7tq is not available
Apr 29 23:10:25.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:25.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:25.495: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.510: INFO: Pod daemon-set-phkq6 is not available
Apr 29 23:10:26.518: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.518: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.518: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 29 23:10:26.526: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.526: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.526: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:26.531: INFO: Number of nodes with available pods: 2
Apr 29 23:10:26.531: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 23:10:27.540: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:27.540: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:27.540: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:27.544: INFO: Number of nodes with available pods: 2
Apr 29 23:10:27.544: INFO: Node k8s-linuxpool-38811533-2 is running more than one daemon pod
Apr 29 23:10:28.541: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:28.541: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:28.541: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:10:28.546: INFO: Number of nodes with available pods: 3
Apr 29 23:10:28.546: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6857, will wait for the garbage collector to delete the pods
Apr 29 23:10:28.627: INFO: Deleting DaemonSet.extensions daemon-set took: 11.819872ms
Apr 29 23:10:29.227: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.179975ms
Apr 29 23:10:42.150: INFO: Number of nodes with available pods: 0
Apr 29 23:10:42.151: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:10:42.155: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33854"},"items":null}

Apr 29 23:10:42.159: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33854"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:10:42.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6857" for this suite.

• [SLOW TEST:42.995 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":235,"skipped":3982,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:10:42.192: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6134
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 23:10:42.406: INFO: Waiting up to 5m0s for pod "pod-423ce544-3f00-421b-bad7-96c50e319b58" in namespace "emptydir-6134" to be "Succeeded or Failed"
Apr 29 23:10:42.410: INFO: Pod "pod-423ce544-3f00-421b-bad7-96c50e319b58": Phase="Pending", Reason="", readiness=false. Elapsed: 3.745123ms
Apr 29 23:10:44.420: INFO: Pod "pod-423ce544-3f00-421b-bad7-96c50e319b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014371945s
Apr 29 23:10:46.436: INFO: Pod "pod-423ce544-3f00-421b-bad7-96c50e319b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029689911s
STEP: Saw pod success
Apr 29 23:10:46.436: INFO: Pod "pod-423ce544-3f00-421b-bad7-96c50e319b58" satisfied condition "Succeeded or Failed"
Apr 29 23:10:46.439: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-423ce544-3f00-421b-bad7-96c50e319b58 container test-container: <nil>
STEP: delete the pod
Apr 29 23:10:46.502: INFO: Waiting for pod pod-423ce544-3f00-421b-bad7-96c50e319b58 to disappear
Apr 29 23:10:46.507: INFO: Pod pod-423ce544-3f00-421b-bad7-96c50e319b58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:10:46.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6134" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":3992,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:10:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4840
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0429 23:10:56.845330      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 23:10:58.870: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:10:58.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4840" for this suite.

• [SLOW TEST:12.367 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":237,"skipped":4007,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:10:58.886: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:10.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-551" for this suite.

• [SLOW TEST:11.292 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":238,"skipped":4010,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:10.178: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Apr 29 23:11:10.383: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-5526 proxy --unix-socket=/tmp/kubectl-proxy-unix545924324/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5526" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":239,"skipped":4010,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:10.461: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1675
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:11:10.628: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:18.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1675" for this suite.

• [SLOW TEST:7.834 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":240,"skipped":4013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:18.298: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-939af209-c961-43b0-bf27-b4b0f4004060
STEP: Creating a pod to test consume secrets
Apr 29 23:11:18.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07" in namespace "projected-3519" to be "Succeeded or Failed"
Apr 29 23:11:18.547: INFO: Pod "pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.840431ms
Apr 29 23:11:20.558: INFO: Pod "pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015319012s
Apr 29 23:11:22.569: INFO: Pod "pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02619811s
STEP: Saw pod success
Apr 29 23:11:22.569: INFO: Pod "pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07" satisfied condition "Succeeded or Failed"
Apr 29 23:11:22.572: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:11:22.601: INFO: Waiting for pod pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07 to disappear
Apr 29 23:11:22.604: INFO: Pod pod-projected-secrets-66630ba6-8222-484b-8f22-f2a785165d07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:22.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3519" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4074,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:22.619: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:11:22.811: INFO: Creating ReplicaSet my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4
Apr 29 23:11:22.825: INFO: Pod name my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4: Found 0 pods out of 1
Apr 29 23:11:27.835: INFO: Pod name my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4: Found 1 pods out of 1
Apr 29 23:11:27.835: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4" is running
Apr 29 23:11:27.839: INFO: Pod "my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4-msrf2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 23:11:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 23:11:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 23:11:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-04-29 23:11:22 +0000 UTC Reason: Message:}])
Apr 29 23:11:27.839: INFO: Trying to dial the pod
Apr 29 23:11:32.868: INFO: Controller my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4: Got expected result from replica 1 [my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4-msrf2]: "my-hostname-basic-6209c7e5-5807-4316-9415-7ffaea4d01c4-msrf2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:32.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8196" for this suite.

• [SLOW TEST:10.269 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":242,"skipped":4098,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:32.888: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7619
STEP: Creating secret with name secret-test-cf129592-70a2-4798-be06-348e7ff758ae
STEP: Creating a pod to test consume secrets
Apr 29 23:11:33.285: INFO: Waiting up to 5m0s for pod "pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc" in namespace "secrets-7537" to be "Succeeded or Failed"
Apr 29 23:11:33.289: INFO: Pod "pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22432ms
Apr 29 23:11:35.300: INFO: Pod "pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014321806s
Apr 29 23:11:37.309: INFO: Pod "pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024073697s
STEP: Saw pod success
Apr 29 23:11:37.310: INFO: Pod "pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc" satisfied condition "Succeeded or Failed"
Apr 29 23:11:37.312: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:11:37.341: INFO: Waiting for pod pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc to disappear
Apr 29 23:11:37.432: INFO: Pod pod-secrets-dc605f14-e638-4138-996f-7239eb7c3edc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:11:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7537" for this suite.
STEP: Destroying namespace "secret-namespace-7619" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:11:37.451: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3831
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:11:37.628: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 23:11:37.692: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:11:39.702: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:11:41.702: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:11:43.703: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:11:45.703: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:11:47.698: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:11:49.704: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:11:51.703: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 23:11:51.708: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:11:53.718: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:11:55.725: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:11:57.725: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:11:59.718: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 29 23:11:59.724: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 29 23:12:03.754: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 29 23:12:03.754: INFO: Breadth first check of 10.244.2.2 on host 10.240.0.5...
Apr 29 23:12:03.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.3:9080/dial?request=hostname&protocol=udp&host=10.244.2.2&port=8081&tries=1'] Namespace:pod-network-test-3831 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:12:03.758: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:12:03.942: INFO: Waiting for responses: map[]
Apr 29 23:12:03.942: INFO: reached 10.244.2.2 after 0/1 tries
Apr 29 23:12:03.942: INFO: Breadth first check of 10.244.3.53 on host 10.240.0.4...
Apr 29 23:12:03.947: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.3:9080/dial?request=hostname&protocol=udp&host=10.244.3.53&port=8081&tries=1'] Namespace:pod-network-test-3831 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:12:03.947: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:12:04.105: INFO: Waiting for responses: map[]
Apr 29 23:12:04.105: INFO: reached 10.244.3.53 after 0/1 tries
Apr 29 23:12:04.105: INFO: Breadth first check of 10.244.4.104 on host 10.240.0.6...
Apr 29 23:12:04.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.3:9080/dial?request=hostname&protocol=udp&host=10.244.4.104&port=8081&tries=1'] Namespace:pod-network-test-3831 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:12:04.110: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:12:04.253: INFO: Waiting for responses: map[]
Apr 29 23:12:04.253: INFO: reached 10.244.4.104 after 0/1 tries
Apr 29 23:12:04.253: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:04.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3831" for this suite.

• [SLOW TEST:26.828 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4123,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:04.279: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-a937241f-81dc-4aca-b3b2-6d485ef2c5d4
STEP: Creating a pod to test consume configMaps
Apr 29 23:12:04.515: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374" in namespace "projected-4095" to be "Succeeded or Failed"
Apr 29 23:12:04.523: INFO: Pod "pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374": Phase="Pending", Reason="", readiness=false. Elapsed: 8.388654ms
Apr 29 23:12:06.535: INFO: Pod "pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019923447s
Apr 29 23:12:08.543: INFO: Pod "pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027997231s
STEP: Saw pod success
Apr 29 23:12:08.543: INFO: Pod "pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374" satisfied condition "Succeeded or Failed"
Apr 29 23:12:08.546: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:12:08.569: INFO: Waiting for pod pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374 to disappear
Apr 29 23:12:08.573: INFO: Pod pod-projected-configmaps-a7204924-7289-42cc-89f4-8454a6250374 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:08.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4095" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":245,"skipped":4127,"failed":0}
SSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:08.584: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-8673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Apr 29 23:12:09.834: INFO: starting watch
STEP: patching
STEP: updating
Apr 29 23:12:09.847: INFO: waiting for watch events with expected annotations
Apr 29 23:12:09.847: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:09.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8673" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":246,"skipped":4130,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:09.932: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3099
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:12:10.133: INFO: Waiting up to 5m0s for pod "busybox-user-65534-de181500-3ef2-4ee6-a275-95e7e324d5b1" in namespace "security-context-test-3099" to be "Succeeded or Failed"
Apr 29 23:12:10.136: INFO: Pod "busybox-user-65534-de181500-3ef2-4ee6-a275-95e7e324d5b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.561117ms
Apr 29 23:12:12.150: INFO: Pod "busybox-user-65534-de181500-3ef2-4ee6-a275-95e7e324d5b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016512561s
Apr 29 23:12:12.150: INFO: Pod "busybox-user-65534-de181500-3ef2-4ee6-a275-95e7e324d5b1" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:12.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3099" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4140,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:12.169: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-732116d2-7dc7-494f-ab14-d056f71f09be
STEP: Creating a pod to test consume secrets
Apr 29 23:12:12.477: INFO: Waiting up to 5m0s for pod "pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138" in namespace "secrets-7576" to be "Succeeded or Failed"
Apr 29 23:12:12.481: INFO: Pod "pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991019ms
Apr 29 23:12:14.492: INFO: Pod "pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014289861s
Apr 29 23:12:16.502: INFO: Pod "pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02464861s
STEP: Saw pod success
Apr 29 23:12:16.502: INFO: Pod "pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138" satisfied condition "Succeeded or Failed"
Apr 29 23:12:16.505: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:12:16.545: INFO: Waiting for pod pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138 to disappear
Apr 29 23:12:16.548: INFO: Pod pod-secrets-9f92907b-d649-4888-b45a-d90a22cc6138 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:16.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7576" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:16.561: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-706c17e0-8720-4532-a436-25fbc99cbf1f
STEP: Creating a pod to test consume secrets
Apr 29 23:12:16.761: INFO: Waiting up to 5m0s for pod "pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c" in namespace "secrets-6105" to be "Succeeded or Failed"
Apr 29 23:12:16.769: INFO: Pod "pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369754ms
Apr 29 23:12:18.778: INFO: Pod "pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016696104s
Apr 29 23:12:20.789: INFO: Pod "pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027729683s
STEP: Saw pod success
Apr 29 23:12:20.789: INFO: Pod "pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c" satisfied condition "Succeeded or Failed"
Apr 29 23:12:20.792: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:12:20.823: INFO: Waiting for pod pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c to disappear
Apr 29 23:12:20.828: INFO: Pod pod-secrets-bafe2a4a-e6a6-428c-972b-c092ca65b90c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:20.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6105" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4175,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:20.911: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:12:21.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":250,"skipped":4182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:12:21.121: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8589
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Apr 29 23:12:21.338: INFO: Found 0 stateful pods, waiting for 3
Apr 29 23:12:31.352: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:12:31.352: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:12:31.352: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 29 23:12:31.390: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 29 23:12:41.438: INFO: Updating stateful set ss2
Apr 29 23:12:41.444: INFO: Waiting for Pod statefulset-8589/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 23:12:51.464: INFO: Waiting for Pod statefulset-8589/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 29 23:13:01.503: INFO: Found 1 stateful pods, waiting for 3
Apr 29 23:13:11.517: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:11.517: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:11.517: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 29 23:13:11.551: INFO: Updating stateful set ss2
Apr 29 23:13:11.557: INFO: Waiting for Pod statefulset-8589/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 23:13:21.597: INFO: Updating stateful set ss2
Apr 29 23:13:21.604: INFO: Waiting for StatefulSet statefulset-8589/ss2 to complete update
Apr 29 23:13:21.604: INFO: Waiting for Pod statefulset-8589/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 23:13:31.621: INFO: Waiting for StatefulSet statefulset-8589/ss2 to complete update
Apr 29 23:13:31.621: INFO: Waiting for Pod statefulset-8589/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Apr 29 23:13:41.628: INFO: Deleting all statefulset in ns statefulset-8589
Apr 29 23:13:41.632: INFO: Scaling statefulset ss2 to 0
Apr 29 23:14:01.679: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:14:01.682: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:01.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8589" for this suite.

• [SLOW TEST:100.588 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":251,"skipped":4229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:01.710: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5064
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:14:01.930: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 23:14:05.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-5064 --namespace=crd-publish-openapi-5064 create -f -'
Apr 29 23:14:06.365: INFO: stderr: ""
Apr 29 23:14:06.365: INFO: stdout: "e2e-test-crd-publish-openapi-602-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 29 23:14:06.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-5064 --namespace=crd-publish-openapi-5064 delete e2e-test-crd-publish-openapi-602-crds test-cr'
Apr 29 23:14:06.501: INFO: stderr: ""
Apr 29 23:14:06.501: INFO: stdout: "e2e-test-crd-publish-openapi-602-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 29 23:14:06.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-5064 --namespace=crd-publish-openapi-5064 apply -f -'
Apr 29 23:14:06.889: INFO: stderr: ""
Apr 29 23:14:06.889: INFO: stdout: "e2e-test-crd-publish-openapi-602-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 29 23:14:06.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-5064 --namespace=crd-publish-openapi-5064 delete e2e-test-crd-publish-openapi-602-crds test-cr'
Apr 29 23:14:06.995: INFO: stderr: ""
Apr 29 23:14:06.995: INFO: stdout: "e2e-test-crd-publish-openapi-602-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 29 23:14:06.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=crd-publish-openapi-5064 explain e2e-test-crd-publish-openapi-602-crds'
Apr 29 23:14:07.308: INFO: stderr: ""
Apr 29 23:14:07.308: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-602-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5064" for this suite.

• [SLOW TEST:9.394 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":252,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:11.104: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:14:11.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468" in namespace "projected-4936" to be "Succeeded or Failed"
Apr 29 23:14:11.296: INFO: Pod "downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03052ms
Apr 29 23:14:13.306: INFO: Pod "downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012979416s
Apr 29 23:14:15.325: INFO: Pod "downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031141277s
STEP: Saw pod success
Apr 29 23:14:15.325: INFO: Pod "downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468" satisfied condition "Succeeded or Failed"
Apr 29 23:14:15.330: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468 container client-container: <nil>
STEP: delete the pod
Apr 29 23:14:15.382: INFO: Waiting for pod downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468 to disappear
Apr 29 23:14:15.385: INFO: Pod downwardapi-volume-5b35d6bb-edc0-49c4-a55c-c4d32560e468 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:15.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4936" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4347,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:15.397: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-a38af098-024f-4f2b-91ce-b8bc1fd2f1eb
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:15.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4777" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":254,"skipped":4351,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:15.600: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4110
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-4110/configmap-test-db59c73f-b251-440f-ac49-40abc2d0da21
STEP: Creating a pod to test consume configMaps
Apr 29 23:14:15.806: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee" in namespace "configmap-4110" to be "Succeeded or Failed"
Apr 29 23:14:15.809: INFO: Pod "pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.123821ms
Apr 29 23:14:17.825: INFO: Pod "pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018616077s
STEP: Saw pod success
Apr 29 23:14:17.825: INFO: Pod "pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee" satisfied condition "Succeeded or Failed"
Apr 29 23:14:17.829: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee container env-test: <nil>
STEP: delete the pod
Apr 29 23:14:17.852: INFO: Waiting for pod pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee to disappear
Apr 29 23:14:17.854: INFO: Pod pod-configmaps-c4828d74-0bce-4c6d-a95d-c24f3ed915ee no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:17.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4110" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4357,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:17.865: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 29 23:14:18.068: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35281 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:14:18.068: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35282 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:14:18.068: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35283 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 29 23:14:28.145: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35345 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:14:28.145: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35346 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:14:28.146: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6956  3b8711dd-651d-427c-b92d-57ab8bf8e01b 35347 0 2021-04-29 23:14:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-04-29 23:14:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:28.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6956" for this suite.

• [SLOW TEST:10.292 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":256,"skipped":4367,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:28.157: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Apr 29 23:14:28.344: INFO: Waiting up to 5m0s for pod "client-containers-560a4780-b844-4679-8a21-f46209e6f9f3" in namespace "containers-1271" to be "Succeeded or Failed"
Apr 29 23:14:28.347: INFO: Pod "client-containers-560a4780-b844-4679-8a21-f46209e6f9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842919ms
Apr 29 23:14:30.355: INFO: Pod "client-containers-560a4780-b844-4679-8a21-f46209e6f9f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011339187s
Apr 29 23:14:32.366: INFO: Pod "client-containers-560a4780-b844-4679-8a21-f46209e6f9f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022273482s
STEP: Saw pod success
Apr 29 23:14:32.366: INFO: Pod "client-containers-560a4780-b844-4679-8a21-f46209e6f9f3" satisfied condition "Succeeded or Failed"
Apr 29 23:14:32.369: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod client-containers-560a4780-b844-4679-8a21-f46209e6f9f3 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:14:32.394: INFO: Waiting for pod client-containers-560a4780-b844-4679-8a21-f46209e6f9f3 to disappear
Apr 29 23:14:32.397: INFO: Pod client-containers-560a4780-b844-4679-8a21-f46209e6f9f3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:14:32.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1271" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4372,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:14:32.408: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-4120
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:14:32.589: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Creating first CR 
Apr 29 23:14:33.201: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:14:33Z]] name:name1 resourceVersion:35396 uid:e2e9f628-729a-4f76-a6bb-64457b6e4e45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 29 23:14:43.222: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:14:43Z]] name:name2 resourceVersion:35438 uid:aa555818-4a5e-4980-aced-62f0cddf08e3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 29 23:14:53.238: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:14:53Z]] name:name1 resourceVersion:35460 uid:e2e9f628-729a-4f76-a6bb-64457b6e4e45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 29 23:15:03.254: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:15:03Z]] name:name2 resourceVersion:35480 uid:aa555818-4a5e-4980-aced-62f0cddf08e3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 29 23:15:13.275: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:14:53Z]] name:name1 resourceVersion:35500 uid:e2e9f628-729a-4f76-a6bb-64457b6e4e45] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 29 23:15:23.294: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-04-29T23:14:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-04-29T23:15:03Z]] name:name2 resourceVersion:35520 uid:aa555818-4a5e-4980-aced-62f0cddf08e3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:15:33.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4120" for this suite.

• [SLOW TEST:61.425 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":258,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:15:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-9cd09556-7c08-45fb-9caa-bd70e08ca485
STEP: Creating a pod to test consume secrets
Apr 29 23:15:34.062: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72" in namespace "projected-4030" to be "Succeeded or Failed"
Apr 29 23:15:34.072: INFO: Pod "pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02397ms
Apr 29 23:15:36.084: INFO: Pod "pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021814756s
Apr 29 23:15:38.096: INFO: Pod "pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03337795s
STEP: Saw pod success
Apr 29 23:15:38.096: INFO: Pod "pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72" satisfied condition "Succeeded or Failed"
Apr 29 23:15:38.099: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72 container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:15:38.134: INFO: Waiting for pod pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72 to disappear
Apr 29 23:15:38.137: INFO: Pod pod-projected-secrets-1201b96f-73d7-461f-91ac-b03284728c72 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:15:38.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4030" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:15:38.150: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-9946
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Apr 29 23:15:38.342: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Apr 29 23:15:38.374: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:15:38.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9946" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":260,"skipped":4448,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:15:38.416: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1199
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:15:38.593: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 29 23:15:38.638: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:15:40.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:42.650: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:44.650: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:46.652: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:48.650: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:50.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr 29 23:15:52.660: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr 29 23:15:52.667: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:15:54.678: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr 29 23:15:56.692: INFO: The status of Pod netserver-1 is Running (Ready = true)
Apr 29 23:15:56.699: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Apr 29 23:16:00.775: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Apr 29 23:16:00.776: INFO: Going to poll 10.244.2.15 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 29 23:16:00.779: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.15 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:16:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:16:01.959: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 29 23:16:01.959: INFO: Going to poll 10.244.3.57 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 29 23:16:01.967: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:16:01.967: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:16:03.109: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 29 23:16:03.109: INFO: Going to poll 10.244.4.107 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Apr 29 23:16:03.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.107 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1199 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 29 23:16:03.118: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:16:04.270: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:16:04.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1199" for this suite.

• [SLOW TEST:25.882 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:16:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 29 23:16:04.538: INFO: Waiting up to 5m0s for pod "downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11" in namespace "downward-api-5909" to be "Succeeded or Failed"
Apr 29 23:16:04.542: INFO: Pod "downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061222ms
Apr 29 23:16:06.552: INFO: Pod "downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013673325s
Apr 29 23:16:08.562: INFO: Pod "downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023854133s
STEP: Saw pod success
Apr 29 23:16:08.562: INFO: Pod "downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11" satisfied condition "Succeeded or Failed"
Apr 29 23:16:08.565: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:16:08.603: INFO: Waiting for pod downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11 to disappear
Apr 29 23:16:08.607: INFO: Pod downward-api-7b4f3574-7160-480b-ae06-80b0f821ba11 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:16:08.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5909" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4488,"failed":0}
SS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:16:08.624: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-3973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Apr 29 23:16:08.810: INFO: Major version: 1
STEP: Confirm minor version
Apr 29 23:16:08.810: INFO: cleanMinorVersion: 20
Apr 29 23:16:08.810: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:16:08.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3973" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":263,"skipped":4490,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:16:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 23:16:09.030: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:09.030: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:09.030: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:09.035: INFO: Number of nodes with available pods: 0
Apr 29 23:16:09.035: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:16:10.043: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:10.043: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:10.043: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:10.047: INFO: Number of nodes with available pods: 0
Apr 29 23:16:10.047: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:16:11.055: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:11.056: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:11.056: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:11.061: INFO: Number of nodes with available pods: 0
Apr 29 23:16:11.061: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:16:12.045: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.045: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.045: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.048: INFO: Number of nodes with available pods: 3
Apr 29 23:16:12.048: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 29 23:16:12.077: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.077: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.077: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:16:12.082: INFO: Number of nodes with available pods: 3
Apr 29 23:16:12.082: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7477, will wait for the garbage collector to delete the pods
Apr 29 23:16:13.171: INFO: Deleting DaemonSet.extensions daemon-set took: 11.068179ms
Apr 29 23:16:13.771: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.211849ms
Apr 29 23:17:22.182: INFO: Number of nodes with available pods: 0
Apr 29 23:17:22.182: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:17:22.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36018"},"items":null}

Apr 29 23:17:22.188: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36018"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:17:22.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7477" for this suite.

• [SLOW TEST:73.392 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":264,"skipped":4498,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:17:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:17:22.438: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515" in namespace "projected-7806" to be "Succeeded or Failed"
Apr 29 23:17:22.442: INFO: Pod "downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515": Phase="Pending", Reason="", readiness=false. Elapsed: 3.274824ms
Apr 29 23:17:24.453: INFO: Pod "downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014294316s
Apr 29 23:17:26.464: INFO: Pod "downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025643118s
STEP: Saw pod success
Apr 29 23:17:26.464: INFO: Pod "downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515" satisfied condition "Succeeded or Failed"
Apr 29 23:17:26.467: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515 container client-container: <nil>
STEP: delete the pod
Apr 29 23:17:26.494: INFO: Waiting for pod downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515 to disappear
Apr 29 23:17:26.497: INFO: Pod downwardapi-volume-6bccde70-ffa3-4c77-b1e5-372358483515 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:17:26.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7806" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4500,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:17:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:17:26.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3" in namespace "downward-api-1115" to be "Succeeded or Failed"
Apr 29 23:17:26.709: INFO: Pod "downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.211023ms
Apr 29 23:17:28.719: INFO: Pod "downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013557825s
Apr 29 23:17:30.727: INFO: Pod "downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021116514s
STEP: Saw pod success
Apr 29 23:17:30.727: INFO: Pod "downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3" satisfied condition "Succeeded or Failed"
Apr 29 23:17:30.731: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3 container client-container: <nil>
STEP: delete the pod
Apr 29 23:17:30.756: INFO: Waiting for pod downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3 to disappear
Apr 29 23:17:30.759: INFO: Pod downwardapi-volume-3219e318-873f-470c-8b9f-d3eb6bfc17e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:17:30.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1115" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":266,"skipped":4517,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:17:30.776: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3779
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-3e48e68f-098b-4b69-978b-00ac7a5afd1f
STEP: Creating configMap with name cm-test-opt-upd-f1c2157e-b5c5-4152-aaa0-9b6d60747c94
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3e48e68f-098b-4b69-978b-00ac7a5afd1f
STEP: Updating configmap cm-test-opt-upd-f1c2157e-b5c5-4152-aaa0-9b6d60747c94
STEP: Creating configMap with name cm-test-opt-create-ad20ebf9-fb11-4e31-9c42-22f5836bb86b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:39.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3779" for this suite.

• [SLOW TEST:68.701 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:39.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3862" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":268,"skipped":4551,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:39.663: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Apr 29 23:18:39.823: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:44.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9835" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":269,"skipped":4561,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:44.077: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 23:18:44.314: INFO: Waiting up to 5m0s for pod "pod-d63adcde-e99a-4c42-8a9b-776e376428d7" in namespace "emptydir-2659" to be "Succeeded or Failed"
Apr 29 23:18:44.324: INFO: Pod "pod-d63adcde-e99a-4c42-8a9b-776e376428d7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.177075ms
Apr 29 23:18:46.336: INFO: Pod "pod-d63adcde-e99a-4c42-8a9b-776e376428d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022284433s
STEP: Saw pod success
Apr 29 23:18:46.337: INFO: Pod "pod-d63adcde-e99a-4c42-8a9b-776e376428d7" satisfied condition "Succeeded or Failed"
Apr 29 23:18:46.341: INFO: Trying to get logs from node k8s-linuxpool-38811533-1 pod pod-d63adcde-e99a-4c42-8a9b-776e376428d7 container test-container: <nil>
STEP: delete the pod
Apr 29 23:18:46.393: INFO: Waiting for pod pod-d63adcde-e99a-4c42-8a9b-776e376428d7 to disappear
Apr 29 23:18:46.396: INFO: Pod pod-d63adcde-e99a-4c42-8a9b-776e376428d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:46.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2659" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:46.412: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 29 23:18:49.628: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:49.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1484" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4653,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:49.665: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 23:18:49.850: INFO: Waiting up to 5m0s for pod "pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c" in namespace "emptydir-3118" to be "Succeeded or Failed"
Apr 29 23:18:49.855: INFO: Pod "pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.791435ms
Apr 29 23:18:51.866: INFO: Pod "pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016080004s
Apr 29 23:18:53.876: INFO: Pod "pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025844067s
STEP: Saw pod success
Apr 29 23:18:53.876: INFO: Pod "pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c" satisfied condition "Succeeded or Failed"
Apr 29 23:18:53.879: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c container test-container: <nil>
STEP: delete the pod
Apr 29 23:18:53.905: INFO: Waiting for pod pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c to disappear
Apr 29 23:18:53.907: INFO: Pod pod-dc06175b-4a8d-4b31-af74-2a64e6ddbd1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:53.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3118" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4667,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:53.919: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 23:18:54.107: INFO: Waiting up to 5m0s for pod "pod-a5305792-d256-42f7-9765-6f01a99bd0b9" in namespace "emptydir-2204" to be "Succeeded or Failed"
Apr 29 23:18:54.110: INFO: Pod "pod-a5305792-d256-42f7-9765-6f01a99bd0b9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.397425ms
Apr 29 23:18:56.127: INFO: Pod "pod-a5305792-d256-42f7-9765-6f01a99bd0b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019829444s
STEP: Saw pod success
Apr 29 23:18:56.127: INFO: Pod "pod-a5305792-d256-42f7-9765-6f01a99bd0b9" satisfied condition "Succeeded or Failed"
Apr 29 23:18:56.135: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-a5305792-d256-42f7-9765-6f01a99bd0b9 container test-container: <nil>
STEP: delete the pod
Apr 29 23:18:56.161: INFO: Waiting for pod pod-a5305792-d256-42f7-9765-6f01a99bd0b9 to disappear
Apr 29 23:18:56.165: INFO: Pod pod-a5305792-d256-42f7-9765-6f01a99bd0b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:56.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2204" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4673,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:18:56.519: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4" in namespace "downward-api-1567" to be "Succeeded or Failed"
Apr 29 23:18:56.523: INFO: Pod "downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529534ms
Apr 29 23:18:58.533: INFO: Pod "downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014451811s
STEP: Saw pod success
Apr 29 23:18:58.533: INFO: Pod "downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4" satisfied condition "Succeeded or Failed"
Apr 29 23:18:58.537: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4 container client-container: <nil>
STEP: delete the pod
Apr 29 23:18:58.569: INFO: Waiting for pod downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4 to disappear
Apr 29 23:18:58.571: INFO: Pod downwardapi-volume-a8ea97ae-f918-42e1-b484-c0d8098285a4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:58.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1567" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:58.582: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 29 23:18:58.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9484  18a0b952-3307-49da-a535-1e41fc7b9f0c 36526 0 2021-04-29 23:18:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-29 23:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:18:58.791: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9484  18a0b952-3307-49da-a535-1e41fc7b9f0c 36527 0 2021-04-29 23:18:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-29 23:18:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 29 23:18:58.808: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9484  18a0b952-3307-49da-a535-1e41fc7b9f0c 36528 0 2021-04-29 23:18:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-29 23:18:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:18:58.809: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9484  18a0b952-3307-49da-a535-1e41fc7b9f0c 36529 0 2021-04-29 23:18:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-04-29 23:18:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:58.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9484" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":275,"skipped":4740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:58.821: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:18:59.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9350" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4781,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:18:59.027: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4356 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4356;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4356 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4356;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4356.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4356.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4356.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4356.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4356.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4356.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4356.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4356.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4356.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.57.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.57.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.57.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.57.234_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4356 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4356;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4356 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4356;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4356.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4356.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4356.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4356.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4356.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4356.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4356.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4356.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4356.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4356.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.57.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.57.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.57.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.57.234_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 23:19:03.290: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.294: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.297: INFO: Unable to read wheezy_udp@dns-test-service.dns-4356 from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4356 from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.307: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.311: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.315: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.338: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.346: INFO: Unable to read jessie_udp@dns-test-service.dns-4356 from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-4356 from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.354: INFO: Unable to read jessie_udp@dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.361: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.365: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4356.svc from pod dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e: the server could not find the requested resource (get pods dns-test-3e284f28-c4ed-4899-9bde-67553390c89e)
Apr 29 23:19:03.388: INFO: Lookups using dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4356 wheezy_tcp@dns-test-service.dns-4356 wheezy_udp@dns-test-service.dns-4356.svc wheezy_tcp@dns-test-service.dns-4356.svc wheezy_udp@_http._tcp.dns-test-service.dns-4356.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4356.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4356 jessie_tcp@dns-test-service.dns-4356 jessie_udp@dns-test-service.dns-4356.svc jessie_tcp@dns-test-service.dns-4356.svc jessie_udp@_http._tcp.dns-test-service.dns-4356.svc jessie_tcp@_http._tcp.dns-test-service.dns-4356.svc]

Apr 29 23:19:08.494: INFO: DNS probes using dns-4356/dns-test-3e284f28-c4ed-4899-9bde-67553390c89e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:19:08.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4356" for this suite.

• [SLOW TEST:9.592 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":277,"skipped":4793,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:19:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:19:09.233: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:19:11.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335149, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335149, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335149, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335149, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:19:14.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:19:14.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4914" for this suite.
STEP: Destroying namespace "webhook-4914-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.949 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":278,"skipped":4797,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:19:14.574: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-333650af-8c5c-40d0-bedb-85612d002d56-4550
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:19:14.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-232" for this suite.
STEP: Destroying namespace "nspatchtest-333650af-8c5c-40d0-bedb-85612d002d56-4550" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":279,"skipped":4813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:19:14.933: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:19:15.508: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Apr 29 23:19:17.524: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335155, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335155, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335155, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335155, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:19:20.563: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:19:20.571: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:19:22.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1389" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.214 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":280,"skipped":4842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:19:22.150: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-dea31bbe-2292-4d66-bc7e-59d21d392ac1 in namespace container-probe-7917
Apr 29 23:19:24.456: INFO: Started pod liveness-dea31bbe-2292-4d66-bc7e-59d21d392ac1 in namespace container-probe-7917
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 23:19:24.460: INFO: Initial restart count of pod liveness-dea31bbe-2292-4d66-bc7e-59d21d392ac1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:23:25.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7917" for this suite.

• [SLOW TEST:243.672 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4875,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:23:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-5917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5917 to expose endpoints map[]
Apr 29 23:23:26.016: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 29 23:23:27.028: INFO: successfully validated that service multi-endpoint-test in namespace services-5917 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5917 to expose endpoints map[pod1:[100]]
Apr 29 23:23:30.067: INFO: successfully validated that service multi-endpoint-test in namespace services-5917 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5917 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 29 23:23:33.115: INFO: successfully validated that service multi-endpoint-test in namespace services-5917 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5917 to expose endpoints map[pod2:[101]]
Apr 29 23:23:33.159: INFO: successfully validated that service multi-endpoint-test in namespace services-5917 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5917 to expose endpoints map[]
Apr 29 23:23:34.210: INFO: successfully validated that service multi-endpoint-test in namespace services-5917 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:23:34.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5917" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.427 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":282,"skipped":4882,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:23:34.250: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 29 23:23:34.501: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2396  3f8bd54c-b00f-4e1b-ae74-574fccddd4a1 37516 0 2021-04-29 23:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-29 23:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:23:34.501: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2396  3f8bd54c-b00f-4e1b-ae74-574fccddd4a1 37517 0 2021-04-29 23:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-04-29 23:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:23:34.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2396" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":283,"skipped":4883,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:23:34.513: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:23:36.724: INFO: Deleting pod "var-expansion-7664c474-19cc-4fd5-80af-d164cee749f5" in namespace "var-expansion-1138"
Apr 29 23:23:36.734: INFO: Wait up to 5m0s for pod "var-expansion-7664c474-19cc-4fd5-80af-d164cee749f5" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:23:38.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1138" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":284,"skipped":4884,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:23:38.766: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:23:38.995: INFO: Create a RollingUpdate DaemonSet
Apr 29 23:23:39.000: INFO: Check that daemon pods launch on every node of the cluster
Apr 29 23:23:39.005: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:39.005: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:39.005: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:39.009: INFO: Number of nodes with available pods: 0
Apr 29 23:23:39.009: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:23:40.018: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:40.018: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:40.018: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:40.022: INFO: Number of nodes with available pods: 0
Apr 29 23:23:40.022: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:23:41.019: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:41.019: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:41.019: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:41.022: INFO: Number of nodes with available pods: 0
Apr 29 23:23:41.022: INFO: Node k8s-linuxpool-38811533-0 is running more than one daemon pod
Apr 29 23:23:42.021: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:42.021: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:42.021: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:42.025: INFO: Number of nodes with available pods: 3
Apr 29 23:23:42.025: INFO: Number of running nodes: 3, number of available pods: 3
Apr 29 23:23:42.025: INFO: Update the DaemonSet to trigger a rollout
Apr 29 23:23:42.038: INFO: Updating DaemonSet daemon-set
Apr 29 23:23:52.059: INFO: Roll back the DaemonSet before rollout is complete
Apr 29 23:23:52.075: INFO: Updating DaemonSet daemon-set
Apr 29 23:23:52.075: INFO: Make sure DaemonSet rollback is complete
Apr 29 23:23:52.079: INFO: Wrong image for pod: daemon-set-t57f8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 29 23:23:52.079: INFO: Pod daemon-set-t57f8 is not available
Apr 29 23:23:52.084: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:52.084: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:52.084: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:53.096: INFO: Wrong image for pod: daemon-set-t57f8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 29 23:23:53.096: INFO: Pod daemon-set-t57f8 is not available
Apr 29 23:23:53.103: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:53.103: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:53.103: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:54.093: INFO: Pod daemon-set-jmjdg is not available
Apr 29 23:23:54.098: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:54.098: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 29 23:23:54.098: INFO: DaemonSet pods can't tolerate node k8s-master-38811533-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9842, will wait for the garbage collector to delete the pods
Apr 29 23:23:54.168: INFO: Deleting DaemonSet.extensions daemon-set took: 9.588473ms
Apr 29 23:23:54.768: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.174303ms
Apr 29 23:24:02.191: INFO: Number of nodes with available pods: 0
Apr 29 23:24:02.191: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:24:02.193: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37723"},"items":null}

Apr 29 23:24:02.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37723"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:02.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9842" for this suite.

• [SLOW TEST:23.463 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":285,"skipped":4889,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:02.230: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:02.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8332" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":286,"skipped":4893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:02.461: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:24:02.641: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 29 23:24:04.694: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:05.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2876" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":287,"skipped":4937,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Apr 29 23:24:05.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-4865 api-versions'
Apr 29 23:24:06.022: INFO: stderr: ""
Apr 29 23:24:06.022: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecrets-store.csi.x-k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:06.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4865" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":288,"skipped":4946,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:23.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4329" for this suite.

• [SLOW TEST:17.298 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":289,"skipped":4949,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:23.333: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:24:23.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 create -f -'
Apr 29 23:24:24.163: INFO: stderr: ""
Apr 29 23:24:24.163: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 29 23:24:24.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 create -f -'
Apr 29 23:24:24.425: INFO: stderr: ""
Apr 29 23:24:24.425: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Apr 29 23:24:25.435: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:24:25.436: INFO: Found 0 / 1
Apr 29 23:24:26.433: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:24:26.433: INFO: Found 1 / 1
Apr 29 23:24:26.433: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 29 23:24:26.436: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 29 23:24:26.436: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 23:24:26.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 describe pod agnhost-primary-mdjlm'
Apr 29 23:24:26.552: INFO: stderr: ""
Apr 29 23:24:26.552: INFO: stdout: "Name:         agnhost-primary-mdjlm\nNamespace:    kubectl-7489\nPriority:     0\nNode:         k8s-linuxpool-38811533-0/10.240.0.5\nStart Time:   Thu, 29 Apr 2021 23:24:24 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           10.244.2.34\nIPs:\n  IP:           10.244.2.34\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://1b68046f21ba73befea9cfa80680fe4f519c731c032ccffd70f04b61c36f4115\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 29 Apr 2021 23:24:25 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-75kcm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-75kcm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-75kcm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7489/agnhost-primary-mdjlm to k8s-linuxpool-38811533-0\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 29 23:24:26.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 describe rc agnhost-primary'
Apr 29 23:24:26.691: INFO: stderr: ""
Apr 29 23:24:26.691: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7489\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-mdjlm\n"
Apr 29 23:24:26.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 describe service agnhost-primary'
Apr 29 23:24:26.817: INFO: stderr: ""
Apr 29 23:24:26.817: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7489\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.0.5.63\nIPs:               10.0.5.63\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.2.34:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 29 23:24:26.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 describe node k8s-linuxpool-38811533-0'
Apr 29 23:24:26.963: INFO: stderr: ""
Apr 29 23:24:26.963: INFO: stdout: "Name:               k8s-linuxpool-38811533-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.azure.com/cluster=tst20r2\n                    kubernetes.azure.com/role=agent\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-linuxpool-38811533-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    node.kubernetes.io/instance-type=Standard_D2_v2\n                    storageprofile=managed\n                    storagetier=Standard_LRS\n                    topology.kubernetes.io/region=redmond\n                    topology.kubernetes.io/zone=1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"secrets-store.csi.k8s.io\":\"k8s-linuxpool-38811533-0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 29 Apr 2021 20:48:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-linuxpool-38811533-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 29 Apr 2021 23:24:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 29 Apr 2021 20:49:22 +0000   Thu, 29 Apr 2021 20:49:22 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Thu, 29 Apr 2021 23:20:35 +0000   Thu, 29 Apr 2021 20:48:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 29 Apr 2021 23:20:35 +0000   Thu, 29 Apr 2021 20:48:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 29 Apr 2021 23:20:35 +0000   Thu, 29 Apr 2021 20:48:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 29 Apr 2021 23:20:35 +0000   Thu, 29 Apr 2021 20:48:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.5\n  Hostname:    k8s-linuxpool-38811533-0\nCapacity:\n  attachable-volumes-azure-disk:  8\n  cpu:                            2\n  ephemeral-storage:              50633320Ki\n  example.com/fakecpu:            1k\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         7113576Ki\n  pods:                           110\n  scheduling.k8s.io/foo:          3\nAllocatable:\n  attachable-volumes-azure-disk:  8\n  cpu:                            2\n  ephemeral-storage:              46663667635\n  example.com/fakecpu:            1k\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         6345576Ki\n  pods:                           110\n  scheduling.k8s.io/foo:          3\nSystem Info:\n  Machine ID:                 893c298e99c44f829e41da94797c27de\n  System UUID:                BB0E60DD-9757-0A4C-BA40-501018272C73\n  Boot ID:                    df808816-4667-46b8-aa39-8389ac6259ed\n  Kernel Version:             4.15.0-1113-azure\n  OS Image:                   Ubuntu 16.04.7 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.20.6\n  Kube-Proxy Version:         v1.20.6\nPodCIDR:                      10.244.2.0/24\nPodCIDRs:                     10.244.2.0/24\nProviderID:                   azure:///subscriptions/e6f80264-2587-4152-a867-5be17b2f18be/resourceGroups/tst20r2/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-38811533-0\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 azure-ip-masq-agent-xstdb                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     155m\n  kube-system                 csi-secrets-store-m7k6q                                    70m (3%)      600m (30%)  140Mi (2%)       600Mi (9%)     26m\n  kube-system                 csi-secrets-store-provider-azure-f6926                     50m (2%)      200m (10%)  100Mi (1%)       200Mi (3%)     26m\n  kube-system                 kube-proxy-zfgdr                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         155m\n  kubectl-7489                agnhost-primary-mdjlm                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            270m (13%)  850m (42%)\n  memory                         290Mi (4%)  1050Mi (16%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  hugepages-1Gi                  0 (0%)      0 (0%)\n  hugepages-2Mi                  0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\n  example.com/fakecpu            0           0\n  scheduling.k8s.io/foo          0           0\nEvents:                          <none>\n"
Apr 29 23:24:26.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7489 describe namespace kubectl-7489'
Apr 29 23:24:27.077: INFO: stderr: ""
Apr 29 23:24:27.077: INFO: stdout: "Name:         kubectl-7489\nLabels:       e2e-framework=kubectl\n              e2e-run=821a7fd5-0184-4739-98d8-647759f1e36d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7489" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":290,"skipped":4966,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:27.095: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6542
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:31.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6542" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":291,"skipped":4973,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:31.356: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-18457db2-f623-4c18-a017-d2dde3171bd7
STEP: Creating a pod to test consume secrets
Apr 29 23:24:31.578: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280" in namespace "projected-2964" to be "Succeeded or Failed"
Apr 29 23:24:31.582: INFO: Pod "pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736229ms
Apr 29 23:24:33.592: INFO: Pod "pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013802605s
Apr 29 23:24:35.602: INFO: Pod "pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024384987s
STEP: Saw pod success
Apr 29 23:24:35.602: INFO: Pod "pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280" satisfied condition "Succeeded or Failed"
Apr 29 23:24:35.605: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:24:35.631: INFO: Waiting for pod pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280 to disappear
Apr 29 23:24:35.634: INFO: Pod pod-projected-secrets-cf404f36-769a-4e1b-aed8-2bbeb78e0280 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:35.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2964" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":4989,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:35.646: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4383
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4383
I0429 23:24:35.876253      21 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4383, replica count: 2
I0429 23:24:38.926598      21 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 23:24:38.926: INFO: Creating new exec pod
Apr 29 23:24:41.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4383 exec execpodz8cvh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 29 23:24:42.250: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 29 23:24:42.250: INFO: stdout: ""
Apr 29 23:24:42.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=services-4383 exec execpodz8cvh -- /bin/sh -x -c nc -zv -t -w 2 10.0.77.155 80'
Apr 29 23:24:42.506: INFO: stderr: "+ nc -zv -t -w 2 10.0.77.155 80\nConnection to 10.0.77.155 80 port [tcp/http] succeeded!\n"
Apr 29 23:24:42.506: INFO: stdout: ""
Apr 29 23:24:42.506: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:42.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4383" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.931 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":293,"skipped":5028,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:42.578: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0429 23:24:43.824092      21 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Apr 29 23:24:45.853: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:45.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8051" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":294,"skipped":5034,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:45.881: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Apr 29 23:24:46.099: INFO: Waiting up to 5m0s for pod "test-pod-1453e119-19bc-436e-a992-a20850524695" in namespace "svcaccounts-1709" to be "Succeeded or Failed"
Apr 29 23:24:46.102: INFO: Pod "test-pod-1453e119-19bc-436e-a992-a20850524695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900422ms
Apr 29 23:24:48.113: INFO: Pod "test-pod-1453e119-19bc-436e-a992-a20850524695": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013589724s
Apr 29 23:24:50.124: INFO: Pod "test-pod-1453e119-19bc-436e-a992-a20850524695": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024219628s
STEP: Saw pod success
Apr 29 23:24:50.124: INFO: Pod "test-pod-1453e119-19bc-436e-a992-a20850524695" satisfied condition "Succeeded or Failed"
Apr 29 23:24:50.127: INFO: Trying to get logs from node k8s-linuxpool-38811533-2 pod test-pod-1453e119-19bc-436e-a992-a20850524695 container agnhost-container: <nil>
STEP: delete the pod
Apr 29 23:24:50.186: INFO: Waiting for pod test-pod-1453e119-19bc-436e-a992-a20850524695 to disappear
Apr 29 23:24:50.189: INFO: Pod test-pod-1453e119-19bc-436e-a992-a20850524695 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:50.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1709" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":295,"skipped":5040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:50.202: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9620
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:24:50.800: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:24:52.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335490, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335490, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335490, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335490, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:24:55.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:24:55.858: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8774-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:24:57.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9620" for this suite.
STEP: Destroying namespace "webhook-9620-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.005 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":296,"skipped":5072,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:24:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-8wq9
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:24:57.534: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8wq9" in namespace "subpath-5679" to be "Succeeded or Failed"
Apr 29 23:24:57.538: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.145732ms
Apr 29 23:24:59.548: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013672641s
Apr 29 23:25:01.561: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 4.026955982s
Apr 29 23:25:03.571: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 6.036856699s
Apr 29 23:25:05.582: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 8.047271223s
Apr 29 23:25:07.591: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 10.057120646s
Apr 29 23:25:09.600: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 12.065818062s
Apr 29 23:25:11.612: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 14.077586905s
Apr 29 23:25:13.623: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 16.088488343s
Apr 29 23:25:15.634: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 18.099177983s
Apr 29 23:25:17.645: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 20.110308229s
Apr 29 23:25:19.654: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Running", Reason="", readiness=true. Elapsed: 22.119176161s
Apr 29 23:25:21.663: INFO: Pod "pod-subpath-test-downwardapi-8wq9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.129146403s
STEP: Saw pod success
Apr 29 23:25:21.664: INFO: Pod "pod-subpath-test-downwardapi-8wq9" satisfied condition "Succeeded or Failed"
Apr 29 23:25:21.667: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-subpath-test-downwardapi-8wq9 container test-container-subpath-downwardapi-8wq9: <nil>
STEP: delete the pod
Apr 29 23:25:21.700: INFO: Waiting for pod pod-subpath-test-downwardapi-8wq9 to disappear
Apr 29 23:25:21.703: INFO: Pod pod-subpath-test-downwardapi-8wq9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8wq9
Apr 29 23:25:21.703: INFO: Deleting pod "pod-subpath-test-downwardapi-8wq9" in namespace "subpath-5679"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:25:21.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5679" for this suite.

• [SLOW TEST:24.508 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":297,"skipped":5086,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:25:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Apr 29 23:25:21.951: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:25:21.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5945" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":298,"skipped":5105,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:25:21.984: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:25:22.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7129 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Apr 29 23:25:22.271: INFO: stderr: ""
Apr 29 23:25:22.272: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Apr 29 23:25:22.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-127715093 --namespace=kubectl-7129 delete pods e2e-test-httpd-pod'
Apr 29 23:25:31.793: INFO: stderr: ""
Apr 29 23:25:31.793: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:25:31.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7129" for this suite.

• [SLOW TEST:9.854 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":299,"skipped":5113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:25:31.839: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 29 23:25:32.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38537 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:25:32.077: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38537 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 29 23:25:42.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38569 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:25:42.125: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38569 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 29 23:25:52.160: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38589 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:25:52.161: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38589 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 29 23:26:02.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38609 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:26:02.194: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9077  3f5ec0cc-b499-42a9-8a13-3a1db7debeb6 38609 0 2021-04-29 23:25:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-04-29 23:25:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 29 23:26:12.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9077  75691c3d-ea25-426d-ae76-78375a0df945 38629 0 2021-04-29 23:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-29 23:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:26:12.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9077  75691c3d-ea25-426d-ae76-78375a0df945 38629 0 2021-04-29 23:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-29 23:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 29 23:26:22.304: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9077  75691c3d-ea25-426d-ae76-78375a0df945 38649 0 2021-04-29 23:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-29 23:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 29 23:26:22.304: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9077  75691c3d-ea25-426d-ae76-78375a0df945 38649 0 2021-04-29 23:26:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-04-29 23:26:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:26:32.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9077" for this suite.

• [SLOW TEST:60.514 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":300,"skipped":5148,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:26:32.354: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Apr 29 23:26:37.126: INFO: Successfully updated pod "labelsupdate56a156c0-198f-4bdf-9bdf-1de32d6b444d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:26:39.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6452" for this suite.

• [SLOW TEST:6.814 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:26:39.170: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-fqxx
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:26:39.375: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-fqxx" in namespace "subpath-5602" to be "Succeeded or Failed"
Apr 29 23:26:39.379: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.914231ms
Apr 29 23:26:41.392: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 2.016282591s
Apr 29 23:26:43.403: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 4.027598445s
Apr 29 23:26:45.412: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 6.036631484s
Apr 29 23:26:47.422: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 8.046756433s
Apr 29 23:26:49.431: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 10.055223972s
Apr 29 23:26:51.445: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 12.069204856s
Apr 29 23:26:53.455: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 14.079884117s
Apr 29 23:26:55.466: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 16.090249277s
Apr 29 23:26:57.484: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 18.108616402s
Apr 29 23:26:59.493: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Running", Reason="", readiness=true. Elapsed: 20.117990459s
Apr 29 23:27:01.503: INFO: Pod "pod-subpath-test-configmap-fqxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.127941123s
STEP: Saw pod success
Apr 29 23:27:01.503: INFO: Pod "pod-subpath-test-configmap-fqxx" satisfied condition "Succeeded or Failed"
Apr 29 23:27:01.507: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod pod-subpath-test-configmap-fqxx container test-container-subpath-configmap-fqxx: <nil>
STEP: delete the pod
Apr 29 23:27:01.538: INFO: Waiting for pod pod-subpath-test-configmap-fqxx to disappear
Apr 29 23:27:01.541: INFO: Pod pod-subpath-test-configmap-fqxx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-fqxx
Apr 29 23:27:01.541: INFO: Deleting pod "pod-subpath-test-configmap-fqxx" in namespace "subpath-5602"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:01.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5602" for this suite.

• [SLOW TEST:22.384 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":302,"skipped":5199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:27:03.133: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:27:05.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335623, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335623, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335623, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63755335623, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:27:08.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:08.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3742" for this suite.
STEP: Destroying namespace "webhook-3742-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":303,"skipped":5223,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:08.398: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 29 23:27:16.661: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:27:16.665: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 23:27:18.665: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:27:18.676: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 23:27:20.665: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:27:20.676: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 23:27:22.665: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:27:22.676: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:22.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7247" for this suite.

• [SLOW TEST:14.307 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5229,"failed":0}
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:22.705: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:26.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4279" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:26.947: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Apr 29 23:27:27.124: INFO: Waiting up to 5m0s for pod "downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c" in namespace "downward-api-4999" to be "Succeeded or Failed"
Apr 29 23:27:27.128: INFO: Pod "downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.362626ms
Apr 29 23:27:29.156: INFO: Pod "downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031432361s
Apr 29 23:27:31.168: INFO: Pod "downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043679075s
STEP: Saw pod success
Apr 29 23:27:31.168: INFO: Pod "downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c" satisfied condition "Succeeded or Failed"
Apr 29 23:27:31.171: INFO: Trying to get logs from node k8s-linuxpool-38811533-0 pod downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:27:31.202: INFO: Waiting for pod downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c to disappear
Apr 29 23:27:31.205: INFO: Pod downward-api-5a5e17d4-bb26-4e07-8d74-498259515e6c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4999" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5267,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:31.217: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1592
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 29 23:27:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
Apr 29 23:27:35.167: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:50.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1592" for this suite.

• [SLOW TEST:18.831 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":307,"skipped":5282,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:50.049: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 23:27:54.838: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4447afd8-ffaa-424a-bdd0-03592d2e96cb"
Apr 29 23:27:54.838: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4447afd8-ffaa-424a-bdd0-03592d2e96cb" in namespace "pods-3438" to be "terminated due to deadline exceeded"
Apr 29 23:27:54.845: INFO: Pod "pod-update-activedeadlineseconds-4447afd8-ffaa-424a-bdd0-03592d2e96cb": Phase="Running", Reason="", readiness=true. Elapsed: 6.686452ms
Apr 29 23:27:56.874: INFO: Pod "pod-update-activedeadlineseconds-4447afd8-ffaa-424a-bdd0-03592d2e96cb": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.035882824s
Apr 29 23:27:56.874: INFO: Pod "pod-update-activedeadlineseconds-4447afd8-ffaa-424a-bdd0-03592d2e96cb" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:27:56.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3438" for this suite.

• [SLOW TEST:6.843 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5300,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:27:56.892: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Apr 29 23:27:57.294: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:27:59.304: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Pending, waiting for it to be Running (with Ready = true)
Apr 29 23:28:01.305: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:03.306: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:05.304: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:07.306: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:09.303: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:11.306: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:13.305: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = false)
Apr 29 23:28:15.304: INFO: The status of Pod test-webserver-d7081eb8-df5b-46db-8005-8ec388cc5f40 is Running (Ready = true)
Apr 29 23:28:15.307: INFO: Container started at 2021-04-29 23:27:58 +0000 UTC, pod became ready at 2021-04-29 23:28:15 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:28:15.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7241" for this suite.

• [SLOW TEST:18.433 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5308,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:28:15.326: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:28:28.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1794" for this suite.

• [SLOW TEST:13.309 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":310,"skipped":5323,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Apr 29 23:28:28.636: INFO: >>> kubeConfig: /tmp/kubeconfig-127715093
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Apr 29 23:28:28.823: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 23:28:28.832: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 23:28:28.834: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-0 before test
Apr 29 23:28:28.843: INFO: azure-ip-masq-agent-xstdb from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.843: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 23:28:28.843: INFO: csi-secrets-store-m7k6q from kube-system started at 2021-04-29 22:58:01 +0000 UTC (3 container statuses recorded)
Apr 29 23:28:28.843: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 23:28:28.843: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 23:28:28.843: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 23:28:28.843: INFO: csi-secrets-store-provider-azure-f6926 from kube-system started at 2021-04-29 22:58:01 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.843: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 23:28:28.843: INFO: kube-proxy-zfgdr from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.843: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:28.843: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:28.843: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Apr 29 23:28:28.843: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:28.843: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-1 before test
Apr 29 23:28:28.853: INFO: azure-ip-masq-agent-vsvkh from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 23:28:28.854: INFO: csi-secrets-store-6rvtw from kube-system started at 2021-04-29 20:49:00 +0000 UTC (3 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 23:28:28.854: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 23:28:28.854: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 23:28:28.854: INFO: csi-secrets-store-provider-azure-mvwgl from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 23:28:28.854: INFO: kube-proxy-kf9sp from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:28.854: INFO: sonobuoy from sonobuoy started at 2021-04-29 21:52:30 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 23:28:28.854: INFO: sonobuoy-e2e-job-b783d86494674f2c from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container e2e ready: true, restart count 0
Apr 29 23:28:28.854: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:28.854: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:28.854: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Apr 29 23:28:28.854: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:28.854: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-38811533-2 before test
Apr 29 23:28:28.865: INFO: azure-ip-masq-agent-twx92 from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.865: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Apr 29 23:28:28.865: INFO: coredns-6bbfb77cb5-rjkn6 from kube-system started at 2021-04-29 20:48:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.865: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:28:28.865: INFO: csi-secrets-store-7k84m from kube-system started at 2021-04-29 20:48:59 +0000 UTC (3 container statuses recorded)
Apr 29 23:28:28.865: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 29 23:28:28.865: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 29 23:28:28.865: INFO: 	Container secrets-store ready: true, restart count 0
Apr 29 23:28:28.865: INFO: csi-secrets-store-provider-azure-9zz42 from kube-system started at 2021-04-29 20:49:01 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.865: INFO: 	Container provider-azure-installer ready: true, restart count 0
Apr 29 23:28:28.865: INFO: kube-proxy-2zjsk from kube-system started at 2021-04-29 20:49:00 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.866: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:28.866: INFO: metrics-server-6c8cc7585b-t8pbb from kube-system started at 2021-04-29 22:17:16 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:28.866: INFO: 	Container metrics-server ready: true, restart count 0
Apr 29 23:28:28.866: INFO: sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v from sonobuoy started at 2021-04-29 21:52:35 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:28.866: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Apr 29 23:28:28.866: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node k8s-linuxpool-38811533-0
STEP: verifying the node has the label node k8s-linuxpool-38811533-1
STEP: verifying the node has the label node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod azure-ip-masq-agent-twx92 requesting resource cpu=50m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod azure-ip-masq-agent-vsvkh requesting resource cpu=50m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod azure-ip-masq-agent-xstdb requesting resource cpu=50m on Node k8s-linuxpool-38811533-0
Apr 29 23:28:28.960: INFO: Pod coredns-6bbfb77cb5-rjkn6 requesting resource cpu=100m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-6rvtw requesting resource cpu=70m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-7k84m requesting resource cpu=70m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-m7k6q requesting resource cpu=70m on Node k8s-linuxpool-38811533-0
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-provider-azure-9zz42 requesting resource cpu=50m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-provider-azure-f6926 requesting resource cpu=50m on Node k8s-linuxpool-38811533-0
Apr 29 23:28:28.960: INFO: Pod csi-secrets-store-provider-azure-mvwgl requesting resource cpu=50m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod kube-proxy-2zjsk requesting resource cpu=100m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod kube-proxy-kf9sp requesting resource cpu=100m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod kube-proxy-zfgdr requesting resource cpu=100m on Node k8s-linuxpool-38811533-0
Apr 29 23:28:28.960: INFO: Pod metrics-server-6c8cc7585b-t8pbb requesting resource cpu=0m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod sonobuoy-e2e-job-b783d86494674f2c requesting resource cpu=0m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-84458 requesting resource cpu=0m on Node k8s-linuxpool-38811533-0
Apr 29 23:28:28.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-ffr7v requesting resource cpu=0m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.960: INFO: Pod sonobuoy-systemd-logs-daemon-set-d833942aa1b34476-tgkn2 requesting resource cpu=0m on Node k8s-linuxpool-38811533-1
STEP: Starting Pods to consume most of the cluster CPU.
Apr 29 23:28:28.960: INFO: Creating a pod which consumes cpu=1211m on Node k8s-linuxpool-38811533-1
Apr 29 23:28:28.971: INFO: Creating a pod which consumes cpu=1141m on Node k8s-linuxpool-38811533-2
Apr 29 23:28:28.980: INFO: Creating a pod which consumes cpu=1211m on Node k8s-linuxpool-38811533-0
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b.167a77f4bc2d947b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9188/filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b to k8s-linuxpool-38811533-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b.167a77f4fbbfea8b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b.167a77f501727017], Reason = [Created], Message = [Created container filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b.167a77f50bf31e43], Reason = [Started], Message = [Started container filler-pod-24f019ff-e883-414c-95db-0a4db23d7b8b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8.167a77f4bd399bdc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9188/filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8 to k8s-linuxpool-38811533-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8.167a77f4fb4daed9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8.167a77f502e5e795], Reason = [Created], Message = [Created container filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8.167a77f50d8f45c1], Reason = [Started], Message = [Started container filler-pod-6b0466a1-a7fa-454d-8cf7-2374993ea0b8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28.167a77f4bb87831b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9188/filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28 to k8s-linuxpool-38811533-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28.167a77f4f9874abe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28.167a77f4fe75f136], Reason = [Created], Message = [Created container filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28.167a77f508c719c3], Reason = [Started], Message = [Started container filler-pod-caeddd58-81de-4b1b-9c8b-5cd9ca7b8a28]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167a77f5ae22f1f9], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: true}, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.167a77f5af143e5f], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: true}, that the pod didn't tolerate.]
STEP: removing the label node off the node k8s-linuxpool-38811533-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-38811533-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-linuxpool-38811533-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Apr 29 23:28:34.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9188" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.501 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":311,"skipped":5329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSApr 29 23:28:34.138: INFO: Running AfterSuite actions on all nodes
Apr 29 23:28:34.138: INFO: Running AfterSuite actions on node 1
Apr 29 23:28:34.138: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 5721.625 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h35m23.679242405s
Test Suite Passed
